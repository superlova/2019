<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Superlova</title>
  
  <subtitle>Welcome...</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://superlova.github.io/"/>
  <updated>2020-08-15T16:12:20.962Z</updated>
  <id>https://superlova.github.io/</id>
  
  <author>
    <name>Superlova</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【学习笔记】使用tf.data对预处理过程优化</title>
    <link href="https://superlova.github.io/2020/08/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8tf-data%E5%AF%B9%E9%A2%84%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E4%BC%98%E5%8C%96/"/>
    <id>https://superlova.github.io/2020/08/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8tf-data%E5%AF%B9%E9%A2%84%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E4%BC%98%E5%8C%96/</id>
    <published>2020-08-15T15:18:43.000Z</published>
    <updated>2020-08-15T16:12:20.962Z</updated>
    
    <content type="html"><![CDATA[<p>本文是关于 <code>tf.data</code> 介绍的第二篇，主要介绍一些数据预处理方面的优化方法，诸如并行化预处理映射函数、使用缓存等。<br><a id="more"></a></p><p>构建一个机器学习模型时，由于数据预处理过程不能使用GPU进行加速，因此格外耗时。背后的原因可能是CPU、网络或者缓存等复杂的因素。因此要研究如何提升数据预处理的效率，首先需要控制实验的变量。想实现这一点，构造一个虚假的数据集比较可行。</p><p>通过构建一个虚假的数据集，从<code>tf.data.Dataset</code>继承的类，称为<code>ArtificialDataset</code>。该数据集模拟三件事：</p><ol><li>生成<code>num_samples</code>样本（默认为3）</li><li>在第一个模拟打开文件的项目之前睡眠一段时间</li><li>在产生每个项目以模拟从文件读取数据之前先休眠一段时间</li></ol><pre><code class="lang-py">class ArtificialDataset(tf.data.Dataset):    def _generator(num_samples):        # Opening the file        time.sleep(0.03)        for sample_idx in range(num_samples):            # Reading data (line, record) from the file            time.sleep(0.015)            yield (sample_idx,)    def __new__(cls, num_samples=3):        return tf.data.Dataset.from_generator(            cls._generator,            output_types=tf.dtypes.int64,            output_shapes=(1,),            args=(num_samples,)        )</code></pre><p>构建 <code>benchmark</code> ，通过模拟训练的方式，计算该数据预处理模式的耗时：</p><pre><code class="lang-py">def benchmark(dataset, num_epochs=2):    start_time = time.perf_counter()    for epoch_num in range(num_epochs):        for sample in dataset:            # Performing a training step            time.sleep(0.01)    tf.print(&quot;Execution time:&quot;, time.perf_counter() - start_time)</code></pre><p>我们先来不加任何优化地运行一次benchmark：</p><pre><code class="lang-py">benchmark(ArtificialDataset())Execution time: 0.33306735700000445</code></pre><p>此时模型的执行时间图如图所示：<br><img src="https://www.tensorflow.org/guide/images/data_performance/naive.svg" srcset="/img/loading.gif" alt="Naive"></p><p>时间消耗是这样的：先是打开文件，然后从文件中获取数据项，然后使用数据进行训练。这种执行方式，当数据进行预处理，模型就空闲；当模型开始训练，管道又空闲下来了。预处理和训练这两部分明显可以重叠。</p><p><code>tf.data</code> API提供了<code>tf.data.Dataset.prefetch</code>转换。它可以用于将数据生成时间与数据消耗时间分开。转换使用后台线程和内部缓冲区预取元素。要预取的元素数量应等于（或可能大于）单个训练步骤消耗的批次数量。将预取的元素数量设置为<code>tf.data.experimental.AUTOTUNE</code> ，这将提示<code>tf.data</code>运行时在运行时动态调整值。</p><pre><code class="lang-py">benchmark(    ArtificialDataset()    .prefetch(tf.data.experimental.AUTOTUNE))Execution time: 0.20504431599999862</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/prefetched.svg" srcset="/img/loading.gif" alt="Prefetched"></p><p>时间有了明显优化，因为数据的生产和消费有了些许重叠。</p><p>在实际工作中，输入数据可以远程存储在其他计算机上。在本地和远程存储之间存在以下差异：</p><ol><li>到达第一个字节的时间：从远程存储读取文件的第一个字节所花费的时间要比从本地存储中读取文件的时间长几个数量级。</li><li>读取吞吐量：虽然远程存储通常提供较大的聚合带宽，但是读取单个文件可能只能使用此带宽的一小部分。</li></ol><p>此外，一旦将原始字节加载到内存中，可能还需要对数据进行反序列化和/或解密，这需要进行额外的计算。不管数据是本地存储还是远程存储，都存在这种开销，但是<strong>如果数据没有有效地预取，则在远程情况下会更糟</strong>。</p><p>可以使用<code>tf.data.Dataset.interleave</code>转换来<strong>并行化数据加载步骤</strong>， <code>cycle_length</code> 表明可以一起处理的数据集数量， <code>num_parallel_calls</code> 则是并行度。</p><pre><code class="lang-py">benchmark(    tf.data.Dataset.range(2)    .interleave(        ArtificialDataset,        num_parallel_calls=tf.data.experimental.AUTOTUNE    )Execution time: 0.18243273299958673</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/parallel_interleave.svg" srcset="/img/loading.gif" alt="Parallel interleave"></p><p>该图可以显示<code>interleave</code>变换的行为，从两个可用的数据集中获取样本。这次，两个数据集的读取并行进行，从而减少了全局数据处理时间</p><h2 id="并行预处理操作"><a href="#并行预处理操作" class="headerlink" title="并行预处理操作"></a>并行预处理操作</h2><p>在准备数据时，可能需要对输入元素进行预处理。可以使用<code>tf.data.Dataset.map(f)</code>转换，其含义为将某个转换<code>f</code>作用于数据集<code>Dataset</code>中的每个元素。这里有个很重要的前提条件，由于输入元素彼此独立，因此预处理可以跨多个CPU内核并行化。因此<code>map</code>转换也提供<code>num_parallel_calls</code>参数来指定并行度。关于并行度的选择上，<code>map</code>转换支持<code>tf.data.experimental.AUTOTUNE</code>，而不必人工定义。</p><p>首先定义伪操作：</p><pre><code class="lang-py">def mapped_function(s):    # Do some hard pre-processing    tf.py_function(lambda: time.sleep(0.03), [], ())    return s</code></pre><p>我们来测试伪操作，此时没有任何并行优化：</p><pre><code class="lang-py">benchmark(    ArtificialDataset()    .map(mapped_function))Execution time: 0.4592052289999913</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/sequential_map.svg" srcset="/img/loading.gif" alt="Sequential mapping"></p><p>现在，使用相同的预处理功能，但将其并行应用于多个样本。</p><pre><code class="lang-py">benchmark(    ArtificialDataset()    .map(        mapped_function,        num_parallel_calls=tf.data.experimental.AUTOTUNE    ))Execution time: 0.3045882669994171</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/parallel_map.svg" srcset="/img/loading.gif" alt="Parallel mapping"></p><p>可以在图上看到预处理步骤重叠，从而减少了单次迭代的总时间。</p><p><code>tf.data.Dataset.cache</code>转换可以在内存中或本地存储上缓存数据集。这样可以避免在每个epoch执行某些重复性操作（例如打开文件和读取数据）。</p><pre><code class="lang-py">benchmark(    ArtificialDataset()    .map(  # Apply time consuming operations before cache        mapped_function    ).cache(    ),    5)Execution time: 0.3795637040002475</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/cached_dataset.svg" srcset="/img/loading.gif" alt="Cached dataset"></p><p>第一个epoch执行一次cache之前的转换（例如文件打开和数据读取）。下一个epoch将重用cache转换所缓存的数据。</p><p>这里涉及到一个<code>map</code>和<code>cache</code>操作谁先谁后的问题。有一个原则，如果<code>map</code>操作很复杂、昂贵，那么先<code>map</code>再<code>cache</code>，下次不用<code>map</code>了。如果<code>cache</code>过大而无法放入缓冲区，则先<code>cache</code>后<code>map</code>，或者试图采用一些数据预处理方法以减少资源使用。</p><h2 id="向量化数据预处理操作"><a href="#向量化数据预处理操作" class="headerlink" title="向量化数据预处理操作"></a>向量化数据预处理操作</h2><p>所谓向量化，即使得<code>mapping</code>操作能够一次处理一<code>batch</code>数据。这样做肯定可以加速，因为避免了繁杂的数据读取时间。对用户定义的函数进行向量化处理，并且对数据集应用<code>batch</code>转换再进入<code>mapping</code>。在某种情况下，这个做法非常有用。</p><p>首先定义一个数据集操作<code>increment</code>，负责把每个元素的值+1。另外之前的例子里面使用了毫秒级别的<code>sleep</code>操作，这会掩盖我们优化的结果。这次我们把它拿掉。</p><p>下面是未经向量化优化的<code>increment</code>操作耗时：</p><pre><code class="lang-py">fast_dataset = tf.data.Dataset.range(10000)def fast_benchmark(dataset, num_epochs=2):    start_time = time.perf_counter()    for _ in tf.data.Dataset.range(num_epochs):        for _ in dataset:            pass    tf.print(&quot;Execution time:&quot;, time.perf_counter() - start_time)def increment(x):    return x+1fast_benchmark(    fast_dataset    # Apply function one item at a time    .map(increment)    # Batch    .batch(256))Execution time: 0.7625284370005829</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/scalar_map.svg" srcset="/img/loading.gif" alt="Scalar map"></p><p>与之对比，经过向量化后，耗时明显减少：</p><pre><code class="lang-py">fast_benchmark(    fast_dataset    .batch(256)    # Apply function on a batch of items    # The tf.Tensor.__add__ method already handle batches    .map(increment))Execution time: 0.04735958700075571</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/vectorized_map.svg" srcset="/img/loading.gif" alt="Vectorized map"></p><h2 id="减少内存占用"><a href="#减少内存占用" class="headerlink" title="减少内存占用"></a>减少内存占用</h2><p>许多转换（包括interleave ， prefetch和shuffle ）各自维护内部缓冲区。如果传递给map转换的用户定义函数更改了元素的大小，则映射转换的顺序以及缓冲元素的转换会影响内存使用。</p><p>通常，我们建议选择导致内存占用减少的顺序，除非需要不同的顺序才能提高性能。</p><p>对于缓存，我们建议除非转换后的数据难以保存到缓冲区，否则一律先<code>map</code>再<code>cache</code>。如果你有两个<code>map</code>，其中一个比较耗时<code>time_consuming_mapping</code>，另一个比较耗内存<code>memory_consuming_mapping</code>，那么其实你可以将其拆分成两部分；</p><pre><code class="lang-py">dataset.map(time_consuming_mapping).cache().map(memory_consuming_mapping)</code></pre><p>这样，耗时部分仅在第一个epoch执行，并且避免了使用过多的缓存空间。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>使用<code>tf.data</code>，并采用合理的优化手段，就能让你的数据预处理过程节约很多时间。这些手段有：</p><ul><li>使用<code>prefetch</code>转换可以使生产者和消费者的工作重叠。</li><li>使用<code>interleave</code>变换并行化数据读取变换。</li><li>通过设置<code>num_parallel_calls</code>参数来并行化<code>map</code>转换 。</li><li>在第一个epoch使用<code>cache</code>转换将数据缓存在内存中</li><li>向量化传递给<code>map</code>转换的用户定义函数</li><li>应用<code>interleave</code> ， <code>prefetch</code>和<code>shuffle</code>转换时， 逐渐减少内存使用 。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是关于 &lt;code&gt;tf.data&lt;/code&gt; 介绍的第二篇，主要介绍一些数据预处理方面的优化方法，诸如并行化预处理映射函数、使用缓存等。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="preprocessing" scheme="https://superlova.github.io/tags/preprocessing/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】tf.data基本使用</title>
    <link href="https://superlova.github.io/2020/08/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91tf-data%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"/>
    <id>https://superlova.github.io/2020/08/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91tf-data%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</id>
    <published>2020-08-15T08:01:09.000Z</published>
    <updated>2020-08-15T15:20:36.629Z</updated>
    
    <content type="html"><![CDATA[<p>使用 <code>tf.data</code> API 可以轻松处理大量数据，支持多样化的数据格式，还可以方便执行复杂的转换。本文介绍了不同类别源数据转化为 <code>tf.data.Dataset</code> 的方法，以及 <code>Dataset</code> 常见的预处理方法。<br><a id="more"></a></p><h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>最近几年我们在机器学习的加速计算领域取得了一些突破。虽然我们进行指标运算和矩阵运算所需的时间大大减少了，但是提供数据加速的CPU却没能跟上相应的步伐，这就成为了预处理中的瓶颈。我们本以为可以通过构建更复杂的模型来减少对硬件的需求，但是CPU的效率还是取决于他们拥有多少RAM。</p><p><img src="/2020/08/15/【学习笔记】tf-data基本使用/2020-08-15-22-34-14.png" srcset="/img/loading.gif" alt></p><p>对于一些数据集很大的问题，解决问题的方法除了提升CPU的性能之外，还有对数据集进行分批次处理，但是分批预处理需要不断进行训练和合并操作，这对预处理增加了难度。<code>tf.data</code> 可以帮助解决数据集过大造成的预处理瓶颈问题。</p><p><code>tf.data</code> 是tensorflow的数据预处理框架。它速度快、灵活且易于使用。</p><p>假设你的数据以TFRecord格式存在磁盘，需要将这些数据读取、处理并训练模型，可以先通过<code>TFRecordDataset</code> 开始处理这些数据：</p><pre><code class="lang-py">dataset = tf.data.TFRecordDataset(&#39;.../*.tfrecord&#39;)</code></pre><p>然后执行一些数据预处理操作，这个过程可能很消耗资源：</p><pre><code class="lang-py">dataset = dataset.map(expensive_preprocess)</code></pre><p>随后你需要打乱数据，以降低模型训练过程中过拟合的可能性：</p><pre><code class="lang-py">dataset = dataset.shuffle(buffer_size=1024)</code></pre><p>然后我们需要分批次，以令模型加速计算</p><pre><code class="lang-py">dataset = dataset.batch(batch_size=32)</code></pre><p>最终要完成pipeline搭建，这样可以保证模型在运行一个batch的数据时，另一批数据进行预处理以提升效率。</p><pre><code class="lang-py">dataset = dataset.prefetch()</code></pre><p>将数据输入到模型，我们可以开始训练了。</p><pre><code class="lang-py">model = tf.keras.Model(...)model.fit(dataset)</code></pre><p>上面就是数据从读取到处理到训练的全部流程，称之为管道（pipeline）。</p><p>处理大量原始数据，要经过多次函数变换，这些函数变换都是可重用的。使用<code>tf.data</code>将这些变换整理成管道，一方面可以简化复杂输入的预处理过程，另一方面，由于<code>Dataset</code> 对象可迭代，可以执行分批处理。使用<code>tf.data.Dataset</code>可以方便地整合操作、构造数据集。</p><p>有两种方法构造可供训练使用的<code>Dataset</code>数据集：</p><ol><li>从文件、内存中直接构建<code>Dataset</code></li><li>从其他<code>Dataset</code>中转化</li></ol><p>如果打算从内存中读取数据构建 <code>Dataset</code> ，有 <code>tf.data.Dataset.from_tensors()</code> 和 <code>tf.data.Dataset.from_tensor_slices()</code> 可供选择；如果打算从 <code>TFRecord</code> 格式的文件中读取数据，可以调用 <code>tf.data.TFRecordDataset()</code> 。</p><p>当 <code>Dataset</code> 对象构建好了之后，通过使用 <code>Dataset.map()</code> 为其中每个元素施加变换、使用 <code>Dataset.batch()</code> 为整批元素添加变换等等对数据进行预处理。</p><h2 id="从内存或文件中构造Dataset"><a href="#从内存或文件中构造Dataset" class="headerlink" title="从内存或文件中构造Dataset"></a>从内存或文件中构造<code>Dataset</code></h2><h3 id="从内存中的array构造Dataset"><a href="#从内存中的array构造Dataset" class="headerlink" title="从内存中的array构造Dataset"></a>从内存中的array构造<code>Dataset</code></h3><p>如果你的所有数据都在内存中，那么最简单构造 <code>Dataset</code> 的方式就是，先将其利用 <code>tf.Tensor</code> 转成tensor，后使用<code>Dataset.from_tensor_slices()</code>。</p><pre><code class="lang-python">train, test = tf.keras.datasets.fashion_mnist.load_data()images, labels = trainimages = images/255dataset = tf.data.Dataset.from_tensor_slices((images, labels))dataset</code></pre><h3 id="从生成器构造Dataset"><a href="#从生成器构造Dataset" class="headerlink" title="从生成器构造Dataset"></a>从生成器构造<code>Dataset</code></h3><p>你也可以利用 <code>Dataset.from_generator</code> 从Python的生成器来构造 <code>Dataset</code> ，比如从 <code>preprocessing.image.ImageDataGenerator</code> 构造 <code>Dataset</code>。但这种方法受制于Python的GIL，因此效率不会太高。</p><p>首先下载花朵图片数据集，一共3670张花朵图片，分成五个类别。</p><pre><code class="lang-python">flowers = tf.keras.utils.get_file(    &#39;flower_photos&#39;,    &#39;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&#39;,    untar=True)</code></pre><p>利用 <code>preprocessing.image.ImageDataGenerator</code> 定义数据增强操作，然后将其套用到花朵数据集上。</p><pre><code class="lang-python">img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)images, labels = next(img_gen.flow_from_directory(flowers))</code></pre><p>最后构造 <code>Dataset</code> 。</p><pre><code class="lang-python">ds = tf.data.Dataset.from_generator(    img_gen.flow_from_directory, args=[flowers],     output_types=(tf.float32, tf.float32),     output_shapes=([32,256,256,3], [32,5]))ds&lt;FlatMapDataset shapes: ((32, 256, 256, 3), (32, 5)), types: (tf.float32, tf.float32)&gt;</code></pre><h3 id="从TFRecord格式文件构造Dataset"><a href="#从TFRecord格式文件构造Dataset" class="headerlink" title="从TFRecord格式文件构造Dataset"></a>从<code>TFRecord</code>格式文件构造<code>Dataset</code></h3><p>有些时候数据不在内存中，而是以特定格式存在磁盘上，比如 <code>TFRecord</code> 格式。这种情况我们可以使用 <code>tf.data.TFRecordDataset</code> 作为数据管道的一部分。</p><pre><code class="lang-python"># Creates a dataset that reads all of the examples from two files.fsns_test_file = tf.keras.utils.get_file(&quot;fsns.tfrec&quot;, &quot;https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001&quot;)dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file])</code></pre><p><code>tf.data.TFRecordDataset</code> API中，<code>filenames</code> 的输入很灵活，既可以是字符串，表明一个文件；也可以是字符串列表，表明多个文件。</p><h3 id="从-txt格式文件构造Dataset"><a href="#从-txt格式文件构造Dataset" class="headerlink" title="从.txt格式文件构造Dataset"></a>从<code>.txt</code>格式文件构造<code>Dataset</code></h3><p>如果是 <code>.txt</code> 格式，那么采用 <code>tf.data.TextLineDataset</code> 也可转成 <code>Dataset</code>。</p><pre><code class="lang-python">directory_url = &#39;https://storage.googleapis.com/download.tensorflow.org/data/illiad/&#39;file_names = [&#39;cowper.txt&#39;, &#39;derby.txt&#39;, &#39;butler.txt&#39;]file_paths = [    &#39;cowper.txt&#39;,&#39;derby.txt&#39;,&#39;butler.txt&#39;]dataset = tf.data.TextLineDataset(file_paths)</code></pre><p>看一下第一个文件的前5行：</p><pre><code class="lang-python">for line in dataset.take(5):  print(line.numpy())b&quot;\xef\xbb\xbfAchilles sing, O Goddess! Peleus&#39; son;&quot;b&#39;His wrath pernicious, who ten thousand woes&#39;b&quot;Caused to Achaia&#39;s host, sent many a soul&quot;b&#39;Illustrious into Ades premature,&#39;b&#39;And Heroes gave (so stood the will of Jove)&#39;</code></pre><p>前五行都是 <code>cowper.txt</code> 中的。如果我们希望生成的 <code>Dataset</code> 能够轮流选取三个文件中的元素，可以在构造之初，使用 <code>Dataset.interleave</code> ，并设置 <code>cycle_length</code>：</p><pre><code class="lang-python">files_ds = tf.data.Dataset.from_tensor_slices(file_paths)lines_ds = files_ds.interleave(tf.data.TextLineDataset, cycle_length=3)for i, line in enumerate(lines_ds.take(9)):  if i % 3 == 0:    print()  print(i, line.numpy())0 b&quot;\xef\xbb\xbfAchilles sing, O Goddess! Peleus&#39; son;&quot;1 b&quot;\xef\xbb\xbfOf Peleus&#39; son, Achilles, sing, O Muse,&quot;2 b&#39;\xef\xbb\xbfSing, O goddess, the anger of Achilles son of Peleus, that brought&#39;3 b&#39;His wrath pernicious, who ten thousand woes&#39;4 b&#39;The vengeance, deep and deadly; whence to Greece&#39;5 b&#39;countless ills upon the Achaeans. Many a brave soul did it send&#39;6 b&quot;Caused to Achaia&#39;s host, sent many a soul&quot;7 b&#39;Unnumbered ills arose; which many a soul&#39;8 b&#39;hurrying down to Hades, and many a hero did it yield a prey to dogs and&#39;</code></pre><p>有的时候我们不希望录入文件的第一行，或者只要文件中满足要求的特定行，可以分别使用 <code>Dataset.skip()</code> 和 <code>Dataset.filter()</code> 。比如下面的泰坦尼克数据集，去掉第一行后，筛选生存下来的人。</p><pre><code class="lang-python">titanic_file = tf.keras.utils.get_file(&quot;train.csv&quot;, &quot;https://storage.googleapis.com/tf-datasets/titanic/train.csv&quot;)titanic_lines = tf.data.TextLineDataset(titanic_file)for line in titanic_lines.take(10):  print(line.numpy())b&#39;survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone&#39;b&#39;0,male,22.0,1,0,7.25,Third,unknown,Southampton,n&#39;b&#39;1,female,38.0,1,0,71.2833,First,C,Cherbourg,n&#39;b&#39;1,female,26.0,0,0,7.925,Third,unknown,Southampton,y&#39;b&#39;1,female,35.0,1,0,53.1,First,C,Southampton,n&#39;b&#39;0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y&#39;b&#39;0,male,2.0,3,1,21.075,Third,unknown,Southampton,n&#39;b&#39;1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n&#39;b&#39;1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n&#39;b&#39;1,female,4.0,1,1,16.7,Third,G,Southampton,n&#39;def survived(line):  return tf.not_equal(tf.strings.substr(line, 0, 1), &quot;0&quot;)survivors = titanic_lines.skip(1).filter(survived)for line in survivors.take(10):  print(line.numpy())b&#39;1,female,38.0,1,0,71.2833,First,C,Cherbourg,n&#39;b&#39;1,female,26.0,0,0,7.925,Third,unknown,Southampton,y&#39;b&#39;1,female,35.0,1,0,53.1,First,C,Southampton,n&#39;b&#39;1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n&#39;b&#39;1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n&#39;b&#39;1,female,4.0,1,1,16.7,Third,G,Southampton,n&#39;b&#39;1,male,28.0,0,0,13.0,Second,unknown,Southampton,y&#39;b&#39;1,female,28.0,0,0,7.225,Third,unknown,Cherbourg,y&#39;b&#39;1,male,28.0,0,0,35.5,First,A,Southampton,y&#39;b&#39;1,female,38.0,1,5,31.3875,Third,unknown,Southampton,n&#39;</code></pre><h3 id="从csv格式构造Dataset"><a href="#从csv格式构造Dataset" class="headerlink" title="从csv格式构造Dataset"></a>从<code>csv</code>格式构造<code>Dataset</code></h3><p>除了<code>TFRecord</code>和<code>txt</code>格式，还有<code>csv</code>格式也很流行。<code>csv</code>格式能够以纯文本保存表格数据。<code>pandas</code>的<code>to_csv</code>是将<code>csv</code>搬运到内存的良好工具。</p><pre><code class="lang-python">titanic_file = tf.keras.utils.get_file(&quot;train.csv&quot;, &quot;https://storage.googleapis.com/tf-datasets/titanic/train.csv&quot;)df = pd.read_csv(titanic_file, index_col=None)df.head()titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))for feature_batch in titanic_slices.take(1):  for key, value in feature_batch.items():    print(&quot;  {!r:20s}: {}&quot;.format(key, value))</code></pre><p>当然<code>tf.data</code>强大之处在于可以处理<code>pandas</code>处理不了的文件大小。<code>experimental.make_csv_dataset</code>函数是用于读取csv文件集的高层接口，它可以自动推导每个<code>column</code>的文件类型。</p><pre><code class="lang-python">titanic_batches = tf.data.experimental.make_csv_dataset(    titanic_file, batch_size=4,    label_name=&quot;survived&quot;)</code></pre><p>查看第一个<code>batch</code>的内容。</p><pre><code class="lang-python">for feature_batch, label_batch in titanic_batches.take(1):  print(&quot;&#39;survived&#39;: {}&quot;.format(label_batch))  print(&quot;features:&quot;)  for key, value in feature_batch.items():    print(&quot;  {!r:20s}: {}&quot;.format(key, value))</code></pre><p>如果只需要<code>csv</code>的某一列，那么可以使用<code>select_columns</code>参数。</p><pre><code class="lang-python">titanic_batches = tf.data.experimental.make_csv_dataset(    titanic_file, batch_size=4,    label_name=&quot;survived&quot;, select_columns=[&#39;class&#39;, &#39;fare&#39;, &#39;survived&#39;])for feature_batch, label_batch in titanic_batches.take(1):  print(&quot;&#39;survived&#39;: {}&quot;.format(label_batch))  for key, value in feature_batch.items():    print(&quot;  {!r:20s}: {}&quot;.format(key, value))&#39;survived&#39;: [1 1 1 0]features:  &#39;sex&#39;               : [b&#39;female&#39; b&#39;female&#39; b&#39;male&#39; b&#39;female&#39;]  &#39;age&#39;               : [35. 31. 45. 28.]  &#39;n_siblings_spouses&#39;: [0 1 0 8]  &#39;parch&#39;             : [0 1 0 2]  &#39;fare&#39;              : [512.3292  20.525    8.05    69.55  ]  &#39;class&#39;             : [b&#39;First&#39; b&#39;Third&#39; b&#39;Third&#39; b&#39;Third&#39;]  &#39;deck&#39;              : [b&#39;unknown&#39; b&#39;unknown&#39; b&#39;unknown&#39; b&#39;unknown&#39;]  &#39;embark_town&#39;       : [b&#39;Cherbourg&#39; b&#39;Southampton&#39; b&#39;Southampton&#39; b&#39;Southampton&#39;]  &#39;alone&#39;             : [b&#39;y&#39; b&#39;n&#39; b&#39;y&#39; b&#39;n&#39;]</code></pre><p>还有一个底层的<code>experimental.CsvDataset</code>类，它可以更精细的控制读取<code>csv</code>的过程。不支持列类型推断。</p><pre><code class="lang-python">titanic_types  = [tf.int32, tf.string, tf.float32, tf.int32, tf.int32, tf.float32, tf.string, tf.string, tf.string, tf.string] dataset = tf.data.experimental.CsvDataset(titanic_file, titanic_types , header=True)for line in dataset.take(10):  print([item.numpy() for item in line])[0, b&#39;male&#39;, 22.0, 1, 0, 7.25, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 38.0, 1, 0, 71.2833, b&#39;First&#39;, b&#39;C&#39;, b&#39;Cherbourg&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 26.0, 0, 0, 7.925, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;y&#39;][1, b&#39;female&#39;, 35.0, 1, 0, 53.1, b&#39;First&#39;, b&#39;C&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][0, b&#39;male&#39;, 28.0, 0, 0, 8.4583, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Queenstown&#39;, b&#39;y&#39;][0, b&#39;male&#39;, 2.0, 3, 1, 21.075, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 27.0, 0, 2, 11.1333, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 14.0, 1, 0, 30.0708, b&#39;Second&#39;, b&#39;unknown&#39;, b&#39;Cherbourg&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 4.0, 1, 1, 16.7, b&#39;Third&#39;, b&#39;G&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][0, b&#39;male&#39;, 20.0, 0, 0, 8.05, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;y&#39;]</code></pre><p><code>CsvDataset</code>还可以指定每列的默认值，供元素为空时填充。</p><p>在Colab中，直接书写<code>csv</code>文件：</p><pre><code class="lang-bash">%%writefile missing.csv1,2,3,4,2,3,41,,3,41,2,,41,2,3,,,,</code></pre><p>设置每列默认值：</p><pre><code class="lang-python"># Creates a dataset that reads all of the records from two CSV files, each with# four float columns which may have missing values.record_defaults = [999,999,999,999]dataset = tf.data.experimental.CsvDataset(&quot;missing.csv&quot;, record_defaults)dataset = dataset.map(lambda *items: tf.stack(items))for line in dataset:  print(line.numpy())[1 2 3 4][999   2   3   4][  1 999   3   4][  1   2 999   4][  1   2   3 999][999 999 999 999]</code></pre><p>你也可以选择删除<code>header</code>，或者指定某列输出</p><pre><code class="lang-python"># Creates a dataset that reads all of the records from two CSV files with# headers, extracting float data from columns 2 and 4.record_defaults = [999, 999] # Only provide defaults for the selected columnsdataset = tf.data.experimental.CsvDataset(&quot;missing.csv&quot;, record_defaults, select_cols=[1, 3])dataset = dataset.map(lambda *items: tf.stack(items))for line in dataset:  print(line.numpy())[2 4][2 4][999   4][2 4][  2 999][999 999]</code></pre><h3 id="从文件夹中的每个文件构造Dataset"><a href="#从文件夹中的每个文件构造Dataset" class="headerlink" title="从文件夹中的每个文件构造Dataset"></a>从文件夹中的每个文件构造<code>Dataset</code></h3><p>如果每个单独的文件都是一个数据项（比如图片数据集），这样的数据集如何整理？</p><pre><code class="lang-python">flowers_root = tf.keras.utils.get_file(    &#39;flower_photos&#39;,    &#39;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&#39;,    untar=True)flowers_root = pathlib.Path(flowers_root)list_ds = tf.data.Dataset.list_files(str(flowers_root/&#39;*/*&#39;))for f in list_ds.take(5):  print(f.numpy())b&#39;/root/.keras/datasets/flower_photos/dandelion/8720503800_cab5c62a34.jpg&#39;b&#39;/root/.keras/datasets/flower_photos/dandelion/16510864164_3afa8ac37f.jpg&#39;b&#39;/root/.keras/datasets/flower_photos/tulips/7136973281_b2a935ce20.jpg&#39;b&#39;/root/.keras/datasets/flower_photos/sunflowers/14623719696_1bb7970208_n.jpg&#39;b&#39;/root/.keras/datasets/flower_photos/dandelion/4560663938_3557a1f831.jpg&#39;</code></pre><p>通过使用<code>tf.io.read_file</code>读取数据，并从路径中提取<code>label</code>，返回<code>(image, label)</code>数据对。</p><pre><code class="lang-python">def process_path(file_path):  label = tf.strings.split(file_path, os.sep)[-2]  return tf.io.read_file(file_path), labellabeled_ds = list_ds.map(process_path)for image_raw, label_text in labeled_ds.take(1):  print(repr(image_raw.numpy()[:100]))  print()  print(label_text.numpy())b&#39;\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x01\x03|\x03|\x00\x00\xff\xe2\x0cXICC_PROFILE\x00\x01\x01\x00\x00\x0cHLino\x02\x10\x00\x00mntrRGB XYZ \x07\xce\x00\x02\x00\t\x00\x06\x001\x00\x00acspMSFT\x00\x00\x00\x00IEC sRGB\x00\x00\x00\x00\x00\x00&#39;b&#39;sunflowers&#39;</code></pre><h2 id="批处理数据集元素"><a href="#批处理数据集元素" class="headerlink" title="批处理数据集元素"></a>批处理数据集元素</h2><p>批处理的最简单形式是将数据集的<code>n</code>个连续元素堆叠为单个元素。</p><p>batched_dataset = dataset.batch(4)</p><batchdataset shapes: ((none,), (none,)), types: (tf.int64, tf.int64)><p>最后一个批次可能未满，使用<code>drop_remainder</code>参数忽略最后一批，使得<code>shape</code>完整：</p><p>batched_dataset = dataset.batch(7, drop_remainder=True)<br>batched_dataset</p><batchdataset shapes: ((7,), (7,)), types: (tf.int64, tf.int64)><p>许多模型（例如序列模型）都可以使用大小可变（例如长度不同的序列）的输入数据。通过<code>Dataset.padded_batch</code>可以将不同长度的<code>tensor</code>转换成一个<code>batch</code>。</p><p>在多个<code>epochs</code>的情况下，需要重复迭代数据集，最简单的方法是将数据集重复<code>epochs</code>遍。可以使用 <code>Dataset.repeat()</code>完成。</p><p>原有的<code>titanic_lines</code>数据集中的数据数量为 628</p><pre><code class="lang-python">count = 0for data in titanic_lines:    count += 1    # print(data.numpy())print(count)</code></pre><p>titanic_lines数据集经过<code>repeat</code>之后数目变为原来的两倍，1256</p><pre><code class="lang-python">count = 0for data in titanic_lines.repeat(2):    count += 1    # print(data.numpy())print(count)</code></pre><p>将数据集打散的方法 <code>Dataset.shuffle()</code> 通过维护一个固定大小的缓冲区来实现。</p><pre><code class="lang-py">dataset = tf.data.Dataset.zip((counter, lines))dataset = dataset.shuffle(buffer_size=100)dataset = dataset.batch(20)</code></pre><p>由于<code>buffer_size</code>为100，而批大小为20，因此第一批不包含索引大于120的元素。</p><p>在实际使用中，<code>repeat</code>操作、<code>batch</code>操作和<code>shuffle</code>操作经常一起混用，但是一定要注意操作的先后顺序。</p><h2 id="预处理数据"><a href="#预处理数据" class="headerlink" title="预处理数据"></a>预处理数据</h2><p><code>Dataset.map(f)</code> 通过函数 <code>f</code> 对数据集执行变换，<code>f</code> 必须以单个Tensor为输入，单个Tensor为输出（这里指的单个tensor，其意思是由原始数据和标签组成的数据对）。</p><p>假设我们定义了单个图像变换函数 <code>parse_image</code> ，只需 <code>images_ds = list_ds.map(parse_image)</code> 即可对 <code>image_ds</code> 数据集中的所有图片执行变换了。</p><p>假设我们需要将照片随机旋转，可以定义函数，然后使用map将其应用于数据集的所有图片上。</p><pre><code class="lang-py">import scipy.ndimage as ndimagedef random_rotate_image(image):  image = ndimage.rotate(image, np.random.uniform(-30, 30), reshape=False)  return imageimage, label = next(iter(images_ds))image = random_rotate_image(image)show(image, label)</code></pre><p><img src="https://www.tensorflow.org/guide/data_files/output__wEyL7bS9S6t_1.png" srcset="/img/loading.gif" alt></p><p>这里使用的是 <code>scipy</code> 中的旋转函数，需套用 <code>tf.py_function()</code> 才能在Tensorflow的eager_mode里面使用。</p><p>接下来使用 <code>Dataset.map</code> ：</p><pre><code class="lang-py">def tf_random_rotate_image(image, label):  im_shape = image.shape  [image,] = tf.py_function(random_rotate_image, [image], [tf.float32])  image.set_shape(im_shape)  return image, label</code></pre><p>在函数内部不但要注意输入和返回值是<code>(image, label)</code>，而且需要描述数据的<code>shape</code>和<code>type</code>，方便调试。</p><h2 id="时间序列数据窗口化"><a href="#时间序列数据窗口化" class="headerlink" title="时间序列数据窗口化"></a>时间序列数据窗口化</h2><p>时间序列数据的标签有所不同，一般以下一时刻的输入数据为标签，对未来进行一步一步的密集预测。比如：</p><pre><code class="lang-py">range_ds = tf.data.Dataset.range(100000)batches = range_ds.batch(10, drop_remainder=True)def dense_1_step(batch):  # Shift features and labels one step relative to each other.  return batch[:-1], batch[1:]predict_dense_1_step = batches.map(dense_1_step)for features, label in predict_dense_1_step.take(3):  print(features.numpy(), &quot; =&gt; &quot;, label.numpy())[0 1 2 3 4 5 6 7 8]  =&gt;  [1 2 3 4 5 6 7 8 9][10 11 12 13 14 15 16 17 18]  =&gt;  [11 12 13 14 15 16 17 18 19][20 21 22 23 24 25 26 27 28]  =&gt;  [21 22 23 24 25 26 27 28 29]</code></pre><p>如果要预测整个时间窗口而不是固定的偏移量，比如</p><pre><code class="lang-py">batches = range_ds.batch(15, drop_remainder=True)def label_next_5_steps(batch):  return (batch[:-5],   # Take the first 5 steps          batch[-5:])   # take the remainderpredict_5_steps = batches.map(label_next_5_steps)for features, label in predict_5_steps.take(3):  print(features.numpy(), &quot; =&gt; &quot;, label.numpy())[0 1 2 3 4 5 6 7 8 9]  =&gt;  [10 11 12 13 14][15 16 17 18 19 20 21 22 23 24]  =&gt;  [25 26 27 28 29][30 31 32 33 34 35 36 37 38 39]  =&gt;  [40 41 42 43 44]</code></pre><p>或者一个批次的标签和下个批次的输入有重叠：</p><pre><code class="lang-py">feature_length = 10label_length = 5features = range_ds.batch(feature_length, drop_remainder=True)labels = range_ds.batch(feature_length).skip(1).map(lambda labels: labels[:-5])predict_5_steps = tf.data.Dataset.zip((features, labels))for features, label in predict_5_steps.take(3):  print(features.numpy(), &quot; =&gt; &quot;, label.numpy())[0 1 2 3 4 5 6 7 8 9]  =&gt;  [10 11 12 13 14][10 11 12 13 14 15 16 17 18 19]  =&gt;  [20 21 22 23 24][20 21 22 23 24 25 26 27 28 29]  =&gt;  [30 31 32 33 34]</code></pre><p>有更方便的方法，那就是使用<code>Dataset.window</code>方法</p><pre><code class="lang-py">def make_window_dataset(ds, window_size, shift, stride):  windows = ds.window(window_size, shift=shift, stride=stride)  def sub_to_batch(sub):    return sub.batch(window_size, drop_remainder=True)  windows = windows.flat_map(sub_to_batch)  return windows</code></pre><p><code>Dataset.window(window_size, shift=shift, stride=stride)</code> 中的window_size代表窗口大小，即每个batch的元素个数；shift代表每次窗口移动的距离；stride代表选择元素的间隔</p><pre><code class="lang-py">ds = make_window_dataset(range_ds, window_size=10, shift = 5, stride=3)for example in ds.take(10):  print(example.numpy())[ 0  3  6  9 12 15 18 21 24 27][ 5  8 11 14 17 20 23 26 29 32][10 13 16 19 22 25 28 31 34 37][15 18 21 24 27 30 33 36 39 42][20 23 26 29 32 35 38 41 44 47][25 28 31 34 37 40 43 46 49 52][30 33 36 39 42 45 48 51 54 57][35 38 41 44 47 50 53 56 59 62][40 43 46 49 52 55 58 61 64 67][45 48 51 54 57 60 63 66 69 72]</code></pre><p>提取这些数据的标签方法：</p><pre><code class="lang-py">dense_labels_ds = ds.map(dense_1_step)for inputs,labels in dense_labels_ds.take(3):  print(inputs.numpy(), &quot;=&gt;&quot;, labels.numpy())[ 0  3  6  9 12 15 18 21 24] =&gt; [ 3  6  9 12 15 18 21 24 27][ 5  8 11 14 17 20 23 26 29] =&gt; [ 8 11 14 17 20 23 26 29 32][10 13 16 19 22 25 28 31 34] =&gt; [13 16 19 22 25 28 31 34 37]</code></pre><h2 id="重采样"><a href="#重采样" class="headerlink" title="重采样"></a>重采样</h2><p>有一些数据集，不同类别的数据分布不均匀。这个时候需要对那些不足的类别进行重采样。</p><p>给定信用卡欺诈（二分类）数据集，下面首先检查数据集中不同类别的占比</p><pre><code class="lang-py">def count(counts, batch):  features, labels = batch  class_1 = labels == 1  class_1 = tf.cast(class_1, tf.int32)  class_0 = labels == 0  class_0 = tf.cast(class_0, tf.int32)  counts[&#39;class_0&#39;] += tf.reduce_sum(class_0)  counts[&#39;class_1&#39;] += tf.reduce_sum(class_1)  return countscounts = creditcard_ds.take(10).reduce(    initial_state={&#39;class_0&#39;: 0, &#39;class_1&#39;: 0},    reduce_func = count)counts = np.array([counts[&#39;class_0&#39;].numpy(),                   counts[&#39;class_1&#39;].numpy()]).astype(np.float32)fractions = counts/counts.sum()print(fractions)[0.9953 0.0047]</code></pre><p>偏差很大，这样训练的二分类器只需全预测为正类，即可达到99.53%的正确率。</p><p>重采样数据集的一种方法是使用<code>sample_from_datasets</code> 。当每个类都有单独的<code>data.Dataset</code>时，此方法更适用。</p><p>正类和反类分别构建 <code>Dataset</code></p><pre><code class="lang-py">negative_ds = (  creditcard_ds    .unbatch()    .filter(lambda features, label: label==0)    .repeat())positive_ds = (  creditcard_ds    .unbatch()    .filter(lambda features, label: label==1)    .repeat())</code></pre><p>要使用<code>tf.data.experimental.sample_from_datasets</code>传递数据集以及每个数据集的权重</p><pre><code class="lang-py">balanced_ds = tf.data.experimental.sample_from_datasets(    [negative_ds, positive_ds], [0.5, 0.5]).batch(10)for features, labels in balanced_ds.take(10):  print(labels.numpy())[0 1 1 0 0 1 0 1 0 0][0 1 0 1 1 0 0 1 1 0][1 1 1 0 0 1 1 0 1 1][0 0 0 0 1 1 0 0 1 0][0 1 1 0 0 0 1 0 0 0][1 1 0 0 0 0 0 0 0 0][0 1 0 1 1 0 0 1 1 1][0 0 1 0 1 0 1 0 1 1][1 0 1 1 0 1 0 0 1 0][0 0 0 1 1 1 1 0 1 1]</code></pre><p>现在数据集就平衡了。</p><p>上述<code>experimental.sample_from_datasets</code>方法的一个问题是，每个类需要一个单独的<code>tf.data.Dataset</code>。</p><p>可以将<code>data.experimental.rejection_resample</code>函数应用于数据集，它仅加载一次，通过将多余元素将从数据集中删除以实现平衡。</p><p><code>data.experimental.rejection_resample</code>采用<code>class_func</code>参数，用于标记每个数据集元素所属的类别。</p><p>由于<code>Dataset</code>已经是<code>(features, label)</code>标记好的状态，因此只需</p><pre><code class="lang-py">def class_func(features, label):  return label</code></pre><p>重采样器输入的数据不能为batch后的Dataset，必须经过unbatch。重采样器还需要目标分布，以及可选的初始分布估计。最后经过map中的函数，直接删除掉extra_label即可。</p><pre><code class="lang-py">resampler = tf.data.experimental.rejection_resample(    class_func, target_dist=[0.5, 0.5], initial_dist=fractions)resample_ds = creditcard_ds.unbatch().apply(resampler).batch(10)balanced_ds = resample_ds.map(lambda extra_label, features_and_label: features_and_label)</code></pre><h2 id="数据集迭代器的checkpoint"><a href="#数据集迭代器的checkpoint" class="headerlink" title="数据集迭代器的checkpoint"></a>数据集迭代器的checkpoint</h2><p>没想到吧？不只是模型能使用checkpoint，Dataset的处理过程也可以使用checkpoint。如果您有一个很大的数据集，并且不想在每次重新启动时都从头开始，则这可能很有用。但是请注意，迭代器检查点可能很大，因为诸如shuffle和prefetch需要迭代器中的缓冲元素。</p><p>下面是示例：</p><pre><code class="lang-py">range_ds = tf.data.Dataset.range(20)iterator = iter(range_ds)ckpt = tf.train.Checkpoint(step=tf.Variable(0), iterator=iterator)manager = tf.train.CheckpointManager(ckpt, &#39;/tmp/my_ckpt&#39;, max_to_keep=3)print([next(iterator).numpy() for _ in range(5)])[0, 1, 2, 3, 4]save_path = manager.save()print([next(iterator).numpy() for _ in range(5)])[5, 6, 7, 8, 9]ckpt.restore(manager.latest_checkpoint)print([next(iterator).numpy() for _ in range(5)])[5, 6, 7, 8, 9]</code></pre><h2 id="在Keras中使用tf-data"><a href="#在Keras中使用tf-data" class="headerlink" title="在Keras中使用tf.data"></a>在Keras中使用<code>tf.data</code></h2><p>数据集的处理：</p><pre><code class="lang-py">train, test = tf.keras.datasets.fashion_mnist.load_data()images, labels = trainimages = images/255.0labels = labels.astype(np.int32)fmnist_train_ds = tf.data.Dataset.from_tensor_slices((images, labels))fmnist_train_ds = fmnist_train_ds.shuffle(5000).batch(32)</code></pre><p>模型构建：</p><pre><code class="lang-py">model = tf.keras.Sequential([  tf.keras.layers.Flatten(),  tf.keras.layers.Dense(10)])model.compile(optimizer=&#39;adam&#39;,              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),               metrics=[&#39;accuracy&#39;])</code></pre><p>模型训练：</p><pre><code class="lang-py">model.fit(fmnist_train_ds, epochs=2)</code></pre><p>如果在fit过程中你要对Dataset进行repeat，只需指定每个epochs使用的数据个数，然后不给repeat指定参数，数据集就会变成无限个，一定会满足epochs的要求。</p><pre><code class="lang-py">model.fit(fmnist_train_ds.repeat(), epochs=2, steps_per_epoch=20)</code></pre><p>同理，evaluate时也是一样的</p><pre><code class="lang-py">loss, accuracy = model.evaluate(fmnist_train_ds.repeat(), steps=10)print(&quot;Loss :&quot;, loss)print(&quot;Accuracy :&quot;, accuracy)Loss : 0.3501795828342438Accuracy : 0.8968750238418579</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>如果在使用机器学习算法解决问题过程中，数据预处理是主要问题的话，采用合适的数据组织手段可以帮助解决问题。 <code>tf.data</code> 能够快速处理大量数据，并将各个来源的数据归一化成合适的 <code>Dataset</code> 格式。</p><p>你可以对构建好的 <code>tf.data.Dataset</code> 做预处理操作，比如随机打乱、分批次、规划时间窗口、重采样等等。</p><p>经过处理后的 <code>Dataset</code> 对象可以直接输入到keras进行训练。</p></batchdataset></batchdataset>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用 &lt;code&gt;tf.data&lt;/code&gt; API 可以轻松处理大量数据，支持多样化的数据格式，还可以方便执行复杂的转换。本文介绍了不同类别源数据转化为 &lt;code&gt;tf.data.Dataset&lt;/code&gt; 的方法，以及 &lt;code&gt;Dataset&lt;/code&gt; 常见的预处理方法。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="preprocessing" scheme="https://superlova.github.io/tags/preprocessing/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】使用TensorBoard分析模型性能</title>
    <link href="https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8TensorBoard%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD/"/>
    <id>https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8TensorBoard%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD/</id>
    <published>2020-08-14T08:39:06.000Z</published>
    <updated>2020-08-15T01:04:23.193Z</updated>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第六篇，讲述的是如何使用TensorBoard分析模型的性能，优化模型的资源消耗。<br><a id="more"></a></p><p>TensorBoard可以监控模型的各个组分运行过程中的时间消耗和资源消耗，并根据这些数据对模型下一步优化提出建议。</p><p>首先我们安装性能分析的插件</p><pre><code class="lang-bash">!pip install -U tensorboard_plugin_profile</code></pre><p>定义TensorBoard的回调函数（数据预处理和模型定义略去不表），注意这里新的参数<code>profile_batch</code>只监控第500到520之间的20个Batch，避免监控过多导致模型运行效率过低。</p><pre><code class="lang-python"># Create a TensorBoard callbacklogs = &quot;logs/&quot; + datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)tboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs,                        histogram_freq=1,                        profile_batch=&#39;500,520&#39;)model.fit(ds_train,    epochs=2,    validation_data=ds_test,    callbacks=[tboard_callback])</code></pre><p>打开TensorBoard：</p><pre><code class="lang-bash"># Load the TensorBoard notebook extension.%load_ext tensorboard# Launch TensorBoard and navigate to the Profile tab to view performance profile%tensorboard --logdir=logs</code></pre><p><img src="/2020/08/14/【学习笔记】使用TensorBoard分析模型性能/2020-08-14-16-47-10.png" srcset="/img/loading.gif" alt></p><p>有非常多有用的信息，比如每个batch消耗的时间都花在哪里了：</p><p><img src="/2020/08/14/【学习笔记】使用TensorBoard分析模型性能/2020-08-14-17-09-32.png" srcset="/img/loading.gif" alt></p><p>还有针对耗时的改进意见：</p><p><img src="/2020/08/14/【学习笔记】使用TensorBoard分析模型性能/2020-08-14-17-09-56.png" srcset="/img/loading.gif" alt></p><p>有耗时最长的10大操作：</p><p><img src="/2020/08/14/【学习笔记】使用TensorBoard分析模型性能/2020-08-14-17-10-27.png" srcset="/img/loading.gif" alt></p><p>有性能监控选项，查看CPU活动和GPU活动。根据一般经验，始终保持设备（GPU / TPU）处于活动状态是我们的优化目标。</p><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/profiler_trace_viewer_bad_ip.png" srcset="/img/loading.gif" alt></p><p>查看事件跟踪，可以看到<code>tf_data_iterator_get_next</code> 操作在CPU上运行时GPU不活动。该操作负责处理输入数据并将其发送到GPU进行训练。因此我们的优化方法可以是使用tf.data API优化输入管道，缓存训练数据集并预取数据，以确保始终有可供GPU处理的数据。</p><pre><code class="lang-python"> (ds_train, ds_test), ds_info = tfds.load(    &#39;mnist&#39;,    split=[&#39;train&#39;, &#39;test&#39;],    shuffle_files=True,    as_supervised=True,    with_info=True,)ds_train = ds_train.map(normalize_img)ds_train = ds_train.batch(128)ds_train = ds_train.cache()ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)ds_test = ds_test.map(normalize_img)ds_test = ds_test.batch(128)ds_test = ds_test.cache()ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)model.fit(ds_train,        epochs=2,        validation_data=ds_test,        callbacks = [tboard_callback])%tensorboard --logdir=logs</code></pre><p>跟踪查看器显示<code>tf_data_iterator_get_next</code>操作执行得更快。因此，GPU获得了稳定的数据流以进行训练，并通过模型训练获得了更好的利用率。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是TensorBoard笔记的第六篇，讲述的是如何使用TensorBoard分析模型的性能，优化模型的资源消耗。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="TensorBoard" scheme="https://superlova.github.io/tags/TensorBoard/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】通过TensorBoard可视化词嵌入空间</title>
    <link href="https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%8D%E5%B5%8C%E5%85%A5%E7%A9%BA%E9%97%B4/"/>
    <id>https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%8D%E5%B5%8C%E5%85%A5%E7%A9%BA%E9%97%B4/</id>
    <published>2020-08-14T08:37:06.000Z</published>
    <updated>2020-08-14T08:38:23.417Z</updated>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第五篇，讲述的是如何使用TensorBoard对词嵌入空间可视化。<br><a id="more"></a></p><p>TensorBoard可以将词嵌入空间二维化，方便我们理解词嵌入空间的含义。</p><pre><code class="lang-python">from tensorboard.plugins import projector</code></pre><p>引入IMDb数据集。tensorflow_datasets是tensorflow的官方数据集库。</p><pre><code class="lang-python">import tensorflow_datasets as tfds(train_data, test_data), info = tfds.load(    &quot;imdb_reviews/subwords8k&quot;,    split=(tfds.Split.TRAIN, tfds.Split.TEST),    with_info=True,    as_supervised=True,)encoder = info.features[&quot;text&quot;].encoder# shuffle and pad the data.train_batches = train_data.shuffle(1000).padded_batch(    10, padded_shapes=((None,), ()))test_batches = test_data.shuffle(1000).padded_batch(    10, padded_shapes=((None,), ()))train_batch, train_labels = next(iter(train_batches))</code></pre><p>训练词嵌入模型</p><pre><code class="lang-python"># Create an embedding layerembedding_dim = 16embedding = tf.keras.layers.Embedding(encoder.vocab_size, embedding_dim)# Train this embedding as part of a keras modelmodel = tf.keras.Sequential(    [        embedding, # The embedding layer should be the first layer in a model.        tf.keras.layers.GlobalAveragePooling1D(),        tf.keras.layers.Dense(16, activation=&quot;relu&quot;),        tf.keras.layers.Dense(1),    ])# Compile modelmodel.compile(    optimizer=&quot;adam&quot;,    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),    metrics=[&quot;accuracy&quot;],)# Train modelhistory = model.fit(    train_batches, epochs=1, validation_data=test_batches, validation_steps=20)</code></pre><p>将数据保存成方便TensorBoard读取的形式</p><pre><code class="lang-python"># Set up a logs directory, so Tensorboard knows where to look for fileslog_dir=&#39;/logs/imdb-example/&#39;if not os.path.exists(log_dir):    os.makedirs(log_dir)# Save Labels separately on a line-by-line manner.with open(os.path.join(log_dir, &#39;metadata.tsv&#39;), &quot;w&quot;) as f:  for subwords in encoder.subwords:    f.write(&quot;{}\n&quot;.format(subwords))  # Fill in the rest of the labels with &quot;unknown&quot;  for unknown in range(1, encoder.vocab_size - len(encoder.subwords)):    f.write(&quot;unknown #{}\n&quot;.format(unknown))# Save the weights we want to analyse as a variable. Note that the first# value represents any unknown word, which is not in the metadata, so# we will remove that value.weights = tf.Variable(model.layers[0].get_weights()[0][1:])# Create a checkpoint from embedding, the filename and key are# name of the tensor.checkpoint = tf.train.Checkpoint(embedding=weights)checkpoint.save(os.path.join(log_dir, &quot;embedding.ckpt&quot;))# Set up configconfig = projector.ProjectorConfig()embedding = config.embeddings.add()# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`embedding.tensor_name = &quot;embedding/.ATTRIBUTES/VARIABLE_VALUE&quot;embedding.metadata_path = &#39;metadata.tsv&#39;projector.visualize_embeddings(log_dir, config)</code></pre><pre><code class="lang-python">%tensorboard --logdir /logs/imdb-example/</code></pre><p>你可以看到2维、3维空间中的Embedding，搜索某个单词在词嵌入空间中的位置，甚至可以采取不同的可视化方法：<br><img src="/2020/08/14/【学习笔记】通过TensorBoard可视化词嵌入空间/2020-08-13-19-45-25.png" srcset="/img/loading.gif" alt></p><p>打开该工具的时候我的电脑很卡。。。希望大家注意。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是TensorBoard笔记的第五篇，讲述的是如何使用TensorBoard对词嵌入空间可视化。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="TensorBoard" scheme="https://superlova.github.io/tags/TensorBoard/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】通过TensorBoard调整超参数</title>
    <link href="https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87TensorBoard%E8%B0%83%E6%95%B4%E8%B6%85%E5%8F%82%E6%95%B0/"/>
    <id>https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87TensorBoard%E8%B0%83%E6%95%B4%E8%B6%85%E5%8F%82%E6%95%B0/</id>
    <published>2020-08-14T08:34:46.000Z</published>
    <updated>2020-08-14T08:36:27.555Z</updated>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第四篇，讲述的是如何借助TensorBoard调整模型的超参数。<br><a id="more"></a></p><p>TensorBoard中的HParams仪表板是比较新颖的工具包，提供了多种调节超参数的工具，并且该工具还在不断更新中。</p><pre><code class="lang-python">from tensorboard.plugins.hparams import api as hp</code></pre><p>用过sklearn进行机器学习模型调参的同学应该体验过交叉验证调参的方法。通过提供许多不同的超参数选项，<code>GridSearchCV</code>将训练多个模型，并取性能最优的模型超参数。</p><pre><code class="lang-python">from sklearn.model_selection import GridSearchCVfrom sklearn.pipeline import make_pipelineX_train, X_test, y_train, y_test = train_test_split(df_train[&#39;text&#39;][:10000], df_train[&#39;label&#39;][:10000], random_state=0)pipe_logis = make_pipeline(TfidfVectorizer(min_df=5, ngram_range=(1,3)), LogisticRegression())param_grid = {&#39;logisticregression__C&#39;: [0.001, 0.01, 0.1, 1, 10]}grid = GridSearchCV(pipe_logis, param_grid, cv=5)grid.fit(X_train, y_train)print(&quot;Best params:\n{}\n&quot;.format(grid.best_params_))print(&quot;Best cross-validation score: {:.2f}&quot;.format(grid.best_score_))print(&quot;Test-set score: {:.2f}&quot;.format(grid.score(X_test, y_test)))</code></pre><p>在上面的程序中，grid中包含一个需要调节的超参数，即逻辑回归的C值。候选C值有5个，因此grid在fit过程中会训练五个模型，每个模型执行5次交叉验证（因为fit中cv参数为5）。</p><p>HParams也是采用类似的方法找超参数。首先我们定义候选超参数的变化范围。我们选择三个参数进行网格搜索，分别是Dense层的Unit数目、dropout的比例和优化器，每个超参数都有两种选择，因此一共需要训练八个模型。</p><p>最终模型的评价标准以Accuracy为准。具体代码如下所示：</p><pre><code class="lang-python">HP_NUM_UNITS = hp.HParam(&#39;num_units&#39;, hp.Discrete([16, 32]))HP_DROPOUT = hp.HParam(&#39;dropout&#39;, hp.RealInterval(0.1, 0.2))HP_OPTIMIZER = hp.HParam(&#39;optimizer&#39;, hp.Discrete([&#39;adam&#39;, &#39;sgd&#39;]))METRIC_ACCURACY = &#39;accuracy&#39;</code></pre><p>设置write句柄，这已经是传统艺能了。</p><pre><code class="lang-python">with tf.summary.create_file_writer(&#39;logs/hparam_tuning&#39;).as_default():  hp.hparams_config(    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],    metrics=[hp.Metric(METRIC_ACCURACY, display_name=&#39;Accuracy&#39;)],  )</code></pre><p>接下来我们定义待训练模型。模型本身非常简单，而且只训练一个epoch，这是考虑到要消耗平时八倍的时间而采取的tradeoff。</p><pre><code class="lang-python">def train_test_model(hparams):  model = tf.keras.models.Sequential([    tf.keras.layers.Flatten(),    tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=tf.nn.relu),    tf.keras.layers.Dropout(hparams[HP_DROPOUT]),    tf.keras.layers.Dense(10, activation=tf.nn.softmax),  ])  model.compile(      optimizer=hparams[HP_OPTIMIZER],      loss=&#39;sparse_categorical_crossentropy&#39;,      metrics=[&#39;accuracy&#39;],  )  model.fit(x_train, y_train, epochs=1) # Run with 1 epoch to speed things up for demo purposes  _, accuracy = model.evaluate(x_test, y_test)  return accuracy</code></pre><p>进行训练并记录模型输出；</p><pre><code class="lang-python">def run(run_dir, hparams):  with tf.summary.create_file_writer(run_dir).as_default():    hp.hparams(hparams)  # record the values used in this trial    accuracy = train_test_model(hparams)    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)</code></pre><p>之后我们进行网格搜索（其实就是遍历每种可能。搜索方法完全是自己定义的，你也可以使用随机搜索方法）：</p><pre><code class="lang-python">session_num = 0for num_units in HP_NUM_UNITS.domain.values:  for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):    for optimizer in HP_OPTIMIZER.domain.values:      hparams = {          HP_NUM_UNITS: num_units,          HP_DROPOUT: dropout_rate,          HP_OPTIMIZER: optimizer,      }      run_name = &quot;run-%d&quot; % session_num      print(&#39;--- Starting trial: %s&#39; % run_name)      print({h.name: hparams[h] for h in hparams})      run(&#39;logs/hparam_tuning/&#39; + run_name, hparams)      session_num += 1</code></pre><p>最后使用TensorBoard进行可视化：</p><pre><code class="lang-bash">%tensorboard --logdir logs/hparam_tuning</code></pre><p><img src="https://www.tensorflow.org/tensorboard/images/hparams_table.png?raw=1" srcset="/img/loading.gif" alt></p><p>我们可以通过TensorBoard发现很多有趣的现象：比如在本模型中，adam优化器比sgd要好等等。</p><p><img src="/2020/08/14/【学习笔记】通过TensorBoard调整超参数/2020-08-13-19-19-41.png" srcset="/img/loading.gif" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是TensorBoard笔记的第四篇，讲述的是如何借助TensorBoard调整模型的超参数。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="TensorBoard" scheme="https://superlova.github.io/tags/TensorBoard/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】用TensorBoard生成模型图</title>
    <link href="https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8TensorBoard%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%9B%BE/"/>
    <id>https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8TensorBoard%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%9B%BE/</id>
    <published>2020-08-14T08:31:40.000Z</published>
    <updated>2020-08-14T08:34:09.831Z</updated>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第三篇，讲述的是如何令TensorBoard生成模型架构图。<br><a id="more"></a></p><p>TensorBoard不但可以展示存在的图片和张量，还可以生成图片，诸如模型图等。通过TensorBoard的GRAPHS选项卡，可以快速查看模型结构的预览图，并确保其符合设计预期。</p><p>比如我们定义模型如下：</p><pre><code class="lang-python">model = keras.models.Sequential([    keras.layers.Flatten(input_shape=(28, 28)),    keras.layers.Dense(32, activation=&#39;relu&#39;),    keras.layers.Dropout(0.2),    keras.layers.Dense(10, activation=&#39;softmax&#39;)])model.compile(    optimizer=&#39;adam&#39;,    loss=&#39;sparse_categorical_crossentropy&#39;,    metrics=[&#39;accuracy&#39;])</code></pre><p><img src="/2020/08/14/【学习笔记】用TensorBoard生成模型图/2020-08-13-13-31-51.png" srcset="/img/loading.gif" alt></p><p>通过TensorBoard中的GRAPHS选项卡，我们看到执行图。图是倒置的，数据从下到上流动，因此与代码相比是上下颠倒的。</p><p>可以更改Tag，选择Keras，选择左边的Conceptual Graph查看概念图，双击Sequential，得到概念图。概念图更像是代码。</p><p><img src="/2020/08/14/【学习笔记】用TensorBoard生成模型图/2020-08-13-13-35-03.png" srcset="/img/loading.gif" alt></p><p>有的时候我们希望得到计算图，研究数据经过了何种计算。比如下面这个函数：</p><pre><code class="lang-python"># Sample data for your function.x = tf.random.uniform((3, 3))y = tf.random.uniform((3, 3))# The function to be traced.@tf.functiondef my_func(x, y):  # A simple hand-rolled layer.  return tf.nn.relu(tf.matmul(x, y))# z = my_func(x, y)</code></pre><p>我们希望得到它的计算图。首先需要使用<code>@tf.function</code>修饰被监控的函数，然后使用<code>tf.summary.trace_on()</code>在<code>z = my_func(x, y)</code>函数运行之前开始记录。</p><p><img src="https://as2.bitinn.net/uploads/legacy/og/cistioqrt008t8q5nh9gtx9og.1200.jpg" srcset="/img/loading.gif" alt="Trace On"></p><pre><code class="lang-python">tf.summary.trace_on(graph=True, profiler=True)z = my_func(x, y)</code></pre><p>定义日志目录名称和文件写入句柄，这些都是刻在DNA里的操作：</p><pre><code class="lang-python"># Set up logging.stamp = datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)logdir = &#39;logs/func/%s&#39; % stampwriter = tf.summary.create_file_writer(logdir)</code></pre><p>最后执行记录：</p><pre><code class="lang-python"># Call only one tf.function when tracing.with writer.as_default():  tf.summary.trace_export(      name=&quot;my_func_trace&quot;,      step=0,      profiler_outdir=logdir)</code></pre><pre><code class="lang-bash">%tensorboard --logdir logs/func</code></pre><p><img src="/2020/08/14/【学习笔记】用TensorBoard生成模型图/2020-08-13-13-47-57.png" srcset="/img/loading.gif" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是TensorBoard笔记的第三篇，讲述的是如何令TensorBoard生成模型架构图。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="TensorBoard" scheme="https://superlova.github.io/tags/TensorBoard/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】用TensorBoard展示图片</title>
    <link href="https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8TensorBoard%E5%B1%95%E7%A4%BA%E5%9B%BE%E7%89%87/"/>
    <id>https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8TensorBoard%E5%B1%95%E7%A4%BA%E5%9B%BE%E7%89%87/</id>
    <published>2020-08-14T08:28:29.000Z</published>
    <updated>2020-08-14T08:31:11.010Z</updated>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第二篇，讲述的是如何令TensorBoard展示外界已有的图片和Tensor。<br><a id="more"></a></p><p>我们可以利用TensorBoard展示图片类数据，或者通过tf.summary将张量类数据转化成图片。下面是对Fashion-MNIST数据集中部分图片的可视化：</p><h3 id="可视化单个图片"><a href="#可视化单个图片" class="headerlink" title="可视化单个图片"></a>可视化单个图片</h3><pre><code class="lang-python">import tensorflow as tffrom tensorflow import kerasfashion_mnist = keras.datasets.fashion_mnist(train_images, train_labels), (test_images, test_labels) = \    fashion_mnist.load_data()</code></pre><p>数据集中每个图像的形状都是2阶张量形状（28、28），分别表示高度和宽度</p><p>但是， tf.summary.image()期望包含(batch_size, height, width, channels)的4级张量。因此，张量需要重塑。</p><pre><code class="lang-python">img = np.reshape(train_images[0], (-1, 28, 28, 1))</code></pre><p>使用<code>tf.summary.image</code>将其转化为tensor，并利用TensorBoard可视化：</p><pre><code class="lang-python"> # Clear out any prior log data.!rm -rf logs# Sets up a timestamped log directory.logdir = &quot;logs/train_data/&quot; + datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)# Creates a file writer for the log directory.file_writer = tf.summary.create_file_writer(logdir)# Using the file writer, log the reshaped image.with file_writer.as_default():  tf.summary.image(&quot;Training data&quot;, img, step=0)</code></pre><p>转化后的图片被<code>tf.summary.create_file_writer</code>输出到logdir里面了。使用TensorBoard看看：</p><pre><code class="lang-bash">%tensorboard --logdir logs/train_data</code></pre><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_single.png" srcset="/img/loading.gif" alt></p><p>加载的过程可能有点慢，注意留足够的内存以免标签页崩溃。</p><p>你也可以使用左边的滑动条调节亮度、对比度和大小。</p><h3 id="可视化多张图片"><a href="#可视化多张图片" class="headerlink" title="可视化多张图片"></a>可视化多张图片</h3><p>调整<code>tf.summary.image</code>里面的参数<code>max_outputs</code>：</p><pre><code class="lang-python"> with file_writer.as_default():    # Don&#39;t forget to reshape.    images = np.reshape(train_images[0:25], (-1, 28, 28, 1))    tf.summary.image(&quot;25 training data examples&quot;, images, max_outputs=25, step=0)</code></pre><pre><code class="lang-bash">%tensorboard --logdir logs/train_data</code></pre><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_multiple.png" srcset="/img/loading.gif" alt></p><h3 id="可视化其他格式的图片"><a href="#可视化其他格式的图片" class="headerlink" title="可视化其他格式的图片"></a>可视化其他格式的图片</h3><p>有些图片不是tensor或者numpy.array，而是由诸如opencv、matplotlib生成的png图像，我们需要将其转化为tensor。</p><p>由于matplotlib适合生成复杂的数据图，因此先利用其他库生成图片，随后利用<code>tf.summary.image</code>将其转化为一个tensor再可视化，是一个比较方便的选择。</p><p>matplotlib生成数据集可视化：</p><pre><code class="lang-python"> # Clear out prior logging data.!rm -rf logs/plotslogdir = &quot;logs/plots/&quot; + datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)file_writer = tf.summary.create_file_writer(logdir)def plot_to_image(figure):  &quot;&quot;&quot;Converts the matplotlib plot specified by &#39;figure&#39; to a PNG image and  returns it. The supplied figure is closed and inaccessible after this call.&quot;&quot;&quot;  # Save the plot to a PNG in memory.  buf = io.BytesIO()  plt.savefig(buf, format=&#39;png&#39;)  # Closing the figure prevents it from being displayed directly inside  # the notebook.  plt.close(figure)  buf.seek(0)  # Convert PNG buffer to TF image  image = tf.image.decode_png(buf.getvalue(), channels=4)  # Add the batch dimension  image = tf.expand_dims(image, 0)  return imagedef image_grid():  &quot;&quot;&quot;Return a 5x5 grid of the MNIST images as a matplotlib figure.&quot;&quot;&quot;  # Create a figure to contain the plot.  figure = plt.figure(figsize=(10,10))  for i in range(25):    # Start next subplot.    plt.subplot(5, 5, i + 1, title=class_names[train_labels[i]])    plt.xticks([])    plt.yticks([])    plt.grid(False)    plt.imshow(train_images[i], cmap=plt.cm.binary)  return figure</code></pre><p>尔后，利用<code>tf.summary.image</code>转化：</p><pre><code class="lang-python"># Prepare the plotfigure = image_grid()# Convert to image and logwith file_writer.as_default():  tf.summary.image(&quot;Training data&quot;, plot_to_image(figure), step=0)</code></pre><p>最后，利用TensorBoard可视化：</p><pre><code class="lang-bash">%tensorboard --logdir logs/plots</code></pre><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_arbitrary.png" srcset="/img/loading.gif" alt></p><h3 id="在图片分类器中使用TensorBoard"><a href="#在图片分类器中使用TensorBoard" class="headerlink" title="在图片分类器中使用TensorBoard"></a>在图片分类器中使用TensorBoard</h3><p>之前我们通过TensorBoard了解了Fashion-MNIST数据集的概要，但是TensorBoard的功能不止于此。</p><p>首先构建分类模型：</p><pre><code class="lang-python">model = keras.models.Sequential([    keras.layers.Flatten(input_shape=(28, 28)),    keras.layers.Dense(32, activation=&#39;relu&#39;),    keras.layers.Dense(10, activation=&#39;softmax&#39;)])model.compile(    optimizer=&#39;adam&#39;,     loss=&#39;sparse_categorical_crossentropy&#39;,    metrics=[&#39;accuracy&#39;])</code></pre><p>我们想使用<a href="https://en.wikipedia.org/wiki/Confusion_matrix" target="_blank" rel="noopener">混淆矩阵</a>详细了解分类器对测试数据的性能。因此接下来定义一个函数，专门计算混淆矩阵。具体来说，</p><ol><li>使用model.predict预测该epoch的所有测试用例的标签，得到<code>test_pred</code></li><li>调用<code>sklearn.metrics.confusion_matrix</code>直接计算混淆矩阵</li><li>使用<code>matplotlib</code>将混淆矩阵可视化</li><li>将<code>matplotlib</code>生成的图片转为tensor，最后变成log储存</li></ol><p>下面是前两步所需的操作：</p><pre><code class="lang-python">def log_confusion_matrix(epoch, logs):    # Use the model to predict the values from the validation dataset.    test_pred_raw = model.predict(test_images)    test_pred = np.argmax(test_pred_raw, axis=1)    # Calculate the confusion matrix.    cm = sklearn.metrics.confusion_matrix(test_labels, test_pred)    # Log the confusion matrix as an image summary.    figure = plot_confusion_matrix(cm, class_names=class_names)    cm_image = plot_to_image(figure)    # Log the confusion matrix as an image summary.    with file_writer_cm.as_default():    tf.summary.image(&quot;Confusion Matrix&quot;, cm_image, step=epoch)</code></pre><p>下面是第三步所需的可视化函数：</p><pre><code class="lang-python">def plot_confusion_matrix(cm, class_names):    &quot;&quot;&quot;    Returns a matplotlib figure containing the plotted confusion matrix.    Args:    cm (array, shape = [n, n]): a confusion matrix of integer classes    class_names (array, shape = [n]): String names of the integer classes    &quot;&quot;&quot;    figure = plt.figure(figsize=(8, 8))    plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=plt.cm.Blues)    plt.title(&quot;Confusion matrix&quot;)    plt.colorbar()    tick_marks = np.arange(len(class_names))    plt.xticks(tick_marks, class_names, rotation=45)    plt.yticks(tick_marks, class_names)    # Normalize the confusion matrix.    cm = np.around(cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis], decimals=2)    # Use white text if squares are dark; otherwise black.    threshold = cm.max() / 2.    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):    color = &quot;white&quot; if cm[i, j] &gt; threshold else &quot;black&quot;    plt.text(j, i, cm[i, j], horizontalalignment=&quot;center&quot;, color=color)    plt.tight_layout()    plt.ylabel(&#39;True label&#39;)    plt.xlabel(&#39;Predicted label&#39;)    return figure</code></pre><p>下面是第四步所需的tensor转化和储存函数以及其他回调函数：</p><pre><code class="lang-bash"># Clear out prior logging data.!rm -rf logs/image</code></pre><pre><code class="lang-python">logdir = &quot;logs/image/&quot; + datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)# Define the basic TensorBoard callback.tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)file_writer_cm = tf.summary.create_file_writer(logdir + &#39;/cm&#39;)</code></pre><p>让我们开始训练：</p><pre><code class="lang-bash"># Start TensorBoard.%tensorboard --logdir logs/image</code></pre><pre><code class="lang-python"># Train the classifier.model.fit(    train_images,    train_labels,    epochs=5,    verbose=0, # Suppress chatty output    callbacks=[tensorboard_callback, cm_callback],    validation_data=(test_images, test_labels),)</code></pre><p>请注意，此时我先调用的TensorBoard，然后开始的训练，并且我设置了verbose=0，意味着信息完全通过TensorBoard动态展示。训练过程中你就可以看到参数的变化。</p><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_accuracy.png" srcset="/img/loading.gif" alt></p><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_cm.png" srcset="/img/loading.gif" alt></p><p>你还可以看到，随着训练的进行，矩阵是如何发生变化的：沿着对角线的正方形会逐渐变暗，而矩阵的其余部分趋向于0和白色。这意味着分类器正在不断改进。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是TensorBoard笔记的第二篇，讲述的是如何令TensorBoard展示外界已有的图片和Tensor。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="TensorBoard" scheme="https://superlova.github.io/tags/TensorBoard/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】TensorBoard简介</title>
    <link href="https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91TensorBoard%E7%AE%80%E4%BB%8B/"/>
    <id>https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91TensorBoard%E7%AE%80%E4%BB%8B/</id>
    <published>2020-08-14T07:28:09.000Z</published>
    <updated>2020-08-14T08:25:08.319Z</updated>
    
    <content type="html"><![CDATA[<p>简单介绍了TensorBoard的用途和使用方法。<br><a id="more"></a></p><h2 id="TensorBoard简介"><a href="#TensorBoard简介" class="headerlink" title="TensorBoard简介"></a>TensorBoard简介</h2><p>TensorBoard是Google开发的模型内部参数跟踪和可视化的调试工具。在Tensorflow中，用TensorBoard可以监控模型的各种指标的变化（如acc、loss的动态变化），可以将模型结构可视化，可以可视化词嵌入空间，可以分析模型性能，可以分析数据集的公平性等等，是一个非常强大且非常简单的工具。</p><p>TensorBoard核心就是回调函数和可视化操作面板。通过编写回调函数获取模型信息，通过命令行启动TensorBoard图形化界面。</p><p>TensorBoard的回调函数API为：</p><pre><code class="lang-python">tf.keras.callbacks.TensorBoard(    log_dir=&#39;logs&#39;, histogram_freq=0, write_graph=True, write_images=False,    update_freq=&#39;epoch&#39;, profile_batch=2, embeddings_freq=0,    embeddings_metadata=None, **kwargs)</code></pre><div class="table-container"><table><thead><tr><th>参数</th><th>含义</th></tr></thead><tbody><tr><td>log_dir</td><td>模型的信息保存目录</td></tr><tr><td>histogram_freq</td><td>模型激活和参数信息记录的频率，每隔几个epochs记录一次</td></tr><tr><td>write_graph</td><td>是否保存模型图文件</td></tr><tr><td>write_images</td><td>是否保存模型参数可视化图</td></tr><tr><td>update_freq</td><td>模型loss和其他metrics的记录频率，每隔几个batch更新一次</td></tr><tr><td>profile_batch</td><td>指定性能分析时使用的批次</td></tr><tr><td>embeddings_freq</td><td>embedding 层更新的频率</td></tr></tbody></table></div><p>在Colab中使用TensorBoard需输入：</p><pre><code class="lang-bash">%load_ext tensorboard</code></pre><p>为了跟踪模型训练过程，需要在模型的<code>fit</code>过程中添加回调函数</p><pre><code class="lang-python">tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)</code></pre><p>其中<code>log_dir</code>为你想储存log的目录，在教程中，<code>log_dir=&quot;logs/fit/&quot; + datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)</code>。</p><pre><code class="lang-python">model.fit(x=x_train,           y=y_train,           epochs=5,           validation_data=(x_test, y_test),           callbacks=[tensorboard_callback])</code></pre><p>在<code>log_dir</code>中生成了一系列的日志文件：</p><p><img src="/2020/08/14/【学习笔记】TensorBoard简介/2020-08-13-12-39-02.png" srcset="/img/loading.gif" alt></p><p>这些日志文件可以通过TensorBoard解析：</p><pre><code class="lang-shell">%tensorboard --logdir logs/fit</code></pre><p>在命令行中， 运行不带“％”的相同命令。结果如下：</p><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/quickstart_model_fit.png" srcset="/img/loading.gif" alt></p><p>在TensorBoard面板上，有四个选项卡：</p><ul><li><strong>Scalars</strong> 显示损失和指标在每个时期如何变化</li><li><strong>Graphs</strong> 可帮助您可视化模型</li><li><strong>Distributions</strong> 和 <strong>Histograms</strong> 显示张量随时间的分布</li></ul><p>如果使用tensorflow原生API训练模型，也可以利用<code>tf.summary</code>记录log，然后利用TensorBoard可视化。具体流程如下：</p><ol><li>使用 <code>tf.summary.create_file_writer()</code> 创建文件编写器；</li><li>使用 <code>tf.summary.scalar()</code> 记录感兴趣的指标</li><li>将 <code>LearningRateScheduler</code> 回调传递给 <code>Model.fit()</code></li><li>使用命令行<code>tensorboard --logdir logs/fit</code>打开可视化界面</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;简单介绍了TensorBoard的用途和使用方法。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="TensorBoard" scheme="https://superlova.github.io/tags/TensorBoard/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】新闻文本分类之注意力机制与预训练模型</title>
    <link href="https://superlova.github.io/2020/08/04/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%8E%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    <id>https://superlova.github.io/2020/08/04/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%8E%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-08-04T12:13:28.000Z</published>
    <updated>2020-08-04T15:56:45.063Z</updated>
    
    <content type="html"><![CDATA[<p>见多了优秀的文章，再写博客的时候就会感叹自己的学识浅薄。<br><a id="more"></a></p><p>本章介绍了Bert的原理和使用，具体包括pretrain和finetune两部分。</p><h2 id="基于深度学习的文本分类"><a href="#基于深度学习的文本分类" class="headerlink" title="基于深度学习的文本分类"></a>基于深度学习的文本分类</h2><h4 id="Transformer原理"><a href="#Transformer原理" class="headerlink" title="Transformer原理"></a>Transformer原理</h4><p>当用神经网络来处理大量的输入信息时，可以借鉴人脑的注意力机制，只选择一些关键的信息输入进行处理，来提高神经网络的效率。注意力机制可以单独使用，但更多地用作神经网络中的一个组件。基于循环神经网络的序列到序列模型的一个缺点是无法并行计算，为了提高并行计算效率以及捕捉长距离的依赖关系，我们可以使用自注意力模型（Self-Attention Model）来建立一个全连接的网络结构。</p><p>Transformer模型是一个基于多头自注意力的序列到序列模型，包含编码器和解码器两部分。</p><p><img src="https://img-blog.csdnimg.cn/20200714211046668.png" srcset="/img/loading.gif" alt></p><h3 id="基于Bert的文本分类"><a href="#基于Bert的文本分类" class="headerlink" title="基于Bert的文本分类"></a>基于Bert的文本分类</h3><p>分成Pretrain和Fine-Tune两部分。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;见多了优秀的文章，再写博客的时候就会感叹自己的学识浅薄。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="Classification" scheme="https://superlova.github.io/tags/Classification/"/>
    
      <category term="Deep Learning" scheme="https://superlova.github.io/tags/Deep-Learning/"/>
    
      <category term="attention" scheme="https://superlova.github.io/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】新闻文本分类之深度学习Word2Vec</title>
    <link href="https://superlova.github.io/2020/07/31/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Word2Vec/"/>
    <id>https://superlova.github.io/2020/07/31/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Word2Vec/</id>
    <published>2020-07-31T14:24:16.000Z</published>
    <updated>2020-07-31T14:27:44.969Z</updated>
    
    <content type="html"><![CDATA[<p>谦虚使人进步，骄傲使人落后。<br><a id="more"></a></p><p>最近课题内容较多时间紧张，同时我认为学习资料非常翔实，遂转发以做笔记。之后会在文章最后总结自己的思路和经验。</p><h1 id="Task5-基于深度学习的文本分类2"><a href="#Task5-基于深度学习的文本分类2" class="headerlink" title="Task5 基于深度学习的文本分类2"></a>Task5 基于深度学习的文本分类2</h1><p>在上一章节，我们通过FastText快速实现了基于深度学习的文本分类模型，但是这个模型并不是最优的。在本章我们将继续深入。</p><h2 id="基于深度学习的文本分类"><a href="#基于深度学习的文本分类" class="headerlink" title="基于深度学习的文本分类"></a>基于深度学习的文本分类</h2><p>本章将继续学习基于深度学习的文本分类。</p><h3 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>学习Word2Vec的使用和基础原理</li><li>学习使用TextCNN、TextRNN进行文本表示</li><li>学习使用HAN网络结构完成文本分类</li></ul><h3 id="文本表示方法-Part3"><a href="#文本表示方法-Part3" class="headerlink" title="文本表示方法 Part3"></a>文本表示方法 Part3</h3><h4 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h4><p>本节通过word2vec学习词向量。word2vec模型背后的基本思想是对出现在上下文环境里的词进行预测。对于每一条输入文本，我们选取一个上下文窗口和一个中心词，并基于这个中心词去预测窗口里其他词出现的概率。因此，word2vec模型可以方便地从新增语料中学习到新增词的向量表达，是一种高效的在线学习算法（online learning）。</p><p>word2vec的主要思路：通过单词和上下文彼此预测，对应的两个算法分别为：</p><ul><li><p>Skip-grams (SG)：预测上下文</p></li><li><p>Continuous Bag of Words (CBOW)：预测目标单词</p></li></ul><p>另外提出两种更加高效的训练方法：</p><ul><li><p>Hierarchical softmax</p></li><li><p>Negative sampling</p></li></ul><p><strong>1. Skip-grams原理和网络结构</strong></p><p>Word2Vec模型中，主要有Skip-Gram和CBOW两种模型，从直观上理解，Skip-Gram是给定input word来预测上下文。而CBOW是给定上下文，来预测input word。</p><p><img src="https://img-blog.csdnimg.cn/20200714210354729.png" srcset="/img/loading.gif" alt="skip_grams"></p><p>Word2Vec模型实际上分为了两个部分，第一部分为建立模型，第二部分是通过模型获取嵌入词向量。</p><p>Word2Vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”。</p><p>Skip-grams过程</p><p>假如我们有一个句子“The dog barked at the mailman”。</p><ol><li><p>首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为input word；</p></li><li><p>有了input word以后，我们再定义一个叫做skip_window的参数，它代表着我们从当前input word的一侧（左边或右边）选取词的数量。如果我们设置skip_window=2，那么我们最终获得窗口中的词（包括input word在内）就是[‘The’, ‘dog’，’barked’, ‘at’]。skip_window=2代表着选取左input word左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小span=2x2=4。另一个参数叫num_skips，它代表着我们从整个窗口中选取多少个不同的词作为我们的output word，当skip_window=2，num_skips=2时，我们将会得到两组 (input word, output word) 形式的训练数据，即 (‘dog’, ‘barked’)，(‘dog’, ‘the’)。</p></li><li><p>神经网络基于这些训练数据将会输出一个概率分布，这个概率代表着我们的词典中的每个词作为input word的output word的可能性。这句话有点绕，我们来看个例子。第二步中我们在设置skip_window和num_skips=2的情况下获得了两组训练数据。假如我们先拿一组数据 (‘dog’, ‘barked’) 来训练神经网络，那么模型通过学习这个训练样本，会告诉我们词汇表中每个单词当’dog’作为input word时，其作为output word的可能性。</p></li></ol><p>也就是说模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现。例如：如果我们向神经网络模型中输入一个单词“Soviet“，那么最终模型的输出概率中，像“Union”， ”Russia“这种相关词的概率将远高于像”watermelon“，”kangaroo“非相关词的概率。因为”Union“，”Russia“在文本中更大可能在”Soviet“的窗口中出现。</p><p>我们将通过给神经网络输入文本中成对的单词来训练它完成上面所说的概率计算。下面的图中给出了一些我们训练样本的例子。我们选定句子“The quick brown fox jumps over lazy dog”，设定我们的窗口大小为2（window_size=2），也就是说我们仅选输入词前后各两个词和输入词进行组合。下图中，蓝色代表input word，方框内代表位于窗口内的单词。</p><p><img src="https://img-blog.csdnimg.cn/20200721190035764.png" srcset="/img/loading.gif" alt="1"></p><p><img src="https://img-blog.csdnimg.cn/20200714210519939.png" srcset="/img/loading.gif" alt="2"></p><p>我们的模型将会从每对单词出现的次数中习得统计结果。例如，我们的神经网络可能会得到更多类似（“Soviet“，”Union“）这样的训练样本对，而对于（”Soviet“，”Sasquatch“）这样的组合却看到的很少。因此，当我们的模型完成训练后，给定一个单词”Soviet“作为输入，输出的结果中”Union“或者”Russia“要比”Sasquatch“被赋予更高的概率。</p><p>PS：input word和output word都会被我们进行one-hot编码。仔细想一下，我们的输入被one-hot编码以后大多数维度上都是0（实际上仅有一个位置为1），所以这个向量相当稀疏，那么会造成什么结果呢。如果我们将一个1 x 10000的向量和10000 x 300的矩阵相乘，它会消耗相当大的计算资源，为了高效计算，它仅仅会选择矩阵中对应的向量中维度值为1的索引行：</p><p><img src="https://img-blog.csdnimg.cn/20200714205344406.png" srcset="/img/loading.gif" alt></p><p><strong>2. Skip-grams训练</strong></p><p>由上部分可知，Word2Vec模型是一个超级大的神经网络（权重矩阵规模非常大）。例如：我们拥有10000个单词的词汇表，我们如果想嵌入300维的词向量，那么我们的输入-隐层权重矩阵和隐层-输出层的权重矩阵都会有 10000 x 300 = 300万个权重，在如此庞大的神经网络中进行梯度下降是相当慢的。更糟糕的是，你需要大量的训练数据来调整这些权重并且避免过拟合。百万数量级的权重矩阵和亿万数量级的训练样本意味着训练这个模型将会是个灾难</p><p>解决方案：</p><ul><li><p>将常见的单词组合（word pairs）或者词组作为单个“words”来处理</p></li><li><p>对高频次单词进行抽样来减少训练样本的个数</p></li><li><p>对优化目标采用“negative sampling”方法，这样每个训练样本的训练只会更新一小部分的模型权重，从而降低计算负担</p></li></ul><p><em>2.1 Word pairs and “phases”</em></p><p>一些单词组合（或者词组）的含义和拆开以后具有完全不同的意义。比如“Boston Globe”是一种报刊的名字，而单独的“Boston”和“Globe”这样单个的单词却表达不出这样的含义。因此，在文章中只要出现“Boston Globe”，我们就应该把它作为一个单独的词来生成其词向量，而不是将其拆开。同样的例子还有“New York”，“United Stated”等。</p><p>在Google发布的模型中，它本身的训练样本中有来自Google News数据集中的1000亿的单词，但是除了单个单词以外，单词组合（或词组）又有3百万之多。</p><p><em>2.2 对高频词抽样</em></p><p>在上一部分中，对于原始文本为“The quick brown fox jumps over the laze dog”，如果使用大小为2的窗口，那么我们可以得到图中展示的那些训练样本。</p><p><img src="https://img-blog.csdnimg.cn/20200714210458879.png" srcset="/img/loading.gif" alt="1"></p><p>但是对于“the”这种常用高频单词，这样的处理方式会存在下面两个问题：</p><ol><li><p>当我们得到成对的单词训练样本时，(“fox”, “the”) 这样的训练样本并不会给我们提供关于“fox”更多的语义信息，因为“the”在每个单词的上下文中几乎都会出现</p></li><li><p>由于在文本中“the”这样的常用词出现概率很大，因此我们将会有大量的（”the“，…）这样的训练样本，而这些样本数量远远超过了我们学习“the”这个词向量所需的训练样本数</p></li></ol><p>Word2Vec通过“抽样”模式来解决这种高频词问题。它的基本思想如下：对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关。</p><p>ωi 是一个单词，Z(ωi) 是 ωi 这个单词在所有语料中出现的频次，例如：如果单词“peanut”在10亿规模大小的语料中出现了1000次，那么 Z(peanut) = 1000/1000000000 = 1e - 6。</p><p>P(ωi) 代表着保留某个单词的概率：</p><p><img src="https://img-blog.csdnimg.cn/20200714205456898.png" srcset="/img/loading.gif" alt></p><p><em>2.3 Negative sampling</em></p><p>训练一个神经网络意味着要输入训练样本并且不断调整神经元的权重，从而不断提高对目标的准确预测。每当神经网络经过一个训练样本的训练，它的权重就会进行一次调整。</p><p>所以，词典的大小决定了我们的Skip-Gram神经网络将会拥有大规模的权重矩阵，所有的这些权重需要通过数以亿计的训练样本来进行调整，这是非常消耗计算资源的，并且实际中训练起来会非常慢。</p><p>负采样（negative sampling）解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。</p><p>当我们用训练样本 ( input word: “fox”，output word: “quick”) 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的词典大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为“negative” word。</p><p>当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“）。</p><p>PS: 在论文中，作者指出指出对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。</p><p>我们使用“一元模型分布（unigram distribution）”来选择“negative words”。个单词被选作negative sample的概率跟它出现的频次有关，出现频次越高的单词越容易被选作negative words。</p><p>每个单词被选为“negative words”的概率计算公式：</p><p><img src="https://img-blog.csdnimg.cn/20200714205545327.png" srcset="/img/loading.gif" alt></p><p>其中 f(ωi)代表着单词出现的频次，而公式中开3/4的根号完全是基于经验的。</p><p>在代码负采样的代码实现中，unigram table有一个包含了一亿个元素的数组，这个数组是由词汇表中每个单词的索引号填充的，并且这个数组中有重复，也就是说有些单词会出现多次。那么每个单词的索引在这个数组中出现的次数该如何决定呢，有公式，也就是说计算出的负采样概率*1亿=单词在表中出现的次数。</p><p>有了这张表以后，每次去我们进行负采样时，只需要在0-1亿范围内生成一个随机数，然后选择表中索引号为这个随机数的那个单词作为我们的negative word即可。一个单词的负采样概率越大，那么它在这个表中出现的次数就越多，它被选中的概率就越大。</p><p><strong>3. Hierarchical Softmax</strong></p><p><em>3.1 霍夫曼树</em></p><p>输入：权值为(w1,w2,…wn)的n个节点</p><p>输出：对应的霍夫曼树</p><ol><li><p>将(w1,w2,…wn)看做是有n棵树的森林，每个树仅有一个节点</p></li><li><p>在森林中选择根节点权值最小的两棵树进行合并，得到一个新的树，这两颗树分布作为新树的左右子树。新树的根节点权重为左右子树的根节点权重之和</p></li><li><p>将之前的根节点权值最小的两棵树从森林删除，并把新树加入森林</p></li><li><p>重复步骤 2 和 3 直到森林里只有一棵树为止</p></li></ol><p>下面我们用一个具体的例子来说明霍夫曼树建立的过程，我们有(a，b，c，d，e，f)共6个节点，节点的权值分布是(16，4，8，6，20，3)。</p><p> 首先是最小的b和f合并，得到的新树根节点权重是7.此时森林里5棵树，根节点权重分别是16，8，6，20，7。此时根节点权重最小的6，7合并，得到新子树，依次类推，最终得到下面的霍夫曼树。</p><p><img src="https://img-blog.csdnimg.cn/20200714210647687.png" srcset="/img/loading.gif" alt="3"></p><p>那么霍夫曼树有什么好处呢？一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。如何编码呢？一般对于一个霍夫曼树的节点（根节点除外），可以约定左子树编码为0，右子树编码为1。如上图，则可以得到c的编码是00。</p><p>  在word2vec中，约定编码方式和上面的例子相反，即约定左子树编码为1，右子树编码为0，同时约定左子树的权重不小于右子树的权重。</p><p>更多原理可参考：<a href="https://blog.csdn.net/lzw66666/article/details/78934893" target="_blank" rel="noopener">霍夫曼树原理</a></p><p><em>3.2Hierarchical Softmax过程</em></p><p>为了避免要计算所有词的softmax概率，word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。</p><p>霍夫曼树的建立：</p><ul><li><p>根据标签（label）和频率建立霍夫曼树（label出现的频率越高，Huffman树的路径越短）</p></li><li><p>Huffman树中每一叶子结点代表一个label</p></li></ul><p><img src="https://img-blog.csdnimg.cn/20200714205623583.png" srcset="/img/loading.gif" alt="4"></p><p>如上图所示：</p><p><img src="https://img-blog.csdnimg.cn/20200714205711676.png" srcset="/img/loading.gif" alt></p><p><img src="https://img-blog.csdnimg.cn/20200714205759860.png" srcset="/img/loading.gif" alt></p><p>注意：此时的theta是一个待定系数，它是由推导最大似然之后求解得到迭代式子。</p><p><img src="https://img-blog.csdnimg.cn/20200714205841871.png" srcset="/img/loading.gif" alt></p><p><strong>使用gensim训练word2vec</strong></p><pre><code>from gensim.models.word2vec import Word2Vecmodel = Word2Vec(sentences, workers=num_workers, size=num_features)</code></pre><p><strong>参考：</strong></p><ol><li><p><a href="http://www.hankcs.com/nlp/word-vector-representations-word2vec.html" target="_blank" rel="noopener">CS224n笔记2 词的向量表示：word2vec</a></p></li><li><p><a href="http://www.52nlp.cn/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E4%BA%8C%E8%AE%B2%E8%AF%8D%E5%90%91%E9%87%8F" target="_blank" rel="noopener">斯坦福大学深度学习与自然语言处理第二讲：词向量 </a></p></li><li><p><a href="https://www.cnblogs.com/iloveai/p/cs224d-lecture3-note.html" target="_blank" rel="noopener">(Stanford CS224d) Deep Learning and NLP课程笔记（三）：GloVe与模型的评估</a></p></li><li><p><a href="http://www.cnblogs.com/pinard/p/7249903.html" target="_blank" rel="noopener">http://www.cnblogs.com/pinard/p/7249903.html</a></p></li><li><p><a href="https://blog.csdn.net/yinkun6514/article/details/79218736" target="_blank" rel="noopener">https://blog.csdn.net/yinkun6514/article/details/79218736</a></p></li><li><p><a href="https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html" target="_blank" rel="noopener">https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html</a></p></li></ol><h4 id="TextCNN"><a href="#TextCNN" class="headerlink" title="TextCNN"></a>TextCNN</h4><p>TextCNN利用CNN（卷积神经网络）进行文本特征抽取，不同大小的卷积核分别抽取n-gram特征，卷积计算出的特征图经过MaxPooling保留最大的特征值，然后将拼接成一个向量作为文本的表示。</p><p>这里我们基于TextCNN原始论文的设定，分别采用了100个大小为2,3,4的卷积核，最后得到的文本向量大小为100*3=300维。</p><p><img src="https://img-blog.csdnimg.cn/20200714205932720.jpeg" srcset="/img/loading.gif" alt></p><h4 id="TextRNN"><a href="#TextRNN" class="headerlink" title="TextRNN"></a>TextRNN</h4><p>TextRNN利用RNN（循环神经网络）进行文本特征抽取，由于文本本身是一种序列，而LSTM天然适合建模序列数据。TextRNN将句子中每个词的词向量依次输入到双向双层LSTM，分别将两个方向最后一个有效位置的隐藏层拼接成一个向量作为文本的表示。</p><p><img src="https://img-blog.csdnimg.cn/20200714210806492.png" srcset="/img/loading.gif" alt="5"></p><h3 id="基于TextCNN、TextRNN的文本表示"><a href="#基于TextCNN、TextRNN的文本表示" class="headerlink" title="基于TextCNN、TextRNN的文本表示"></a>基于TextCNN、TextRNN的文本表示</h3><h4 id="TextCNN-1"><a href="#TextCNN-1" class="headerlink" title="TextCNN"></a>TextCNN</h4><ul><li>模型搭建</li></ul><pre><code>self.filter_sizes = [2, 3, 4]  # n-gram windowself.out_channel = 100self.convs = nn.ModuleList([nn.Conv2d(1, self.out_channel, (filter_size, input_size), bias=True) for filter_size in self.filter_sizes])</code></pre><ul><li>前向传播</li></ul><pre><code>pooled_outputs = []for i in range(len(self.filter_sizes)):    filter_height = sent_len - self.filter_sizes[i] + 1    conv = self.convs[i](batch_embed)    hidden = F.relu(conv)  # sen_num x out_channel x filter_height x 1    mp = nn.MaxPool2d((filter_height, 1))  # (filter_height, filter_width)    # sen_num x out_channel x 1 x 1 -&gt; sen_num x out_channel    pooled = mp(hidden).reshape(sen_num, self.out_channel)    pooled_outputs.append(pooled)</code></pre><h4 id="TextRNN-1"><a href="#TextRNN-1" class="headerlink" title="TextRNN"></a>TextRNN</h4><ul><li>模型搭建</li></ul><pre><code>input_size = config.word_dimsself.word_lstm = LSTM(    input_size=input_size,    hidden_size=config.word_hidden_size,    num_layers=config.word_num_layers,    batch_first=True,    bidirectional=True,    dropout_in=config.dropout_input,    dropout_out=config.dropout_hidden,)</code></pre><ul><li>前向传播</li></ul><pre><code>hiddens, _ = self.word_lstm(batch_embed, batch_masks)  # sent_len x sen_num x hidden*2hiddens.transpose_(1, 0)  # sen_num x sent_len x hidden*2if self.training:    hiddens = drop_sequence_sharedmask(hiddens, self.dropout_mlp)</code></pre><h3 id="使用HAN用于文本分类"><a href="#使用HAN用于文本分类" class="headerlink" title="使用HAN用于文本分类"></a>使用HAN用于文本分类</h3><p><a href="https://link.zhihu.com/?target=http%3A//www.aclweb.org/anthology/N16-1174" target="_blank" rel="noopener">Hierarchical Attention Network for Document Classification</a>(HAN)基于层级注意力，在单词和句子级别分别编码并基于注意力获得文档的表示，然后经过Softmax进行分类。其中word encoder的作用是获得句子的表示，可以替换为上节提到的TextCNN和TextRNN，也可以替换为下节中的BERT。</p><p><img src="https://img-blog.csdnimg.cn/20200714210015326.png" srcset="/img/loading.gif" alt="Sequence Intent Classification Using Hierarchical Attention..."></p><h3 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h3><p>本章介绍了Word2Vec的使用，以及TextCNN、TextRNN的原理和训练，最后介绍了用于长文档分类的HAN。</p><h3 id="本章作业"><a href="#本章作业" class="headerlink" title="本章作业"></a>本章作业</h3><ul><li>尝试通过Word2Vec训练词向量</li><li>尝试使用TextCNN、TextRNN完成文本表示</li><li>尝试使用HAN进行文本分类</li></ul><p><strong>参考：</strong></p><ol><li><a href="https://mp.weixin.qq.com/s/I-yeHQopTFdNk67Ir_iWiA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/I-yeHQopTFdNk67Ir_iWiA</a></li><li><a href="https://github.com/hecongqing/2018-daguan-competition" target="_blank" rel="noopener">https://github.com/hecongqing/2018-daguan-competition</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;谦虚使人进步，骄傲使人落后。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="Classification" scheme="https://superlova.github.io/tags/Classification/"/>
    
      <category term="Deep Learning" scheme="https://superlova.github.io/tags/Deep-Learning/"/>
    
      <category term="word2vec" scheme="https://superlova.github.io/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】新闻文本分类之深度学习FastText</title>
    <link href="https://superlova.github.io/2020/07/27/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0FastText/"/>
    <id>https://superlova.github.io/2020/07/27/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0FastText/</id>
    <published>2020-07-27T13:56:03.000Z</published>
    <updated>2020-07-27T15:11:39.605Z</updated>
    
    <content type="html"><![CDATA[<p>当你写东西或讲话的时候，始终要想到使每个普通工人都懂得，都相信你的号召，都决心跟着你走。要想到你究竟为什么人写东西，向什么人讲话。——《反对党八股》<br><a id="more"></a></p><p>在上一章节，我们使用传统机器学习算法来解决了文本分类问题，从本章开始我们将尝试使用深度学习方法。与传统机器学习不同，深度学习既提供特征提取功能，也可以完成分类的功能。</p><p>本次学习我们主要介绍FastText。</p><p>fastText是一个快速文本分类算法，与基于神经网络的分类算法相比有两大优点：<br>1、fastText在保持高精度的情况下加快了训练速度和测试速度<br>2、fastText不需要预训练好的词向量，fastText会自己训练词向量<br>3、fastText两个重要的优化：层级 Softmax、N-gram</p><pre><code class="lang-python">import fasttextmodel = fasttext.train_supervised(&#39;train.csv&#39;, lr=1.0, wordNgrams=2, verbose=2, minCount=1, epoch=25, loss=&quot;hs&quot;)val_pred = [model.predict(x)[0][0].split(&#39;__&#39;)[-1] for x in df_train.iloc[-5000:][&#39;text&#39;]]print(f1_score(df_train[&#39;label&#39;].values[-5000:].astype(str), val_pred, average=&#39;macro&#39;))0.8256254253081777</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当你写东西或讲话的时候，始终要想到使每个普通工人都懂得，都相信你的号召，都决心跟着你走。要想到你究竟为什么人写东西，向什么人讲话。——《反对党八股》&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="Classification" scheme="https://superlova.github.io/tags/Classification/"/>
    
      <category term="Deep Learning" scheme="https://superlova.github.io/tags/Deep-Learning/"/>
    
      <category term="FastText" scheme="https://superlova.github.io/tags/FastText/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】新闻文本分类之机器学习文本分类</title>
    <link href="https://superlova.github.io/2020/07/23/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    <id>https://superlova.github.io/2020/07/23/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</id>
    <published>2020-07-22T17:43:15.000Z</published>
    <updated>2020-07-27T14:18:20.439Z</updated>
    
    <content type="html"><![CDATA[<p>今我睹子之难穷也，吾非至于子之门则殆矣。<br><a id="more"></a></p><h1 id="文本表示方法实践"><a href="#文本表示方法实践" class="headerlink" title="文本表示方法实践"></a>文本表示方法实践</h1><p>自然语言总需要转换成数值等表示才能被线性模型等处理。下面利用task1&amp;2提到的编码方式进行实践。</p><h2 id="Label"><a href="#Label" class="headerlink" title="Label"></a>Label</h2><p>这种编码方式是把自然语言首先分词成基本语素单元，然后把不同的单元赋予唯一的整数编码。比如本次比赛提供的数据集就是该编码方式。每条数据都是整数序列，最大整数为7549，最小整数为0。</p><p>这样做的好处是相对节约内存，且实现简单；坏处是破坏了自然语言中部分语义，无法进一步进行诸如删除停用词、词根提取等操作。另外，编码的大小可能会让算法误以为词和词之间有大小关系。</p><p>因为原来的数据集就是此编码方法，故不再赘述。</p><h2 id="Bag-of-Words-CountVectorizer"><a href="#Bag-of-Words-CountVectorizer" class="headerlink" title="Bag of Words: CountVectorizer"></a>Bag of Words: CountVectorizer</h2><p>用于机器学习的文本表示有一种最简单的方法，也是最有效且最常用的方法，就是使用词袋（bag-of-words）表示。使用这种表示方式时，我们舍弃了输入文本中的大部分结构，如章节、段落、句子和格式，<strong>只计算语料库中每个单词在每个文本中的出现频次</strong>。舍弃结构并仅计算单词出现次数，这会让脑海中出现将文本表示为“袋”的画面。</p><pre><code class="lang-python">from sklearn.feature_extraction.text import CountVectorizerbagvect = CountVectorizer(max_df=.15)bagvect.fit(corpus)feature_names = bagvect.get_feature_names()print(&quot;Number of features: {}&quot;.format(len(feature_names)))print(&quot;First 20 features:\n{}&quot;.format(feature_names[:20]))Number of features: 4740First 20 features:[&#39;10&#39;, &#39;100&#39;, &#39;1000&#39;, &#39;1001&#39;, &#39;1002&#39;, &#39;1004&#39;, &#39;1005&#39;, &#39;1006&#39;, &#39;1007&#39;, &#39;1008&#39;, &#39;1009&#39;, &#39;101&#39;, &#39;1010&#39;, &#39;1012&#39;, &#39;1013&#39;, &#39;1014&#39;, &#39;102&#39;, &#39;1020&#39;, &#39;1022&#39;, &#39;1023&#39;]bag_of_words = bagvect.transform(corpus)print(&quot;bag_of_words: {}&quot;.format(repr(bag_of_words)))print(bag_of_words[0].toarray())bag_of_words: &lt;10000x4740 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39;    with 813365 stored elements in Compressed Sparse Row format&gt;[[0 0 0 ... 0 0 0]]</code></pre><p>词袋表示保存在一个 SciPy 稀疏矩阵中，这种数据格式只保存非零元素</p><p>矩阵的形状为 10000x4740，每行对应于两个数据点之一，每个特征对应于词表中的一个单词。当然，我们在这里选了10000个样本，如果把所有数据集都给转化成词袋向量，那么矩阵形状将会是 200000×6859。</p><h2 id="Hash编码实践"><a href="#Hash编码实践" class="headerlink" title="Hash编码实践"></a>Hash编码实践</h2><p><a href="https://chenk.tech/posts/eb79fc5f.html" target="_blank" rel="noopener">https://chenk.tech/posts/eb79fc5f.html</a></p><pre><code class="lang-python">from sklearn.feature_extraction.text import HashingVectorizerhvec = HashingVectorizer(n_features=10000)hvec.fit(corpus)h_words = hvec.transform(corpus)print(&quot;h_words: {}&quot;.format(repr(h_words)))h_words: &lt;10000x10000 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;    with 2739957 stored elements in Compressed Sparse Row format&gt;</code></pre><p>我选取了10000个样本，将其映射到10000个特征的Hash向量中。</p><pre><code class="lang-python">print(h_words[0]) (0, 0)    -0.009886463280261834  (0, 6)    -0.04943231640130917  (0, 9)    0.03954585312104734  (0, 26)    -0.04943231640130917  (0, 42)    0.01977292656052367  (0, 70)    0.01977292656052367  (0, 74)    -0.12852402264340385  (0, 94)    0.009886463280261834  (0, 99)    0.01977292656052367  (0, 109)    -0.009886463280261834  :    :  (0, 9642)    -0.009886463280261834  (0, 9650)    0.01977292656052367  (0, 9653)    0.009886463280261834  (0, 9659)    -0.04943231640130917  (0, 9715)    -0.009886463280261834  (0, 9721)    0.009886463280261834  (0, 9729)    -0.009886463280261834  (0, 9741)    0.009886463280261834  (0, 9765)    -0.01977292656052367  (0, 9781)    0.01977292656052367  (0, 9821)    -0.009886463280261834  (0, 9862)    0.009886463280261834  (0, 9887)    -0.009886463280261834  (0, 9932)    0.009886463280261834</code></pre><p>输出的含义，前面的元组代表了该特征在词袋中的位置，后面的数值代表了对应的Hash值。</p><h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><p>TF-IDF（Term Frequency-inverse Document Frequency）是一种针对关键词的统计分析方法，用于评估一个词对一个文件集或者一个语料库的重要程度。</p><pre><code class="lang-python">from sklearn.feature_extraction.text import TfidfVectorizertvec = TfidfVectorizer()tvec.fit(corpus)feature_names = tvec.get_feature_names()print(&quot;Number of features: {}&quot;.format(len(feature_names)))print(&quot;First 20 features:\n{}&quot;.format(feature_names[:20]))Number of features: 5333First 20 features:[&#39;10&#39;, &#39;100&#39;, &#39;1000&#39;, &#39;1001&#39;, &#39;1002&#39;, &#39;1004&#39;, &#39;1005&#39;, &#39;1006&#39;, &#39;1007&#39;, &#39;1008&#39;, &#39;1009&#39;, &#39;101&#39;, &#39;1010&#39;, &#39;1012&#39;, &#39;1013&#39;, &#39;1014&#39;, &#39;1018&#39;, &#39;102&#39;, &#39;1020&#39;, &#39;1022&#39;]t_words = tvec.transform(corpus)print(&quot;t_words: {}&quot;.format(repr(t_words)))print(t_words[0].toarray())t_words: &lt;10000x5333 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;    with 2797304 stored elements in Compressed Sparse Row format&gt;[[0. 0. 0. ... 0. 0. 0.]]# 找到数据集中每个特征的最大值max_value = t_words.max(axis=0).toarray().ravel()sorted_by_tfidf = max_value.argsort()# 获取特征名称feature_names = np.array(tvec.get_feature_names())print(&quot;Features with lowest tfidf:\n{}&quot;.format(feature_names[sorted_by_tfidf[:20]]))print(&quot;Features with highest tfidf: \n{}&quot;.format(feature_names[sorted_by_tfidf[-20:]]))sorted_by_idf = np.argsort(tvec.idf_)print(&quot;Features with lowest idf:\n{}&quot;.format(feature_names[sorted_by_idf[:100]]))Features with lowest tfidf:[&#39;6844&#39; &#39;6806&#39; &#39;7201&#39; &#39;5609&#39; &#39;5585&#39; &#39;5485&#39; &#39;3453&#39; &#39;7390&#39; &#39;2322&#39; &#39;2083&#39; &#39;1222&#39; &#39;2360&#39; &#39;319&#39; &#39;2520&#39; &#39;6268&#39; &#39;3105&#39; &#39;6049&#39; &#39;4888&#39; &#39;2390&#39; &#39;2849&#39;]Features with highest tfidf: [&#39;3198&#39; &#39;2346&#39; &#39;5480&#39; &#39;4375&#39; &#39;6296&#39; &#39;1710&#39; &#39;682&#39; &#39;354&#39; &#39;4381&#39; &#39;482&#39; &#39;5990&#39; &#39;2798&#39; &#39;5907&#39; &#39;3992&#39; &#39;418&#39; &#39;513&#39; &#39;4759&#39; &#39;6250&#39; &#39;6220&#39; &#39;1633&#39;]Features with lowest idf:[&#39;3750&#39; &#39;900&#39; &#39;648&#39; &#39;6122&#39; &#39;7399&#39; &#39;2465&#39; &#39;4811&#39; &#39;4464&#39; &#39;1699&#39; &#39;299&#39; &#39;2400&#39; &#39;3659&#39; &#39;3370&#39; &#39;2109&#39; &#39;4939&#39; &#39;669&#39; &#39;5598&#39; &#39;5445&#39; &#39;4853&#39; &#39;5948&#39; &#39;2376&#39; &#39;7495&#39; &#39;4893&#39; &#39;5410&#39; &#39;340&#39; &#39;619&#39; &#39;4659&#39; &#39;1460&#39; &#39;6065&#39; &#39;1903&#39; &#39;5560&#39; &#39;6017&#39; &#39;2252&#39; &#39;4516&#39; &#39;1519&#39; &#39;2073&#39; &#39;5998&#39; &#39;5491&#39; &#39;2662&#39; &#39;5977&#39; &#39;6093&#39; &#39;1324&#39; &#39;5780&#39; &#39;3915&#39; &#39;3800&#39; &#39;5393&#39; &#39;2210&#39; &#39;5915&#39; &#39;3223&#39; &#39;4490&#39; &#39;2490&#39; &#39;1375&#39; &#39;803&#39; &#39;1635&#39; &#39;7539&#39; &#39;4411&#39; &#39;4128&#39; &#39;7543&#39; &#39;5602&#39; &#39;1866&#39; &#39;5176&#39; &#39;2799&#39; &#39;4646&#39; &#39;3700&#39; &#39;5858&#39; &#39;307&#39; &#39;913&#39; &#39;25&#39; &#39;6045&#39; &#39;1702&#39; &#39;4822&#39; &#39;3099&#39; &#39;5330&#39; &#39;1920&#39; &#39;1567&#39; &#39;2614&#39; &#39;4190&#39; &#39;1080&#39; &#39;5510&#39; &#39;4149&#39; &#39;3166&#39; &#39;3530&#39; &#39;192&#39; &#39;5659&#39; &#39;3618&#39; &#39;4525&#39; &#39;3686&#39; &#39;6038&#39; &#39;1767&#39; &#39;5589&#39; &#39;5736&#39; &#39;6831&#39; &#39;7377&#39; &#39;4969&#39; &#39;1394&#39; &#39;6104&#39; &#39;7010&#39; &#39;6407&#39; &#39;5430&#39; &#39;23&#39;]</code></pre><h2 id="多个单词的词袋：N-gram《》"><a href="#多个单词的词袋：N-gram《》" class="headerlink" title="多个单词的词袋：N-gram《》"></a>多个单词的词袋：N-gram《》</h2><p>使用词袋表示的主要缺点之一是完全舍弃了单词顺序。因此，“it’s bad, not good at all”（电影很差，一点也不好）和“it’s good, not bad at all”（电影很好，还不错）这两个字符串的词袋表示完全相同，尽管它们的含义相反。将“not”（不）放在单词前面，这只是上下文很重要的一个例子（可能是一个极端的例子）。幸运的是，使用词袋表示时有一种获取上下文的方法，就是不仅考虑单一词例的计数，而且还考虑相邻的两个或三个词例的计数。两个词例被称为二元分词（bigram），三个词例被称为三元分词（trigram），更一般的词例序列被称为 n 元分词（n-gram）。我们可以通过改变 CountVectorizer 或 TfidfVectorizer 的 ngram_range 参数来改变作为特征的词例范围。ngram_range 参数是一个元组，包含要考虑的词例序列的最小长度和最大长度。</p><p>在大多数情况下，添加二元分词会有所帮助。添加更长的序列（一直到五元分词）也可能有所帮助，但这会导致特征数量的大大增加，也可能会导致过拟合，因为其中包含许多非常具体的特征。原则上来说，二元分词的数量是一元分词数量的平方，三元分词的数量是一元分词数量的三次方，从而导致非常大的特征空间。在实践中，更高的 n 元分词在数据中的出现次数实际上更少，原因在于（英语）语言的结构，不过这个数字仍然很大。</p><pre><code class="lang-python">from sklearn.feature_extraction.text import TfidfVectorizertvec = TfidfVectorizer(ngram_range=(1,3), min_df=5)tvec.fit(corpus)# 找到数据集中每个特征的最大值max_value = t_words.max(axis=0).toarray().ravel()sorted_by_tfidf = max_value.argsort()# 获取特征名称feature_names = np.array(tvec.get_feature_names())print(&quot;Features with lowest tfidf:\n{}&quot;.format(feature_names[sorted_by_tfidf[:20]]))print(&quot;Features with highest tfidf: \n{}&quot;.format(feature_names[sorted_by_tfidf[-20:]]))sorted_by_idf = np.argsort(tvec.idf_)print(&quot;Features with lowest idf:\n{}&quot;.format(feature_names[sorted_by_idf[:100]]))Features with lowest tfidf:[&#39;1008 5612&#39; &#39;100 5560&#39; &#39;1018 4089 5491&#39; &#39;101 648 900&#39; &#39;1006 3750 826&#39; &#39;1018 1066 3231&#39; &#39;101 5560 3568&#39; &#39;100 5589&#39; &#39;1018 2119 281&#39; &#39;101 5560 3659&#39; &#39;1018 1066 3166&#39; &#39;100 5598 1465&#39; &#39;1000 5011&#39; &#39;101 2662 4939&#39; &#39;100 5602&#39; &#39;101 873 648&#39; &#39;1006 2265 648&#39; &#39;1008 5640&#39; &#39;1008 5689&#39; &#39;101 856 531&#39;]Features with highest tfidf: [&#39;101 2087&#39; &#39;1018 1066 281&#39; &#39;101 648 3440&#39; &#39;1006 5640 3641&#39; &#39;101 760 4233&#39; &#39;1006 6017&#39; &#39;1018 1066 6983&#39; &#39;1014 3750 3659&#39; &#39;100 5430 2147&#39; &#39;100 5510 2471&#39; &#39;1018 1141&#39; &#39;1006 1866 5977&#39; &#39;1018 2119 3560&#39; &#39;1018 2662 3068&#39; &#39;101 1844 4486&#39; &#39;101 2304 3659&#39; &#39;1006 3750 5330&#39; &#39;101 5589&#39; &#39;1008 900 3618&#39; &#39;100 6122 2489&#39;]Features with lowest idf:[&#39;3750&#39; &#39;900&#39; &#39;648&#39; &#39;2465&#39; &#39;6122&#39; &#39;7399&#39; &#39;4811&#39; &#39;4464&#39; &#39;1699&#39; &#39;3659&#39; &#39;2400&#39; &#39;299&#39; &#39;3370&#39; &#39;2109&#39; &#39;4939&#39; &#39;5598&#39; &#39;669&#39; &#39;5445&#39; &#39;4853&#39; &#39;2376&#39; &#39;5948&#39; &#39;7495&#39; &#39;4893&#39; &#39;5410&#39; &#39;340&#39; &#39;619&#39; &#39;4659&#39; &#39;1460&#39; &#39;6065&#39; &#39;4516&#39; &#39;1903&#39; &#39;5560&#39; &#39;6017&#39; &#39;2252&#39; &#39;2073&#39; &#39;1519&#39; &#39;5491&#39; &#39;5998&#39; &#39;2662&#39; &#39;5977&#39; &#39;1324&#39; &#39;5780&#39; &#39;6093&#39; &#39;3915&#39; &#39;5393&#39; &#39;2210&#39; &#39;3800&#39; &#39;3223&#39; &#39;5915&#39; &#39;4490&#39; &#39;2490&#39; &#39;803&#39; &#39;1635&#39; &#39;4128&#39; &#39;1375&#39; &#39;7539&#39; &#39;4411&#39; &#39;7543&#39; &#39;5602&#39; &#39;2799&#39; &#39;1866&#39; &#39;5176&#39; &#39;5858&#39; &#39;4646&#39; &#39;3700&#39; &#39;307&#39; &#39;6045&#39; &#39;1702&#39; &#39;25&#39; &#39;913&#39; &#39;5330&#39; &#39;4822&#39; &#39;2614&#39; &#39;3099&#39; &#39;1920&#39; &#39;1567&#39; &#39;4190&#39; &#39;4149&#39; &#39;5510&#39; &#39;1080&#39; &#39;3166&#39; &#39;3659 3370&#39; &#39;3530&#39; &#39;192&#39; &#39;3618&#39; &#39;4525&#39; &#39;5659&#39; &#39;3686&#39; &#39;6038&#39; &#39;1767&#39; &#39;5736&#39; &#39;7377&#39; &#39;5589&#39; &#39;6831&#39; &#39;3370 3370&#39; &#39;1394&#39; &#39;4969&#39; &#39;5430&#39; &#39;7010&#39; &#39;6104&#39;]</code></pre><h1 id="无监督探索"><a href="#无监督探索" class="headerlink" title="无监督探索"></a>无监督探索</h1><h2 id="PCA可视化"><a href="#PCA可视化" class="headerlink" title="PCA可视化"></a>PCA可视化</h2><pre><code class="lang-python">from sklearn.decomposition import PCApca = PCA(n_components=2)pca_vec = pca.fit_transform(t_words.toarray())pca_vec.shape, pca.explained_variance_ratio_((10000, 2), array([0.03817303, 0.02684457]))</code></pre><p>我们成功将tfidf转化后的词向量压缩成2维向量，这样就能够在二维平面可视化了。</p><p>后面的<code>explained_variance_ratio_</code>代表着经过PCA算法压缩后，保留的信息量。这个数值还是偏低，因此这种PCA压缩方法仅适用于实验。</p><pre><code class="lang-python">plt.figure(figsize=(12,10))plt.scatter(pca_vec[:,0], pca_vec[:,1], c=labels)</code></pre><p><img src="/2020/07/23/【竞赛打卡】新闻文本分类之机器学习文本分类/1.png" srcset="/img/loading.gif" alt></p><h2 id="主题建模"><a href="#主题建模" class="headerlink" title="主题建模"></a>主题建模</h2><p>常用于文本数据的一种特殊技术是主题建模（topic modeling），这是描述将每个文档分配给一个或多个主题的任务（通常是无监督的）的概括性术语。这方面一个很好的例子是新闻数据，它们可以被分为“政治”“体育”“金融”等主题。如果为每个文档分配一个主题，那么这是一个文档聚类任务。我们学到的每个成分对应于一个主题，文档表示中的成分系数告诉我们这个文档与该主题的相关性强弱。通常来说，人们在谈论主题建模时，他们指的是一种叫作隐含狄利克雷分布（Latent Dirichlet Allocation，LDA）的特定分解方法。</p><p>我们将 LDA 应用于新闻数据集，来看一下它在实践中的效果。对于无监督的文本文档模型，通常最好删除非常常见的单词，否则它们可能会支配分析过程。我们将删除至少在15% 的文档中出现过的单词，并在删除前 15% 之后，将词袋模型限定为最常见的 10 000 个单词：</p><pre><code class="lang-python">vect = CountVectorizer(max_features=10000, max_df=.15)X = vect.fit_transform(corpus)from sklearn.decomposition import LatentDirichletAllocationlda = LatentDirichletAllocation(n_components=14, learning_method=&quot;batch&quot;, max_iter=25, random_state=0)# 我们在一个步骤中构建模型并变换数据# 计算变换需要花点时间，二者同时进行可以节省时间document_topics = lda.fit_transform(bag_of_words)# 对于每个主题（components_的一行），将特征排序（升序）# 用[:, ::-1]将行反转，使排序变为降序sorting = np.argsort(lda.components_, axis=1)[:, ::-1]# 从向量器中获取特征名称feature_names = np.array(bagvect.get_feature_names())plt.figure()plt.bar(x=range(14), height=document_topics[0])plt.xticks(list(range(14)))</code></pre><p><img src="/2020/07/23/【竞赛打卡】新闻文本分类之机器学习文本分类/2.png" srcset="/img/loading.gif" alt></p><p>由LDA确定的主题词如下：</p><pre><code class="lang-python">topic 0       topic 1       topic 2       topic 3       topic 4       --------      --------      --------      --------      --------      6654          1970          7349          3464          4412          4173          2716          7354          7436          7363          1219          4553          1684          5562          6689          6861          7042          5744          3289          4986          5006          5822          6569          5105          2506          7400          5099          1999          5810          3056          3508          3654          1351          3134          6220          6223          3021          56            3648          5117          6227          4967          4036          6308          6319          7257          3396          4223          1706          2695          topic 5       topic 6       topic 7       topic 8       topic 9       --------      --------      --------      --------      --------      4967          1334          1934          4114          7328          7528          5166          1146          3198          5547          3644          6143          532           517           4768          1899          2695          4802          812           3231          6678          1616          419           3090          5492          5744          7532          4089          4163          4080          6047          368           3725          5305          4120          1252          2918          2851          177           2331          5814          2968          6227          7251          3019          1170          3032          6639          2835          6613          topic 10      topic 11      topic 12      topic 13      --------      --------      --------      --------      3523          4902          5178          5122          3342          1258          6014          6920          6722          343           5920          5519          6352          4089          4603          7154          3186          5226          3648          4381          5179          810           4042          4760          4369          6284          1724          4412          3501          3477          4450          4595          2334          7127          657           3377          2722          7077          5803          7006</code></pre><h2 id="t-SNE可视化"><a href="#t-SNE可视化" class="headerlink" title="t-SNE可视化"></a>t-SNE可视化</h2><p>t-SNE是当前最流行的数据可视化方法。将TF-IDF转化后的向量可视化如下：</p><pre><code class="lang-python">from sklearn.manifold import TSNEwords_emb = TSNE(n_components=2).fit_transform(t_words)plt.figure(figsize=(12,10))plt.scatter(words_emb[:,0], words_emb[:,1], c=labels)</code></pre><p><img src="/2020/07/23/【竞赛打卡】新闻文本分类之机器学习文本分类/3.png" srcset="/img/loading.gif" alt></p><p>将HashingVectorizer转化后的向量经过t-SNE算法可视化结果分享如下：</p><pre><code class="lang-python">hash_words_emb = TSNE(n_components=2).fit_transform(h_words)plt.figure(figsize=(12,10))plt.scatter(hash_words_emb[:,0], hash_words_emb[:,1], c=labels)</code></pre><p><img src="/2020/07/23/【竞赛打卡】新闻文本分类之机器学习文本分类/4.png" srcset="/img/loading.gif" alt></p><h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><p>首先导入相关库</p><pre><code class="lang-python">from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizerfrom sklearn.pipeline import make_pipelinefrom sklearn.linear_model import LogisticRegression, RidgeClassifierfrom sklearn.model_selection import GridSearchCV, train_test_split, cross_val_scorefrom sklearn.multiclass import OneVsRestClassifierfrom sklearn.svm import SVCfrom sklearn.metrics import f1_score</code></pre><p><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model" target="_blank" rel="noopener">https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model</a></p><h2 id="RidgeClassifier-CountVectorizer"><a href="#RidgeClassifier-CountVectorizer" class="headerlink" title="RidgeClassifier+CountVectorizer"></a>RidgeClassifier+CountVectorizer</h2><p>首先我们使用教程中的范例</p><pre><code class="lang-python">X, y = df_train[&#39;text&#39;][:10000], df_train[&#39;label&#39;][:10000]vectorizer = CountVectorizer(max_features=3000)X = vectorizer.fit_transform(X)clf = RidgeClassifier()clf.fit(X[:5000], y[:5000]) # 使用前5000个样本进行训练val_pred = clf.predict(X[5000:]) # 使用后5000个样本进行预测print(f1_score(y[5000:], val_pred, average=&#39;macro&#39;))0.6322204326986258</code></pre><p>我们把10000个样本中，前5000个用于训练，后5000个用于测试，最终结果以F1指标展示，结果为0.63，不太令人满意。</p><h2 id="LogisticRegression-TFIDF"><a href="#LogisticRegression-TFIDF" class="headerlink" title="LogisticRegression+TFIDF"></a>LogisticRegression+TFIDF</h2><p>最常见的线性分类算法是 Logistic 回归。虽然 LogisticRegression 的名字中含有回归（regression），但它是一种分类算法，并不是回归算法，不应与 LinearRegression 混淆。<br>我们可以将 LogisticRegression 和 LinearSVC 模型应用到经过tfidf处理的新闻文本数据集上。</p><p>我们使用了sklearn中的划分数据集的方法<code>train_test_split</code>，将数据集划分成训练集和测试集两部分。但是这样一来，数据集中的测试集部分将不能被训练，未免有点可惜。</p><p>我们在训练时，采用了pipeline方式，Pipeline 类可以将多个处理步骤合并（glue）为单个 scikit-learn 估计器。Pipeline 类本身具有 fit、predict 和 score 方法，其行为与 scikit-learn 中的其 他模型相同。Pipeline 类最常见的用例是将预处理步骤（比如数据缩放）与一个监督模型 （比如分类器）链接在一起。</p><pre><code class="lang-python">%%timeX_train, X_test, y_train, y_test = train_test_split(df_train[&#39;text&#39;][:10000], df_train[&#39;label&#39;][:10000], random_state=0)pipe_logis = make_pipeline(TfidfVectorizer(min_df=5, ngram_range=(1,3)), LogisticRegression())param_grid = {&#39;logisticregression__C&#39;: [0.001, 0.01, 0.1, 1, 10]}grid = GridSearchCV(pipe_logis, param_grid, cv=5)grid.fit(X_train, y_train)print(&quot;Best params:\n{}\n&quot;.format(grid.best_params_))print(&quot;Best cross-validation score: {:.2f}&quot;.format(grid.best_score_))print(&quot;Test-set score: {:.2f}&quot;.format(grid.score(X_test, y_test)))Best params:{&#39;logisticregression__C&#39;: 10}Best cross-validation score: 0.91Test-set score: 0.92</code></pre><h2 id="SVC-tfidf"><a href="#SVC-tfidf" class="headerlink" title="SVC+tfidf"></a>SVC+tfidf</h2><p>这次我们使用非线性模型中大名鼎鼎的SVM模型，并采用交叉验证的方法划分数据集，不浪费任何一部分数据。</p><pre><code class="lang-python">X_train, y_train = df_train[&#39;text&#39;][:10000], df_train[&#39;label&#39;][:10000]pipe_svc = make_pipeline(TfidfVectorizer(min_df=5), SVC()) # decision_function_shape=&#39;ovr&#39;scores = cross_val_score(pipe_svc, X_train, y_train, cv=5, n_jobs=-1)print(scores.mean())0.8924</code></pre><h1 id="模型评价"><a href="#模型评价" class="headerlink" title="模型评价"></a>模型评价</h1><p>TR，FN，precision，recall等的进一步解释，请参考以下链接：<br><a href="https://www.zhihu.com/question/30643044" target="_blank" rel="noopener">https://www.zhihu.com/question/30643044</a></p><h1 id="进一步优化"><a href="#进一步优化" class="headerlink" title="进一步优化"></a>进一步优化</h1><p>使用机器学习模型+巧妙的特征工程，我们可以达到90%以上的精度，这在14分类问题中已经很惊人了。然而我们的工作并没有结束，还有许许多多的问题等着我们去探索。比如</p><ul><li>删除停用词、罕见词、其他常见词和不能反映特征的词</li><li>类别不平衡问题</li></ul><p>周志华《机器学习》中介绍到，分类学习方法都有一个共同的基本假设，即不同类别的训练样例数目相当。如果不同类别的训练样例数目稍有差别，对学习结果的影响通常也不大，但若样本类别数目差别很大，属于极端不均衡，则会对学习过程（模型训练）造成困扰。这些学习算法的设计背后隐含的优化目标是数据集上的分类准确度，而这会导致学习算法在不平衡数据上更偏向于含更多样本的多数类。多数不平衡学习（imbalance learning）算法就是为了解决这种“对多数类的偏好”而提出的。如果正负类样本类别不平衡比例超过4:1，那么其分类器会大大地因为数据不平衡性而无法满足分类要求</p><p>关于如何解决类别不平衡的问题，可以参考以下链接：<br><a href="https://zhuanlan.zhihu.com/p/84322912" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/84322912</a><br><a href="https://zhuanlan.zhihu.com/p/36381828" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/36381828</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今我睹子之难穷也，吾非至于子之门则殆矣。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="Classification" scheme="https://superlova.github.io/tags/Classification/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析</title>
    <link href="https://superlova.github.io/2020/07/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E5%88%86%E6%9E%90/"/>
    <id>https://superlova.github.io/2020/07/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E5%88%86%E6%9E%90/</id>
    <published>2020-07-22T04:03:05.000Z</published>
    <updated>2020-07-25T05:22:02.670Z</updated>
    
    <content type="html"><![CDATA[<p>不要对我有任何期待哦！<br><a id="more"></a></p><h1 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h1><p>Colab</p><pre><code class="lang-python">import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitimport seaborn as snsimport scipyfrom collections import Counter</code></pre><h1 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h1><pre><code class="lang-python">df_train = pd.read_csv(train_path, sep=&#39;\t&#39;)df_test = pd.read_csv(test_path, sep=&#39;\t&#39;)</code></pre><h1 id="数据探索"><a href="#数据探索" class="headerlink" title="数据探索"></a>数据探索</h1><h2 id="简单查看数据"><a href="#简单查看数据" class="headerlink" title="简单查看数据"></a>简单查看数据</h2><pre><code class="lang-python">df_train.head(), len(df_train)(   label                                               text 0      2  2967 6758 339 2021 1854 3731 4109 3792 4149 15... 1     11  4464 486 6352 5619 2465 4802 1452 3137 5778 54... 2      3  7346 4068 5074 3747 5681 6093 1777 2226 7354 6... 3      2  7159 948 4866 2109 5520 2490 211 3956 5520 549... 4      3  3646 3055 3055 2490 4659 6065 3370 5814 2465 5..., 200000)</code></pre><p>发现text域的数据是字符串。我们想要得到整数序列。可以用字符串分割<code>split()</code>。</p><pre><code class="lang-python">print(len(df_train[&#39;text&#39;][0]), type(df_train[&#39;text&#39;][0]))df_train.head()</code></pre><p><img src="/2020/07/22/【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析/1.png" srcset="/img/loading.gif" alt></p><h2 id="长度分布"><a href="#长度分布" class="headerlink" title="长度分布"></a>长度分布</h2><h3 id="describe"><a href="#describe" class="headerlink" title="describe"></a>describe</h3><p>这里与教程中的方法有所不同。vectorize是numpy中很方便的函数，作用和pandas中<code>apply</code>差不多。用法：</p><p><code>np.vectorize(function)(array)</code></p><p>输入待处理的array，以及逐元素处理函数function，返回经过处理后的ndarray。原来的array则不受影响。</p><p>当前我使用的函数<code>split_df</code>负责将一行数据按空格切分成整数列表，然后计算该列表的长度。</p><pre><code class="lang-python">def split_df(df_row):    return len(str(df_row).split())len_dist = np.vectorize(split_df)(df_train[&#39;text&#39;])len_test_dist = np.vectorize(split_df)(df_test[&#39;text&#39;])</code></pre><p>使用describe函数查看训练集和测试集中的数据长度分布</p><pre><code class="lang-python">print(pd.Series(len_dist).describe())print(pd.Series(len_test_dist).describe())count    200000.000000mean        907.207110std         996.029036min           2.00000025%         374.00000050%         676.00000075%        1131.000000max       57921.000000dtype: float64count    50000.000000mean       909.844960std       1032.313375min         14.00000025%        370.00000050%        676.00000075%       1133.000000max      41861.000000dtype: float64</code></pre><p>通过数据描述可以看到</p><p>训练集共200,000条新闻，每条新闻平均907个字符，最短的句子长度为2，最长的句子长度为57921，其中75%以下的数据长度在1131以下。</p><p>测试集共50,000条新闻，每条新闻平均909个字符，最短句子长度为14，最长句子41861,75%以下的数据长度在1133以下。</p><p>训练集和测试集就长度来说似乎是同一分布。</p><h3 id="直方图"><a href="#直方图" class="headerlink" title="直方图"></a>直方图</h3><p>绘制直方图查看训练集和测试集中的数据长度分布</p><pre><code class="lang-python">fig, ax = plt.subplots(1,1,figsize=(12,6))ax = plt.hist(x=len_dist, bins=100)ax = plt.hist(x=len_test_dist, bins=100)plt.xlim([0, max(max(len_dist), max(len_test_dist))])plt.xlabel(&quot;length of sample&quot;)plt.ylabel(&quot;number of sample&quot;)plt.legend([&#39;train_len&#39;,&#39;test_len&#39;])plt.show()</code></pre><p><img src="/2020/07/22/【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析/2.png" srcset="/img/loading.gif" alt></p><p>使用seaborn绘制更好的图</p><p>seaborn计算的纵坐标是频率，而不是出现次数。由于训练集和测试集的数据量不一样，因此用频率更加科学、更能看出是否符合同一分布。</p><pre><code class="lang-python">plt.figure(figsize=(15,5))ax = sns.distplot(len_dist, bins=100)ax = sns.distplot(len_test_dist, bins=100)plt.xlim([0, max(max(len_dist), max(len_test_dist))])plt.xlabel(&quot;length of sample&quot;)plt.ylabel(&quot;prob of sample&quot;)plt.legend([&#39;train_len&#39;,&#39;test_len&#39;])</code></pre><p><img src="/2020/07/22/【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析/3.png" srcset="/img/loading.gif" alt></p><p>通过直方图，我们能直观感受到训练集和测试集的长度分布都属于右偏分布。按理说分析到这份儿上就该停了。</p><h3 id="同分布验证"><a href="#同分布验证" class="headerlink" title="同分布验证"></a>同分布验证</h3><pre><code class="lang-python">import scipyscipy.stats.ks_2samp(len_dist, len_test_dist)Ks_2sampResult(statistic=0.004049999999999998, pvalue=0.5279614323123156)</code></pre><p>P值为0.52，比指定的显著水平（假设为5%）大，我们认为二者同分布。</p><h3 id="截断位置"><a href="#截断位置" class="headerlink" title="截断位置"></a>截断位置</h3><p>在输入模型进行训练之前，我们要把所有的数据长度统一化，数据肯定要截断。但是在什么位置截断合适呢？</p><p>考虑到数据长度分布是长尾分布，log一下看看是不是正态分布，如果是正态分布，使用3sigma法则作为截断的参考。如果不是，则就只能瞎猜了</p><p>测量拟合分布的均值和方差sigma原则</p><p>$1\sigma$原则：数值分布在$(\mu-\sigma,\mu+\sigma)$中的概率为0.6526；</p><p>$2\sigma$原则：数值分布在$(\mu-2\sigma,\mu+2\sigma)$中的概率为0.9544；</p><p>$3\sigma$原则：数值分布在$(\mu-3\sigma,\mu+3\sigma)$中的概率为0.9974；</p><p>由于“小概率事件”和假设检验的基本思想 “小概率事件”通常指发生的概率小于5%的事件，认为在一次试验中该事件是几乎不可能发生的。由此可见X落在$(\mu-3\sigma,\mu+3\sigma)$以外的概率小于千分之三，在实际问题中常认为相应的事件是不会发生的，基本上可以把区间$(\mu-3\sigma,\mu+3\sigma)$看作是随机变量X实际可能的取值区间，这称之为正态分布的“$3\sigma$”原则。</p><pre><code class="lang-python">log_len_dist = np.log(1+len_dist)log_len_test_dist = np.log(1+len_test_dist)plt.figure(figsize=(15,5))ax = sns.distplot(log_len_dist)ax = sns.distplot(log_len_test_dist)plt.xlabel(&quot;log length of sample&quot;)plt.ylabel(&quot;prob of log&quot;)plt.legend([&#39;train_len&#39;,&#39;test_len&#39;])</code></pre><p><img src="/2020/07/22/【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析/4.png" srcset="/img/loading.gif" alt></p><p>从log图上也能看出二者（很像）同分布。</p><p>下面我想验证一下我的猜想：该分布为正态分布，且训练集和测试集为同分布。</p><p>先验证训练集分布为正态分布：</p><pre><code class="lang-python">_, lognormal_ks_pvalue = scipy.stats.kstest(rvs=log_len_dist, cdf=&#39;norm&#39;)print(&#39;P value is &#39;, lognormal_ks_pvalue)P value is  0.0</code></pre><p>？0？？？拟合优度检验，p值为0，意思就是说这不是一个正态分布。<br>关于分布检验，参考<a href="https://blog.csdn.net/QimaoRyan/article/details/72861387" target="_blank" rel="noopener">这篇文章</a></p><p>之前我们把数据log了一下，但是这里有更科学的变换方式。log只是box-cox变换的特殊形式。我们使用box-cox变换再次做一下验证，是否为正态分布：</p><pre><code class="lang-python">trans_data, lam = scipy.stats.boxcox(len_dist+1)scipy.stats.normaltest(trans_data)NormaltestResult(statistic=1347.793358118494, pvalue=2.1398873511704724e-293)</code></pre><p>e后面跟了那么多负数，我佛了。这说明我们的假设不成立。</p><p>但总归是要猜一个截断值的。看log图上8.5的位置比较靠谱。np.exp(8.5)=4914约等于5000，因此我初步决定把截断长度定为5000。</p><h2 id="类别信息"><a href="#类别信息" class="headerlink" title="类别信息"></a>类别信息</h2><h3 id="简单查看类别信息表"><a href="#简单查看类别信息表" class="headerlink" title="简单查看类别信息表"></a>简单查看类别信息表</h3><p>先改造一下df_train，多加几个字段，分别是</p><ul><li>text-split，将text字段分词</li><li>len，每条新闻长度</li><li>first_char，新闻第一个字符</li><li>last_char，新闻最后一个字符</li><li>most_freq，新闻最常出现的字符</li></ul><pre><code class="lang-python">df_train[&#39;text_split&#39;] = df_train[&#39;text&#39;].apply(lambda x:x.split())df_train[&#39;len&#39;] = df_train[&#39;text&#39;].apply(lambda x:len(x.split()))df_train[&#39;first_char&#39;] = df_train[&#39;text_split&#39;].apply(lambda x:x[0])df_train[&#39;last_char&#39;] = df_train[&#39;text_split&#39;].apply(lambda x:x[-1])df_train[&#39;most_freq&#39;] = df_train[&#39;text_split&#39;].apply(lambda x:np.argmax(np.bincount(x)))df_train.head()</code></pre><p><img src="/2020/07/22/【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析/2020-07-25-12-22-01.png" srcset="/img/loading.gif" alt></p><p>构建一个类别信息表。</p><ul><li>count，该类别新闻个数</li><li>len_mean，该类别新闻平均长度</li><li>len_std，该类别新闻长度标准差</li><li>len_min，该类别新闻长度最小值</li><li>len_max，该类别新闻长度最大值</li><li>freq_fc，该类别新闻最常出现的第一个字符</li><li>freq_lc，该类别新闻最常出现的最后一个字符</li><li>freq_freq，该类别新闻最常出现的字符</li></ul><pre><code class="lang-python">df_train_info = pd.DataFrame(columns=[&#39;count&#39;,&#39;len_mean&#39;,&#39;len_std&#39;,&#39;len_min&#39;,&#39;len_max&#39;,&#39;freq_fc&#39;,&#39;freq_lc&#39;,&#39;freq_freq&#39;])for name, group in df_train.groupby(&#39;label&#39;):    count = len(group) # 该类别新闻数    len_mean = np.mean(group[&#39;len&#39;]) # 该类别长度平均值    len_std = np.std(group[&#39;len&#39;]) # 长度标准差    len_min = np.min(group[&#39;len&#39;]) # 最短的新闻长度    len_max = np.max(group[&#39;len&#39;]) # 最长的新闻长度    freq_fc = np.argmax(np.bincount(group[&#39;first_char&#39;])) # 最频繁出现的首词    freq_lc = np.argmax(np.bincount(group[&#39;last_char&#39;])) # 最频繁出现的末词    freq_freq = np.argmax(np.bincount(group[&#39;most_freq&#39;])) # 该类别最频繁出现的词    df_train_info.loc[name] = [count,len_mean,len_std,len_min,len_max,freq_fc,freq_lc,freq_freq]df_train_info</code></pre><p><img src="/2020/07/22/【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析/2020-07-25-12-30-24.png" srcset="/img/loading.gif" alt></p><h3 id="类别分布"><a href="#类别分布" class="headerlink" title="类别分布"></a>类别分布</h3><p>之前的讨论是从数据集总体验证同分布的，我们还需要验证训练集的类别足够均匀。</p><p>在数据集中标签的对应的关系如下</p><pre><code class="lang-python">label_2_index_dict = {&#39;科技&#39;: 0, &#39;股票&#39;: 1, &#39;体育&#39;: 2, &#39;娱乐&#39;: 3, &#39;时政&#39;: 4, &#39;社会&#39;: 5, &#39;教育&#39;: 6, &#39;财经&#39;: 7, &#39;家居&#39;: 8, &#39;游戏&#39;: 9, &#39;房产&#39;: 10, &#39;时尚&#39;: 11, &#39;彩票&#39;: 12, &#39;星座&#39;: 13}index_2_label_dict = {v:k for k,v in label_2_index_dict.items()}plt.figure()plt.bar(x=range(14), height=np.bincount(df_train[&#39;label&#39;]))plt.xlabel(&quot;label&quot;)plt.ylabel(&quot;number of sample&quot;)plt.xticks(range(14), list(index_2_label_dict.values()), fontproperties=zhfont, rotation=60)plt.show()</code></pre><p><img src="/2020/07/22/【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析/6.png" srcset="/img/loading.gif" alt></p><p>从统计结果可以看出</p><p>赛题的数据集类别分布存在较为不均匀的情况。在训练集中科技类新闻最多，其次是股票类新闻，最少的新闻是星座新闻。</p><p>科技类新闻最多，星座类新闻最少。这个国家的人大部分是唯物主义者哈，神秘学受众比较少（啊这，我在分析什么？）。</p><p>由于类别不均衡，会严重影响模型的精度。但是我们也是有办法应对的。</p><h3 id="类别长度"><a href="#类别长度" class="headerlink" title="类别长度"></a>类别长度</h3><pre><code class="lang-python">df_train[&#39;len&#39;] = df_train[&#39;text&#39;].apply(lambda x: len(x.split()))plt.figure()ax = sns.catplot(x=&#39;label&#39;, y=&#39;len&#39;, data=df_train, kind=&#39;strip&#39;)plt.xticks(range(14), list(index_2_label_dict.values()), fontproperties=zhfont, rotation=60)</code></pre><p><img src="/2020/07/22/【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析/7.png" srcset="/img/loading.gif" alt></p><p>在散点图中，股票类新闻的长度都飘到天上去了，可以看出股票分析类文章真的很容易写得又臭又长啊（发现：不同类别的文章长度不同，可以把长度作为一个Feature，以供机器学习模型训练）！</p><h2 id="字符分布"><a href="#字符分布" class="headerlink" title="字符分布"></a>字符分布</h2><p>训练集中总共包括6869个字，最大数字为7549，最小数字为0，其中编号3750的字出现的次数最多，编号3133的字出现的次数最少，仅出现一次。</p><pre><code class="lang-python"># 内存警告！！！没有8G内存不要运行该代码all_lines = &#39; &#39;.join(list(df_train[&#39;text&#39;]))word_count = Counter(all_lines.split(&quot; &quot;))word_count = sorted(word_count.items(), key=lambda d:d[1], reverse=True)print(len(word_count))# 6869print(word_count[0])# (&#39;3750&#39;, 7482224)print(word_count[-1])# (&#39;3133&#39;, 1)</code></pre><p>下面代码统计了不同字符在多少个句子中出现过，其中字符3750、字符900和字符648在20w新闻的覆盖率接近99%，很有可能是标点符号。</p><pre><code class="lang-python">%%timedf_train[&#39;text_unique&#39;] = df_train[&#39;text&#39;].apply(lambda x: &#39; &#39;.join(list(set(x.split(&#39; &#39;)))))all_lines = &#39; &#39;.join(list(df_train[&#39;text_unique&#39;]))word_count = Counter(all_lines.split(&quot; &quot;))word_count = sorted(word_count.items(), key=lambda d:int(d[1]), reverse=True)# 打印整个训练集中覆盖率前5的词for i in range(5):    print(&quot;{} occurs {} times, {}%&quot;.format(word_count[i][0], word_count[i][1], (word_count[i][1]/200000)*100))</code></pre><p>3750 occurs 197997 times, 98.9985%<br>900 occurs 197653 times, 98.8265%<br>648 occurs 191975 times, 95.9875%<br>2465 occurs 177310 times, 88.655%<br>6122 occurs 176543 times, 88.2715%</p><h2 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h2><p><strong>假设字符3750，字符900和字符648是句子的标点符号，请分析赛题每篇新闻平均由多少个句子构成？</strong></p><p>如果这是英文文章，那么3750应该是空格吧？如果3750是逗号怎么办？先要判断哪个是句号。</p><p>思路：该新闻的句子数为该个数。每条新闻最后的字符往往是句号，先看看每条新闻最后一个字符是什么：</p><pre><code class="lang-python">last_char = np.vectorize(lambda x:int(x.split()[-1]))(df_train[&#39;text&#39;])last_char_count = Counter(last_char)last_char_count = sorted(last_char_count.items(), key=lambda d:d[1], reverse=True)# 打印出现次数最多的前十个for i in range(10):    print(&quot;{}在新闻末尾出现了{}次&quot;.format(last_char_count[i][0], last_char_count[i][1]))900在新闻末尾出现了85040次2662在新闻末尾出现了39273次885在新闻末尾出现了14473次1635在新闻末尾出现了7379次2465在新闻末尾出现了7076次57在新闻末尾出现了3284次3231在新闻末尾出现了2758次1633在新闻末尾出现了2706次3568在新闻末尾出现了1504次2265在新闻末尾出现了1389次</code></pre><p>因此我们有理由认为900是句号。至于3750应该是逗号吧？猜的，理由是3750不太容易在新闻末尾出现。</p><p>但是除了句号之外，感叹号和问号照样能划分句子，我们试着将2662当作感叹号，将885当作问号。什么理由？猜的。</p><p>下面开始计算每篇新闻所含标点符号（900、2662、885）的个数，</p><pre><code class="lang-python">def sum_of_sep(row):    counter = Counter(row.split())    return counter.get(&#39;900&#39;, 0)+counter.get(&#39;2662&#39;, 0)+counter.get(&#39;885&#39;, 0)sum_sep = np.vectorize(sum_of_sep)(df_train[&#39;text&#39;])print(&quot;平均每条新闻的句子个数约为：&quot;, np.round(np.mean(sum_sep)))pd.Series(sum_sep).describe()平均每条新闻的句子个数约为： 19.0count    200000.000000mean         19.070155std          21.463798min           0.00000025%           7.00000050%          14.00000075%          24.000000max        1392.000000dtype: float64</code></pre><p>平均长度为19，其实这是把那些股票文章也算上了，拉高了平均值。75%的新闻长度都在24个句子以下。</p><p>给df_train_info新加一列sent_num，计算分词后的句子个数；sent_len为句子长度。</p><pre><code class="lang-python">list_num_sentence = []for name, group in df_train.groupby(&#39;label&#39;):    sum_sep_label = np.vectorize(sum_of_sep)(group[&#39;text&#39;])    num_sentence = np.mean(sum_sep_label)    list_num_sentence.append(num_sentence)df_train_info[&#39;sent_num&#39;] = list_num_sentencedf_train_info[&#39;sent_len&#39;] = df_train_info[&#39;len_mean&#39;] / df_train_info[&#39;sent_num&#39;]df_train_info</code></pre><p>不同类别的新闻，其句子长度和个数也是不同的。</p><p>之前我们分析，股票类文章往往很长，而社会（label=5）和教育（label=6）类文章的句子最多。家居（label=8）和时尚（label=11）类新闻的句子最少。游戏类（label=9）句子是最长的，社会（label=5）句子是最短的（发现：句子和个数长度也可以作为特征）。</p><p><img src="/2020/07/22/【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析/2020-07-25-13-00-28.png" srcset="/img/loading.gif" alt></p><p><strong>每类新闻中出现次数前10</strong></p><p>在每类新闻中出现频率最高的词汇，就是df_train_info表中的freq_freq列。可以看到，清一色的3750，这个字符我们在后期处理时可以拿掉。</p><pre><code class="lang-python">word_count_dict = {}for name, df in df_train.groupby(&#39;label&#39;):    # print(name, type(df))    all_text = &#39; &#39;.join(list(df[&#39;text&#39;].apply(lambda x: &#39; &#39;.join(list(x.split(&#39; &#39;))))))    word_count_single_class = Counter(all_text.split(&quot; &quot;))    word_count_single_class = sorted(word_count_single_class.items(), key=lambda d:int(d[1]), reverse = True)    word_count_dict[name] = word_count_single_classfor label in range(14):    print(index_2_label_dict[label], [x for x,_ in word_count_dict[label][:10]])科技 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;3370&#39;, &#39;4464&#39;, &#39;2465&#39;, &#39;6122&#39;, &#39;3659&#39;, &#39;7399&#39;, &#39;4939&#39;]股票 [&#39;3750&#39;, &#39;648&#39;, &#39;3370&#39;, &#39;900&#39;, &#39;4464&#39;, &#39;3659&#39;, &#39;5036&#39;, &#39;6250&#39;, &#39;1633&#39;, &#39;6065&#39;]体育 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;7399&#39;, &#39;6122&#39;, &#39;4939&#39;, &#39;4704&#39;, &#39;1667&#39;, &#39;5598&#39;, &#39;669&#39;]娱乐 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;6122&#39;, &#39;4939&#39;, &#39;4893&#39;, &#39;7399&#39;, &#39;669&#39;, &#39;803&#39;, &#39;1635&#39;]时政 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;4411&#39;, &#39;7399&#39;, &#39;4893&#39;, &#39;6122&#39;, &#39;4464&#39;, &#39;2400&#39;, &#39;4853&#39;]社会 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;6122&#39;, &#39;5598&#39;, &#39;4893&#39;, &#39;7399&#39;, &#39;4939&#39;, &#39;3370&#39;, &#39;669&#39;]教育 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;6248&#39;, &#39;2555&#39;, &#39;5620&#39;, &#39;2465&#39;, &#39;6122&#39;, &#39;5560&#39;, &#39;3370&#39;]财经 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;3370&#39;, &#39;5296&#39;, &#39;4464&#39;, &#39;6835&#39;, &#39;3659&#39;, &#39;6122&#39;, &#39;7399&#39;]家居 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;6122&#39;, &#39;4939&#39;, &#39;913&#39;, &#39;5560&#39;, &#39;7399&#39;, &#39;3961&#39;, &#39;4811&#39;]游戏 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;7328&#39;, &#39;6122&#39;, &#39;7399&#39;, &#39;5547&#39;, &#39;4939&#39;, &#39;3370&#39;, &#39;2465&#39;]房产 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;3370&#39;, &#39;2465&#39;, &#39;5560&#39;, &#39;3686&#39;, &#39;4464&#39;, &#39;3523&#39;, &#39;6122&#39;]时尚 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;4939&#39;, &#39;6122&#39;, &#39;5560&#39;, &#39;669&#39;, &#39;4811&#39;, &#39;7539&#39;, &#39;4893&#39;]彩票 [&#39;3750&#39;, &#39;4464&#39;, &#39;3370&#39;, &#39;648&#39;, &#39;2465&#39;, &#39;900&#39;, &#39;3659&#39;, &#39;6065&#39;, &#39;1667&#39;, &#39;2614&#39;]星座 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;4939&#39;, &#39;669&#39;, &#39;6122&#39;, &#39;4893&#39;, &#39;3864&#39;, &#39;4811&#39;, &#39;1465&#39;]</code></pre><h1 id="分析结果"><a href="#分析结果" class="headerlink" title="分析结果"></a>分析结果</h1><p>数据分析肯定要有结论，没有结论的数据分析是不完整的。</p><ol><li><p>训练集共200,000条新闻，每条新闻平均907个字符，最短的句子长度为2，最长的句子长度为57921，其中75%以下的数据长度在1131以下。测试集共50,000条新闻，每条新闻平均909个字符，最短句子长度为14，最长句子41861,75%以下的数据长度在1133以下。</p></li><li><p>训练集和测试集就长度来说似乎是同一分布，但是不属于正态分布。</p></li><li><p>把截断长度定为5000？</p></li><li><p>赛题的数据集类别分布存在较为不均匀的情况。在训练集中科技类新闻最多，其次是股票类新闻，最少的新闻是星座新闻。需要用采样方法解决。文章最长的是股票类新闻。不同类别的文章长度不同，可以把长度和句子个数作为一个Feature，以供机器学习模型训练。</p></li><li><p>训练集中总共包括6869个字，最大数字为7549，最小数字为0，其中编号3750的字出现的次数最多，编号3133的字出现的次数最少，仅出现一次，其中字符3750、字符900和字符648在20w新闻的覆盖率接近99%，很有可能是标点符号。</p></li><li><p>900很有可能是句号，2662和885则很有可能为感叹号和问号，3750出现频率很高但是基本不在新闻最后出现，因此初步判断为逗号。按照这种划分，训练集中每条新闻平均句子个数约为19。</p></li><li><p>在训练集中，不同类别新闻出现词汇有特色。但是需要把共有的常用词停用。自然想到利用TF-IDF编码方式。</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不要对我有任何期待哦！&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Feature Engineering" scheme="https://superlova.github.io/tags/Feature-Engineering/"/>
    
      <category term="Data Science" scheme="https://superlova.github.io/tags/Data-Science/"/>
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】零基础入门NLP之新闻文本分类之赛题理解</title>
    <link href="https://superlova.github.io/2020/07/21/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/"/>
    <id>https://superlova.github.io/2020/07/21/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/</id>
    <published>2020-07-21T15:51:00.000Z</published>
    <updated>2020-07-21T16:18:20.216Z</updated>
    
    <content type="html"><![CDATA[<p>这项任务不好做呀，稍不小心就占用了task2的内容。<br><a id="more"></a></p><h1 id="零基础入门NLP之新闻文本分类之赛题理解"><a href="#零基础入门NLP之新闻文本分类之赛题理解" class="headerlink" title="零基础入门NLP之新闻文本分类之赛题理解"></a>零基础入门NLP之新闻文本分类之赛题理解</h1><h2 id="一、现在公开的情报"><a href="#一、现在公开的情报" class="headerlink" title="一、现在公开的情报"></a>一、现在公开的情报</h2><h3 id="1-比赛内容"><a href="#1-比赛内容" class="headerlink" title="1. 比赛内容"></a>1. 比赛内容</h3><p>本次比赛的任务为文本的分类任务。虽然简单，但是想要取得高分还是不容易。</p><p>待分类文本为新闻文本。新闻文本根据来源，分为财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐，共14类。</p><h3 id="2-数据内容"><a href="#2-数据内容" class="headerlink" title="2. 数据内容"></a>2. 数据内容</h3><p>训练集和测试集在<a href="https://tianchi.aliyun.com/competition/entrance/531810/introduction" target="_blank" rel="noopener">官网</a>下载。</p><p>其中训练集是带正确标签的，测试集不带标签，是真正的题目。我们的任务是训练模型，正确分类测试集中每一条新闻的标签。</p><p>训练集由20万条新闻构成，测试集五万条数据。</p><p>每条新闻都被编码为整数序列。每个单词对应一个整数。</p><p>数据集中标签的对应的关系如下：</p><p><code>{&#39;科技&#39;: 0, &#39;股票&#39;: 1, &#39;体育&#39;: 2, &#39;娱乐&#39;: 3, &#39;时政&#39;: 4, &#39;社会&#39;: 5, &#39;教育&#39;: 6, &#39;财经&#39;: 7, &#39;家居&#39;: 8, &#39;游戏&#39;: 9, &#39;房产&#39;: 10, &#39;时尚&#39;: 11, &#39;彩票&#39;: 12, &#39;星座&#39;: 13}</code></p><h3 id="3-赛制"><a href="#3-赛制" class="headerlink" title="3. 赛制"></a>3. 赛制</h3><ul><li>第一阶段（7月15日-9月7日），每天两次提交自己答案的机会，系统根据成绩自动排名，排行榜每小时更新。该排行与最终成绩无关。</li><li>第二阶段（9月7日～9月8日）清空排行榜，7日11：00放出新测试数据。同样每天只能提交两次，每小时更新榜单，9月8日晚上20点的排行即为最终成绩。</li><li>排行前13名选手在9月11日12:00前提交代码，获得奖励。</li></ul><h3 id="4-结果提交"><a href="#4-结果提交" class="headerlink" title="4. 结果提交"></a>4. 结果提交</h3><p>将测试集的label保存成csv格式，上传到<a href="https://tianchi.aliyun.com/competition/entrance/531810/submission/" target="_blank" rel="noopener">这里</a>。</p><p>注意第一行是标题label，从第二行开始写入标签。</p><h3 id="5-评分标准"><a href="#5-评分标准" class="headerlink" title="5. 评分标准"></a>5. 评分标准</h3><p>F1评价指标</p><h3 id="6-其他"><a href="#6-其他" class="headerlink" title="6. 其他"></a>6. 其他</h3><p>可以充分发挥自己的特长来完成各种特征工程，不限制使用任何外部数据和模型。</p><h2 id="二、赛题理解"><a href="#二、赛题理解" class="headerlink" title="二、赛题理解"></a>二、赛题理解</h2><h3 id="1-数据编码"><a href="#1-数据编码" class="headerlink" title="1. 数据编码"></a>1. 数据编码</h3><p>赛题使用的数据为新闻，但是数据已经编码成了整数序列。分词我们不必操心了，但是这种编码方式我们有必要熟悉一下。</p><p>文本数据的编码方式通常有：</p><h4 id="1-1-根据单词表编码"><a href="#1-1-根据单词表编码" class="headerlink" title="1.1 根据单词表编码"></a>1.1 根据单词表编码</h4><p>本题就是根据单词表编码的，每个单词被编码为在单词表中的位置。比如整数40就是单词表中第40个单词。</p><p>本题中最大的数据就是7549，因此推测单词表大小为7550。剩下的特征工程留给下次打卡，要不没得写了XD</p><p>当然你可以基于单词表编码的方法，使用大名鼎鼎的<strong>One-Hot编码</strong>方法，把每个单词对应的整数映射成一个7550维的向量$\mathbf{x}$，该向量的第$i$维$\mathbf{x}_i=1$，其他维度为0。</p><p>One-Hot编码方法的坏处显而易见，那就是数据太过稀疏。好处则是，实践证明，深度学习模型是可以从这种稀疏表示的特征中高效地学习到知识的。</p><h4 id="1-2-词袋模型"><a href="#1-2-词袋模型" class="headerlink" title="1.2 词袋模型"></a>1.2 词袋模型</h4><p>文本数据通常被表示为由字符组成的字符串。我们需要先处理数据，然后才能对其应用机器学习算法。</p><p>在文本分析的语境中，数据集通常被称为语料库（corpus），每个由单个文本表示的数据点被称为文档（document）。</p><p>最简单的处理方法，是<strong>只计算语料库中每个单词在每个文本中的出现频次</strong>。这种文本处理模型称之为<strong>词袋模型</strong>。</p><p>不考虑词语出现的顺序，每个出现过的词汇单独作为一列特征，这些不重复的特征词汇集合为词表。</p><p>每一个文本都可以在很长的词表上统计出一个很多列的特征向量。如果每个文本都出现的词汇，一般被标记为<strong>停用词</strong>不计入特征向量。</p><p>为了搞清楚词袋模型，也就是<code>CountVectorizer</code>到底做了什么，我们执行以下代码：</p><pre><code class="lang-python">bards_words =[&quot;The fool doth think he is wise,&quot;,    &quot;but the wise man knows himself to be a fool&quot;]</code></pre><p>我们导入 CountVectorizer 并将其实例化，然后对 bards_words 进行拟合，如下所示：</p><pre><code class="lang-python">from sklearn.feature_extraction.text import CountVectorizervect = CountVectorizer()vect.fit(bards_words)</code></pre><p>拟合 CountVectorizer 包括训练数据的分词与词表的构建，我们可以通过 vocabulary_ 属性来访问词表：</p><pre><code class="lang-python">print(&quot;Vocabulary size: {}&quot;.format(len(vect.vocabulary_)))print(&quot;Vocabulary content:\n {}&quot;.format(vect.vocabulary_))#------------------#Vocabulary size: 13Vocabulary content:{&#39;the&#39;: 9, &#39;himself&#39;: 5, &#39;wise&#39;: 12, &#39;he&#39;: 4, &#39;doth&#39;: 2, &#39;to&#39;: 11, &#39;knows&#39;: 7,&#39;man&#39;: 8, &#39;fool&#39;: 3, &#39;is&#39;: 6, &#39;be&#39;: 0,  &#39;think&#39;: 10, &#39;but&#39;: 1}</code></pre><p>词表共包含 13 个词，从 “be” 到 “wise”。<br>我们可以调用 transform 方法来创建训练数据的词袋表示：</p><pre><code class="lang-python">bag_of_words = vect.transform(bards_words)print(&quot;bag_of_words: {}&quot;.format(repr(bag_of_words)))#--------------------#bag_of_words: &lt;2x13 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39;with 16 stored elements in Compressed Sparse Row format&gt;</code></pre><p>词袋表示保存在一个 SciPy 稀疏矩阵中，这种数据格式只保存非零元素。这个矩阵的形状为 2×13，每行对应于两个数据点之一，每个特征对应于词表中的一个单词。要想查看稀疏矩阵的实际内容，可以使用 toarray 方法将其转换为“密集的”NumPy 数组（保存所有 0 元素）：</p><pre><code class="lang-python">print(&quot;Dense representation of bag_of_words:\n{}&quot;.format(    bag_of_words.toarray()))#---------------------#Dense representation of bag_of_words:[[0 0 1 1 1 0 1 0 0 1 1 0 1][1 1 0 1 0 1 0 1 1 1 0 1 1]]</code></pre><p>删除没有信息量的单词，除了使用<code>min_df</code>参数设定词例至少需要在多少个文档中出现过之外，还可以通过添加停用词的方法。</p><h4 id="1-3-用tf-idf编码数据"><a href="#1-3-用tf-idf编码数据" class="headerlink" title="1.3 用tf-idf编码数据"></a>1.3 用tf-idf编码数据</h4><p>词频 - 逆向文档频率（term frequency–inverse document frequency，tf-idf）方法，对在某个特定文档中经常出现的术语给予很高的权重，但对在语料库的许多文档中都经常出现的术语给予的权重却不高。</p><p>scikit-learn 在两个类中实现了 tf-idf 方法：TfidfTransformer 和 TfidfVectorizer，前者接受 CountVectorizer 生成的稀疏矩阵并将其变换，后者接受文本数据并完成词袋特征提取与 tf-idf 变换。</p><p>单词w在文档d中的tf-idf分数为：</p><script type="math/tex; mode=display">\operatorname{tfidf}(w, d)=\operatorname{tf} \log \left(\frac{N+1}{N_{w}+1}\right)+1</script><p>式中，tf为词频，Term Frequency, 表示一个词在一个文档中的出现频率。该频率最后要除以该文档的长度，用以归一化。</p><p>式中，$N$为总文档数，$N_w$为带有单词$w$的文档数。由于分子比分母大，所以该 $\log$ 值必不可能小于零。</p><pre><code class="lang-python">from sklearn.feature_extraction.text import TfidfVectorizercorpus=[&quot;I come to China to travel&quot;,&quot;This is a car polupar in China&quot;,&quot;I love tea and Apple &quot;,&quot;The work is to write some papers in science&quot;]tfidf = TfidfVectorizer()vector = tfidf.fit_transform(corpus)print(vector)#---------------#(0, 16)    0.4424621378947393(0, 3)    0.348842231691988(0, 15)    0.697684463383976(0, 4)    0.4424621378947393(1, 5)    0.3574550433419527(1, 9)    0.45338639737285463(1, 2)    0.45338639737285463(1, 6)    0.3574550433419527(1, 14)    0.45338639737285463(1, 3)    0.3574550433419527(2, 1)    0.5(2, 0)    0.5(2, 12)    0.5(2, 7)    0.5(3, 10)    0.3565798233381452(3, 8)    0.3565798233381452(3, 11)    0.3565798233381452(3, 18)    0.3565798233381452(3, 17)    0.3565798233381452(3, 13)    0.3565798233381452(3, 5)    0.2811316284405006(3, 6)    0.2811316284405006(3, 15)    0.2811316284405006</code></pre><p>返回值什么意思呢？(0, 16)代表第0个文档，第一个单词在单词表（词袋）中的位置是第16个，该单词的tf-idf值为0.44246213；第二个单词在词袋中第3个位置……</p><p>显然这是个经过压缩的系数矩阵，每一行的元组表明该元素在稀疏矩阵中的位置，其值为右边的tf-idf值，代表一个单词。可以通过<code>.toarray()</code>方法令其恢复到系数矩阵状态。</p><pre><code class="lang-python">print(vector.toarray().shape)print(len(vector.toarray()))print(type(vector.toarray()))print(vector.toarray())#-----------------------------#(4, 19)4&lt;class &#39;numpy.ndarray&#39;&gt;[[0. 0. 0. 0.34884223 0.44246214 0.  0. 0. 0. 0. 0. 0.  0. 0. 0. 0.69768446 0.44246214 0.  0. ] [0. 0. 0.4533864  0.35745504 0. 0.35745504  0.35745504 0. 0. 0.4533864  0. 0.  0. 0. 0.4533864  0. 0. 0.  0. ] [0.5 0.5 0. 0. 0. 0.  0. 0.5 0. 0. 0. 0.  0.5 0. 0. 0. 0. 0.  0. ] [0. 0. 0. 0. 0. 0.28113163  0.28113163 0. 0.35657982 0. 0.35657982 0.35657982  0. 0.35657982 0. 0.28113163 0. 0.35657982  0.35657982]]</code></pre><h4 id="1-4-Hash编码"><a href="#1-4-Hash编码" class="headerlink" title="1.4 Hash编码"></a>1.4 Hash编码</h4><p>无论采用什么编码，只要令每个特征能够独一无二地表示即可。可采用Hash思想。</p><p>对于类别数量很多的分类变量，利用哈希函数将一个数据点转换成一个向量。相比较One-Hot模型，哈希编码维度下降了很多。</p><p>若采用哈希函数</p><pre><code>h(the) mod 5 = 0h(quick) mod 5 = 1h(brown) mod 5 = 1h(fox) mod 5 = 3</code></pre><p>则对于某句话：<br><code>the quick brown fox</code><br>来说，其使用哈希特转换的向量就是：<br><code>(1,2,0,1,0)</code><br>对比one-hot编码向量（在单词表里就这四个单词的情况下）：<br><code>(0001,0010,0100,1000)</code></p><p>在实践中，哈希编码通过调用sklearn的HashingVectorizer实现。</p><p>关于数据的编码及其他特征工程，请看<a href="https://superlova.github.io/2020/07/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/#%E4%B8%80%E3%80%81%E7%B1%BB%E5%88%AB%E7%89%B9%E5%BE%81">这里</a>。</p><h3 id="2-评分标准"><a href="#2-评分标准" class="headerlink" title="2. 评分标准"></a>2. 评分标准</h3><p>分类模型的评分标准非常丰富，通用评价指标有精度和错误率</p><script type="math/tex; mode=display">\operatorname{accuracy}=\frac{T P+T N}{N}, \text { Error Rate }=\frac{F P+F N}{N}</script><p>其中N是样本总数，TP、FP、TN、FN的含义如下表</p><p><img src="/2020/07/21/【竞赛打卡】零基础入门NLP之新闻文本分类之赛题理解/table.png" srcset="/img/loading.gif" alt></p><p>除此之外，准确率、召回率、F1值也是常用的评价指标。</p><p><img src="/2020/07/21/【竞赛打卡】零基础入门NLP之新闻文本分类之赛题理解/metrics.png" srcset="/img/loading.gif" alt></p><p>P-R曲线即召回率R为横轴、精确率P为纵轴画的曲线。分类器的P-R曲线下面积越大，表明分类性能越好。</p><p>ROC曲线分析的是二元分类模型，也就是输出结果只有两种类别的模型。ROC以伪阳性率（FPR）为 X 轴，以真阳性率（TPR）为 Y 轴绘制曲线。AUC（Area Under Curve）被定义为ROC曲线下与坐标轴围成的面积，完美分类器的AUC=1。</p><p><img src="/2020/07/21/【竞赛打卡】零基础入门NLP之新闻文本分类之赛题理解/metrics2.png" srcset="/img/loading.gif" alt></p><h3 id="3-实验环境"><a href="#3-实验环境" class="headerlink" title="3. 实验环境"></a>3. 实验环境</h3><p>你还不知道Colab吗？我不允许有人不知道这么好的东西！Colab是一款在线Python编程工具。使用Colab，让你再也不用下载和安装Anaconda，再也不用纠结显卡驱动！有了它就可以白嫖Google的GPU服务器啦！(<a href="https://colab.research.google.com/" target="_blank" rel="noopener">https://colab.research.google.com/</a>)</p><p>Colab深度学习乞丐炼丹师的最爱！但是想说爱你不容易，各种掉线、内存不足，心酸……有钱还是买带显卡的服务器吧！</p><p>关于如何使用Colab，可以参考知乎的<a href="https://zhuanlan.zhihu.com/p/35063343" target="_blank" rel="noopener">这篇文章</a>。</p><p>（其实使用Colab最大的障碍是，你得有个稳定的VPN……）</p><h3 id="4-解题思路"><a href="#4-解题思路" class="headerlink" title="4. 解题思路"></a>4. 解题思路</h3><p>文本分类问题嘛，相比大家都用LSTM分类过IMDb影评，相当于Hello World之于程序员了。用LSTM分类IMDb影评的笔记我都写好了：<a href="https://superlova.github.io/2020/06/03/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8LSTM%E8%AE%AD%E7%BB%83imdb%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/">这里</a>。<a href="https://www.tensorflow.org/tutorials/text/text_classification_rnn?hl=zh-cn" target="_blank" rel="noopener">Tensorflow的官方教程</a>也有用LSTM分类IMDb影评的Notebook。</p><p>因此使用LSTM的方法可以作为Baseline。</p><h3 id="5-小试牛刀"><a href="#5-小试牛刀" class="headerlink" title="5. 小试牛刀"></a>5. 小试牛刀</h3><p><img src="/2020/07/21/【竞赛打卡】零基础入门NLP之新闻文本分类之赛题理解/score.png" srcset="/img/loading.gif" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这项任务不好做呀，稍不小心就占用了task2的内容。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Feature Engineering" scheme="https://superlova.github.io/tags/Feature-Engineering/"/>
    
      <category term="Data Science" scheme="https://superlova.github.io/tags/Data-Science/"/>
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】特征工程</title>
    <link href="https://superlova.github.io/2020/07/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    <id>https://superlova.github.io/2020/07/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</id>
    <published>2020-07-20T08:53:34.000Z</published>
    <updated>2020-07-20T08:59:27.422Z</updated>
    
    <content type="html"><![CDATA[<p>本文翻译自HJ van Veen的Feature Engineering一文，总结了数据竞赛中常用的特征工程方法。<br><a id="more"></a></p><h1 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h1><p>本文翻译自HJ van Veen的Feature Engineering一文。由于原文以PPT方式呈现，信息高度压缩，因此在我整理本文过程中，添加了自己的理解。如有错误，敬请指正！</p><p><img src="/2020/07/20/【学习笔记】特征工程/title.png" srcset="/img/loading.gif" alt></p><p><strong>全文概览：</strong></p><p><img src="/2020/07/20/【学习笔记】特征工程/特征工程.png" srcset="/img/loading.gif" alt></p><p>机器学习的特征工程是将原始的输入数据转换成特征，以便于更好的表示潜在的问题、提高预测模型准确性的过程。</p><p>特征工程是数据科学中最具创造性的部分，决定了机器学习模型的性能上限。</p><p>即便是号称端到端的神经网络也需要特征工程，比如cv需要获得HOG，SIFT，whitening, perturbation, image pyramids, rotation, z-scaling, log-scaling, frame-grams, external semantic data等信息。</p><p>机器学习的输入特征包括几种：</p><ul><li>类别特征：如ID、性别等，值的大小无意义。</li><li>数值特征：包括整型、浮点型等，值的大小有意义。</li><li>时间特征：如月份、年份、季度、日期、小时等。</li><li>空间特征：经纬度等，可以转换成邮编，城市等。</li><li>文本特征：文档，自然语言，程序语句等。</li></ul><h2 id="一、类别特征"><a href="#一、类别特征" class="headerlink" title="一、类别特征"></a>一、类别特征</h2><p>分类特征的特点是几乎总是需要处理后才能输入算法。</p><p>类别特征难以估算缺失数据，类别太多（数据维度高）会导致数据稀疏，因此类别特征是特征工程的重点。</p><h3 id="One-Hot编码"><a href="#One-Hot编码" class="headerlink" title="One-Hot编码"></a>One-Hot编码</h3><p>将每个特征编码成长度为K的向量，每个向量只有一个维度值为1，其他值为0。这样做的好处是简单易实现，且数据样本的长度也归一化了。</p><p>比如分类特征{黄，绿，蓝}可以编码成{001,010,100}；{男，女}=&gt;{01,10}；{1,2,3,4,5,6,7,8,9}=&gt;{000000001,…,100000000}</p><p><strong>通常丢弃第一列以避免线性相关。</strong></p><p>比如Pandas中，<code>get_dummies</code>函数有一个参数为<code>drop_first</code>，如果为True就丢弃One-Hot编码后数据的第一列，因为丢弃的一列可以通过其他剩余的k-1列计算得到，这一列也就变成了重复数据。</p><p>缺陷：容易造成数据稀疏，且现有的One-Hot编码实现方式，较难应对缺失数据和没出现过的变量。</p><p>对于类别不多的分类变量，可以采用独热编码。但其实自然语言处理任务中也广泛使用了One-Hot编码。要知道自然语言处理任务的词典大小动辄上万，那么一个单词的One-Hot向量长度也上万，一篇文档转化成One-Hot矩阵，这个数据量就非常可观了。</p><h3 id="Hash编码"><a href="#Hash编码" class="headerlink" title="Hash编码"></a>Hash编码</h3><p>无论采用什么编码，只要令每个特征能够独一无二地表示即可。可采用Hash思想。</p><p>对于类别数量很多的分类变量，利用哈希函数将一个数据点转换成一个向量。相比较One-Hot模型，哈希编码维度下降了很多。</p><p>若采用哈希函数</p><pre><code>h(the) mod 5 = 0h(quick) mod 5 = 1h(brown) mod 5 = 1h(fox) mod 5 = 3</code></pre><p>则对于某句话：<br><code>the quick brown fox</code><br>来说，其使用哈希特转换的向量就是：<br><code>(1,2,0,1,0)</code><br>对比one-hot编码向量（在单词表里就这四个单词的情况下）：<br><code>(0001,0010,0100,1000)</code></p><p>哈希表有如下特性：</p><ul><li>相同的输入可能有相同的输出（一般情况下比例不高）</li><li>不同的输出一定对应不同的输入</li><li>正向计算很简单，反向计算很困难</li><li>根据输入查找输出效率很高</li></ul><p>本部分参考自 <a href="https://www.datalearner.com/blog/1051537932880901" target="_blank" rel="noopener">https://www.datalearner.com/blog/1051537932880901</a></p><h3 id="标签编码"><a href="#标签编码" class="headerlink" title="标签编码"></a>标签编码</h3><p>给予每个类别变量一个独一无二的数字ID。</p><p>假如有三种颜色特征：红、黄、蓝，那么你可能想令红=1，黄=2，蓝=3，这就是标签编码。</p><p>这种编码对于树算法等非线性算法比较有用，好处是不会升维，坏处是会让机器学习算法误以为颜色存在数值大小关系。</p><h3 id="计数编码"><a href="#计数编码" class="headerlink" title="计数编码"></a>计数编码</h3><p>将每个类别的编码定义为其在数据集中出现的次数。比如在数据集中’红’出现了300次，那么’红’的编码就是300。</p><p><code>df.groupby([&#39;category&#39;])[&#39;target&#39;].transform(sum)</code><br>这种做法很常见，也很简单。缺点是对异常值敏感，且不同类别出现次数相同时可能引入冲突。</p><h3 id="计数排序编码"><a href="#计数排序编码" class="headerlink" title="计数排序编码"></a>计数排序编码</h3><p>根据类别变量在训练集中出现的次数<strong>排序</strong></p><p>对异常值不敏感，不会引入冲突。在实际比赛中效果可能出乎意料的好。</p><h3 id="Target编码"><a href="#Target编码" class="headerlink" title="Target编码"></a>Target编码</h3><p>将类别变量编码为分类为正的比例，类似于该类别对正类的贡献指数。</p><p>比如职业类别特征{‘manager’,’engineer’,’scientist’}在数据集中，但凡scientist都使得target=1，那么scientist就编码成1.00。</p><p><img src="/2020/07/20/【学习笔记】特征工程/2020-07-20-11-20-25.png" srcset="/img/loading.gif" alt></p><h3 id="类别嵌入"><a href="#类别嵌入" class="headerlink" title="类别嵌入"></a>类别嵌入</h3><p>由神经网络得到每个类别的嵌入表达，将特征投影到更高维度的空间。在进行文档分类等，原本具有语义相似性的单词映射之后的向量之间的距离也比较小，进而可以帮助我们进一步进行机器学习的应用，这一点比独热模型好很多。</p><p>参考 <a href="https://arxiv.org/abs/1604.06737" target="_blank" rel="noopener">https://arxiv.org/abs/1604.06737</a></p><p><img src="/2020/07/20/【学习笔记】特征工程/2020-07-20-11-21-54.png" srcset="/img/loading.gif" alt></p><h3 id="把Nan也当作一个类别"><a href="#把Nan也当作一个类别" class="headerlink" title="把Nan也当作一个类别"></a>把Nan也当作一个类别</h3><p>只在Nan确定有意义时使用。</p><h3 id="多元编码"><a href="#多元编码" class="headerlink" title="多元编码"></a>多元编码</h3><p>将两个二元分类变量组合，形成四元向量表示</p><p><img src="/2020/07/20/【学习笔记】特征工程/2020-07-19-14-09-28.png" srcset="/img/loading.gif" alt></p><h3 id="扩张编码"><a href="#扩张编码" class="headerlink" title="扩张编码"></a>扩张编码</h3><p>将一个Feature拆分成多个Feature</p><h3 id="合并编码"><a href="#合并编码" class="headerlink" title="合并编码"></a>合并编码</h3><p>将多个Feature合并成一个Feature</p><h2 id="二、数值特征"><a href="#二、数值特征" class="headerlink" title="二、数值特征"></a>二、数值特征</h2><h3 id="舍入"><a href="#舍入" class="headerlink" title="舍入"></a>舍入</h3><p>过高的精度有时是噪音。</p><p>舍入后的数据可以被当作是类别变量。</p><p>舍入前可以log一下，log转换可以将范围很大的值缩小在一定范围内，这对某些异常值的处理也很有效。</p><p><img src="http://www.datalearner.com/resources/blog_images/6a4db741-d9bb-4391-8d24-7214ecd07b3b.jpg" srcset="/img/loading.gif" alt></p><h3 id="按数值大小分箱（Binning）"><a href="#按数值大小分箱（Binning）" class="headerlink" title="按数值大小分箱（Binning）"></a>按数值大小分箱（Binning）</h3><p>数据分箱是一种将多个或多或少连续值分组为较少数量的“箱(bin)”的方法。通过分箱，数值特征转换成类别特征了。</p><p>例如一天24小时可以分成早晨[5,8)，上午[8,11)，中午[11,14)，下午[14,19)，夜晚[10,22)，深夜[19,24)和[24,5)。因为比如中午11点和12点其实没有很大区别，可以使用分箱技巧处理之后可以减少这些“误差”。</p><h3 id="特征缩放（Scaling）"><a href="#特征缩放（Scaling）" class="headerlink" title="特征缩放（Scaling）"></a>特征缩放（Scaling）</h3><p>也称为数据标准化。可以将很大范围的数据限定在指定范围内。由于原始数据的值范围变化很大，在一些机器学习算法中，如果没有标准化，目标函数将无法正常工作。</p><p>特征缩放方法主要有：</p><ul><li>转为标准正态</li><li>最大最小缩放</li><li>对数缩放</li><li>开方缩放 root scale</li></ul><h3 id="估算缺失数据（Imputation）"><a href="#估算缺失数据（Imputation）" class="headerlink" title="估算缺失数据（Imputation）"></a>估算缺失数据（Imputation）</h3><p>由于各种原因，许多真实世界的数据集包含缺失的值，通常编码为空白、NAN或其他占位符。简单删除该条数据项就太可惜了，一个更好的策略是估算缺失的值， 即从数据的已知部分推断它们。</p><p>缺失的值可以用提供的常量值来计算，或使用缺失值所在的每一列的统计数据(平均值不如中位数鲁棒)。处理缺失数据的方法还有很多。</p><p>sklearn的SimpleImputer类就是用来估算缺失值的。<br><a href="https://www.studyai.cn/modules/impute.html" target="_blank" rel="noopener">https://www.studyai.cn/modules/impute.html</a></p><h3 id="特征交叉-feature-interactions"><a href="#特征交叉-feature-interactions" class="headerlink" title="特征交叉(feature interactions)"></a>特征交叉(feature interactions)</h3><p>在回归模型中加入交互项是一种非常常见的处理方式。它可以极大的拓展回归模型对变量之间的依赖的解释。</p><p>举个例子：<a href="https://www.datalearner.com/blog/1051508158689792" target="_blank" rel="noopener">来源</a></p><p>不同特征之间可能相互影响，比如阳光和土壤质量同时决定树木生长高度，我们可以建模：</p><script type="math/tex; mode=display">\text{Tree}=a\times \text{Sun}+b\times \text{Soil}+c</script><p>但是阳光本身也可能影响土壤质量，而我们建立的线性模型事实上是把土壤和阳光当作独立变量的。想要学习线性相关性，我们可以增加一个特征:</p><script type="math/tex; mode=display">\text{Tree}=a\cdot\text{Sun}+b\cdot\text{Soil}+c\cdot(\text{Sun}\cdot\text{Soil})+d</script><p>特征交叉的方法有很多，比如不同特征间进行加减乘除、指数操作等。</p><h3 id="非线性特征在线性模型中的应用"><a href="#非线性特征在线性模型中的应用" class="headerlink" title="非线性特征在线性模型中的应用"></a>非线性特征在线性模型中的应用</h3><ul><li><p>通过多项式核函数(polynomial kernel)和径向基核函数(RBF kernel)将线性不可分的数据映射到高维空间去</p></li><li><p>Leafcoding（随机森林嵌入）（acebook的gbdt+lr这种思路）</p></li><li><p>遗传算法（典型代表gplearn）</p></li><li><p>局部线性嵌入 Locally Linear Embedding，频谱嵌入 Spectral Embedding，t-SNE （降维提取重要特征）</p></li></ul><h3 id="行统计"><a href="#行统计" class="headerlink" title="行统计"></a>行统计</h3><p>统计一行数据中Nan个数、0的个数、负数个数、最大值、最小值、中位数、峰度偏度等</p><h2 id="三、时间特征"><a href="#三、时间特征" class="headerlink" title="三、时间特征"></a>三、时间特征</h2><p>时间变量非常容易出错，需要更好的局部验证方案（如回测）</p><h3 id="将时间映射成环（周期性变量）"><a href="#将时间映射成环（周期性变量）" class="headerlink" title="将时间映射成环（周期性变量）"></a>将时间映射成环（周期性变量）</h3><p>Turn single features, like day_of_week, into two coordinates on a circle</p><p>Ensures that distance between max and min is the same as min and min +1.</p><p>含义是指周六和周天，与周天和周一的距离是一样的。</p><p>Use for day_of_week, day_of_month, hour_of_day, etc.</p><p>就是将大时间项，比如2019年11月11日分解成小时间项的意思。</p><h3 id="趋势线"><a href="#趋势线" class="headerlink" title="趋势线"></a>趋势线</h3><p>数据的变化趋势本身也是信息。因此不要使用诸如total_spend这种总结式变量，要有一些中间变量，诸如spend_in_last_week，spend_in_last_month。展现数据趋势，利于模型获取信息。</p><h3 id="事件编码"><a href="#事件编码" class="headerlink" title="事件编码"></a>事件编码</h3><p>将某日期与重大节日之间的距离也作为特征，比如法定假日、重大体育赛事、周末、每月的第一个星期六等。</p><h2 id="四、空间特征"><a href="#四、空间特征" class="headerlink" title="四、空间特征"></a>四、空间特征</h2><h3 id="将地点视作分类特征"><a href="#将地点视作分类特征" class="headerlink" title="将地点视作分类特征"></a>将地点视作分类特征</h3><ul><li>克里金法 Kriging，空间插值方法</li><li>K-means 聚类</li><li>原始经纬度</li><li>将城市转换为经纬度</li><li>在街道名称中添加邮政编码</li></ul><h3 id="将某地点与关键地点之间的距离也作为特征"><a href="#将某地点与关键地点之间的距离也作为特征" class="headerlink" title="将某地点与关键地点之间的距离也作为特征"></a>将某地点与关键地点之间的距离也作为特征</h3><p>诸如大城市、超市等，对你的任务有重要影响的地区。</p><h3 id="位置事件数据可以指示可疑行为"><a href="#位置事件数据可以指示可疑行为" class="headerlink" title="位置事件数据可以指示可疑行为"></a>位置事件数据可以指示可疑行为</h3><p>比如同时出现在不同城市的两笔交易、在与住所或送货地址不同的城镇中消费、从不在同一个位置消费。</p><h2 id="五、数据探索"><a href="#五、数据探索" class="headerlink" title="五、数据探索"></a>五、数据探索</h2><p>数据探索的目的是提前发现数据的潜在问题，诸如异常值、噪音；然后探索数据的特征工程方法、清洗方法，为数据预处理做准备。</p><p>一开始尝试简单统计量：min、max。</p><p>Incorporate the target so find correlation between signal.</p><p>我的理解是，探索该特征与该数据的label的相关性。</p><h3 id="迭代和Debugging"><a href="#迭代和Debugging" class="headerlink" title="迭代和Debugging"></a>迭代和Debugging</h3><p>特征工程是一个迭代的过程，确保你的Pipeline能够快速迭代。</p><p>Use sub-linear debugging: Output intermediate information on the process, do spurious logging</p><p>使用sub-linear debugging：输出有关过程的中间信息，进行伪记录。</p><p>使用一些帮助快速实验的工具。</p><p>一鸟在手胜过双鸟在林，想法太多不容易成功。</p><h3 id="Label工程"><a href="#Label工程" class="headerlink" title="Label工程"></a>Label工程</h3><p>可以把数据集的标签label给变换一下，当成数据的特征（有点泄漏答案的意思）。</p><ul><li>log变换：$y\rightarrow\log{(y+1)}$、$\exp{y_{pred}}-1$</li><li>平方变换</li><li>Box-Cox变换</li><li>创建一个评分，用来把二元分类target变成回归问题</li><li>训练回归模型，用于预测测试集中不可获取的特征</li></ul><h2 id="六、文本特征"><a href="#六、文本特征" class="headerlink" title="六、文本特征"></a>六、文本特征</h2><p>与类别特征类似，特征工程手段更为丰富，举例：</p><ul><li>Lowercasing,</li><li>Removing non-alphanumeric,</li><li>Repairing,</li><li>Encoding punctuation marks,</li><li>Tokenizing,</li><li>Token-grams,</li><li>skipgrams,</li><li>char-grams,</li><li>Removing stopwords,</li><li>Removing rare words</li><li>and very common words,</li><li>Spelling Correction,</li><li>Chopping,</li><li>Stemming,</li><li>Lemmatization,</li><li>Document features,</li><li>Entitity Insertion &amp; Extraction</li><li>Simplification,</li><li>Word2Vec and GloVe / Doc2Vec,</li><li>String Similarity,</li><li>Reading level,</li><li>Nearest Neighbors,</li><li>TF-IDF,</li><li>BayesSVM, Vectorization, LDA, LSA.</li></ul><h3 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h3><p>从大写字母转换成小写，从unicode转换成ascii，移除非字母字符，修复源文本中的格式问题等。</p><h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><ul><li>将句子分成单词token序列，英文可能好做一点，中文之类的语言就需要特定的工具了，比如jieba分词。</li><li>将标点符号也硬编码为token，因为标点可能也代表有用的信息。</li><li>词袋模型（bag-of-word model）</li><li>N元分词：“I like the Beatles” -&gt; [“I like”, “like the”, “the Beatles”]</li><li>Skip-grams：“I like Beatles” -&gt; [“I the”, “like Beatles”]</li><li>Char-grams：“Beatles” -&gt; [“Bea”, “eat”, “atl”, “tle”, “les”]</li><li>Affixes：Same as char-grams, but only the postfixes and prefixe</li></ul><h3 id="删除词"><a href="#删除词" class="headerlink" title="删除词"></a>删除词</h3><ul><li>删除停用词</li><li>删除罕见词</li><li>删除其他常见词，不能反映特征的词</li></ul><h3 id="还原词形-root"><a href="#还原词形-root" class="headerlink" title="还原词形 root"></a>还原词形 root</h3><ul><li>拼写检查</li><li>Chop，只取每个token的前8个字符</li><li>Stem，将token缩减为词根形式</li><li>Lemmatize，词形还原</li></ul><h3 id="更多特征"><a href="#更多特征" class="headerlink" title="更多特征"></a>更多特征</h3><ul><li>文档特征，诸如统计空格、tab、换行、字符、token出现次数等</li><li>添加一些通用的描述，“Microsoft releases Windows” -&gt; “Microsoft (company) releases Windows (application)”</li><li>语法分析树 Parse Tree，NLTK工具包有实现</li><li>Reading level: Compute the reading level of a document.（国外的阅读分级制度）</li></ul><h3 id="相似度"><a href="#相似度" class="headerlink" title="相似度"></a>相似度</h3><ul><li>token相似度：计算两段文本中相同token数</li><li>压缩距离 Compression distance：一段句子是否能被压缩成另外一段</li><li>Levenshitein、Hamming、Jaccard距离，用来衡量两个string的距离，计算将该文本转为另一个文本所需的最小操作次数</li><li>word2vec、glove：计算两向量的余弦距离</li></ul><h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><ul><li>Term Frequency: 能够降低长文本造成的bias</li><li>Inverse Document Frequency: 降低常用词造成的Bias</li><li>TF-IDF: 辨别在document中最重要的token，删除不重要的token；或者用在数据预处理上，可以使维度下降</li></ul><h3 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h3><ul><li>PCA: 将文本压缩到50~100维的向量</li><li>SVD: 同上</li><li>LDA: 与TF-IDF配套</li><li>LSA: 创建主题向量</li></ul><h3 id="外部模型"><a href="#外部模型" class="headerlink" title="外部模型"></a>外部模型</h3><ul><li>感情分析器：获得text的情感倾向</li><li>主题模型：使用一个dataset创建主题向量，用于另一个任务</li></ul><h2 id="七、泄露的特征-黄金特征"><a href="#七、泄露的特征-黄金特征" class="headerlink" title="七、泄露的特征 / 黄金特征"></a>七、泄露的特征 / 黄金特征</h2><p>特征工程能帮助发现泄露的特征，这些特征可能对你帮助很大。</p><ul><li>比如“逆向”工程：<ul><li>用彩虹表破解MD5哈希。<a href="https://www.cnblogs.com/by-3ks/articles/4137562.html" target="_blank" rel="noopener">彩虹表（Rainbow Table）</a>是一种破解哈希算法的技术，可以破解MD5、HASH等多种密码。</li><li>利用TF-IDF获得术语频率。</li><li>编码样本数据集的顺序。</li><li>编码文件创建日期。</li></ul></li><li>比如规则挖掘：<ul><li>查找简单的规则（并对它们进行编码）以帮助模型决策。</li></ul></li></ul><h2 id="八、案例研究"><a href="#八、案例研究" class="headerlink" title="八、案例研究"></a>八、案例研究</h2><p>Quora重复问题数据集，约440000个问题，将其分成重复问题或非重复问题。作者将自己解决Quora重复问题的过程分享如下：</p><ul><li>First attempt: 词袋模型+逻辑回归</li><li>Second attempt: token之间进行数据的多项式交互</li><li>Third attempt: 使用NLTK的SnowballStemmer进行词干提取</li><li>Fourth attempt: 使用2-grams分词</li><li>Fifth attempt: 添加以下手工构造的特征：<ul><li>归一化问答对的长度</li><li>归一化问答对的compression距离</li><li>计算问答对的词向量之间的余弦距离</li><li>Chargram co-occurence between question pairs.</li><li>计算word出现次数：which，what，where</li></ul></li><li>还能想到更多的改进方法吗？<ul><li>外部或预训练模型？</li><li>Search engine models?</li><li>Logic based models?</li></ul></li></ul><h2 id="九、其他资源参考"><a href="#九、其他资源参考" class="headerlink" title="九、其他资源参考"></a>九、其他资源参考</h2><ul><li><strong>Kaggle forums &amp; kernels:</strong> Far0n, KazAnova, Fchollet, Abhishek, Gilberto Titericz, Leustagos, Owen Zhang, Gert Jacobusse …</li><li><strong>Introduction:</strong> <a href="http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/" target="_blank" rel="noopener">http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/</a></li><li><strong>Books:</strong><ul><li>Mastering Feature Engineering (Alice Zheng),</li><li>Feature Extraction (Isabelle Guyon et al.)</li></ul></li><li><strong>Blogs:</strong><ul><li><a href="https://smerity.com/articles/2016/architectures_are_the_new_feature_engineering.html" target="_blank" rel="noopener">https://smerity.com/articles/2016/architectures_are_the_new_feature_engineering.html</a></li><li><a href="http://hunch.net/~jl/projects/hash_reps/" target="_blank" rel="noopener">http://hunch.net/~jl/projects/hash_reps/</a></li><li><a href="https://blogs.technet.microsoft.com/machinelearning/2014/09/24/online-learning-and-sub-linear-debugging/" target="_blank" rel="noopener">https://blogs.technet.microsoft.com/machinelearning/2014/09/24/online-learning-and-sub-linear-debugging/</a></li><li><a href="http://blog.kaggle.com/2015/12/03/dato-winners-interview-1st-place-mad-professors/" target="_blank" rel="noopener">http://blog.kaggle.com/2015/12/03/dato-winners-interview-1st-place-mad-professors/</a></li><li><a href="http://blog.kaggle.com/2016/08/24/avito-duplicate-ads-detection-winners-interview-1st-place-team-devil-team-stanislav-dmitrii/" target="_blank" rel="noopener">http://blog.kaggle.com/2016/08/24/avito-duplicate-ads-detection-winners-interview-1st-place-team-devil-team-stanislav-dmitrii/</a></li><li><a href="http://www.slideshare.net/DataRobot/featurizing-log-data-before-xgboost" target="_blank" rel="noopener">http://www.slideshare.net/DataRobot/featurizing-log-data-before-xgboost</a></li></ul></li><li><strong>Data:</strong> <a href="https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs" target="_blank" rel="noopener">https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs</a></li><li><strong>Software:</strong> <a href="https://github.com/trevorstephens/gplearn" target="_blank" rel="noopener">https://github.com/trevorstephens/gplearn</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文翻译自HJ van Veen的Feature Engineering一文，总结了数据竞赛中常用的特征工程方法。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
      <category term="translation" scheme="https://superlova.github.io/categories/notes/translation/"/>
    
      <category term="转载" scheme="https://superlova.github.io/categories/notes/translation/%E8%BD%AC%E8%BD%BD/"/>
    
    
      <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
      <category term="Feature Engineering" scheme="https://superlova.github.io/tags/Feature-Engineering/"/>
    
      <category term="Data Science" scheme="https://superlova.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读笔记】CAGFuzz: Coverage-Guided Adversarial Generative Fuzzing Testing of Deep Learning Systems</title>
    <link href="https://superlova.github.io/2020/06/28/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91CAGFuzz-Coverage-Guided-Adversarial-Generative-Fuzzing-Testing-of-Deep-Learning-Systems/"/>
    <id>https://superlova.github.io/2020/06/28/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91CAGFuzz-Coverage-Guided-Adversarial-Generative-Fuzzing-Testing-of-Deep-Learning-Systems/</id>
    <published>2020-06-27T16:02:49.000Z</published>
    <updated>2020-07-20T08:51:57.349Z</updated>
    
    <content type="html"><![CDATA[<p>提出了CAGFuzz，这是一种覆盖率指导的灰盒对抗性生成模糊测试方法，可以为目标DNN生成对抗性示例，以发现其潜在缺陷。<br><a id="more"></a></p><p>DeepXplore，使用多个DNN来发现并生成位于这些DNN决策边界之间的对抗性示例。</p><p>DeepHunter，使用变形突变策略来生成新的测试示例。</p><p>DeepGauge，提出了深度神经网络的新覆盖标准。</p><ol><li>为了提高泛化能力，仅从数据的角度添加较小的扰动就非常重要。</li><li>现有研究完全忽略了包含高级语义信息（例如图像对象类别和场景语义）的深度特征约束。使用L0和L∞来限制对抗性示例的像素级变化，这样的约束只能代表对抗实例和原始实例之间的视觉一致性，而不能保证对抗实例和原始实例的高级语义信息之间的一致性。</li></ol><p>具体地，</p><ol><li>双重GAN构造对抗样本生成器AEG<br>CAGFuzz的目标是最大化神经元覆盖范围，并在对目标DNN产生较小扰动的情况下尽可能多地生成对抗性测试示例。同时，生成的示例适用于不同种类的DNN。</li></ol><p>CycleGAN [19]的目标是将图像A转换为具有不同样式的图像B。</p><p>基于CycleGAN，我们的目标是将图像B转换回图像A，以获得与原始图像A类似的图像A’。</p><p>因此，我们将两个具有相反功能的CycleGAN的生成器组合为对抗示例生成器。</p><p>基于几个特定的​​数据集对AEG进行训练，并且不需要依赖任何特定的DNN模型。</p><ol><li>采用网络信息确保两样本语义一致<br>我们提取原始示例和对抗示例的深度特征，并通过相似性度量使其尽可能相似。</li></ol><p>我们使用VGG-19网络[20]提取原始示例和对抗示例的深度语义信息，并使用余弦相似度测量方法来确保对抗示例的深度语义信息与原始示例一致 越多越好。</p><ol><li>结果<br>AGFuzz可以有效地改善目标DNN模型的神经元覆盖范围。证明了由CAGFuzz生成的对抗示例可以发现目标DNN模型中的隐藏缺陷。通过AEG训练的DNN模型的准确性和鲁棒性已得到显着提高。</li></ol><h1 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h1><h2 id="覆盖率引导的灰盒模糊测试"><a href="#覆盖率引导的灰盒模糊测试" class="headerlink" title="覆盖率引导的灰盒模糊测试"></a>覆盖率引导的灰盒模糊测试</h2><p>最新的CGF方法主要包括三个部分：突变，反馈指导和模糊策略</p><h2 id="CycleGAN"><a href="#CycleGAN" class="headerlink" title="CycleGAN"></a>CycleGAN</h2><p>生成对抗性示例的想法是增加人们无法与原始示例区分开的干扰。这与GAN [27]生成示例的想法非常相似。</p><p>GAN的生成器G和鉴别器D根据噪声数据交替生成与原始示例非常相似但不完全相同的对抗示例。</p><p>考虑到不同目标DL系统（例如某些带有标签数据的DL系统和其他DL系统的数据集）的差异，我们选择CycleGAN [19]作为对抗性示例生成器的训练模型，因为CycleGAN不需要数据匹配集和标签信息。</p><p>CycleGAN的目标是学习X和Y之间的映射关系G和F。有两个对抗判别器$D_x$和$D_y$，$D_x$分辨图片$x$和转换后的图片$F(x)$。$D_y$相似。</p><p>和其他GAN一样，用损失函数优化映射函数。这里用最小二乘损失。</p><p>本示例的目的是将真实图片和梵高风格的绘画相互转化。<br><img src="/2020/06/28/【论文阅读笔记】CAGFuzz-Coverage-Guided-Adversarial-Generative-Fuzzing-Testing-of-Deep-Learning-Systems/2020-06-28-00-36-54.png" srcset="/img/loading.gif" alt></p><h2 id="VGG-19"><a href="#VGG-19" class="headerlink" title="VGG-19"></a>VGG-19</h2><p>VGG-19网络可以从图像中提取高级语义信息[29]，[30]，可用于识别图像之间的相似性。</p><p>本文将最后一个完整连接层的输出作为特征向量进行融合，以比较对抗性示例与原始示例之间的相似性，并作为过滤生成的对抗性示例的阈值。</p><h2 id="神经网络覆盖率"><a href="#神经网络覆盖率" class="headerlink" title="神经网络覆盖率"></a>神经网络覆盖率</h2><p><img src="/2020/06/28/【论文阅读笔记】CAGFuzz-Coverage-Guided-Adversarial-Generative-Fuzzing-Testing-of-Deep-Learning-Systems/2020-06-28-00-39-59.png" srcset="/img/loading.gif" alt></p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p><img src="/2020/06/28/【论文阅读笔记】CAGFuzz-Coverage-Guided-Adversarial-Generative-Fuzzing-Testing-of-Deep-Learning-Systems/2020-06-28-00-41-11.png" srcset="/img/loading.gif" alt></p><p>第一步是数据收集和训练对抗性示例生成器</p><p>将数据集分为两个子集，并作为CycleGAN的输入来训练AEG。</p><p>在根据存储时间等设置了优先级之后，将这些示例放入处理池中，作为模糊测试的原始示例集。</p><p>第二步是对抗示例生成。</p><p>每次从处理池中选择优先的原始示例，并将其用作AEG的输入以生成对抗示例。</p><p>VGG-19处理对抗性示例和原始示例，以提取示例的特征矩阵。</p><p>计算对抗性示例与原始示例之间的特征矩阵的余弦相似度，以确保对抗性示例的深层语义与原始示例一致。</p><p>第三步是使用神经元覆盖率来指导生成过程。</p><p>第二步中生成的对抗示例将输入到被测DNN中以进行覆盖率分析。</p><p>如果发生新的覆盖，则对抗性示例将作为数据集的一部分放入处理池中。</p><p>新的覆盖范围意味着对抗示例的神经元覆盖范围高于原始示例的神经元覆盖范围。</p><p><img src="/2020/06/28/【论文阅读笔记】CAGFuzz-Coverage-Guided-Adversarial-Generative-Fuzzing-Testing-of-Deep-Learning-Systems/2020-06-28-00-44-35.png" srcset="/img/loading.gif" alt></p><p>CAGFuzz的输入包括目标数据集D，给定深度神经网络DNN，最大迭代数N，每个原始示例生成的对抗性示例N1以及top-k的参数K。</p><p>输出是生成的测试示例，可改善目标DNN的覆盖范围。</p><p>在整个模糊测试过程之前，我们需要处理数据集。</p><p>一方面，它被分为两个相等的数据字段（第1行），以训练对抗性示例生成器AEG（第2行）。</p><p>另一方面，所有示例都经过预处理（第3行），并存储在处理池中（第4行）。</p><p>在每个迭代过程中（第5行），根据时间优先级（第6、7行）从处理池中选择原始示例父对象。</p><p>然后，每个原始示例父对象都会生成多次（第8行）。</p><p>对于每一代，对抗性示例生成器AEG用于变异原始示例性父级，以生成对抗性示例数据（第9行）。</p><p>分别提取原始示例父对象和对抗示例数据的深度特征，并计算它们之间的余弦相似度（第10-11行）。</p><p>最后，将原始样本产生的所有对抗性示例按照相似性从高到低排序，并选择其中的前k个作为目标示例（第13行）。</p><p>排名前k位的对抗性示例是具有覆盖范围的反馈（第15行）。</p><p>如果对抗性示例增加了目标DNN的覆盖范围，它们将被存储在处理池中并设置时间优先级（第16-19行）。</p><h2 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h2><p>首先对数据集进行排序，将排好序的数据集储存在队列中。然后将队列分成两部分，训练cycleGAN使用。</p><h2 id="训练对抗样本生成器"><a href="#训练对抗样本生成器" class="headerlink" title="训练对抗样本生成器"></a>训练对抗样本生成器</h2><p>如何把握突变程度degree是关键。本文提出了一个新的对抗样本生成方法，能够保证深层次semantic信息的不变性，且扰动对人类是不可察觉的。</p><p>这个新的方法就是引用了cyclegan，依赖adversarial loss函数添加对抗扰动，且通过cyclic consistency loss控制扰动在人类不可察觉的范围内。</p><p>CycleGAN是怎么做的呢？首先将数据集均匀分成两部分X、Y，模型试图学习两个映射，分别是从X到Y的映射P，以及从Y到X的映射Q。对应地，有两个对抗判别器$D_x$和$D_y$，分别负责判断当前的$x$是来自X的样本还是来自Q生成的样本，以及负责判断当前的$y$是来自Y的样本还是来自P生成的样本。</p><p>对于映射函数P和对应的对抗样本判别器$D_y$，对抗损失函数为：</p><script type="math/tex; mode=display">\begin{aligned}\min _{P} \max _{D} Y V\left(P, D_{Y}, X, Y\right)=E_{y \sim P_{\text {data}}(y)}\left[\log D_{Y}(y)\right] &+\\E_{x \sim P_{\text {data}}(x)}\left[\log \left(1-D_{Y}(P(x))\right)\right] &]\end{aligned}</script><p>最小化映射函数P，最大化对抗判别器$D_y$。</p><p>P映射的目的是生成对抗样本$y’=P(x)$，让$y’$看起来像是从Y中来的一样。这个过程可以理解成，把大量的来自于Y的特性给添加到来自于X的$x$中。同时训练一个辨别器$D_y$，辨别真正的$y$与P生成出来的$y’$。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;提出了CAGFuzz，这是一种覆盖率指导的灰盒对抗性生成模糊测试方法，可以为目标DNN生成对抗性示例，以发现其潜在缺陷。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://superlova.github.io/categories/paper/"/>
    
    
      <category term="Robustness" scheme="https://superlova.github.io/tags/Robustness/"/>
    
      <category term="Deep Neural Networks" scheme="https://superlova.github.io/tags/Deep-Neural-Networks/"/>
    
      <category term="Fuzzing" scheme="https://superlova.github.io/tags/Fuzzing/"/>
    
      <category term="Coverage" scheme="https://superlova.github.io/tags/Coverage/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读笔记】Towards Evaluating the Robustness of Neural Networks</title>
    <link href="https://superlova.github.io/2020/06/27/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Towards-Evaluating-the-Robustness-of-Neural-Networks/"/>
    <id>https://superlova.github.io/2020/06/27/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Towards-Evaluating-the-Robustness-of-Neural-Networks/</id>
    <published>2020-06-27T13:17:13.000Z</published>
    <updated>2020-06-27T16:01:41.571Z</updated>
    
    <content type="html"><![CDATA[<p>2017年的攻击文章，目标是当时防御性较高的<strong>防御性蒸馏</strong>模型，属于图像分类模型。结果是成功令其鲁棒性下降。文章最后的结论是，做防御的人应该用较先进的攻击手段测试自己的模型，同时应该注意防御对抗样本的迁移特性，即在其他弱模型上生成样本后在目标模型上测试的行为。<br><a id="more"></a></p><blockquote><p>警告！本文为简读，大量省略了原文内容，只留下了我关注的细节。</p></blockquote><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>防御性模型蒸馏（Defensive distillation）是近年来抵御对抗样本攻击的最有效方法之一。防御性模型蒸馏应用于前馈神经网络，只需一次重训练，就能将攻击成功率从95%降至0.5%。</p><p>目前有两种衡量神经网络鲁棒性的方法，一是试图证明一个下界，二是攻击模型，试图展示一个上界。第一种方法显然更加难以实践，但是如果攻击算法不够好，第二种方法得到的结果也没有说服力。</p><p>本文构建一系列攻击手段，可以用于构建鲁棒性上界。</p><p>本文还建议使用高置信度（high-confidence）的对抗性示例来评估模型的鲁棒性。</p><p>对抗样本有转移性，可以使用不安全的模型找出对抗样本，这些对抗样本在采取蒸馏防御的模型上同样有效。</p><p>换句话说，有效的防御手段必须破坏转移性</p><p>总的来说，本文的贡献有以下几点：</p><ol><li>引入了攻击手段，分别以$L_0$，$L_2$，$L_{infty}$作为距离指标。</li><li>调研了优化目标对生成对抗样本的影响。</li></ol><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>目前生成对抗样本的关键问题之一是添加distortion的程度。不同领域的distance metric肯定是不同的。</p><p>本文默认的攻击方法是白盒攻击，因为我们可以通过迁移的方法，将白盒生成的对抗样本迁移到目标样本的黑盒测试中。</p><p>神经网络的robustness定义为生成对抗样本的容易程度。</p><p>本文攻击的对象是以模型蒸馏为防御方法的模型。这种方法能够防御住迄今为止的大部分攻击手段。</p><p>本文攻击的模型皆为图像识别领域的m分类器，输出层为softmax层。这也就意味着输出向量维度为m，$y_i$即为样本分类为i的概率。</p><p>对抗攻击分为带目标的和不带目标的。本文将带目标攻击分成三种情况：</p><p>一般情况：在不是正确标签的标签中随机选择目标类别。</p><p>最佳情况：对所有不正确的类别进行攻击，并报告最难攻击的目标类别。</p><p>在所有评估中，我们执行所有三种类型的攻击：最佳情况，平均情况和最坏情况。</p><p>请注意，如果分类器在80％的时间中仅是准确的，则最佳案例攻击将需要在20％的案例中将其更改为0。</p><p>无论是L0,L2,Loo，没有距离度量是人类感知相似性的完美度量，并且我们没有准确判断哪个距离度量是最佳的。</p><p><strong>防御性蒸馏</strong>：以标准方式在训练数据上训练具有相同架构的网络。当我们在训练该网络时计算softmax时，将其替换为更平滑的softmax版本（将对数除以某个常数T）。</p><p>训练结束后，输入训练集得到模型推断的标签，收集这些标签并代替真实标签，再训练第二个网络。</p><p>这样做的理由是，提出蒸馏防御的人认为，对抗样本是模型过拟合的体现，那么避免过拟合可能会消除高维空间中的盲点。</p><p>但事实上蒸馏不会增加网络健壮性。</p><h1 id="攻击方法"><a href="#攻击方法" class="headerlink" title="攻击方法"></a>攻击方法</h1><h2 id="L-BFGS"><a href="#L-BFGS" class="headerlink" title="L-BFGS"></a>L-BFGS</h2><p>找对抗样本的过程建模为优化过程。<br><img src="/2020/06/27/【论文阅读笔记】Towards-Evaluating-the-Robustness-of-Neural-Networks/2020-06-27-23-41-04.png" srcset="/img/loading.gif" alt><br>loss函数可以取交叉熵。上式求解的是c，使用的距离是$L_2$</p><h2 id="FGSM"><a href="#FGSM" class="headerlink" title="FGSM"></a>FGSM</h2><p>用的是$L_{\infty}$</p><h2 id="JSMA"><a href="#JSMA" class="headerlink" title="JSMA"></a>JSMA</h2><p>Jacobian-based Saliency Map Attack</p><p>JSMA使用的不是softmax的输出，而是softmax的输入，换句话说就是前一层的输出。$L_0$</p><h2 id="DeepFool"><a href="#DeepFool" class="headerlink" title="DeepFool"></a>DeepFool</h2><p>$L_2$</p><h1 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h1><p><img src="/2020/06/27/【论文阅读笔记】Towards-Evaluating-the-Robustness-of-Neural-Networks/2020-06-27-23-54-11.png" srcset="/img/loading.gif" alt></p><p><img src="/2020/06/27/【论文阅读笔记】Towards-Evaluating-the-Robustness-of-Neural-Networks/2020-06-27-23-54-31.png" srcset="/img/loading.gif" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2017年的攻击文章，目标是当时防御性较高的&lt;strong&gt;防御性蒸馏&lt;/strong&gt;模型，属于图像分类模型。结果是成功令其鲁棒性下降。文章最后的结论是，做防御的人应该用较先进的攻击手段测试自己的模型，同时应该注意防御对抗样本的迁移特性，即在其他弱模型上生成样本后在目标模型上测试的行为。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://superlova.github.io/categories/paper/"/>
    
    
      <category term="Robustness" scheme="https://superlova.github.io/tags/Robustness/"/>
    
      <category term="Deep Neural Networks" scheme="https://superlova.github.io/tags/Deep-Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读笔记】Fuzzing: A Survey</title>
    <link href="https://superlova.github.io/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing-A-Survey/"/>
    <id>https://superlova.github.io/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing-A-Survey/</id>
    <published>2020-06-20T03:59:40.000Z</published>
    <updated>2020-06-26T17:04:36.951Z</updated>
    
    <content type="html"><![CDATA[<p>2018年CyberSecurity收录的一篇关于软件测试中模糊测试的综述。作者来自清华大学。文章名越短越霸气。<br><a id="more"></a></p><blockquote><p>警告！本文在写作过程中大量使用了谷歌翻译。</p></blockquote><h1 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h1><p>模糊测试几乎不需要了解目标，并且可以轻松扩展到大型应用程序，因此已成为最受欢迎的漏洞发现解决方案</p><p>模糊的随机性和盲目性导致发现错误的效率低下</p><p>反馈驱动的模糊模式（feedback-driven fuzzing mode）和遗传算法（genetic algorithms）的结合提供了更灵活和可自定义的模糊框架，并使模糊过程更加智能和高效。</p><h1 id="二、背景"><a href="#二、背景" class="headerlink" title="二、背景"></a>二、背景</h1><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/2020-06-26-22-37-23.png" srcset="/img/loading.gif" alt></p><h2 id="1-静态分析"><a href="#1-静态分析" class="headerlink" title="1.静态分析"></a>1.静态分析</h2><p>静态分析（Static Analysis）是对在没有实际执行程序的情况下执行的程序的分析。</p><p>通常对源代码执行静态分析，有时还对目标代码执行静态分析。通过分析词法，语法，语义特征以及数据流分析，模型检查，静态分析，可以检测到隐藏的错误。</p><p>静态分析的优点是检测速度快。</p><p>由于缺乏易于使用的漏洞检测模型，因此静态分析工具容易产生大量误报。因此，确定静态分析的结果仍然是一项艰巨的工作。</p><h2 id="2-动态分析"><a href="#2-动态分析" class="headerlink" title="2.动态分析"></a>2.动态分析</h2><p>动态分析（Dynamic Analysis）与静态分析相比，分析人员需要在实际系统或仿真器中执行目标程序。</p><p>通过监视运行状态并分析运行时知识，动态分析工具可以精确地检测程序错误。</p><p>动态分析的优点是精度高，缺点是速度慢，效率低，对测试人员的技术水平要求高，可扩展性差，并且难以进行大规模测试。</p><h2 id="3-符号执行"><a href="#3-符号执行" class="headerlink" title="3.符号执行"></a>3.符号执行</h2><p>符号执行（Symbolic Execution）是另一种发现漏洞的技术。</p><p>通过符号化程序输入，符号执行为每个执行路径维护了一组约束（constraint）。</p><p>执行之后，约束求解器（constraint solvers）将用于求解约束并确定导致执行的输入。</p><p>从技术上讲，符号执行可以覆盖程序中的任何执行路径，并且在小型程序的测试中已显示出良好的效果，但也存在许多限制。</p><p>首先，路径爆炸问题。随着程序规模的增长，执行状态会爆炸，这超出了约束求解器的求解能力。</p><p>第二，环境的相互作用。在符号执行中，当目标程序执行与符号执行环境之外的组件交互时，例如系统调用，处理信号等，可能会出现一致性问题。</p><p>因此符号执行仍然很难扩展到大型应用程序。</p><h2 id="4-模糊测试"><a href="#4-模糊测试" class="headerlink" title="4.模糊测试"></a>4.模糊测试</h2><p>模糊测试（Fuzzing）是目前最流行的漏洞发现技术。</p><p>从概念上讲，模糊测试从为目标应用程序生成大量正常和异常输入开始，并尝试通过将生成的输入提供给目标应用程序并监视执行状态来检测异常。</p><p>与其他技术相比，模糊测试易于部署并且具有良好的可扩展性和适用性，并且可以在有或没有源代码的情况下执行。</p><p>此外，由于模糊测试是在实际执行中执行的，因此它具有很高的准确性。</p><p>而且，模糊测试几乎不需要了解目标应用程序，并且可以轻松扩展到大型应用程序。</p><p>尽管模糊测试存在许多缺点，例如效率低和代码覆盖率低，但是，缺点却胜过缺点，模糊处理已成为当前最有效，最高效的最新漏洞发现技术。</p><h1 id="三、模糊测试介绍"><a href="#三、模糊测试介绍" class="headerlink" title="三、模糊测试介绍"></a>三、模糊测试介绍</h1><h2 id="1-模糊测试的工作过程"><a href="#1-模糊测试的工作过程" class="headerlink" title="1.模糊测试的工作过程"></a>1.模糊测试的工作过程</h2><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/2020-06-26-22-38-53.png" srcset="/img/loading.gif" alt></p><p>模糊测试包括四个主要阶段，即测试用例生成阶段，测试用例运行阶段，程序执行状态监视和异常分析。</p><p>模糊测试从生成一堆程序输入（即测试用例）开始。生成的测试用例的质量直接影响测试效果。</p><p>一方面，输入应尽可能满足测试程序对输入格式的要求。</p><p>另一方面，应充分破坏输入，以便对这些输入进行处理很可能会使程序失败。</p><p>根据目标程序，输入可以是具有不同文件格式的文件，网络通信数据，具有指定特征的可执行二进制文件等。</p><p>如何生成足够多的测试用例是Fuzzer面临的主要挑战。</p><p>在上一阶段生成测试用例后，将它们馈送到目标程序。</p><p>模糊测试器自动开始和完成目标程序的过程，并驱动目标程序的测试用例处理过程。</p><p>在执行之前，分析人员可以配置目标程序的启动和完成方式，并预定义参数和环境变量。</p><p>通常，模糊处理过程在预定义的超时、程序执行挂起或崩溃时停止。</p><p>模糊器在目标程序执行期间监视执行状态，以防异常和崩溃。</p><p>常用的异常监视方法包括监视特定的系统信号，崩溃和其他违规（violations）。</p><p>当捕获到违规时，模糊器将存储相应的测试用例，以供以后重播和分析。</p><p>在分析阶段，分析人员尝试确定捕获的违规的位置和根本原因。</p><p>自动崩溃分析是另一个重要的研究领域。</p><h2 id="2-模糊测试器的种类"><a href="#2-模糊测试器的种类" class="headerlink" title="2.模糊测试器的种类"></a>2.模糊测试器的种类</h2><h3 id="A-基于生成和基于突变"><a href="#A-基于生成和基于突变" class="headerlink" title="A.基于生成和基于突变"></a>A.基于生成和基于突变</h3><p>模糊器可以分为基于生成（generation based）和基于突变（mutation based）两类。</p><p>对于基于生成的模糊器，需要有关程序输入的知识。</p><p>对于文件格式模糊测试（file format fuzzing），通常会提供一个预定义文件格式的配置文件。测试用例根据配置文件生成。</p><p>有了给定的文件格式知识（file format knowledge），基于生成的模糊器生成的测试用例就可以更轻松地通过程序的验证，并且更有可能测试目标程序的更深层代码。</p><p>但是，如果没有友好的文档，分析文件格式将是一项艰巨的工作。</p><p>因此，基于变异的模糊器更容易启动并且更适用，并且被最新的模糊器广泛使用。</p><p>对于基于突变的模糊器，需要一组有效的初始输入（a set of valid initial inputs）。</p><p>测试用例是通过对初始输入和测试过程中产生的测试用例进行变异而生成的。</p><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/2020-06-26-22-43-30.png" srcset="/img/loading.gif" alt></p><h3 id="B-黑盒白盒灰盒"><a href="#B-黑盒白盒灰盒" class="headerlink" title="B.黑盒白盒灰盒"></a>B.黑盒白盒灰盒</h3><p>关于对程序源代码的依赖性（the dependence on program source<br>code）和程序分析的程度（the degree of program analysis），模糊器可以分为白盒，灰盒和黑盒（white box, gray box and black box）。</p><p>白盒模糊测试器可以访问程序的源代码，因此可以通过对源代码进行分析以及测试用例如何影响程序运行状态来收集更多信息。</p><p>黑盒模糊器会在不了解目标程序内部的情况下进行模糊测试。</p><p>灰箱模糊器也不使用源代码，但可以通过程序分析获得目标程序的内部信息。</p><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/2020-06-26-22-44-18.png" srcset="/img/loading.gif" alt></p><h3 id="C-定向模糊和基于覆盖的模糊"><a href="#C-定向模糊和基于覆盖的模糊" class="headerlink" title="C.定向模糊和基于覆盖的模糊"></a>C.定向模糊和基于覆盖的模糊</h3><p>根据探索程序的策略（the strategies of exploring the programs），模糊器可以分为定向模糊（directed fuzzing）和基于覆盖的模糊（coverage-based fuzzing）。</p><p>定向模糊器旨在生成覆盖目标代码和程序目标路径的测试用例（cover target code and target paths of programs），而基于覆盖率的模糊器旨在生成覆盖尽可能多的程序代码的测试用例（cover as much code of programs as possible）。</p><p>定向模糊器期望对程序进行更快的测试，而基于覆盖率的模糊器期望进行更彻底的测试并检测到尽可能多的错误。</p><p>对于定向模糊器和基于覆盖的模糊器，如何提取执行路径的信息是一个关键问题。</p><h3 id="D-哑模糊和智能模糊"><a href="#D-哑模糊和智能模糊" class="headerlink" title="D.哑模糊和智能模糊"></a>D.哑模糊和智能模糊</h3><p>根据对程序执行状态的监视和测试用例的生成之间是否存在反馈（whether there is a feedback between the monitoring of program execution state and testcase generation），模糊器可以分为哑类（dumb fuzz）和智能类（smart fuzz）。</p><p>智能模糊器会根据收集的信息（测试用例如何影响程序行为）来调整测试用例的生成。</p><p>对于基于变异的模糊测试器，反馈信息可用于确定应该对测试用例的哪一部分进行变异以及对它们进行变异的方式。</p><p>哑模糊测试器（Dumb fuzzers）具有更好的测试速度（speed），而智能模糊测试器可以生成更好的测试用例并获得更高的效率（efficiency）。</p><h2 id="3-模糊测试面临的挑战"><a href="#3-模糊测试面临的挑战" class="headerlink" title="3.模糊测试面临的挑战"></a>3.模糊测试面临的挑战</h2><h3 id="A-如何改变种子输入的挑战"><a href="#A-如何改变种子输入的挑战" class="headerlink" title="A.如何改变种子输入的挑战"></a>A.如何改变种子输入的挑战</h3><p>基于变异的模糊测试工具在进行变异时需要回答两个问题：（1）变异的位置，以及（2）变异的方式。</p><p>只有几个关键位置的突变会影响执行的控制流程。因此，如何在测试用例中定位这些关键位置非常重要。</p><p>此外，模糊器改变关键位置的方式是另一个关键问题，即如何确定可以将测试定向到程序有趣路径的值。</p><p>简而言之，测试用例的盲目突变会严重浪费测试资源，更好的变异策略可以显着提高模糊测试的效率。</p><h3 id="B-低代码覆盖率的挑战"><a href="#B-低代码覆盖率的挑战" class="headerlink" title="B.低代码覆盖率的挑战"></a>B.低代码覆盖率的挑战</h3><p>较高的代码覆盖率表示对程序执行状态的覆盖率更高，并且测试更加彻底。发现错误的可能性更高。</p><p>但是，大多数测试用例仅涵盖相同的少数路径。</p><p>因此，仅通过大量生成测试用例并投入测试资源来实现高覆盖率是不明智的选择。</p><p>基于覆盖率的模糊器试图借助程序分析技术（例如程序检测，program instrumentation）来解决问题。</p><h3 id="C-通过验证的挑战"><a href="#C-通过验证的挑战" class="headerlink" title="C.通过验证的挑战"></a>C.通过验证的挑战</h3><p>程序通常在解析和处理之前验证（Validation）输入。</p><p>验证可以保护程序，节省计算资源，并保护程序免受无效输入和恶意构造输入造成的损坏。</p><p>黑盒和灰盒模糊器生成的测试用例很难通过盲目生成策略（blind generation strategy）的验证，从而导致效率很低。</p><h1 id="四、覆盖引导的模糊测试"><a href="#四、覆盖引导的模糊测试" class="headerlink" title="四、覆盖引导的模糊测试"></a>四、覆盖引导的模糊测试</h1><p>为了实现深入而彻底的程序模糊测试，模糊测试人员应尝试<strong>遍历尽可能多的程序运行状态</strong>。</p><p>但是，由于程序行为的不确定性，因此<strong>没有用于程序状态的简单度量</strong>。</p><p>测量代码覆盖率成为一种近似的替代解决方案。代码覆盖率的增加代表了新的程序状态。此外，代码覆盖率很容易测量。</p><p>但是，代码覆盖率是一种近似的度量，因为在实践中，恒定的代码覆盖率并不表示恒定的程序状态数。使用此指标可能会导致某些信息丢失。</p><h2 id="1-代码覆盖率计数"><a href="#1-代码覆盖率计数" class="headerlink" title="1.代码覆盖率计数"></a>1.代码覆盖率计数</h2><p>在程序分析中，程序由基本块（basic blocks）组成。基本块是具有单个入口和出口点的代码段，基本块中的指令将顺序执行，并且只会执行一次。</p><p>在代码覆盖率测量中，最新技术将基本块作为最佳粒度（granularity）。原因包括：（1）基本块是程序执行的最小连贯单位；（2）测量功能或指令会导致信息丢失或冗余；（3）基本块可以由第一条指令的地址标识；以及基本块信息可通过代码检测轻松提取。</p><p>当前，有两种基于基本块的基本测量选择：简单地计算已执行的基本块和计算基本块的跃迁（transitions）。</p><p>在后一种方法中，程序被解释为图，顶点用于表示基本块，边用于表示基本块之间的跃迁。</p><p>后一种方法记录边缘，而前一种方法记录顶点。</p><p>但是实验表明仅对已执行的基本块进行计数将导致严重的信息丢失。</p><p>如图所示，如果首先执行程序路径（BB1，BB2，BB3，BB4），然后执行时遇到路径（BB1，BB2，BB4），则新边（BB2，BB4）信息为丢失。</p><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/2020-06-27-00-11-48.png" srcset="/img/loading.gif" alt></p><h2 id="2-基于覆盖的模糊测试的工作过程"><a href="#2-基于覆盖的模糊测试的工作过程" class="headerlink" title="2.基于覆盖的模糊测试的工作过程"></a>2.基于覆盖的模糊测试的工作过程</h2><p>算法1显示了基于覆盖的模糊器的一般工作过程。</p><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/alg1.png" srcset="/img/loading.gif" alt></p><p>测试从初始给定的种子输入开始。</p><p>在主模糊循环中，模糊器反复选择一个有趣的种子来进行后续的突变和测试用例的生成。然后在模糊器的监视下驱动目标程序以执行生成的测试用例。</p><p>收集触发崩溃的测试用例，并将其他有趣的用例添加到种子库中。</p><p>对于基于覆盖的模糊测试，达到新控制流<strong>边缘的测试用例</strong>被认为很有趣。</p><p>主模糊循环在预先配置的超时或中止信号时停止。</p><p>在模糊测试过程中，模糊器通过各种方法跟踪执行情况。</p><p>基本上，模糊器出于两个目的跟踪执行，即<strong>代码覆盖率</strong>和<strong>安全性违规</strong>（security violations）。</p><p>代码覆盖率信息用于进行彻底的程序状态探索，而安全违规跟踪则用于更好地发现错误。</p><p>下图显示了AFL（American Fuzzy Lop，一个非常有代表性的基于覆盖的模糊器）的工作过程。</p><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/2020-06-27-00-18-50.png" srcset="/img/loading.gif" alt></p><p>在执行覆盖范围收集之前，模糊器将对目标应用程序进行检测。</p><p>在主模糊循环中，提供初始种子输入后，（1）模糊器根据种子选择策略从种子库中选择喜欢的种子，比如AFL则选择最快和最小的种子。（2）根据变异策略对种子文件进行变异，并生成一堆测试用例。</p><p>AFL当前采用一些随机修改和测试用例拼接方法，包括长度和步长变化的顺序位翻转，小整数的顺序加法和减法以及已知有趣的整数（如0、1，INT_MAX等）的顺序插入。（3）测试用例已执行，并且执行情况正在跟踪中。</p><p>收集覆盖率信息以确定有趣的测试用例，即达到新控制流边缘的测试用例。</p><p>有趣的测试用例将添加到种子池中，以进行下一轮运行。</p><h2 id="3-关键问题"><a href="#3-关键问题" class="headerlink" title="3.关键问题"></a>3.关键问题</h2><p>前面的介绍表明，要运行一个高效的、基于覆盖的模糊，需要解决很多问题。最近一些研究如下表。</p><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/2020-06-27-00-22-32.png" srcset="/img/loading.gif" alt></p><h3 id="A-如何获得初始输入？"><a href="#A-如何获得初始输入？" class="headerlink" title="A.如何获得初始输入？"></a>A.如何获得初始输入？</h3><p>大多数最先进的基于覆盖的模糊器都采用了基于突变的测试用例生成策略，这在很大程度上取决于初始种子输入的质量。</p><p>良好的初始种子输入可以显著提高模糊的效率和效果。</p><p>具体来说，</p><p>(1)提供格式良好的种子输入可以节省构建一个种子输入所消耗的大量cpu时间；</p><p>(2)良好的初始输入可以满足复杂文件格式的要求，而复杂文件格式在突变阶段是很难猜测的；</p><p>(3)基于格式良好的种子输入的突变更容易产生测试用例，可以达到更深层次和难以达到的路径；</p><p>(4)良好的种子输入可以在多次测试中重复使用。</p><p>常用的收集种子输入的方法包括使用标准基准、从互联网上抓取和使用现有的POC（Proof of Concept）样本。</p><p>考虑到目标应用输入的多样性，从互联网上抓取是最直观的方法。</p><p>过量的种子输入会导致第一次干运行的时间浪费，从而带来另一个问题，即如何提炼初始语料。</p><p>AFL提供了一个工具，它可以提取一个最小的输入集，以达到相同的代码覆盖率。</p><h3 id="B-如何生成测试用例？"><a href="#B-如何生成测试用例？" class="headerlink" title="B.如何生成测试用例？"></a>B.如何生成测试用例？</h3><p>testcases的质量是影响模糊测试效率和效果的重要因素。</p><p>首先，好的testcases可以在较短的时间内探索更多的程序执行状态，覆盖更多的代码。</p><p>此外，好的测试用例可以针对潜在的脆弱位置，带来更快的程序bug发现。</p><p>因此，如何基于种子输入生成好的测试用例是一个重要的问题。</p><p>种子输入的突变涉及两个关键问题：在哪里突变和用什么值进行突变。</p><p>随着机器学习技术的发展和广泛应用，一些研究尝试使用机器学习技术来辅助生成testcases。</p><p>微软研究院的Godefroid等（2017）利用基于神经网络的统计机器学习技术来自动生成testcases。</p><p>具体来说，他们首先通过机器学习技术从一堆有效输入中学习输入格式，然后利用学习到的知识指导测试用例的生成。</p><p>他们介绍了微软Edge浏览器中PDF解析器的模糊过程。</p><p>虽然实验并没有给出一个令人鼓舞的结果，但仍是一个不错的尝试。</p><p>微软的Rajpal等人（2017）使用神经网络从过去的模糊探索中学习，并预测输入文件中哪些字节要突变。</p><p>Nichols等人（2017）使用生成对抗网络（GAN）模型来帮助用新颖的种子文件重新初始化系统。</p><p>实验表明，GAN比LSTM更快、更有效，并且有助于发现更多的代码路径。</p><h3 id="C-如何从种子池中选择种子？"><a href="#C-如何从种子池中选择种子？" class="headerlink" title="C.如何从种子池中选择种子？"></a>C.如何从种子池中选择种子？</h3><p>模糊器在主模糊循环中新一轮测试开始时，反复从种子池中选择种子进行突变。</p><p>以往的工作已经证明，良好的种子选择策略可以显著提高模糊效率，并帮助更快地找到更多的Bugs。</p><p>通过良好的种子选择策略，模糊器可以</p><ul><li>（1）优先选择更有帮助的种子，包括覆盖更多的代码，更容易触发漏洞；</li><li>（2）减少重复执行路径的浪费，节省计算资源；</li><li>（3）优化选择覆盖更深、更易受攻击的代码的种子，帮助更快地识别隐藏的漏洞。</li></ul><p>Böhme等人（2017）提出了AFLFast，一个基于覆盖的灰盒模糊器。在模糊过程中，AFLFast会测量执行路径的频率，优先处理被模糊次数较少的种子，并为行使低频路径的种子分配更多的能量。</p><p>Rawat等(2017)综合了静态和动态分析来识别难以到达的深层路径，并对到达深层路径的种子进行优先级排序。</p><p>AFLGo将一些易受攻击的代码定义为目标位置，并优化选择离目标位置较近的测试case。</p><p>已知漏洞的特征也可用于种子选择策略。然而，收集耗费资源的信息带来了沉重的开销，降低了模糊的效率。</p><h3 id="D-如何高效地测试应用程序？"><a href="#D-如何高效地测试应用程序？" class="headerlink" title="D.如何高效地测试应用程序？"></a>D.如何高效地测试应用程序？</h3><p>目标应用程序由主模糊循环中的模糊器反复启动和完成。</p><p>对于用户区应用程序的模糊处理，程序的创建和完成将消耗大量的cpu时间。频繁创建和完成该程序将严重降低模糊测试的效率。</p><p>AFL使用forkserver方法，该方法创建一个已加载程序的完全相同的克隆，并在每次运行时重复使用该克隆。</p><h1 id="五、模糊集成技术"><a href="#五、模糊集成技术" class="headerlink" title="五、模糊集成技术"></a>五、模糊集成技术</h1><p>使用随机突变方法的模糊模糊测试策略会导致大量无效测试用例，并且模糊测试效率较低。</p><p>当前，最先进的模糊器通常采用智能模糊策略。</p><p>智能模糊器通过程序分析技术来收集程序控制流和数据流信息，并因此利用收集到的信息来改进测试用例的生成。</p><p>由智能模糊器生成的测试用例具有更好的针对性，更有可能满足程序对数据结构和逻辑判断的要求。</p><p>下图描绘了智能模糊的示意图。</p><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/2020-06-27-00-36-55.png" srcset="/img/loading.gif" alt></p><p>表5中总结了模糊测试中集成的主要技术。</p><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/2020-06-27-00-37-51.png" srcset="/img/loading.gif" alt></p><h2 id="1-测试用例生成"><a href="#1-测试用例生成" class="headerlink" title="1.测试用例生成"></a>1.测试用例生成</h2><p>如前所述，模糊测试中的测试用例是基于生成的方法或基于变异的方法生成的。</p><p>如何生成满足复杂数据结构要求并更有可能触发难以到达的路径的测试用例是一个关键挑战。</p><p>在基于生成的模糊测试中，生成器根据输入数据格式的知识生成测试用例。</p><p>Work（Godefroid et al.2017）使用机器学习技术（特别是递归神经网络）来学习输入文件的语法，并因此使用所学的语法来生成满足格式要求的测试用例。</p><p>更多的最先进的模糊器采用基于变异的模糊策略。通过在突变过程中修改部分种子输入来生成测试用例。</p><p>在盲目的突变模糊化过程中，变异者使用随机值或几个特殊值随机修改种子字节，这被证明是效率很低的。因此，如何确定要修改的位置以及修改中使用的值是另一个关键挑战。</p><h2 id="2-程序执行"><a href="#2-程序执行" class="headerlink" title="2.程序执行"></a>2.程序执行</h2><p>执行阶段涉及的两个关键问题是<strong>如何指导模糊测试过程</strong>以及<strong>如何探索新路径</strong>。</p><p>仪表（Instrumentation）技术用于记录路径执行情况并计算基于覆盖率的模糊测试中的覆盖率信息。</p><p>测试执行过程中的另一个问题是探索新路径。符号执行技术在路径探索中具有天然的优势。</p><p>通过求解约束集，符号执行技术可以计算出满足特定条件要求的值。</p><p>反馈驱动的模糊测试提供了一种有效的引导测试方法，传统和新技术都可以发挥传感器的作用，以在测试执行过程中获取各种信息，并准确地指导模糊测试。</p><h1 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a>六、总结</h1><ul><li>反馈驱动的模糊模式（feedback-driven fuzzing mode）和遗传算法（genetic algorithms）的结合</li><li>测试用例生成阶段，测试用例运行阶段，程序执行状态监视和异常分析</li><li>根据探索程序的策略（the strategies of exploring the programs），模糊器可以分为定向模糊（directed fuzzing）和基于覆盖的模糊（coverage-based fuzzing）。</li><li>定向模糊器期望对程序进行更快的测试，而基于覆盖率的模糊器期望进行更彻底的测试并检测到尽可能多的错误。</li><li>对于定向模糊器和基于覆盖的模糊器，如何提取执行路径的信息是一个关键问题。</li><li>根据对程序执行状态的监视和测试用例的生成之间是否存在反馈（whether there is a feedback between the monitoring of program execution state and testcase generation），模糊器可以分为哑类（dumb fuzz）和智能类（smart fuzz）。</li><li>智能模糊器会根据收集的信息（测试用例如何影响程序行为）来调整测试用例的生成。</li><li>对于基于变异的模糊测试器，反馈信息可用于确定应该对测试用例的哪一部分进行变异以及对它们进行变异的方式。</li><li>基于变异的模糊测试工具在进行变异时需要回答两个问题：（1）变异的位置，以及（2）变异的方式。</li><li>覆盖率的定义。定义覆盖率时要防止信息丢失。代码覆盖率的增加代表了新的程序状态。此外，代码覆盖率很容易测量。边缘的测试用例被认为很有趣</li><li>通过验证的挑战。验证可以保护程序，节省计算资源，并保护程序免受无效输入和恶意构造输入造成的损坏。</li><li>一些研究尝试使用机器学习技术来辅助生成testcases。GAN比LSTM更快、更有效，并且有助于发现更多的代码路径。</li><li>如何从种子池中选择种子？</li><li>如何高效地测试应用程序？</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2018年CyberSecurity收录的一篇关于软件测试中模糊测试的综述。作者来自清华大学。文章名越短越霸气。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://superlova.github.io/categories/paper/"/>
    
    
      <category term="Fuzzing" scheme="https://superlova.github.io/tags/Fuzzing/"/>
    
      <category term="Software Testing" scheme="https://superlova.github.io/tags/Software-Testing/"/>
    
      <category term="Survey" scheme="https://superlova.github.io/tags/Survey/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】LSTM网络结构简介与对应的keras实现</title>
    <link href="https://superlova.github.io/2020/06/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91LSTM%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AF%B9%E5%BA%94%E7%9A%84keras%E5%AE%9E%E7%8E%B0/"/>
    <id>https://superlova.github.io/2020/06/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91LSTM%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AF%B9%E5%BA%94%E7%9A%84keras%E5%AE%9E%E7%8E%B0/</id>
    <published>2020-06-20T03:50:37.000Z</published>
    <updated>2020-06-26T12:39:35.089Z</updated>
    
    <content type="html"><![CDATA[<p>从理论和代码两个层面介绍了LSTM网络。<br><a id="more"></a></p><h2 id="一、理论来一波"><a href="#一、理论来一波" class="headerlink" title="一、理论来一波"></a>一、理论来一波</h2><p>循环神经网络（Recurrent Neural Network，RNN）是一类有短期记忆能力的神经网络。在循环神经网络中，神经元不但可以接受其他神经元的信息，也可以接受自身的信息，形成具有环路的网络结构。</p><p><img src="/2020/06/20/【学习笔记】LSTM网络结构简介与对应的keras实现/2020-06-20-11-51-21.png" srcset="/img/loading.gif" alt></p><p>长短期记忆网络（Long Short-Term Memory Network，LSTM）[Gers et al.,2000; Hochreiter et al., 1997] 是循环神经网络的一个变体，可以有效地解决简单循环神经网络的梯度爆炸或消失问题。</p><p><img src="/2020/06/20/【学习笔记】LSTM网络结构简介与对应的keras实现/2020-06-20-11-51-13.png" srcset="/img/loading.gif" alt></p><p>LSTM 网络引入一个新的内部状态（internal state） $\boldsymbol{c}_{t}\in \mathbb{R}^{\boldsymbol{D}}$ 专门进行线性的循环信息传递，同时（非线性地）输出信息给隐藏层的外部状态 $\boldsymbol{h}_{t}\in \mathbb{R}^{\boldsymbol{D}}$ 。这两个状态通过下式计算：</p><script type="math/tex; mode=display">\begin{array}{l}\boldsymbol{c}_{t}=\boldsymbol{f}_{t} \odot \boldsymbol{c}_{t-1}+\boldsymbol{i}_{t} \odot \tilde{\boldsymbol{c}}_{t} \\\boldsymbol{h}_{t}=\boldsymbol{o}_{t} \odot \tanh \left(\boldsymbol{c}_{t}\right)\end{array}</script><p>其中，$\odot$为向量逐元素乘积（代表左右两边向量维度相同）；$\boldsymbol{c}_{t-1}$为上一时刻的记忆单元；$\tilde{\boldsymbol{c}}\in \mathbb{R}^{\boldsymbol{D}}$是通过非线性函数得到的候选状态：</p><script type="math/tex; mode=display">\tilde{\boldsymbol{c}}_{t}=\tanh \left(\boldsymbol{W}_{c} \boldsymbol{x}_{t}+\boldsymbol{U}_{c} \boldsymbol{h}_{t-1}+\boldsymbol{b}_{c}\right)</script><p>在每个时刻 $t$，LSTM 网络的内部状态 $\boldsymbol{c}_{t}$ 记录了到当前时刻为止的历史信息。</p><p>LSTM内部多了三个gate，分别是forget、input、output。$\boldsymbol{f}_{t}\in [0,1]^{\boldsymbol{D}}$、$\boldsymbol{i}_{t}\in [0,1]^{\boldsymbol{D}}$、$\boldsymbol{o}_{t}\in [0,1]^{\boldsymbol{D}}$。这三个门与输入、隐状态和输出的维度应该相同，都是维度为输入序列维度n的向量（其实应该为n+1）$=D$。</p><p>与此同时，三个门的值依赖于$t$时刻的输入$x_t$、$t-1$时刻的隐变量$h_{t-1}$以及不同的权重矩阵($W_i$/$W_f$/$W_o$/$U_i$/$U_f$/$U_o$)。</p><p>门控机制（Gating Mechanism）是用来控制信息传递的路径的手段。</p><ul><li>遗忘门 $\boldsymbol{f}_{t}$ 控制上一个时刻的内部状态$\boldsymbol{c}_{t-1}$ 需要遗忘多少信息。</li><li>输入门 $\boldsymbol{i}_{t}$ 控制当前时刻的候选状态 ̃$\tilde{\boldsymbol{c}}_{t}$ 有多少信息需要保存。</li><li>输出门 $\boldsymbol{o}_{t}$ 控制当前时刻的内部状态 $\boldsymbol{c}_{t}$ 有多少信息需要输出给外部状态 $\boldsymbol{h}_{t}$。</li></ul><p>举个例子，当$\boldsymbol{f}_{t}=\mathbf{0}, \boldsymbol{i}_{t}=\mathbf{1}$时，记忆单元将历史信息清空，并将候选状态向量$\tilde{\boldsymbol{c}}_{t}$写入。但此时记忆单元 $\boldsymbol{c}_{t}$ 依然和上一时刻的历史信息相关。当$\boldsymbol{f}_{t}=\mathbf{1}, \boldsymbol{i}_{t}=\mathbf{0}$时，记忆单元将复制上一时刻的内容，不写入新的信息。</p><p>LSTM 网络中的“门”是一种“软”门，取值在 (0, 1) 之间，表示以一定的比例允许信息通过．三个门的计算方式为：</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{i}_{t} &=\sigma\left(\boldsymbol{W}_{i} \boldsymbol{x}_{t}+\boldsymbol{U}_{i} \boldsymbol{h}_{t-1}+\boldsymbol{b}_{i}\right) \\\boldsymbol{f}_{t} &=\sigma\left(\boldsymbol{W}_{f} \boldsymbol{x}_{t}+\boldsymbol{U}_{f} \boldsymbol{h}_{t-1}+\boldsymbol{b}_{f}\right) \\\boldsymbol{o}_{t} &=\sigma\left(\boldsymbol{W}_{o} \boldsymbol{x}_{t}+\boldsymbol{U}_{o} \boldsymbol{h}_{t-1}+\boldsymbol{b}_{o}\right)\end{aligned}</script><p>其中$\sigma(\cdot)$ 为 Logistic 函数，其输出区间为 (0, 1)；$\boldsymbol{x}_{t}$为当前时刻的输入。</p><h2 id="二、还是得看代码"><a href="#二、还是得看代码" class="headerlink" title="二、还是得看代码"></a>二、还是得看代码</h2><p>下面是我定义的一个专用于IMDb影评情感分析的二分类模型，包装在一个函数中。输入训练集、测试集及其标签，设定好参数就可以运行、训练。可以选择是否保存模型到本地。最后函数返回训练好的模型。</p><p>这个二分类模型中，输入是长度为80的整数列表（maxlen=80），代表着80个不同的单词构成的一句话。</p><p>如果有影评不够80个词，就在影评前面加足够的0，直到这条影评达到80个词为止。如果影评单词量大于80个，便截取前面的80个词。</p><p>每个整数都代表一个单词表中的单词。当然单词表的大小是固定的（num_words=10000个单词），如果出现不在单词表中的单词，固定将其编码成2，表示UNKNOWN（这条设置不在下面的代码中，属于数据预处理）。</p><p>第一层是Embedding层，负责将一句话中的每个单词映射成固定维度的词向量；</p><p>注意，每个单词（在这里是每个整数）都会变成固定维度（embedding_dim=128）的向量，因此每条影评从Embedding层输出后，都会变成80*128的矩阵。</p><p>第二层是LSTM层。如果你看了理论部分的叙述，就知道LSTM层中无论是隐状态$\boldsymbol{c}$、$\boldsymbol{h}$还是三个门$\boldsymbol{f}$、$\boldsymbol{i}$、$\boldsymbol{o}$，他们的维度都是$\boldsymbol{D}$。这个$\boldsymbol{D}$的大小就需要我们用参数<code>lstm_dim=32</code>来定义。这个参数越大，代表LSTM层的参数越多、泛化能力越强，也更难训练、更容易过拟合。</p><p>第三层是单个神经元的sigmoid层，在这里就直接转换成概率并分类了。</p><pre><code class="lang-python">def train_lstm(x_train, y_train, x_test, y_test,                num_words=10000,                maxlen=80,                embedding_dim=128,                lstm_dim=32,                batch_size=32,                epochs=10):    # 接收一个含有 100 个整数的序列，每个整数在 1 到 20000 之间    inputs = Input(shape=(maxlen,), dtype=&#39;int32&#39;, name=&#39;main_input&#39;)    # Embedding 层将输入序列编码为一个稠密向量的序列，    # 每个向量维度为 512。    x = Embedding(input_dim=num_words,                   input_length=maxlen,                   output_dim=embedding_dim,                   name=&#39;embedding&#39;)(inputs)    # LSTM 层把向量序列转换成单个向量，    # 它包含整个序列的上下文信息    lstm_output = LSTM(lstm_dim, name=&#39;lstm&#39;)(x)    # 插入辅助损失，    #使得即使在模型主损失很高的情况下，LSTM 层和 Embedding 层都能被平稳地训练    outputs = Dense(1, activation=&#39;sigmoid&#39;, name=&#39;output&#39;)(lstm_output)    model = Model(inputs=inputs, outputs=outputs)    model.compile(optimizer=&#39;adam&#39;,            loss=&#39;binary_crossentropy&#39;,            metrics=[&#39;accuracy&#39;])    model.fit(x_train, y_train,        batch_size=batch_size,        epochs=epochs,        validation_data=(x_test, y_test,))    score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)    print(&#39;Test score:&#39;, score)    print(&#39;Test accuracy:&#39;, acc)    # model.save(&quot;lstm_imdb.h5&quot;)    return model</code></pre><h2 id="三、LSTM返回所有时间步的hidden-state向量"><a href="#三、LSTM返回所有时间步的hidden-state向量" class="headerlink" title="三、LSTM返回所有时间步的hidden state向量"></a>三、LSTM返回所有时间步的hidden state向量</h2><p>数据经过LSTM层，输出的是最后一个时间步得到的output向量（即$\boldsymbol{h}_{finally}$），维度为$\boldsymbol{D}$。</p><p>其实LSTM能够在每个时间步都输出output（即$\boldsymbol{h}_{t}$），只不过我们把这些没到时间的半成品output选择性忽略了。</p><p>如果你想要堆叠LSTM层，也就是LSTM层下面还有LSTM，或者你<strong>需要所有时间步的</strong>$\boldsymbol{h}_{t}$，那么你可以在训练的时候把<code>return_sequences=True</code>写进LSTM参数之中。</p><p><img src="/2020/06/20/【学习笔记】LSTM网络结构简介与对应的keras实现/堆叠rnn.png" srcset="/img/loading.gif" alt></p><p>下面让我们来比较一下<code>return_sequences</code>参数开启之后输出值的变化。</p><h3 id="return-sequences-False"><a href="#return-sequences-False" class="headerlink" title="return_sequences=False"></a>return_sequences=False</h3><p>首先固定随机数种子。</p><pre><code class="lang-python">np.random.seed(0)tf.random.set_seed(0)</code></pre><p>然后构建输入Input向量和LSTM层，此时LSTM层使用默认参数<code>return_sequences=False</code>。</p><pre><code class="lang-python">input1 = Input(shape=(3,1)) # 输入是三维向量lstm1 = LSTM(1)(input1) # 内部hidden和cell的维度为1model = Model(inputs=input1, outputs=lstm1)</code></pre><p>构造一批输入，包括6个句子，每个句子三个单词，然后输入LSTM，查看LSTM层的输出。</p><pre><code class="lang-python">data = np.array([[0.1, 0.2, 0.3],                    [0.3, 0.2, 0.1],                    [0.2, 0.6, 0.3],                    [0.8, 0.2, 0.3],                    [0.3, 0.5, 0.1],                    [0.2, 0.6, 0.2]])print(model.predict(data))</code></pre><p>此时输出为：</p><pre><code class="lang-python">[[0.00844267] [0.00617958] [0.01279002] [0.01231858] [0.009055  ] [0.01108878]]Process finished with exit code 0</code></pre><h3 id="return-sequences-True"><a href="#return-sequences-True" class="headerlink" title="return_sequences=True"></a>return_sequences=True</h3><p>然后打开<code>return_sequences</code>的开关</p><pre><code class="lang-python">lstm1 = LSTM(1, return_sequences=True)(input1)</code></pre><p>此时的输出为：</p><pre><code>[[[0.00190693]  [0.00490441]  [0.00844267]] #  [[0.0055262 ]  [0.00704476]  [0.00617958]] # [[0.00374958]  [0.01259477]  [0.01279002]] # [[0.01337298]  [0.01142679]  [0.01231858]] # [[0.0055262 ]  [0.01206062]  [0.009055  ]] # [[0.00374958]  [0.01259477]  [0.01108878]]] #Process finished with exit code 0</code></pre><p>此为输出所有时间步的hidden state。鉴于一共6个测试输入，每个输入有3个feature，所以时间步也就三步。LSTM的输出结果从6个hidden state变成了6*3个hidden state。</p><h3 id="return-state-True"><a href="#return-state-True" class="headerlink" title="return_state=True"></a>return_state=True</h3><p>我们再来看另一个参数，这个参数能够控制LSTM输出cell state。</p><pre><code class="lang-python">lstm1 = LSTM(1, return_state=True)(input1)</code></pre><pre><code>[array([[0.00844267],       [0.00617958],       [0.01279002],       [0.01231858],       [0.009055  ],       [0.01108878]], dtype=float32), array([[0.00844267],       [0.00617958],       [0.01279002],       [0.01231858],       [0.009055  ],       [0.01108878]], dtype=float32), array([[0.01655067],       [0.01227413],       [0.02506882],       [0.02414548],       [0.01798305],       [0.02187706]], dtype=float32)]Process finished with exit code 0</code></pre><p>开启<code>return_state=True</code>之后，LSTM返回3个array，第一个array和第二个array一样，都是hidden state，和默认返回的一样。第三个array就是最后一个时间步的cell state。</p><h3 id="return-state-True-return-sequences-True"><a href="#return-state-True-return-sequences-True" class="headerlink" title="return_state=True, return_sequences=True"></a>return_state=True, return_sequences=True</h3><p>如果两个开关都打开，则结果变成</p><pre><code class="lang-python">lstm1 = LSTM(1, return_state=True, return_sequences=True)(input1)</code></pre><pre><code>[array([[[0.00190693],        [0.00490441],        [0.00844267]],       [[0.0055262 ],        [0.00704476],        [0.00617958]],       [[0.00374958],        [0.01259477],        [0.01279002]],       [[0.01337298],        [0.01142679],        [0.01231858]],       [[0.0055262 ],        [0.01206062],        [0.009055  ]],       [[0.00374958],        [0.01259477],        [0.01108878]]], dtype=float32), array([[0.00844267],       [0.00617958],       [0.01279002],       [0.01231858],       [0.009055  ],       [0.01108878]], dtype=float32), array([[0.01655067],       [0.01227413],       [0.02506882],       [0.02414548],       [0.01798305],       [0.02187706]], dtype=float32)]Process finished with exit code 0</code></pre><p>还是返回三个array，第一个是所有时间步的hidden state，这是开启<code>return_sequences=True</code>的效果；第二个则是原本LSTM的输出hidden state；第三个是开启<code>return_state=True</code>的效果，返回最后一个时间步的cell state</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;从理论和代码两个层面介绍了LSTM网络。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="RNN" scheme="https://superlova.github.io/tags/RNN/"/>
    
      <category term="LSTM" scheme="https://superlova.github.io/tags/LSTM/"/>
    
      <category term="Keras" scheme="https://superlova.github.io/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读笔记】Deep Text Classification Can be Fooled</title>
    <link href="https://superlova.github.io/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/"/>
    <id>https://superlova.github.io/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/</id>
    <published>2020-06-20T01:20:42.000Z</published>
    <updated>2020-06-23T12:41:07.656Z</updated>
    
    <content type="html"><![CDATA[<p>国内人民大学的一篇论文，被IJCAI-2018接收。主要研究文本领域的对抗样本生成，被测模型是文本分类领域的模型。<br><a id="more"></a></p><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>当前的对抗样本生成领域集中在图像的扰动和生成，文本领域少有涉及。</p><p><strong>文本对抗样本应当满足</strong>：</p><ul><li>使模型分类错误</li><li>与原样本相比，扰动难以察觉</li><li>实用性，即文本含义不发生改变</li></ul><p><strong>本文采用的生成思路</strong>：</p><ul><li>三种扰动策略：插入、修改和删除</li><li>自然语言水印</li><li>白盒+黑盒</li></ul><blockquote><p>自然语言水印（Natural Language Watermarking）是通过修改文本元素（如线条，文字或字符）的外观，或更改文本格式或字体（例如，通过-文字中的单词和字母间距）来嵌入信息的技术。</p></blockquote><h2 id="2-目标模型和数据集"><a href="#2-目标模型和数据集" class="headerlink" title="2. 目标模型和数据集"></a>2. 目标模型和数据集</h2><p><strong>字符级模型</strong>：</p><ul><li>以字母为单位，编码长度为字母表大小（26+空白符个数）</li><li>结构：6conv + 3fc</li><li>数据集：DBPedia，包含14个类别，56000训练、70000测试</li></ul><p><strong>单词级模型</strong>：</p><ul><li>以单词为单位，编码长度为单词表大小（数万到数十万）</li><li>结构：embedding + conv + maxpooling + fc(带dropout) + softmax</li><li>数据集：影评MR、产品评价CR、产品意见MPQA，都是二分类</li></ul><h2 id="3-白盒攻击"><a href="#3-白盒攻击" class="headerlink" title="3. 白盒攻击"></a>3. 白盒攻击</h2><p>所谓白盒攻击，就是在已知被测模型内部信息的情况下开展的攻击。由于已知信息较多，所以攻击起来也比较容易。</p><p>白盒攻击生成对抗样本的思想在图像领域用的比较多。比如利用网络参数和损失函数进行指导攻击过程的FGSM算法。</p><p>本文采用的白盒攻击手段，也是利用模型内部的参数和网络训练时的损失函数作为引导，但是不会直接生成对抗样本，而是<strong>首先识别对分类有重大贡献的文本项</strong>。</p><h3 id="HTP：Hot-Training-Phrases"><a href="#HTP：Hot-Training-Phrases" class="headerlink" title="HTP：Hot Training Phrases"></a>HTP：Hot Training Phrases</h3><p>以字符级模型为例，识别过程如下：</p><ol><li>输入训练样本x，计算cost gradients ∇x C(M, x, label)；</li><li>每个样本x选取令gradient最大的前50个字符定义为<strong>hot character</strong>；</li><li>包含3个及以上hot characters的单词定义为<strong>hot word</strong>；</li><li>相邻的hot words定义为<strong>hot phrase</strong>；</li><li>不同类别有不同的hot phrase，代表着该类对分类贡献最大的词组；</li><li>每个类别中最常出现的hot phrase定义为<strong>Hot Training Phrases (HTPs)</strong>。</li></ol><p>下图是DBPedia的Building类别文本数据集中的HTP词汇排名前10。<br><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/2020-06-23-20-11-30.png" srcset="/img/loading.gif" alt></p><h3 id="HSP：Hot-Sample-Phrases"><a href="#HSP：Hot-Sample-Phrases" class="headerlink" title="HSP：Hot Sample Phrases"></a>HSP：Hot Sample Phrases</h3><p>给定样本x，识别x中的hot phrase作为操作位置，该位置的单词或词组就定义为<strong>Hot Sample Phrases (HSPs)</strong>。</p><h3 id="Attacking-Character-level-DNN"><a href="#Attacking-Character-level-DNN" class="headerlink" title="Attacking Character-level DNN"></a>Attacking Character-level DNN</h3><h4 id="Insertion"><a href="#Insertion" class="headerlink" title="Insertion"></a><strong>Insertion</strong></h4><p>通过在样本的HSP位置插入以下内容实现变异操作：</p><ul><li>HTP；</li><li>可有可无的事实（文本水印算法生成）；</li><li>不伤害文本主语义的伪造事实（文本水印算法生成）。</li></ul><p>下面是三个通过插入红色文本造成标签改变的例子。<br><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/2020-06-23-20-11-43.png" srcset="/img/loading.gif" alt="Figure2"><br><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/2020-06-23-20-11-56.png" srcset="/img/loading.gif" alt="Figure3"><br><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Figure4.png" srcset="/img/loading.gif" alt="Figure4"></p><h4 id="Modification"><a href="#Modification" class="headerlink" title="Modification"></a><strong>Modification</strong></h4><p>对HSP稍加操作，比如typo-based watermarking technique（基于错别字的水印技术）：</p><ul><li>(1)替换以常见的错误拼写（需要有错别字语料库）</li><li>(2)替换以外观相似的字符</li></ul><p>下图是替换操作后生成对抗样本的一个例子。<br><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Figure5.png" srcset="/img/loading.gif" alt></p><p>下图是将film替换成flim后模型内部损失函数梯度的改变。<br><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Figure6.png" srcset="/img/loading.gif" alt></p><h4 id="Removal"><a href="#Removal" class="headerlink" title="Removal"></a><strong>Removal</strong></h4><p>删除HSP可降低模型对样本的confidence。只能删除HSPs中起辅助作用的词，要不然会改变本来的含义。</p><p>下面是通过删除来导致置信程度下降的例子。<br><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Figure7.png" srcset="/img/loading.gif" alt></p><h4 id="Combination"><a href="#Combination" class="headerlink" title="Combination"></a><strong>Combination</strong></h4><p>组合上述三种手法。</p><p><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Figure8.png" srcset="/img/loading.gif" alt></p><h3 id="Attacking-Word-level-DNN"><a href="#Attacking-Word-level-DNN" class="headerlink" title="Attacking Word-level DNN"></a>Attacking Word-level DNN</h3><p>单词级模型也是同理，不仅如此，甚至省了hot-character这一步。下面是几个例子。</p><p><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Figure9.png" srcset="/img/loading.gif" alt></p><p><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Figure10.png" srcset="/img/loading.gif" alt></p><h2 id="4-黑盒攻击"><a href="#4-黑盒攻击" class="headerlink" title="4. 黑盒攻击"></a>4. 黑盒攻击</h2><p>黑盒攻击显然不能通过比较Cost Gradient的方式确定HTP和HSP了。但是我们可以采用其他方法确定HTP和HSP。</p><p>具体地，我们通过生成一些测试样本来探测目标模型，判断哪些是Hot Phrases。</p><p>生成方法：</p><ul><li>用若干空格逐个代替单词（空格个数与单词长度相同）</li><li>将测试样本的分类结果与种子进行比较</li><li>偏差越大，相应单词对正确分类的重要性就越大</li><li>带来最大偏差的单词被标识为种子样本的HSP</li></ul><p><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Figure11.png" srcset="/img/loading.gif" alt></p><p>下面是黑盒攻击确定的HTP之后进行攻击的例子：</p><p><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Figure12.png" srcset="/img/loading.gif" alt></p><h2 id="5-Evaluation"><a href="#5-Evaluation" class="headerlink" title="5. Evaluation"></a>5. Evaluation</h2><h3 id="Q1-Can-our-method-perform-effective-source-target-misclassification-attack"><a href="#Q1-Can-our-method-perform-effective-source-target-misclassification-attack" class="headerlink" title="Q1: Can our method perform effective source/target misclassification attack?"></a>Q1: Can our method perform effective source/target misclassification attack?</h3><p>这个问题是问本方法能不能对模型实行定向的攻击，即“指哪打哪”，无论哪个类别的样本都能通过适当修改，突变成指定类别。</p><p><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Fig13.png" srcset="/img/loading.gif" alt></p><p><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Fig14.png" srcset="/img/loading.gif" alt></p><p>通过上表可知，source栏是源类别。target栏是目标类别，No栏是样本编号。本来都是类别source中定义的样本，经过右边三栏的突变方法，最终都以较高置信度被模型分类成了target栏中的类别。可以证明此方法确实能实现定向突变、定向攻击。</p><h3 id="Q2-Can-the-adversarial-samples-avoid-being-distinguished-by-human-observers-and-still-keep-the-utility"><a href="#Q2-Can-the-adversarial-samples-avoid-being-distinguished-by-human-observers-and-still-keep-the-utility" class="headerlink" title="Q2: Can the adversarial samples avoid being distinguished by human observers and still keep the utility?"></a>Q2: Can the adversarial samples avoid being distinguished by human observers and still keep the utility?</h3><p>这个问题是问对抗样本是不是能够避免被人类识别。毕竟文本突变还是很容易被人类识别的。</p><p>本论文是这么设计实验的：</p><ul><li>找23名学生，每个人都提供了20个文本样本，其中一半带有扰动，对每个样本进行手动分类</li><li>如果他们认为样品是人为修改的，则要求他们查明修改的位置</li><li>原样本准确率：94.2%</li><li>扰动样本准确率：94.8%</li><li>总共生成594个变化，有240个被受试者标记为已修改的位置，其中12个正确。准确率为12/240 = 5.0％，召回率为12/594 = 2.0％</li></ul><p>可以看到虽然样本数比较小，但是结果还是很显著的，在那23个同学都比较靠谱的前提下，该算法还是能够保证生成的文本与原文本差距不大的。</p><h3 id="Q3-Is-our-method-efficient-enough"><a href="#Q3-Is-our-method-efficient-enough" class="headerlink" title="Q3: Is our method efficient enough?"></a>Q3: Is our method efficient enough?</h3><p>算法效率其实不是很重要，毕竟在实践中，攻击者往往愿意花费更多时间来制作理想的对抗性样本。</p><p>白盒攻击（计算梯度、确定HTP），总共116小时，平均每类8.29小时；<br>黑盒攻击（生成样本、确定HTP），总共107小时，平均每类7.63小时。</p><h3 id="Q4-White-box-and-black-box-which-is-more-powerful"><a href="#Q4-White-box-and-black-box-which-is-more-powerful" class="headerlink" title="Q4: White-box and black-box, which is more powerful?"></a>Q4: White-box and black-box, which is more powerful?</h3><p>两种方式都有效并且彼此互补。</p><p>下图分别是黑盒和白盒生成的HTP比较，可以看到都是比较类似的。</p><p><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Fig15.png" srcset="/img/loading.gif" alt></p><h2 id="6-读后感"><a href="#6-读后感" class="headerlink" title="6. 读后感"></a>6. 读后感</h2><p>其实本篇文章的思想并不复杂，核心是确定一段文本的HTP和HSP。所谓HTP可以认为是，模型一看到这种词就相信这句话是该类别的了。那如果把类别1的句子x中的HSP给替换成类别2的HTP，的确可能让模型以为句子x是类别2的句子了。</p><p>所以延伸出来一个方向，那就是确定HSP和HTP的方法上。对于白盒攻击，还是查看内部的信息，然后计算梯度，这是一种比较传统的方法。对于黑盒攻击，则是遍历所有可能删除的单词，从结果上来看，比较这些删除单词的重要程度。</p><p>所以说有没有其他方法能够衡量单词的重要程度？这是一个值得研究的方向。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;国内人民大学的一篇论文，被IJCAI-2018接收。主要研究文本领域的对抗样本生成，被测模型是文本分类领域的模型。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://superlova.github.io/categories/paper/"/>
    
    
      <category term="NLP" scheme="https://superlova.github.io/tags/NLP/"/>
    
      <category term="testing" scheme="https://superlova.github.io/tags/testing/"/>
    
      <category term="DNN" scheme="https://superlova.github.io/tags/DNN/"/>
    
  </entry>
  
</feed>
