<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Superlova</title>
  
  <subtitle>Be a better man...</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://superlova.github.io/"/>
  <updated>2020-05-20T09:14:04.102Z</updated>
  <id>https://superlova.github.io/</id>
  
  <author>
    <name>Superlova</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解</title>
    <link href="https://superlova.github.io/2020/05/20/Datawhale%E5%B0%8F%E7%BB%84%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%E4%BB%BB%E5%8A%A1%E2%80%94%E2%80%94Task01%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/"/>
    <id>https://superlova.github.io/2020/05/20/Datawhale%E5%B0%8F%E7%BB%84%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%E4%BB%BB%E5%8A%A1%E2%80%94%E2%80%94Task01%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/</id>
    <published>2020-05-20T08:38:22.000Z</published>
    <updated>2020-05-20T09:14:04.102Z</updated>
    
    <content type="html"><![CDATA[<p>本次新人赛是Datawhale与天池联合发起的0基础入门系列赛事第二场 —— 零基础入门CV之街景字符识别比赛。<br><a id="more"></a></p><h1 id="Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解"><a href="#Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解" class="headerlink" title="Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解"></a>Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解</h1><h2 id="0-学习目标"><a href="#0-学习目标" class="headerlink" title="0. 学习目标"></a>0. 学习目标</h2><ul><li>理解赛题背景和赛题数据</li><li>完成赛题报名和数据下载，理解赛题的解题思路</li><li>了解赛题</li></ul><h2 id="1-大赛简介"><a href="#1-大赛简介" class="headerlink" title="1. 大赛简介"></a>1. 大赛简介</h2><p>本次新人赛是Datawhale与天池联合发起的0基础入门系列赛事第二场 —— 零基础入门CV之街景字符识别比赛。</p><h3 id="1-1-赛题数据介绍"><a href="#1-1-赛题数据介绍" class="headerlink" title="1.1 赛题数据介绍"></a>1.1 赛题数据介绍</h3><p>赛题来源自Google街景图像中的门牌号数据集（The Street View House Numbers Dataset, SVHN），该数据来自真实场景的门牌号。</p><p>训练集数据包括3W张照片，验证集数据包括1W张照片，每张照片包括颜色图像和对应的编码类别和具体位置</p><h3 id="1-2-参赛规则"><a href="#1-2-参赛规则" class="headerlink" title="1.2 参赛规则"></a>1.2 参赛规则</h3><ul><li>比赛允许使用CIFAR-10和ImageNet数据集的预训练模型，不允许使用其他任何预训练模型和任何外部数据；</li><li>报名成功后，选手下载数据，在本地调试算法，提交结果；</li><li>提交后将进行实时评测；每天排行榜更新时间为12:00和20:00，按照评测指标得分从高到低排序；排行榜将选择历史最优成绩进行展示。</li></ul><h3 id="1-3-数据集简介"><a href="#1-3-数据集简介" class="headerlink" title="1.3 数据集简介"></a>1.3 数据集简介</h3><p>所有的数据（训练集、验证集和测试集）的标注使用JSON格式，并使用文件名进行索引。</p><div class="table-container"><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td>top</td><td>左上角坐标X</td><td></td></tr><tr><td>height</td><td>字符高度</td><td></td></tr><tr><td>left</td><td>左上角最表Y</td><td></td></tr><tr><td>width</td><td>字符宽度</td><td></td></tr><tr><td>label</td><td>字符编码</td><td></td></tr></tbody></table></div><p>字符的坐标具体如下所示：<br><img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/字符坐标.png" srcset="/img/loading.gif" alt="坐标">  </p><p>在比赛数据（训练集和验证集）中，同一张图片中可能包括一个或者多个字符，因此在比赛数据的JSON标注中，会有两个字符的边框信息：<br>|原始图片|图片JSON标注|<br>|——|——-|<br><img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/原始图片.png" srcset="/img/loading.gif" alt="19">    | <img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/原始图片标注.png" srcset="/img/loading.gif" alt="标注">  |</p><h3 id="1-4-成绩评定方式"><a href="#1-4-成绩评定方式" class="headerlink" title="1.4 成绩评定方式"></a>1.4 成绩评定方式</h3><p>评价标准为准确率。<br>选手提交结果与实际图片的编码进行对比，以编码整体识别准确率为评价指标，结果越大越好，具体计算公式如下：</p><p> Score=编码识别正确的数量/测试集图片数量   </p><h3 id="1-5-结果提交格式"><a href="#1-5-结果提交格式" class="headerlink" title="1.5 结果提交格式"></a>1.5 结果提交格式</h3><p>提交前请确保预测结果的格式与sample_submit.csv中的格式一致，以及提交文件后缀名为csv。<br>形式如下：<br>file_name, file_code<br>0010000.jpg,451<br>0010001.jpg,232<br>0010002.jpg,45<br>0010003.jpg,67<br>0010004.jpg,191<br>0010005.jpg,892 </p><h2 id="2-数据读取"><a href="#2-数据读取" class="headerlink" title="2. 数据读取"></a>2. 数据读取</h2><p>JSON中标签的读取方式：  </p><pre><code class="lang-python">import jsontrain_json = json.load(open(&#39;../input/train.json&#39;))# 数据标注处理def parse_json(d):    arr = np.array([        d[&#39;top&#39;], d[&#39;height&#39;], d[&#39;left&#39;],  d[&#39;width&#39;], d[&#39;label&#39;]    ])    arr = arr.astype(int)    return arrimg = cv2.imread(&#39;../input/train/000000.png&#39;)arr = parse_json(train_json[&#39;000000.png&#39;])plt.figure(figsize=(10, 10))plt.subplot(1, arr.shape[1]+1, 1)plt.imshow(img)plt.xticks([]); plt.yticks([])for idx in range(arr.shape[1]):    plt.subplot(1, arr.shape[1]+1, idx+2)    plt.imshow(img[arr[0, idx]:arr[0, idx]+arr[1, idx],arr[2, idx]:arr[2, idx]+arr[3, idx]])    plt.title(arr[4, idx])    plt.xticks([]); plt.yticks([])</code></pre><p><img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/19.png" srcset="/img/loading.gif" alt="19"></p><h2 id="3-解题思路"><a href="#3-解题思路" class="headerlink" title="3. 解题思路"></a>3. 解题思路</h2><p>赛题思路分析：赛题本质是分类问题，需要对图片的字符进行识别。但赛题给定的数据图片中不同图片中包含的字符数量不等，如下图所示。有的图片的字符个数为2，有的图片字符个数为3，有的图片字符个数为4。 </p><div class="table-container"><table><thead><tr><th>字符属性</th><th>图片</th></tr></thead><tbody><tr><td>字符：42   字符个数：2</td><td><img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/42.png" srcset="/img/loading.gif" alt="标注"></td></tr><tr><td>字符：241   字符个数：3</td><td><img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/2411.png" srcset="/img/loading.gif" alt="标注"></td></tr><tr><td>字符：7358   字符个数：4</td><td><img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/7358.png" srcset="/img/loading.gif" alt="标注"></td></tr></tbody></table></div><p>因此本次赛题的难点是需要对不定长的字符进行识别，与传统的图像分类任务有所不同。为了降低参赛难度，我们提供了一些解题思路供大家参考：</p><ul><li>简单入门思路：定长字符识别    </li></ul><p>可以将赛题抽象为一个定长字符识别问题，在赛题数据集中大部分图像中字符个数为2-4个，最多的字符    个数为6个。<br>因此可以对于所有的图像都抽象为6个字符的识别问题，字符23填充为23XXXX，字符231填充为231XXX。<br><img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/23xxxxxx.png" srcset="/img/loading.gif" alt="标注">   </p><p>经过填充之后，原始的赛题可以简化了6个字符的分类问题。在每个字符的分类中会进行11个类别的分类，假如分类为填充字符，则表明该字符为空。    </p><ul><li>专业字符识别思路：不定长字符识别 </li></ul><p><img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/不定长字符识别.png" srcset="/img/loading.gif" alt="标注"> </p><p>在字符识别研究中，有特定的方法来解决此种不定长的字符识别问题，比较典型的有CRNN字符识别模型。<br>在本次赛题中给定的图像数据都比较规整，可以视为一个单词或者一个句子。   </p><ul><li>专业分类思路：检测再识别</li></ul><p>在赛题数据中已经给出了训练集、验证集中所有图片中字符的位置，因此可以首先将字符的位置进行识别，利用物体检测的思路完成。   </p><p><img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/检测.png" srcset="/img/loading.gif" alt="IMG"> </p><p>此种思路需要参赛选手构建字符检测模型，对测试集中的字符进行识别。选手可以参考物体检测模型SSD或者YOLO来完成。    </p><h2 id="4-Baseline思路：将不定长字符转换为定长字符的识别问题，并使用CNN完成训练和验证"><a href="#4-Baseline思路：将不定长字符转换为定长字符的识别问题，并使用CNN完成训练和验证" class="headerlink" title="4. Baseline思路：将不定长字符转换为定长字符的识别问题，并使用CNN完成训练和验证"></a>4. Baseline思路：将不定长字符转换为定长字符的识别问题，并使用CNN完成训练和验证</h2><h3 id="4-1-步骤"><a href="#4-1-步骤" class="headerlink" title="4.1 步骤"></a>4.1 步骤</h3><ul><li>赛题数据读取（封装为Pytorch的Dataset和DataLoder）</li><li>构建CNN模型（使用Pytorch搭建）</li><li>模型训练与验证</li><li>模型结果预测</li></ul><h3 id="4-2-运行环境及安装示例"><a href="#4-2-运行环境及安装示例" class="headerlink" title="4.2 运行环境及安装示例"></a>4.2 运行环境及安装示例</h3><ul><li>运行环境要求：Python2/3，Pytorch1.x，内存4G，有无GPU都可以。         </li></ul><p>下面给出python3.7+ torch1.3.1gpu版本的环境安装示例：      </p><ul><li><p>首先在Anaconda中创建一个专门用于本次天池练习赛的虚拟环境。          </p><blockquote><p>$conda create -n py37_torch131 python=3.7      </p></blockquote></li><li><p>激活环境，并安装pytorch1.3.1                                     </p><blockquote><p>$source activate py37_torch131<br>$conda install pytorch=1.3.1 torchvision cudatoolkit=10.0                     </p></blockquote></li><li><p>通过下面的命令一键安装所需其它依赖库     </p><blockquote><p>$pip install jupyter tqdm opencv-python matplotlib pandas                                  </p></blockquote></li><li><p>启动notebook，即可开始baseline代码的学习                  </p><blockquote><p>$jupyter-notebook   </p></blockquote></li><li><p>假设所有的赛题输入文件放在../input/目录下，首先导入常用的包：<br>```python<br>import os, sys, glob, shutil, json<br>os.environ[“CUDA_VISIBLE_DEVICES”] = ‘0’<br>import cv2</p></li></ul><p>from PIL import Image<br>import numpy as np</p><p>from tqdm import tqdm, tqdm_notebook</p><p>import torch<br>torch.manual_seed(0)<br>torch.backends.cudnn.deterministic = False<br>torch.backends.cudnn.benchmark = True</p><p>import torchvision.models as models<br>import torchvision.transforms as transforms<br>import torchvision.datasets as datasets<br>import torch.nn as nn<br>import torch.nn.functional as F<br>import torch.optim as optim<br>from torch.autograd import Variable<br>from torch.utils.data.dataset import Dataset</p><pre><code>#### 步骤1：定义好读取图像的Dataset```pythonclass SVHNDataset(Dataset):    def __init__(self, img_path, img_label, transform=None):        self.img_path = img_path        self.img_label = img_label         if transform is not None:            self.transform = transform        else:            self.transform = None    def __getitem__(self, index):        img = Image.open(self.img_path[index]).convert(&#39;RGB&#39;)        if self.transform is not None:            img = self.transform(img)        # 设置最长的字符长度为5个        lbl = np.array(self.img_label[index], dtype=np.int)        lbl = list(lbl)  + (5 - len(lbl)) * [10]        return img, torch.from_numpy(np.array(lbl[:5]))    def __len__(self):        return len(self.img_path)</code></pre><h4 id="步骤2：定义好训练数据和验证数据的Dataset"><a href="#步骤2：定义好训练数据和验证数据的Dataset" class="headerlink" title="步骤2：定义好训练数据和验证数据的Dataset"></a>步骤2：定义好训练数据和验证数据的Dataset</h4><pre><code class="lang-python">train_path = glob.glob(&#39;../input/train/*.png&#39;)train_path.sort()train_json = json.load(open(&#39;../input/train.json&#39;))train_label = [train_json[x][&#39;label&#39;] for x in train_json]print(len(train_path), len(train_label))train_loader = torch.utils.data.DataLoader(    SVHNDataset(train_path, train_label,                transforms.Compose([                    transforms.Resize((64, 128)),                    transforms.RandomCrop((60, 120)),                    transforms.ColorJitter(0.3, 0.3, 0.2),                    transforms.RandomRotation(5),                    transforms.ToTensor(),                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])    ])),     batch_size=40,     shuffle=True,     num_workers=10,)val_path = glob.glob(&#39;../input/val/*.png&#39;)val_path.sort()val_json = json.load(open(&#39;../input/val.json&#39;))val_label = [val_json[x][&#39;label&#39;] for x in val_json]print(len(val_path), len(val_label))val_loader = torch.utils.data.DataLoader(    SVHNDataset(val_path, val_label,                transforms.Compose([                    transforms.Resize((60, 120)),                    # transforms.ColorJitter(0.3, 0.3, 0.2),                    # transforms.RandomRotation(5),                    transforms.ToTensor(),                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])    ])),     batch_size=40,     shuffle=False,     num_workers=10,)</code></pre><h4 id="步骤3：定义好字符分类模型，使用renset18的模型作为特征提取模块"><a href="#步骤3：定义好字符分类模型，使用renset18的模型作为特征提取模块" class="headerlink" title="步骤3：定义好字符分类模型，使用renset18的模型作为特征提取模块"></a>步骤3：定义好字符分类模型，使用renset18的模型作为特征提取模块</h4><pre><code class="lang-python">class SVHN_Model1(nn.Module):    def __init__(self):        super(SVHN_Model1, self).__init__()        model_conv = models.resnet18(pretrained=True)        model_conv.avgpool = nn.AdaptiveAvgPool2d(1)        model_conv = nn.Sequential(*list(model_conv.children())[:-1])        self.cnn = model_conv        self.fc1 = nn.Linear(512, 11)        self.fc2 = nn.Linear(512, 11)        self.fc3 = nn.Linear(512, 11)        self.fc4 = nn.Linear(512, 11)        self.fc5 = nn.Linear(512, 11)    def forward(self, img):                feat = self.cnn(img)        # print(feat.shape)        feat = feat.view(feat.shape[0], -1)        c1 = self.fc1(feat)        c2 = self.fc2(feat)        c3 = self.fc3(feat)        c4 = self.fc4(feat)        c5 = self.fc5(feat)        return c1, c2, c3, c4, c5</code></pre><h4 id="步骤4：定义好训练、验证和预测模块"><a href="#步骤4：定义好训练、验证和预测模块" class="headerlink" title="步骤4：定义好训练、验证和预测模块"></a>步骤4：定义好训练、验证和预测模块</h4><pre><code class="lang-python">def train(train_loader, model, criterion, optimizer):    # 切换模型为训练模式    model.train()    train_loss = []    for i, (input, target) in enumerate(train_loader):        if use_cuda:            input = input.cuda()            target = target.cuda()        c0, c1, c2, c3, c4 = model(input)        loss = criterion(c0, target[:, 0]) + \                criterion(c1, target[:, 1]) + \                criterion(c2, target[:, 2]) + \                criterion(c3, target[:, 3]) + \                criterion(c4, target[:, 4])        # loss /= 6        optimizer.zero_grad()        loss.backward()        optimizer.step()        if i % 100 == 0:            print(loss.item())        train_loss.append(loss.item())    return np.mean(train_loss)def validate(val_loader, model, criterion):    # 切换模型为预测模型    model.eval()    val_loss = []    # 不记录模型梯度信息    with torch.no_grad():        for i, (input, target) in enumerate(val_loader):            if use_cuda:                input = input.cuda()                target = target.cuda()            c0, c1, c2, c3, c4 = model(input)            loss = criterion(c0, target[:, 0]) + \                    criterion(c1, target[:, 1]) + \                    criterion(c2, target[:, 2]) + \                    criterion(c3, target[:, 3]) + \                    criterion(c4, target[:, 4])            # loss /= 6            val_loss.append(loss.item())    return np.mean(val_loss)def predict(test_loader, model, tta=10):    model.eval()    test_pred_tta = None    # TTA 次数    for _ in range(tta):        test_pred = []        with torch.no_grad():            for i, (input, target) in enumerate(test_loader):                if use_cuda:                    input = input.cuda()                c0, c1, c2, c3, c4 = model(input)                output = np.concatenate([                    c0.data.numpy(),                     c1.data.numpy(),                    c2.data.numpy(),                     c3.data.numpy(),                    c4.data.numpy()], axis=1)                test_pred.append(output)        test_pred = np.vstack(test_pred)        if test_pred_tta is None:            test_pred_tta = test_pred        else:            test_pred_tta += test_pred    return test_pred_tta</code></pre><h4 id="步骤5：迭代训练和验证模型"><a href="#步骤5：迭代训练和验证模型" class="headerlink" title="步骤5：迭代训练和验证模型"></a>步骤5：迭代训练和验证模型</h4><pre><code class="lang-python">model = SVHN_Model1()criterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), 0.001)best_loss = 1000.0use_cuda = Falseif use_cuda:    model = model.cuda()for epoch in range(2):    train_loss = train(train_loader, model, criterion, optimizer, epoch)    val_loss = validate(val_loader, model, criterion)    val_label = [&#39;&#39;.join(map(str, x)) for x in val_loader.dataset.img_label]    val_predict_label = predict(val_loader, model, 1)    val_predict_label = np.vstack([        val_predict_label[:, :11].argmax(1),        val_predict_label[:, 11:22].argmax(1),        val_predict_label[:, 22:33].argmax(1),        val_predict_label[:, 33:44].argmax(1),        val_predict_label[:, 44:55].argmax(1),    ]).T    val_label_pred = []    for x in val_predict_label:        val_label_pred.append(&#39;&#39;.join(map(str, x[x!=10])))    val_char_acc = np.mean(np.array(val_label_pred) == np.array(val_label))    print(&#39;Epoch: {0}, Train loss: {1} \t Val loss: {2}&#39;.format(epoch, train_loss, val_loss))    print(val_char_acc)    # 记录下验证集精度    if val_loss &lt; best_loss:        best_loss = val_loss        torch.save(model.state_dict(), &#39;./model.pt&#39;)</code></pre><p>训练两个2 Epoch后，输出的训练日志为：</p><p>Epoch: 0, Train loss: 3.1      Val loss: 3.4 验证集精度：0.3439<br>Epoch: 1, Train loss: 2.1      Val loss: 2.9 验证集精度：0.4346     </p><h4 id="步骤6：对测试集样本进行预测，生成提交文件"><a href="#步骤6：对测试集样本进行预测，生成提交文件" class="headerlink" title="步骤6：对测试集样本进行预测，生成提交文件"></a>步骤6：对测试集样本进行预测，生成提交文件</h4><pre><code class="lang-python">test_path = glob.glob(&#39;../input/test_a/*.png&#39;)test_path.sort()test_label = [[1]] * len(test_path)print(len(val_path), len(val_label))test_loader = torch.utils.data.DataLoader(    SVHNDataset(test_path, test_label,                transforms.Compose([                    transforms.Resize((64, 128)),                    transforms.RandomCrop((60, 120)),                    # transforms.ColorJitter(0.3, 0.3, 0.2),                    # transforms.RandomRotation(5),                    transforms.ToTensor(),                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])    ])),     batch_size=40,     shuffle=False,     num_workers=10,)test_predict_label = predict(test_loader, model, 1)test_label = [&#39;&#39;.join(map(str, x)) for x in test_loader.dataset.img_label]test_predict_label = np.vstack([    test_predict_label[:, :11].argmax(1),    test_predict_label[:, 11:22].argmax(1),    test_predict_label[:, 22:33].argmax(1),    test_predict_label[:, 33:44].argmax(1),    test_predict_label[:, 44:55].argmax(1),]).Ttest_label_pred = []for x in test_predict_label:    test_label_pred.append(&#39;&#39;.join(map(str, x[x!=10])))import pandas as pddf_submit = pd.read_csv(&#39;../input/test_A_sample_submit.csv&#39;)df_submit[&#39;file_code&#39;] = test_label_preddf_submit.to_csv(&#39;renset18.csv&#39;, index=None)</code></pre><p><strong>在训练完成2个Epoch后，模型在测试集上的成绩应该在0.33左右。</strong>    </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次新人赛是Datawhale与天池联合发起的0基础入门系列赛事第二场 —— 零基础入门CV之街景字符识别比赛。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="datawhale" scheme="https://superlova.github.io/tags/datawhale/"/>
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>numpy中的einsum使用方法</title>
    <link href="https://superlova.github.io/2020/05/19/numpy%E4%B8%AD%E7%9A%84einsum%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>https://superlova.github.io/2020/05/19/numpy%E4%B8%AD%E7%9A%84einsum%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</id>
    <published>2020-05-19T06:52:52.000Z</published>
    <updated>2020-05-20T02:48:40.857Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍爱因斯坦求和约定，以及在numpy中的使用<br><a id="more"></a></p><p>numpy里面有很多奇技淫巧，爱因斯坦求和约定就是其中之一。</p><p>爱因斯坦求和约定能够很方便和简介地表示点积、外积、转置、矩阵-向量乘法、矩阵-矩阵乘法等，这在深度学习公式推导中的用处很大。</p><p>其实我不认为einsum在numpy中用处很大，我认为其顶多就是一种统一的矩阵运算写法罢了。这种技巧，是在牺牲可读性基础上，对代码的简化。而且由于numpy对其他运算也有进行优化，所以仅凭借爱因斯坦乘数法还不一定能提升代码执行效率。</p><p>可能是我还没有体会到高维张量相互计算时的痛苦吧。</p><p>先看一下einsum的api：</p><pre><code class="lang-python">np.einsum(equation, *arr)</code></pre><p>最开始需要一个字符串，用以描述想要完成的计算。后面是计算需要的操作数，也就是你的矩阵等。</p><p>来看具体的例子：</p><h3 id="对于向量"><a href="#对于向量" class="headerlink" title="对于向量"></a>对于向量</h3><pre><code class="lang-python">arr1 = np.arange(5) # 0,1,2,3,4arr2 = np.arange(5) # 0,1,2,3,4</code></pre><ol><li>计算向量所有分量的和，即<code>np.sum(arr)</code>。如何利用einsum完成？</li></ol><pre><code class="lang-python">np.einsum(&quot;i-&gt;&quot;, arr) # 10</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c = \sum_{i} a_i,\quad i = 1, 2, \dots</script><ol><li>计算两向量内积，即<code>np.dot(arr1, arr2)</code>或<code>np.inner(arr1, arr2)</code></li></ol><pre><code class="lang-python"># 0*0 + 1*1 + 2*2 + 3*3 + 4*4np.einsum(&quot;i,i-&gt;&quot;, arr1, arr2) # 30</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c = \sum_{i} a_i \times b_i,\quad i = 1, 2, \dots</script><ol><li>计算两向量逐元素乘积，即<code>arr1 * arr2</code></li></ol><pre><code class="lang-python">np.einsum(&quot;i,i-&gt;i&quot;, arr1, arr2) # 0,1,4,9,16</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_i = a_i \times b_i,\quad i = 1, 2, \dots</script><ol><li>计算两向量外积，即<code>np.outer(arr1, arr2)</code></li></ol><pre><code class="lang-python">[[ 0  0  0  0  0] [ 0  1  2  3  4] [ 0  2  4  6  8] [ 0  3  6  9 12] [ 0  4  8 12 16]]np.einsum(&quot;i,j-&gt;ij&quot;, arr1, arr2)</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i,j} = a_i \times b_j,\quad i,j = 1, 2, \dots</script><h3 id="对于矩阵"><a href="#对于矩阵" class="headerlink" title="对于矩阵"></a>对于矩阵</h3><pre><code class="lang-python">A = np.arange(4).reshape(2,2)B = np.arange(4,8).reshape(2,2)[[0 1] [2 3]][[4 5] [6 7]]</code></pre><ol><li>计算矩阵转置，即<code>A.T</code></li></ol><pre><code class="lang-python">[[0 2] [1 3]]print(np.einsum(&quot;ij-&gt;ji&quot;, A))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i,j} = a_{j,i},\quad i,j = 1, 2, \dots</script><ol><li>计算矩阵各元素求和，即<code>np.sum(A)</code></li></ol><pre><code class="lang-python">6print(np.einsum(&quot;ij-&gt;&quot;, A))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c = \sum_{i}\sum_{j}a_{i,j},\quad i,j = 1, 2, \dots</script><ol><li>计算矩阵按列求和，即<code>np.sum(A, axis=0)</code></li></ol><pre><code class="lang-python">[2 4]print(np.einsum(&quot;ij-&gt;j&quot;, A))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{j} = \sum_{i}a_{i,j},\quad i,j = 1, 2, \dots</script><ol><li>计算矩阵按行求和，即<code>np.sum(A, axis=1)</code></li></ol><pre><code class="lang-python">[1 5]print(np.einsum(&quot;ij-&gt;i&quot;, A))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i} = \sum_{j}a_{i,j},\quad i,j = 1, 2, \dots</script><ol><li>求矩阵对角线元素，即<code>np.diag(A)</code></li></ol><pre><code class="lang-python">[0 3]print(np.einsum(&quot;ii-&gt;i&quot;, A))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i} = a_{i,i},\quad i = 1, 2, \dots</script><ol><li>计算矩阵的迹，即对角线元素和，即<code>np.trace(A)</code></li></ol><pre><code class="lang-python">3print(np.einsum(&quot;ii-&gt;&quot;, A))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c = \sum_{i}a_{i,i},\quad i = 1, 2, \dots</script><ol><li>计算两矩逐元素乘积，即<code>A*B</code></li></ol><pre><code class="lang-python">[[ 0  5] [12 21]] print(np.einsum(&quot;ij,ij-&gt;ij&quot;, A, B))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i,j} = a_{i,j} \times b_{i,j}, i,j = 1, 2, \dots</script><ol><li>计算<code>A*B.T</code></li></ol><pre><code class="lang-python">[[ 0  6] [10 21]]print(np.einsum(&quot;ij,ji-&gt;ij&quot;, A, B))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i,j} = a_{i,j} \times b_{j,i}, i,j = 1, 2, \dots</script><ol><li>计算两矩阵乘积<code>np.dot(A, B)</code></li></ol><pre><code class="lang-python">[[ 6  7] [26 31]]print(np.einsum(&quot;ij,jk-&gt;ik&quot;, A, B))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i,k} = a_{i,j} \times b_{j,k}, i,j = 1, 2, \dots</script><p>停一下，停一下。</p><p><img src="/2020/05/19/numpy中的einsum使用方法/2020-05-20-09-45-48.png" srcset="/img/loading.gif" alt></p><p>你们懂了吗？反正我没有。网上的文章指望着我们光看例子就能学会，这是把我们都当成模型训练了吗？</p><p>仔细看一下上面的两个例子，其实每个equation都拥有一个箭头<code>-&gt;</code>。对应数学公式不难得出，箭头左边对应数学公式右边，箭头右边对应数学公式左边。</p><p>比如这个式子：</p><pre><code class="lang-python">np.einsum(&quot;ij,ji-&gt;i&quot;, A, B)</code></pre><p><code>&quot;ij,ji-&gt;i&quot;</code>解释成自然语言：将A中第<code>{i,j}</code>个元素与B中第<code>{j,i}</code>个元素相乘（逗号理解成相乘），结果中没有j分量，只有i分量，所以所有j分量求和。</p><p>就是对应这个数学公式：</p><script type="math/tex; mode=display">c_i = \sum_{j}a_{i,j}\times b_{j,i}</script><p>实际含义代表：<code>np.sum(A*B.T, axis=1)</code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍爱因斯坦求和约定，以及在numpy中的使用&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="numpy" scheme="https://superlova.github.io/tags/numpy/"/>
    
      <category term="einsum" scheme="https://superlova.github.io/tags/einsum/"/>
    
  </entry>
  
  <entry>
    <title>numpy中axis的简单理解</title>
    <link href="https://superlova.github.io/2020/05/19/numpy%E4%B8%ADaxis%E7%9A%84%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3/"/>
    <id>https://superlova.github.io/2020/05/19/numpy%E4%B8%ADaxis%E7%9A%84%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3/</id>
    <published>2020-05-19T06:52:25.000Z</published>
    <updated>2020-05-19T10:36:47.657Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍numpy中的axis<br><a id="more"></a></p><p>我对于numpy中的axis的理解，一直处于似懂非懂、似是而非的状态。看到网上大神的文章，也只能点个赞之后，该不会还是不会。每次看完博客，都会觉得自己懂了；但是每次使用的时候，又要想老半天才行。因此今天我想借此机会，彻底扫清使用numpy时，axis的障碍。</p><p>在numpy中，数据的基本类型是array。array有个基本的数据属性，是它的维度。</p><p>比如下面的这个array，在逻辑上来看这就是个2维的数据，是一个矩阵。</p><pre><code class="lang-python">A = np.random.randint(0, 19, 9).reshape(3, 3)print(A)[[12 15  0] [ 3  3  7] [ 9 18  4]]</code></pre><p>接下来我要对其中的元素进行求和。</p><pre><code class="lang-python">print(np.sum(A))print(np.sum(A, axis=0))print(np.sum(A, axis=1))71[24 36 11][27 13 31]</code></pre><p>显然，第一个sum是对所有元素累加。第二个参数为axis=0的求和，则是这样计算的：</p><p><code>A[0][X] + A[1][X] + A[2][X]</code><br><code>--|---------|---------|----</code></p><p>也就是说，axis=0意味着在求和的过程中，只有A的第0个分量会变化，将第0个分量的所有情况穷举出来，再作为被操作元素，求和之。</p><p>第0个分量的元素计算完毕、得到一个结果时，计算并没有结束，因为我们的X还有很多种可能。</p><p>同理，axis=1时，变化的只有A的第1个（从逻辑上讲是第二个）分量有变化：</p><p><code>A[X][0] + A[X][1] + A[X][2]</code><br><code>-----|---------|---------|-</code></p><p>把该结论推广到更高维度的数据也不会有问题。我们看一个4维的张量是如何指定axis求和的：</p><pre><code class="lang-python">np.random.seed(0)A = np.random.randint(0, 9, 16).reshape(2, 2, 2, 2)print(&quot;orignal A&quot;, A)orignal A [[[[5 0]   [3 3]]  [[7 3]   [5 2]]] [[[4 7]   [6 8]]  [[8 1]   [6 7]]]]</code></pre><pre><code class="lang-python">print(np.sum(A))75</code></pre><pre><code class="lang-python">print(np.sum(A, axis=0))# 相当于print(A[0,:,:,:]+A[1,:,:,:])[[[ 9  7]  [ 9 11]] [[15  4]  [11  9]]]</code></pre><pre><code class="lang-python">print(np.sum(A, axis=1))# 相当于print(A[:,0,:,:] + A[:,1,:,:])[[[12  3]  [ 8  5]] [[12  8]  [12 15]]]</code></pre><pre><code class="lang-python">print(np.sum(A, axis=2))# 相当于print(A[:,:,0,:] + A[:,:,1,:])[[[ 8  3]  [12  5]] [[10 15]  [14  8]]]</code></pre><pre><code class="lang-python">print(np.sum(A, axis=3))# 相当于print(A[:,:,:,0]+A[:,:,:,1])[[[ 5  6]  [10  7]] [[11 14]  [ 9 13]]]</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍numpy中的axis&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="numpy" scheme="https://superlova.github.io/tags/numpy/"/>
    
      <category term="axis" scheme="https://superlova.github.io/tags/axis/"/>
    
  </entry>
  
  <entry>
    <title>闭包的迷思</title>
    <link href="https://superlova.github.io/2020/05/19/%E9%97%AD%E5%8C%85%E7%9A%84%E8%BF%B7%E6%80%9D/"/>
    <id>https://superlova.github.io/2020/05/19/%E9%97%AD%E5%8C%85%E7%9A%84%E8%BF%B7%E6%80%9D/</id>
    <published>2020-05-19T06:50:52.000Z</published>
    <updated>2020-05-19T08:05:48.783Z</updated>
    
    <content type="html"><![CDATA[<p>闭包是什么？如果你与我有同样的疑问，敬请阅读。<br><a id="more"></a></p><h2 id="什么是闭包？"><a href="#什么是闭包？" class="headerlink" title="什么是闭包？"></a><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-closure/#note_1" target="_blank" rel="noopener">什么是闭包？</a></h2><p>这个问题困扰了我很长时间。</p><p>第一次接触闭包这个概念，是在“形式语言”这门课上。好像“离散数学”这门课上也教过闭包，但是这都不重要，因为我们这里讨论的闭包与数学上的闭包没什么关系。本文讨论的闭包，是程序设计语言中的闭包。</p><h3 id="专业概念："><a href="#专业概念：" class="headerlink" title="专业概念："></a>专业概念：</h3><p><img src="/2020/05/19/闭包的迷思/2020-05-19-15-20-04.png" srcset="/img/loading.gif" alt></p><p>闭包是在其词法上下文中引用了自由变量的<strong>函数</strong>，自由变量是指除局部变量以外的变量。</p><p>又有一种说法是闭包<strong>不是函数</strong>，而是由函数和与其相关的引用环境组合而成的实体。</p><p><a href="https://zh.wikipedia.org/wiki/%E9%97%AD%E5%8C%85_(%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6" target="_blank" rel="noopener">维基百科</a>)的解释：闭包在实现上是一个结构体，它存储了一个函数（通常是其入口地址）和一个关联的环境（相当于一个符号查找表）。</p><p>看到这里我彻底懵逼了。是是是，你们说的都对！<br><img src="/2020/05/19/闭包的迷思/2020-05-18-14-59-42.png" srcset="/img/loading.gif" alt></p><p>身为新手小白，我需要通过判断闭包是做什么的，之后再讨论为什么叫做闭包。</p><h2 id="闭包有什么用？"><a href="#闭包有什么用？" class="headerlink" title="闭包有什么用？"></a>闭包有什么用？</h2><p>如果你是从C++来的，那么阅读下面没有什么障碍。如果不是也没有关系，反正各种语言的设计原理都是类似的，只要你掌握的语言有<strong>匿名函数</strong>的功能即可。</p><p>我们都知道，C++11标准引入了lambda表达式，就是一个匿名函数。这个函数长成这样：</p><pre><code class="lang-cpp">[](const string&amp;a, const string&amp;b) {    return a.size() &lt; b.size();};</code></pre><p>上面的这个匿名函数负责比较两个字符串的大小。匿名函数的好处就是节省代码。</p><p>比如我现在想要实现自定义字符串排序函数，按照字符串长度从小到大排序，而不是按照字典排序。这个排序函数就可以用lambda表达式定义。</p><pre><code class="lang-cpp">stable_sort(words.begin(), words.end(),             [](const string&amp;a, const string&amp;b) {                return a.size() &lt; b.size();};)</code></pre><p>lambda前面的中括号是干啥的？是用来捕获外部变量的。比如我想判断字符串长度有没有大于阈值threshold，这个threshold是在函数外面定义的。按照C++的语法，一般的函数不能访问函数外部的变量。但是lambda可以把外部的变量“捕获”，就像下面这样：</p><pre><code class="lang-cpp">int threshold = 10;[threshold](const string&amp; a) {    return a.size() &gt; threshold;};</code></pre><p>可以看到，这个lambda不但使用了lambda内部的变量和参数，而且还“偷取”了不属于它的全局变量threshold。<strong>我们把lambda表达式定义的这种函数叫做闭包。</strong></p><h2 id="为什么叫做闭包？"><a href="#为什么叫做闭包？" class="headerlink" title="为什么叫做闭包？"></a>为什么叫做闭包？</h2><p>有人说这不是脑子有坑吗，闭包哪里“闭”了？这明明比普通函数更“开放”好吧？是不是名字起错了？</p><p>其实不然。闭包并不是对内部封闭，而是给当前外部环境取了个快照，相当于封闭了外部状态。下面是著名营养快线经销商vczh的回答：</p><p><img src="/2020/05/19/闭包的迷思/2020-05-19-15-40-47.png" srcset="/img/loading.gif" alt></p><h2 id="Python中的闭包"><a href="#Python中的闭包" class="headerlink" title="Python中的闭包"></a>Python中的闭包</h2><p>Python中写闭包就要方便多了，毕竟Python的设计哲学就是“一切皆对象”，函数都是对象。</p><p>我们来看这样一个问题：利用闭包和生成器返回一个计数器函数，每次调用它返回递增整数。</p><pre><code class="lang-python"># 利用闭包和生成器返回一个计数器函数，每次调用它返回递增整数。def createCounter():        [...]# 检验部分counterA = createCounter()print(counterA(), counterA(), counterA(), counterA(), counterA()) # 1 2 3 4 5counterB = createCounter()if [counterB(), counterB(), counterB(), counterB()] == [1, 2, 3, 4]:    print(&#39;测试通过!&#39;)else:    print(&#39;测试失败!&#39;)</code></pre><p>你想怎么写？我能想到的，就是在函数内部定义一个生成器，每次调用生成一个整数；然后利用next函数构造一个迭代器，每次调用让这个整数+1，最后返回这个迭代器。</p><pre><code class="lang-python">def createCounter():        def counter():        &#39;&#39;&#39;定义一个生成器        &#39;&#39;&#39;        n = 0        while 1:            n += 1            yield n    g = counter() # 取生成器    def g_fn():        &#39;&#39;&#39;定义一个迭代器，利用next迭代生成器g        &#39;&#39;&#39;        return next(g)    return g_fn # 返回这个迭代器</code></pre><p>我们看一下上面这个函数，函数内部定义的<code>g_fn</code>函数，它使用了外部变量<code>g</code>，也就是说<code>g_fn</code>是个闭包。</p><h2 id="总结一下："><a href="#总结一下：" class="headerlink" title="总结一下："></a>总结一下：</h2><p><strong>引用了自由变量的函数，就是闭包。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;闭包是什么？如果你与我有同样的疑问，敬请阅读。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="C++" scheme="https://superlova.github.io/tags/C/"/>
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="closure" scheme="https://superlova.github.io/tags/closure/"/>
    
      <category term="lambda" scheme="https://superlova.github.io/tags/lambda/"/>
    
  </entry>
  
  <entry>
    <title>Python装饰器为什么这么难以理解</title>
    <link href="https://superlova.github.io/2020/05/18/Python%E8%A3%85%E9%A5%B0%E5%99%A8%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E9%9A%BE%E4%BB%A5%E7%90%86%E8%A7%A3/"/>
    <id>https://superlova.github.io/2020/05/18/Python%E8%A3%85%E9%A5%B0%E5%99%A8%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E9%9A%BE%E4%BB%A5%E7%90%86%E8%A7%A3/</id>
    <published>2020-05-18T04:43:08.000Z</published>
    <updated>2020-05-19T10:06:38.373Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍Python中的装饰器，以及设计模式中的装饰模式。<br><a id="more"></a></p><p>从C/C++或Java迁移来的新Python程序员一定会对Python的装饰器功能感到陌生，尤其是在函数定义前加<code>@func</code>这一功能感到困惑。装饰器到底是什么？Python背后做了什么？在仔细研究网上的资料之后，我总结了此文，与大家分享。</p><p><a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017451662295584" target="_blank" rel="noopener">参考文章</a></p><h2 id="1-提出需求"><a href="#1-提出需求" class="headerlink" title="1. 提出需求"></a>1. 提出需求</h2><p>我们想在函数增加一点功能，比如每次函数执行之前打印一段话，但是又不想更改函数的定义。</p><p>这种想要给原来函数增加需求的同时，不修改原来代码的行为，非常有“面向对象编程思想”内味儿，因为它符合“开放封闭原则”。</p><p>现在就有请大名鼎鼎的设计模式之——装饰器模式登场！</p><blockquote><p>装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。</p></blockquote><p><img src="/2020/05/18/Python装饰器为什么这么难以理解/2020-05-19-17-27-00.png" srcset="/img/loading.gif" alt></p><h2 id="2-Python中的装饰器模式"><a href="#2-Python中的装饰器模式" class="headerlink" title="2. Python中的装饰器模式"></a>2. Python中的装饰器模式</h2><p>在Python中实现装饰器模式很方便。在Python中，有个功能模块直接就叫装饰器。在Python中的装饰器是指一个返回其他函数的函数。外部的高阶函数在执行内部的原函数的前后，再私藏一点干货，然后把修改后的函数对象赋值给原来的函数变量。这样就能在不修改原函数的基础上，增加一些功能。</p><p>总结下来，实现装饰器三步走：</p><ol><li>定义原函数</li><li>定义高阶函数，在里面除了执行原函数之外，再添加一些功能</li><li>将高阶函数对象赋值为原函数变量，以后调用原函数的时候都会执行高阶函数了</li></ol><pre><code class="lang-python">def log(func):    def wrapper(*args, **kw):        print(&#39;call %s():&#39; % func.__name__)        return func(*args, **kw)    return wrapper</code></pre><p>上面的函数，输入参数为原函数变量，在内部构造了一个高阶函数对象wrapper，wrapper里面负责执行一个print语句。最后返回构造好的wrapper。</p><p>以后我们使用<code>func</code>的时候，只要使用<code>log(func)</code>就可以在执行<code>func</code>的同时，打印一段话了。</p><p>看起来不咋地啊，毕竟我们还是修改了代码，把<code>func</code>全都替换成<code>log(func)</code>才能执行。</p><p>或者我们来这样一句：</p><pre><code class="lang-python">func = log(func)</code></pre><p>这个log函数就是一个装饰器，它现在装饰的是func函数。</p><h2 id="3-Python的语法糖"><a href="#3-Python的语法糖" class="headerlink" title="3. Python的语法糖"></a>3. Python的语法糖</h2><p>借助Python的@语法，把decorator置于函数的定义处，我们可以直接完成<code>func = log(func)</code>的操作。</p><pre><code class="lang-python">@logdef basic_fun():    print(&quot;basic_func&quot;)</code></pre><p>以后使用basic_func就会默认执行log(basic_func)了。</p><h2 id="4-改函数名"><a href="#4-改函数名" class="headerlink" title="4. 改函数名"></a>4. 改函数名</h2><p>Python的设计思想就是“一切皆对象”，就连函数也不例外。既然是对象，那么对象可以赋值给一个变量，也可以直接使用。通过变量也可以调用该函数对象。</p><pre><code class="lang-python">def f():    return 0f_obj = f # 注意，这里f为函数名，不加括号则为将函数对象赋值为变量f_res = f() # f后面跟了括号，则此时执行函数，并把返回值赋值给变量</code></pre><p>Python有个特别方便的功能，那就是函数对象可以在运行时打印自己的名字。接上面的代码：</p><pre><code class="lang-python">print(f.__name__) # fprint(f_obj.__name__) # 本质上还是调用上面的函数对象，结果仍为f</code></pre><p>前面我们做了赋值操作<code>func = log(func)</code>，但是其变量代表的函数名称发生了变化。</p><pre><code class="lang-python">print(func.__name__) # funcfunc = log(func)print(func.__name__) # wrapper</code></pre><p>我们希望装饰器完全包裹原函数，也就是说令外界环境感觉不到内部逻辑的变化。那么就需要我们把函数名字也给保持住。这个功能不难，我们使用<code>functools</code>库中自带的装饰器<code>wraps</code>就可以保持函数名称了。</p><pre><code class="lang-python">import functoolsdef log(func):    @functools.wraps(func) # 将被装饰函数名变成参数中函数名    def wrapper(*args, **kw):        print(&#39;call %s():&#39; % func.__name__)        return func(*args, **kw)    return wrapper</code></pre><h2 id="5-带参数的装饰器"><a href="#5-带参数的装饰器" class="headerlink" title="5. 带参数的装饰器"></a>5. 带参数的装饰器</h2><p>在上面我们可以看到，装饰器也是可以带参数的。这是怎么做到的呢？</p><p>其实我们不难想到，只需装饰一个装饰器即可。比如下面这个问题：</p><p><strong>实现log(str)：在函数每次执行前打印str和函数名</strong></p><pre><code class="lang-python">@log(&#39;end&#39;)def now():    print(np.datetime64(&#39;today&#39;, &#39;D&#39;))&gt;&gt;&gt; now()end now():2019-10-13</code></pre><p>解法如下：</p><pre><code class="lang-python">import functoolsdef log(text):    def decorator(func):        @functools.wraps(func)        def wrapper(*args, **kw):            print(&#39;%s %s():&#39; % (text, func.__name__))            return func(*args, **kw)        return wrapper    return decorator</code></pre><p>相当于<code>fun = log(&#39;text&#39;)(fun)</code>，实际上函数变成了<code>wrapper</code><br>但是由于<code>@functools.wraps(func)</code>，函数的<code>__name__</code>不变</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍Python中的装饰器，以及设计模式中的装饰模式。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="decorator" scheme="https://superlova.github.io/tags/decorator/"/>
    
  </entry>
  
  <entry>
    <title>深度学习工作站调研--结合政府采购网信息</title>
    <link href="https://superlova.github.io/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%E8%B0%83%E7%A0%94-%E7%BB%93%E5%90%88%E6%94%BF%E5%BA%9C%E9%87%87%E8%B4%AD%E7%BD%91%E4%BF%A1%E6%81%AF/"/>
    <id>https://superlova.github.io/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%E8%B0%83%E7%A0%94-%E7%BB%93%E5%90%88%E6%94%BF%E5%BA%9C%E9%87%87%E8%B4%AD%E7%BD%91%E4%BF%A1%E6%81%AF/</id>
    <published>2020-05-11T03:59:50.000Z</published>
    <updated>2020-05-20T02:36:25.578Z</updated>
    
    <content type="html"><![CDATA[<h1 id="服务器调研-2020年5月10日"><a href="#服务器调研-2020年5月10日" class="headerlink" title="服务器调研 2020年5月10日"></a>服务器调研 2020年5月10日</h1><p><strong>调研目标：</strong></p><ul><li>目前典型的计算机，包括商用台式机、工作站、服务器</li><li>搭配目前典型的GPU卡</li><li>GPU适配计算机，需要厂家网站公开的列表，特别是对于服务器。如果厂家没有，需要致电厂商（非销售商）的技术支持。</li></ul><h2 id="1-制约性能的典型项目"><a href="#1-制约性能的典型项目" class="headerlink" title="1. 制约性能的典型项目"></a>1. 制约性能的典型项目</h2><h3 id="1-1-主板"><a href="#1-1-主板" class="headerlink" title="1.1 主板"></a>1.1 主板</h3><p>Intel部分芯片组不支持PCIe 3.0接口，无法发挥显卡的最佳速度。</p><h4 id="名词解释："><a href="#名词解释：" class="headerlink" title="名词解释："></a>名词解释：</h4><p><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-21-33-30.png" srcset="/img/loading.gif" alt><br><strong>PCI Express / PCI-e</strong><br>PCI-E的全名叫PCI Express，简称PCI-E，官方简称PCIe，他是计算机内部的一种高速总线。PCI-E既是通道，也是接口，当他以接口形式存在的时候，就是我们主板上那长长的槽。PCI-E接口目前最大的作用就是插显卡<br><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-21-32-30.png" srcset="/img/loading.gif" alt></p><p><strong>PCI Express 修订版 / PCIe版本</strong><br>PCIe所能承受的带宽一般以版本和长度来区分，目前最流行的PCIe版本是3.0，最新的版本是4.0，目前只有高端主板支持4.0，只有比2080ti还要高端的显卡才需要4.0。</p><p><strong>PCI Express 配置</strong><br>通俗的说就是插槽长度。X1长度是最短的，所能承受的带宽大约是986MB/S。X2长度就是2GB/S，X4长度就是4GB/S，那X16长度就是16GB/S。当前主流显卡，均采用PCIE×16插槽结构。只要具有PCIE×16插槽的主板，都是可以安装独立显卡的。<br>英特尔官网的意义没大看懂，真正有意义的是“支持的处理器 PCI Express 端口配置”这一项。</p><p><strong>支持的处理器 PCI Express 端口配置</strong><br>以Z390主板为例，该主板1x16 or 2x8 or 1x8+2x4，意思就是可以插1个长度为16X的显卡，也可以插两个长度为8X的固态硬盘之类的，但是如果同时插上显卡和固态硬盘，就会出现抢通道的现象：显卡占用16个通道，两个固态占用16个通道，然而<strong>PCI Express 通道数的最大值</strong>就只有24个，通道不够用就会导致限速，甚至无法正常运转。</p><p><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-22-13-34.png" srcset="/img/loading.gif" alt><br><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-22-13-57.png" srcset="/img/loading.gif" alt><br><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-22-14-07.png" srcset="/img/loading.gif" alt><br><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-22-14-15.png" srcset="/img/loading.gif" alt></p><p>以上是当前在售处理器搭配主板（芯片组）的特性支持情况，仅供参考，并不是说某块主板用了上述某个芯片组芯片就会具备这么多的扩展接口及能力，具体还要看主板厂商针对这个版型作出什么样的“阉割”调整。</p><h3 id="1-2-电源"><a href="#1-2-电源" class="headerlink" title="1.2 电源"></a>1.2 电源</h3><p><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-22-11-24.png" srcset="/img/loading.gif" alt="顶级游戏显卡及需要的电源功率大小"></p><h2 id="2-目前典型计算机"><a href="#2-目前典型计算机" class="headerlink" title="2. 目前典型计算机"></a>2. 目前典型计算机</h2><p>服务器对显卡的支持不如工作站，台式机的性能过低，因此本调查汇聚于工作站查询。</p><h3 id="2-1-服务器"><a href="#2-1-服务器" class="headerlink" title="2.1 服务器"></a>2.1 服务器</h3><p>服务器按外形划分可以划分为：塔式服务器、机架式服务器、刀片式服务器。<br>服务器除了一些低端的塔式机能用显卡以外，其他的都不支持显度卡，当然机架式服务器很薄根本就没有显卡的空间。<br>如果购买服务器，官方售后将不会主动为你安装个人家用系列显卡，转而推销商业计算卡。<br><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-15-26-49.png" srcset="/img/loading.gif" alt></p><p>截至2020年5月，服务器热销品牌Top-10（取自<a href="http://top.zol.com.cn/compositor/server.html" target="_blank" rel="noopener">ZOL网</a>）：<br>Dell、华为、浪潮、联想、惠普、H3C、ThinkServer、中科曙光、宝德、IBM。</p><h3 id="2-2-工作站"><a href="#2-2-工作站" class="headerlink" title="2.2 工作站"></a>2.2 工作站</h3><p>工作站的机箱主要以塔式为主，和一般家用主机机箱差距不大。<br>工作站对显卡的支持比服务器强很多，具体来说，工作站的主板对PCI-E的接口支持更好。</p><p>以下价格和资料全部取自于北京市政府采购网。</p><p><strong>神舟</strong><br>HFMPB2O8型号支持双路2080ti或TITAN<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=1r6e15444412038777n5&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">HFMPB2O8</a>    78,016.00 自带2080ti<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=2g0v15681729710791z0&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">HFMPB99K</a>  55,691.28   自带2080ti<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=6u7m15621197126648s6&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">HFMPB3IR</a>  32,870.00   C422可更换更高级显卡<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=8n5x15444417182251i6&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">HFMPB3J9</a>  29,980.00   自带2080</p><p><strong>联想</strong><br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=6h7y15571256822209c5&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">Think Station P520</a>   46,920.00    C422可更换更高级显卡<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=1s8z15571265520623u2&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">ThinkStation P720</a> 35,000.00   C622可更换更高级 </p><p><strong>宏碁</strong><br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=7w0g15281019011939v9&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">AP150 F4</a> 38,500.00    C622可更换更高级 </p><p><strong>浪潮</strong><br>浪潮是自研主板，不过其主板支持PCIe 16x，理论上只要供电足够即可安装包括2080Ti在内的显卡<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=3f4m15287143876139q2&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">P8000</a>    37,260.00</p><p><strong>惠普</strong><br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=4g2q15281081290990x3&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">HP Z4 G4</a>  15,900.00   C622可更换更高级<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=4v9p15281082398434h6&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">HP Z6 G4</a>  23,500.00   C622可更换更高级<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=0z0m15314586871347z2&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">HP Z8 G4</a>  35,800.00   C622可更换更高级 </p><p><strong>苹果</strong><br>苹果的主板仅支持AMD的显卡，A卡不能用作深度学习。</p><h3 id="2-3-商用台式机"><a href="#2-3-商用台式机" class="headerlink" title="2.3 商用台式机"></a>2.3 商用台式机</h3><p>即普通台式机。普通台式机难以支撑深度学习任务。</p><h2 id="3-显卡介绍"><a href="#3-显卡介绍" class="headerlink" title="3. 显卡介绍"></a>3. 显卡介绍</h2><p>显卡分为Nvidia显卡和AMD显卡，其中Nvidia显卡可以用来深度学习训练和推理。</p><p>比较显卡性能，可以去<a href="https://versus.com/cn" target="_blank" rel="noopener">这个网站</a></p><h3 id="3-1-Nvidia显卡简介"><a href="#3-1-Nvidia显卡简介" class="headerlink" title="3.1 Nvidia显卡简介"></a>3.1 Nvidia显卡简介</h3><p><a href="https://www.bybusa.com/gpu-rank" target="_blank" rel="noopener">2020年显卡天梯图</a></p><p><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-22-23-18.png" srcset="/img/loading.gif" alt="2020年显卡天梯图"></p><p>目前最强的显卡是2080ti。预计在2020年底的3080ti发布之前，2080ti还会持续称霸显卡江湖。</p><h3 id="3-2-游戏显卡"><a href="#3-2-游戏显卡" class="headerlink" title="3.2 游戏显卡"></a>3.2 游戏显卡</h3><p>对游戏显卡的调研，参考<a href="https://post.smzdm.com/p/a6lrwk3e/" target="_blank" rel="noopener">“什么值得买”上的调研</a>以及<a href="https://www.cnblogs.com/xiaozhi_5638/p/10923351.html" target="_blank" rel="noopener">这个网址</a>。</p><p><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-22-09-38.png" srcset="/img/loading.gif" alt="游戏显卡一览"></p><p><strong>Geforce系列</strong></p><p>这个系列是销量最多、大众最为熟悉的显卡，一般用来打游戏。价格便宜，最新出来的旗舰卡RTX 2080Ti京东售价大概1w左右，根据不同的品牌，价格有所波动。低配置的便宜的一千就能买到。官方定位是消费级，但是它在深度学习上的表现也非常不错，很多人用来做推理、训练，单张卡的性能跟深度学习专业卡Tesla系列比起来其实差不太多，但是性价比却高很多。比如已经停产的GTX 1080显卡的参数基本和深度学习入门级显卡Tesla P4一样，用来做训练和推理的效果比Tesla P4还要好，可是GTX 1080一张卡才卖5000~6000左右，而Tesla P4要卖到1.4w。</p><p>究其原因，很大程度上在于英伟达官方禁止使用GTX、RTX系列显卡用于深度学习等用途，一经使用，自动过保。除了商业考虑外，还包括：Tesla多块显卡合起来的性能不会受很大影响，且Tesla系列显卡功耗优化非常明显，基本都是被动散热，不提供风扇，更适合数据中心机房工作环境等。</p><h3 id="3-3-计算显卡"><a href="#3-3-计算显卡" class="headerlink" title="3.3 计算显卡"></a>3.3 计算显卡</h3><p>专业级显卡的介绍参考<a href="https://product.pconline.com.cn/itbk/diy/graphics/1802/10846244.html" target="_blank" rel="noopener">“什么值得买”上的调研</a>以及<a href="https://www.cnblogs.com/xiaozhi_5638/p/10923351.html" target="_blank" rel="noopener">这个网址</a>。</p><p><strong>Quadro系列</strong><br>Quadro系列显卡一般用于特定行业，比如设计、建筑等，图像处理专业显卡，比如CAD、Maya等软件，一般人很少用到，价格相对来讲也稍微贵一些，最新的包括RTX 3000/4000/6000/8000型号。</p><p><strong>Tesla系列</strong><br>Tesla系列显卡定位并行计算，一般用于数据中心，具体点，比如用于深度学习，做训练、推理等。阿里云、Amazon云有非常多的GPU服务器，基本都采用Tesla系列显卡。这个系列显卡有个特别明显的特征，那就是贵。Tesla系列入门级显卡 Tesla P4，前面提到过，用来做深度学习的效果比GTX 1080还差，但是价格是后者的3倍多。像其他更高级别的Tesla V100、Tesla P100 价格高达8w、4w，这种价位的显卡虽然性能强劲，但是一般人是买不起的，只有企业数据中心才会部署这种显卡。</p><h3 id="3-4-显卡性能指标"><a href="#3-4-显卡性能指标" class="headerlink" title="3.4 显卡性能指标"></a>3.4 显卡性能指标</h3><p>本部分请参考<a href="https://www.cnblogs.com/xiaozhi_5638/p/10923351.html" target="_blank" rel="noopener">这里</a>。</p><h3 id="3-4-显卡罗列"><a href="#3-4-显卡罗列" class="headerlink" title="3.4 显卡罗列"></a>3.4 显卡罗列</h3><p>政府采购网上，值得采购的显卡如下</p><div class="table-container"><table><thead><tr><th>型号</th><th>价格</th></tr></thead><tbody><tr><td>p5000</td><td>27000</td></tr><tr><td>p6000</td><td>43500</td></tr><tr><td>2080</td><td>13500</td></tr><tr><td>k4000</td><td>42450</td></tr><tr><td>p4000</td><td>6800</td></tr><tr><td>2070s</td><td>8000</td></tr><tr><td>8000</td><td>93350</td></tr><tr><td>2080ti</td><td>16000</td></tr><tr><td>1080ti</td><td>8620</td></tr><tr><td>p4</td><td>28000</td></tr><tr><td>2080</td><td>10290</td></tr><tr><td>2060</td><td>5000</td></tr><tr><td>titan rtx</td><td>30000</td></tr><tr><td>p1000</td><td>3500</td></tr><tr><td>2070</td><td>7500</td></tr><tr><td>m2000</td><td>2982</td></tr><tr><td>titan v</td><td>37500</td></tr></tbody></table></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;服务器调研-2020年5月10日&quot;&gt;&lt;a href=&quot;#服务器调研-2020年5月10日&quot; class=&quot;headerlink&quot; title=&quot;服务器调研 2020年5月10日&quot;&gt;&lt;/a&gt;服务器调研 2020年5月10日&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;调研目标：&lt;
      
    
    </summary>
    
    
      <category term="record" scheme="https://superlova.github.io/categories/record/"/>
    
    
      <category term="PCIe" scheme="https://superlova.github.io/tags/PCIe/"/>
    
      <category term="显卡" scheme="https://superlova.github.io/tags/%E6%98%BE%E5%8D%A1/"/>
    
      <category term="工作站" scheme="https://superlova.github.io/tags/%E5%B7%A5%E4%BD%9C%E7%AB%99/"/>
    
  </entry>
  
  <entry>
    <title>chrome升级版本失败解决办法</title>
    <link href="https://superlova.github.io/2020/05/11/chrome%E5%8D%87%E7%BA%A7%E7%89%88%E6%9C%AC%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <id>https://superlova.github.io/2020/05/11/chrome%E5%8D%87%E7%BA%A7%E7%89%88%E6%9C%AC%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</id>
    <published>2020-05-11T03:46:55.000Z</published>
    <updated>2020-05-11T04:22:34.367Z</updated>
    
    <content type="html"><![CDATA[<h2 id="错误描述："><a href="#错误描述：" class="headerlink" title="错误描述："></a>错误描述：</h2><p>在Win7电脑上试图将Chrome从32位的72版本升级到64位的80版本时发生问题，升级进度到62%报错：<br>Chrome安装 未知错误导致安装失败  “0x80040902”</p><p>从chrome官网下载“chromesetup.exe”，打开梯子之后下载成功，在安装过程中也出现未知错误。<br>从Chrome官网下载“Chromestandalonesetup64.exe”，即离线安装包，最后也出现同样的错误。<br>重新启动、进入安全模式、试图结束所有有关google的进程的方法对我都没用。</p><h2 id="最后有效的方法："><a href="#最后有效的方法：" class="headerlink" title="最后有效的方法："></a>最后有效的方法：</h2><p>把原来的Chrome从控制面板的“添加删除程序”中卸载；</p><p>按住windows+R，在“开始”运行中输入“regedit”，打开注册表编辑器，依次进入HKEY_CURRENT_USER\Software\Google\Chrome；</p><p>把Chrome这一项删除，然后重启。再安装就不会存在问题了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;错误描述：&quot;&gt;&lt;a href=&quot;#错误描述：&quot; class=&quot;headerlink&quot; title=&quot;错误描述：&quot;&gt;&lt;/a&gt;错误描述：&lt;/h2&gt;&lt;p&gt;在Win7电脑上试图将Chrome从32位的72版本升级到64位的80版本时发生问题，升级进度到62%报错：&lt;br&gt;
      
    
    </summary>
    
    
      <category term="record" scheme="https://superlova.github.io/categories/record/"/>
    
    
      <category term="chrome" scheme="https://superlova.github.io/tags/chrome/"/>
    
  </entry>
  
  <entry>
    <title>testRNN--Coverage-guided Testing on Recurrent Neural Networks 论文阅读笔记</title>
    <link href="https://superlova.github.io/2020/03/25/testRNN%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>https://superlova.github.io/2020/03/25/testRNN%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</id>
    <published>2020-03-25T02:37:35.000Z</published>
    <updated>2020-05-20T02:37:02.929Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;

      
    
    </summary>
    
    
      <category term="paper" scheme="https://superlova.github.io/categories/paper/"/>
    
    
      <category term="RNN" scheme="https://superlova.github.io/tags/RNN/"/>
    
      <category term="testing" scheme="https://superlova.github.io/tags/testing/"/>
    
      <category term="testRNN" scheme="https://superlova.github.io/tags/testRNN/"/>
    
  </entry>
  
  <entry>
    <title>RNN-Test--Adversarial Testing Framework for Recurrent Neural Network Systems 论文阅读笔记</title>
    <link href="https://superlova.github.io/2020/03/25/RNN-Test%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>https://superlova.github.io/2020/03/25/RNN-Test%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</id>
    <published>2020-03-25T02:37:16.000Z</published>
    <updated>2020-05-20T02:36:55.897Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;

      
    
    </summary>
    
    
      <category term="paper" scheme="https://superlova.github.io/categories/paper/"/>
    
    
      <category term="RNN" scheme="https://superlova.github.io/tags/RNN/"/>
    
      <category term="testing" scheme="https://superlova.github.io/tags/testing/"/>
    
      <category term="RNN-Test" scheme="https://superlova.github.io/tags/RNN-Test/"/>
    
  </entry>
  
  <entry>
    <title>lintcode-138 子数组求和问题</title>
    <link href="https://superlova.github.io/2020/03/24/lintcode-138-%E5%AD%90%E6%95%B0%E7%BB%84%E4%B9%8B%E5%92%8C/"/>
    <id>https://superlova.github.io/2020/03/24/lintcode-138-%E5%AD%90%E6%95%B0%E7%BB%84%E4%B9%8B%E5%92%8C/</id>
    <published>2020-03-24T09:30:31.000Z</published>
    <updated>2020-03-24T12:04:57.628Z</updated>
    
    <content type="html"><![CDATA[<ul><li>给定一个整数数组，找到和为零的子数组。</li><li>你的代码应该返回满足要求的子数组的起始位置和结束位置<a id="more"></a><h1 id="lintcode-138：子数组之和"><a href="#lintcode-138：子数组之和" class="headerlink" title="lintcode 138：子数组之和"></a>lintcode 138：子数组之和</h1><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2></li></ul><p>给定一个整数数组，找到和为零的子数组。你的代码应该返回满足要求的子数组的起始位置和结束位置<br><strong>样例 1:</strong><br>输入: [-3, 1, 2, -3, 4]<br>输出: [0,2] 或 [1,3]<br>样例解释： 返回任意一段和为0的区间即可。<br><strong>样例 2:</strong><br>输入: [-3, 1, -4, 2, -3, 4]<br>输出: [1,5]<br><strong>注意事项</strong><br>至少有一个子数组的和为 0</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>子数组之和问题。看看哪个区间段，段内所有元素加起来刚好等于0（或者某个值）。类似这种求区间段，段内元素满足什么条件的问题。</p><p>关键是下面这个结论：</p><p>准备一个数组array，其中第i个元素保存num[0]~num[i]之和。那么index_value中一旦出现两个元素其值相同，就说明这两个下标之间所有元素加起来等于0。</p><script type="math/tex; mode=display">\begin{aligned}& if & \sum_{i=0}^{\operatorname{index_1}}nums(i) = \sum_{i=0}^{\operatorname{index_2}}nums(i) \\ & then\quad & return \left[ \operatorname{index_1}+1, \operatorname{index_2} \right] \end{aligned}</script><p>举个例子：对于数组<code>num = [-3, 1, 2, -3, 4]</code>，我们可以构建array数组如下：</p><div class="table-container"><table><thead><tr><th>index</th><th>nums[index]</th><th>$\sum_{i=0}^{index}nums(i)$</th></tr></thead><tbody><tr><td>0</td><td>-3</td><td>-3</td></tr><tr><td>1</td><td>1</td><td>-2</td></tr><tr><td>2</td><td>2</td><td>0</td></tr><tr><td>3</td><td>-3</td><td>-3</td></tr><tr><td>4</td><td>4</td><td>1</td></tr></tbody></table></div><p>返回 [0, 2] 或 [1, 3]</p><p>在代码实现中，当我们采用数组实现array时，会受限于查询array内元素的线型时间复杂度，为了找某个值对应的下标，遍历array数组的过程，可能耗费线性复杂度的时间，导致代码TLE超时。</p><p>因此我们采用散列，将散列的key设置为前i个元素的和值，value为该值对应的下标位置。</p><p>在Python中查找元素，用<strong>字典</strong>可以大大加快查找速度。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><pre><code class="lang-python">class Solution:    &quot;&quot;&quot;    @param nums: A list of integers    @return: A list of integers includes the index of the first number and the index of the last number    &quot;&quot;&quot;    def subarraySum(self, nums):        index_value = {}        accumulator = 0        for i in range(len(nums)):            accumulator += nums[i]            if accumulator in index_value:                return [index_value[accumulator] + 1, i]            else:                index_value[accumulator] = i        else:            if accumulator == 0:                return [0, i]        return [0, 0]</code></pre><h2 id="变种：子数组元素之和等于k"><a href="#变种：子数组元素之和等于k" class="headerlink" title="变种：子数组元素之和等于k"></a>变种：子数组元素之和等于k</h2><script type="math/tex; mode=display">\begin{aligned}& if & \sum_{i=0}^{\operatorname{index_1}}nums(i) - \bold{k} = \sum_{i=0}^{\operatorname{index_2}}nums(i) \\ & then\quad & return \left[ \operatorname{index_1}+1, \operatorname{index_2} \right] \end{aligned}</script><pre><code class="lang-python">class Solution:    &quot;&quot;&quot;    @param nums: A list of integers    @return: A list of integers includes the index of the first number and the index of the last number    &quot;&quot;&quot;    def subarraySum(self, nums, obj_num):        index_value = {}        accumulator = 0        for i in range(len(nums)):            accumulator += nums[i]            if accumulator - obj_num in index_value:                return [index_value[accumulator - obj_num] + 1, i]            else:                index_value[accumulator] = i        else:            if accumulator == 0:                return [0, i]        return [0, 0]</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;给定一个整数数组，找到和为零的子数组。&lt;/li&gt;
&lt;li&gt;你的代码应该返回满足要求的子数组的起始位置和结束位置&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
      <category term="code_exercises" scheme="https://superlova.github.io/categories/code-exercises/"/>
    
    
      <category term="algorithm" scheme="https://superlova.github.io/tags/algorithm/"/>
    
      <category term="lintcode" scheme="https://superlova.github.io/tags/lintcode/"/>
    
      <category term="array" scheme="https://superlova.github.io/tags/array/"/>
    
  </entry>
  
  <entry>
    <title>C++字符串高级操作总结</title>
    <link href="https://superlova.github.io/2020/03/23/C-%E5%AD%97%E7%AC%A6%E4%B8%B2%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E6%80%BB%E7%BB%93/"/>
    <id>https://superlova.github.io/2020/03/23/C-%E5%AD%97%E7%AC%A6%E4%B8%B2%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E6%80%BB%E7%BB%93/</id>
    <published>2020-03-23T15:50:02.000Z</published>
    <updated>2020-03-25T02:32:11.470Z</updated>
    
    <content type="html"><![CDATA[<p>C++的字符串操作非常多，功能也非常多样化，熟练使用标准库提供的字符串操作函数能够高效提升我们编写代码的效率和可读性。除了常用的<string>库中包含的几项基本操作之外，本文总结了几项特别好用而又不为人所知的高级操作。<br><a id="more"></a></string></p><h2 id="常见的基本操作回顾"><a href="#常见的基本操作回顾" class="headerlink" title="常见的基本操作回顾"></a>常见的基本操作回顾</h2><p>必须指明，<string>中字符串方法可以按照输入参数的类型不同调用不同的重载方法，这些函数名相同，但是参数类型和顺序完全不同，返回值也略有差别。</string></p><p>部分函数，包括insert和erase等函数可分为两类，如果输入的位置参数<code>pos</code>为整数<code>int</code>，此时返回值为<strong>被插入字符串</strong>的引用；而输入的位置参数类型为迭代器<code>iterator</code>，则会调用返回迭代器的函数，该迭代器<strong>指向被插入部分的头部</strong>。</p><p>而且<string>部分函数为了兼容C原生字符串，提供了一批适用于C String构造接口，这又产生了一大批只有参数顺序不同的同名函数。比如对于string构造方法上，输入原生string和输入C String的参数含义完全不同。</string></p><p>这些同名、功能相似但不同参数的函数使得C++新人学习标准库时容易产生极大的困扰。</p><h3 id="1-构造string"><a href="#1-构造string" class="headerlink" title="1. 构造string"></a>1. 构造string</h3><pre><code class="lang-cpp">const char * cp = &quot;Hello World!!!&quot;;char noNull[] = {&#39;H&#39;, &#39;i&#39;};</code></pre><p>输入一个char类型的指针，以及偏移量。转换从指针开始的偏移量个字符。<br>如果未指定偏移量，则默认转化到碰到’\0’为止。<br>没有’\0’结尾则该行为未定义。  </p><pre><code class="lang-cpp">string s1(cp); // 从C风格字符串转化string s2(noNull, 2); // 指定转化的字符个数string s3(noNull); // 未定义，因为noNull不是以空字符结尾string s4(cp + 6, 5);</code></pre><p>拷贝构造函数，从其他string拷贝<br>指定拷贝位置和拷贝字符个数<br>不指定pos则默认从头拷贝<br>不指定len则默认从pos开始全拷贝<br>pos越界则抛出异常<br>len越界没问题，只到’\0’  </p><pre><code class="lang-cpp">string s5(s1, 6, 5);string s6(s1, 6);string s7(s1, 6, 20);string s8(s1, 16);</code></pre><p>substr函数，输入pos和len<br>返回由该字符串的第pos位置拷贝len个字符组成的新子串  </p><pre><code class="lang-cpp">string s9 = s1.substr(0, 5);</code></pre><h3 id="2-改变string"><a href="#2-改变string" class="headerlink" title="2. 改变string"></a>2. 改变string</h3><p>以insert为例，简单介绍不同参数重载的不同insert。</p><p>insert除了接受迭代器的版本之外，还有直接接受下标的版本。返回值为被插入字符串的引用。<br>s.insert(pos, count, char)<br>s.insert(pos, char_ptr, len)<br>s.insert(pos, string, pos, len)</p><p>第一个位置总会是pos，表示被插入位置；</p><p>第二个参数如果是个数，那么你调用的是第一个insert函数，其含义为重复插入第三个参数char所制定的内容；</p><p>第二个参数如果是C风格字符串，那么第三个参数可以指出插入长度，不指名就默认把该C风格字符串全插到pos的位置；</p><p>第二个参数如果是string，那么你还需在string参数后指定从哪个pos开始插，并且指定len表示插入多少个。相对于插入C风格字符串的insert来说，插入string更灵活。</p><p>其他函数及其说明见下表。</p><p><img src="/2020/03/23/C-字符串高级操作总结/2020-03-24-20-38-14.png" srcset="/img/loading.gif" alt><br><img src="/2020/03/23/C-字符串高级操作总结/2020-03-25-08-58-36.png" srcset="/img/loading.gif" alt></p><h2 id="搜索字符串"><a href="#搜索字符串" class="headerlink" title="搜索字符串"></a>搜索字符串</h2><p><string>定义了六种不同的搜索方法，每种方法拥有四个重载版本。</string></p><p><img src="/2020/03/23/C-字符串高级操作总结/2020-03-25-09-00-50.png" srcset="/img/loading.gif" alt><br><img src="/2020/03/23/C-字符串高级操作总结/2020-03-25-09-02-09.png" srcset="/img/loading.gif" alt></p><p>需要注意的有两点：</p><ol><li>搜索函数返回类型为string::size_type，为无符号整数类型。</li><li>搜索失败时，返回string::npos，该值为-1，也就是无符号整数最大的值。</li></ol><h2 id="正则表达式库"><a href="#正则表达式库" class="headerlink" title="正则表达式库"></a>正则表达式库</h2><p>正则表达式是字符串匹配的有力工具。C++11加入了对正则表达式的支持，具体定义位于<regex>头文件中。</regex></p><p>在C++中，正则表达式可以做的工作有：</p><ul><li>Match 将整个输入拿来比对（匹配）某个正则表达式</li><li>Search 查找与正则表达式吻合的子串</li><li>Tokenize 根据正则表达式切分字符串</li><li>Replace 根据正则表达式替换字符串</li></ul><h3 id="第一种应用：Match和Search"><a href="#第一种应用：Match和Search" class="headerlink" title="第一种应用：Match和Search"></a>第一种应用：Match和Search</h3><p>具体流程可概括为：定义、匹配、判断</p><pre><code class="lang-cpp">regex reg(&quot;&lt;.*&gt;.*&lt;/.*&gt;&quot;); // 定义bool isExist = regex_match(string, reg); // 匹配整体//orbool isExist = regex_search(string, reg); // 匹配部分cout &lt;&lt; boolalpha &lt;&lt; isExist &lt;&lt; endl; // 判断</code></pre><p>你可能已经注意到了，regex_match和regex_search返回的仅仅是一个bool值，表明是否匹配。我们还需要匹配的位置。此时我们需要一个<code>match</code>对象来保存结果。<code>match</code>对象的方法如下所示。</p><pre><code class="lang-cpp">smatch m;bool isExist = regex_search(string, m, reg); // 结果保存在m中m.empty()m.size() // 返回匹配个数m.str(i) // 类似于python中group，返回第i个匹配位置的字符串。i=0则返回全部m.length(i) // 同上，返回第i个匹配字符串的长度m.position(i) // 同上，返回第i个匹配字符串的位置m.prefix().str() // 已匹配位置之前的字符串，字符串前缀m.suffix().str() // 已匹配位置之后的字符串，字符串后缀for (auto pos = m.begin(); pos != m.end(); ++pos) {    cout &lt;&lt; *pos &lt;&lt; endl;}代码中的参数i，代表了正则表达式中存在分组，i为提取分组i的被匹配内容。类似python中的group。分组操作是正则表达式的语法，本文不再赘述。</code></pre><p><code>match</code>对象根据保存内容类型不同分成</p><ul><li><code>smatch</code> 匹配string</li><li><code>cmatch</code> 匹配C风格字符串</li><li><code>wsmatch</code> 匹配wstring</li><li><code>wcmatch</code> 匹配const wchar_t*</li></ul><h3 id="第二种应用：Regex-Iterator"><a href="#第二种应用：Regex-Iterator" class="headerlink" title="第二种应用：Regex Iterator"></a>第二种应用：Regex Iterator</h3><p>data可能很长，reg可能会多次匹配。为了迭代所有的匹配成果，我们可以使用regex_iterator。根据类型不同，分别是<br><code>sregex_iterator</code><br><code>cregex_iterator</code><br><code>wsregex_iterator</code><br><code>wcregex_iterator</code></p><pre><code class="lang-cpp">regex reg(&quot;...&quot;);sregex_iterator pos(data.cbegin(), data.cend(), reg);sregex_iterator end;for (; pos != end; ++pos) {    cout &lt;&lt; pos-&gt;str() &lt;&lt; endl;}</code></pre><h3 id="第三种应用：Regex-Token-Iterator"><a href="#第三种应用：Regex-Token-Iterator" class="headerlink" title="第三种应用：Regex Token Iterator"></a>第三种应用：Regex Token Iterator</h3><p>你可能关注的不是被匹配的字符串，而是其余的字符串。此时正则表达式串就像是切割刀一样，将data分割成不含被匹配串的几部分。我们可以利用此功能实现C++中一直没能实现的字符串分割函数split。</p><pre><code class="lang-cpp">string data = &quot;qqq www    eee rrr&quot;;regex r(&quot;\\s+&quot;);sregex_token_iterator pos(data.cbegin(), data.cend(), r, -1); // -1代表你对正则表达式匹配的内容不感兴趣sregex_token_iterator end;for (; pos != end; ++pos) {    cout &lt;&lt; *pos &lt;&lt; endl;}</code></pre><h3 id="第四种应用：替换"><a href="#第四种应用：替换" class="headerlink" title="第四种应用：替换"></a>第四种应用：替换</h3><p>下面的代码将</p><pre><code class="lang-html">&lt;person&gt;&lt;first&gt;Nico&lt;/first&gt;&lt;/person&gt;</code></pre><p>替换成</p><pre><code class="lang-html">&lt;person&gt;&lt;first value=&quot;Nico&quot;/&gt;&lt;/person&gt;</code></pre><pre><code class="lang-cpp">string data = &quot;&lt;person&gt;&lt;first&gt;Nico&lt;/first&gt;&lt;/person&gt;&quot;regex reg(&quot;&lt;(.*)&gt;(.*)&lt;/(\\1)&gt;&quot;);string replace_pattern = &quot;&lt;$1 value=\&quot;$2\&quot;/&gt;&quot;;cout &lt;&lt; regex_replace(data, reg, replace_pattern) &lt;&lt; endl;</code></pre><p>模式替换串用$n指定第几个匹配部分group(n)<br>$1 value=$2 含义即为原来是group(1)的部分替换成group(2)的内容。</p><p><img src="/2020/03/23/C-字符串高级操作总结/2020-03-25-10-21-38.png" srcset="/img/loading.gif" alt></p><h2 id="string-view"><a href="#string-view" class="headerlink" title="string_view"></a>string_view</h2><p>C++17加入了string_view对象，能够避免string类型的复制临时对象操作。</p><ul><li>string_view对象由两部分组成，分别是<strong>数据的起始指针</strong>和<strong>数据的长度</strong>。有点类似于带其他语言表示字符串的方法，不依赖<code>&#39;\0&#39;</code>在结尾，而是通过一个变量记忆长度。</li><li>string_view只读，不能修改。可以很好地作为函数的参数和返回值</li></ul><pre><code class="lang-cpp">//使用string的拷贝操作string s(1000, &#39;0&#39;);string sub_s = s.substr(100, 200); // O(n)//使用view则不需要拷贝string_view sv(s); // no copystring_view sv2 = sv.substr(100, 200); // O(1)</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;C++的字符串操作非常多，功能也非常多样化，熟练使用标准库提供的字符串操作函数能够高效提升我们编写代码的效率和可读性。除了常用的&lt;string&gt;库中包含的几项基本操作之外，本文总结了几项特别好用而又不为人所知的高级操作。&lt;br&gt;&lt;/string&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="C++" scheme="https://superlova.github.io/tags/C/"/>
    
      <category term="STL" scheme="https://superlova.github.io/tags/STL/"/>
    
      <category term="string" scheme="https://superlova.github.io/tags/string/"/>
    
      <category term="regex" scheme="https://superlova.github.io/tags/regex/"/>
    
  </entry>
  
  <entry>
    <title>C++关联容器学习笔记</title>
    <link href="https://superlova.github.io/2020/03/20/C-%E5%85%B3%E8%81%94%E5%AE%B9%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>https://superlova.github.io/2020/03/20/C-%E5%85%B3%E8%81%94%E5%AE%B9%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</id>
    <published>2020-03-19T16:10:13.000Z</published>
    <updated>2020-03-20T11:21:43.587Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-关联容器特点简介"><a href="#1-关联容器特点简介" class="headerlink" title="1. 关联容器特点简介"></a>1. 关联容器特点简介</h2><p>关联容器和顺序容器是两种适用范围不同的容器。许多C++程序员只用过顺序容器诸如vector和string，但他们从未使用过set和map等关联数据结构。</p><p><img src="/2020/03/20/C-关联容器学习笔记/2020-03-20-09-47-21.png" srcset="/img/loading.gif" alt></p><p><code>set</code>是元素的简单集合，用来保存类型相同的一组元素。当你只是想知道一个值<strong>是否存在</strong>时，<code>set</code>是最有用的。</p><ul><li><code>set</code><strong>不会出现重复元素</strong></li><li>内部元素永远<strong>有序</strong></li><li><code>set</code>中的元素一经添加就<strong>不能修改</strong>。</li></ul><p><code>map</code>可以看做特殊的<code>vector</code>，其特殊之处在于此<code>map</code>中的每个元素都由两部分 <code>(key, value)</code> 构成，C++将每个这样的 <code>(key, value)</code> 封装成一个对象，其类别为<code>pair</code>。<code>map</code>内部元素都为<code>pair</code>类型。</p><ul><li><code>map</code><strong>不会出现重复<code>key</code></strong></li><li>内部元素永远有序，按照<code>pair</code>类型元素的key字段排序（key字段必须能够被排序）。</li><li><code>map</code>中的key一经添加就<strong>不能修改</strong>。</li></ul><h2 id="2-有序容器基本操作"><a href="#2-有序容器基本操作" class="headerlink" title="2. 有序容器基本操作"></a>2. 有序容器基本操作</h2><p>以下所有操作需要添加头文件：</p><pre><code class="lang-cpp">#include &lt;map&gt;#include &lt;set&gt;</code></pre><h3 id="2-1-初始化"><a href="#2-1-初始化" class="headerlink" title="2.1 初始化"></a>2.1 初始化</h3><h4 id="2-1-1-map"><a href="#2-1-1-map" class="headerlink" title="2.1.1 map"></a>2.1.1 map</h4><pre><code class="lang-cpp">map&lt;int, int&gt; m; // 默认初始化map&lt;int, int&gt; m{{1, 2}, {2, 4}}; // 初始化列表map&lt;int, int&gt; m2(m); // 拷贝构造函数map&lt;int, int&gt; m = {{1, 2}, {2, 4}}; // 初始化列表 + 拷贝构造函数// pair和make_pair()在#include&lt;utility&gt;中map&lt;int, int&gt; m3 = {    std::pair&lt;int, int&gt;(1, 2), std::pair&lt;int, int&gt;(2, 4)}; // 初始化列表的方法展开来说就是这样map&lt;int, int&gt; m4{    std::make_pair(1, 2), std::make_pair(2, 4)}; // 或者使用make_pair函数，免得输入参数类型，效果等价。// std::begin()和end()在#include &lt;iterator&gt; 中map&lt;int, int&gt; m5 = {std::begin(m), std::end(m)}; // 迭代器，前提是被迭代的对象内部元素类型是pair</code></pre><h4 id="2-1-2-set"><a href="#2-1-2-set" class="headerlink" title="2.1.2 set"></a>2.1.2 set</h4><pre><code class="lang-cpp">set&lt;int&gt; s; // 默认初始化set&lt;int&gt; s{1, 2, 3, 6, 9, 10}; // 初始化列表set&lt;int&gt; s2(s); // 拷贝构造函数set&lt;int&gt; s = {1, 2, 3, 6, 9, 10}; // 初始化列表 + 拷贝构造函数// std::begin()和end()在#include &lt;iterator&gt; 中int arr[] = {1, 2, 5, 8, 9};set&lt;int&gt; s2{std::begin(arr), std::end(arr)}; // 迭代器</code></pre><h4 id="2-1-3-定义排序方法"><a href="#2-1-3-定义排序方法" class="headerlink" title="2.1.3 定义排序方法"></a>2.1.3 定义排序方法</h4><p>可以给map或set初始化时输入一个比较器，用以替代原有的比较key大小的方法（原来一般是小于号）。这个比较器可以是一个函数，也可以是一个当做函数用的函数对象。下面是一个示例。</p><pre><code class="lang-cpp">struct comp {    template&lt;typename T&gt;    bool operator() (const T&amp; l, const T&amp; r) const {        return l &gt; r;    }};int main() {    map&lt;int, int, comp&gt; m = {        {1, 5}, {2, 3}, {7, 6}    };    for (auto&amp;&amp; [key, val] : m) {        cout &lt;&lt; key &lt;&lt; &quot; &quot; &lt;&lt; val &lt;&lt; endl;    }}output&gt;&gt; 7 6&gt;&gt; 2 3&gt;&gt; 1 5</code></pre><p>实现自己写的比较器，简单来说就是实现一个自定义的“&lt;”小于号。<br>原有的比较器<code>bool compare(object&amp; left, object&amp; right)</code>作用如下：</p><ol><li>compare输出true，map 认为 left 小于 right ，把left放在right前面。</li><li>compare输出false，map 认为 left 大于等于 right。</li><li>如果compare(left, right)为false，compare(right, left)也为false，就认为left==right。否则left放在right后面。</li></ol><blockquote><p>自己实现的比较器，必须让关键字集合满足以下性质：</p><ul><li>关键字自己不能小于自己</li><li>两个关键字不能互相小于对方</li><li>如果按照比较器，key1小于key2，key2小于key3，则key1小于key3</li></ul><p>学过离散数学的同学应该知道，上面描述的这种二元关系满足反自反性、反对称性和传递性。满足这三个性质的关系称之为“严格偏序关系”。我们日常生活中见到的数字比较的小于号、集合中“真包含于”都是这种关系。</p></blockquote><h3 id="2-2-添加元素"><a href="#2-2-添加元素" class="headerlink" title="2.2 添加元素"></a>2.2 添加元素</h3><h4 id="2-2-1-insert"><a href="#2-2-1-insert" class="headerlink" title="2.2.1 insert"></a>2.2.1 insert</h4><p>对于map</p><pre><code class="lang-cpp">map&lt;string, int&gt; m;m.insert({&quot;str&quot;, 1});m.insert(make_pair(&quot;ser&quot;, 1));m.insert(pair&lt;string, int&gt;(&quot;ssr&quot;, 1));m.insert(map&lt;string, int&gt;::value_type(&quot;sdr&quot;, 1));</code></pre><p>对于set</p><pre><code class="lang-cpp">vector&lt;int&gt; ivec = {2, 4, 6, 8};set&lt;int&gt; set2;set2.insert(1);set2.insert({2, 4, 6, 8});set2.insert(ivec.cbegin(), ivec.cend());</code></pre><p>insert函数和emplace函数返回pair对象，pair.first为迭代器，指向刚插入的元素，pair.second为bool，表示插入是否成功。如果由于存在重复导致插入失败，则除了second为false之外，first指向那个重复元素。</p><pre><code class="lang-cpp">set&lt;int&gt; s;const auto [iter, success] = s.insert(x); // 返回值拆成两个</code></pre><h4 id="2-2-2-对map使用下标-操作"><a href="#2-2-2-对map使用下标-操作" class="headerlink" title="2.2.2 对map使用下标[]操作"></a>2.2.2 对map使用下标[]操作</h4><p>map使用下标操作首先会查找该key的元素，找不到就新建一个key的pair，将其初始化。最后执行赋值操作。</p><pre><code class="lang-cpp">map&lt;char, int&gt; mp;mp[&#39;a&#39;] = 5;mp[&#39;b&#39;] = 4;mp[&#39;c&#39;] = 3;</code></pre><h3 id="2-3-访问和查找元素"><a href="#2-3-访问和查找元素" class="headerlink" title="2.3 访问和查找元素"></a>2.3 访问和查找元素</h3><p>map除了使用下标操作访问元素之外，还可以用<code>at()</code>函数。</p><pre><code class="lang-cpp">map&lt;char, int&gt; mp;mp.at(k) // 查找关键字为k的元素，找不到就抛出异常</code></pre><p>关联容器内置的<code>find</code>函数和<code>count</code>函数可以执行查找操作</p><pre><code class="lang-cpp">//c为一个map容器c.find(k) // 返回一个迭代器，指向关键字为k的元素。若k不在容器中，则返回尾后迭代器c.count(k) // 返回关键字等于k的元素数量。对于map和set而言，返回值永远是0或1。</code></pre><p>当我们要在map容器中查找一个元素时，我们可以使用find函数查找。</p><pre><code class="lang-cpp">auto it = word_count.find(&quot;foobar&quot;);if(it==word_count.end())   cout&lt;&lt;&quot;foobar is not in the map&quot;&lt;&lt;endl;else   cout&lt;&lt;it-&gt;first&lt;&lt;&quot; &quot;&lt;&lt;it-&gt;second&lt;&lt;endl;</code></pre><p>在有序容器中，我们还可以找到关键字k附近的元素。</p><pre><code class="lang-cpp">s.lower_bound(k); // 返回迭代器，指向第一个关键字**不小于**k的元素s.upper_bound(k); // 返回迭代器，指向第一个关键字**大于**k的元素s.equal_range(k); // 返回pair&lt;iter, iter&gt;，表示关键字为k的元素范围。适用于multiset/multimap。若是没有k，则返回两个end()</code></pre><p>在对于允许重复关键字的容器来说，查找元素的过程稍微复杂些，因为一个关键字可能对应多个值，我们需要把这么对应的值都找出来。<br>如果multimap中有多个元素具有相同的关键字，则这些关键字在容器中会相邻存储。我们可以通过这一特性，将一个关键字对应的多个值全部找出来。</p><pre><code class="lang-cpp">//《C++ Primer》示例，查找某作者对应的所有书籍//authors是一个multimap容器string search_item(&quot;Alain&quot;);int numbers=authors.count(search_item);auto it=authors.find(search_item);while(numbers){   cout&lt;&lt;iter-&gt;second&lt;&lt;endl;   ++it;   numbers--;}// 或者采用一种其他方式for (auto beg = authors.lower_bound(search_item),          end = authors.upper_bound(search_item);     beg != end; ++beg) {    cout &lt;&lt; beg-&gt;second &lt;&lt; endl; }// 或者采用一种更加直接的方式for (auto pos = authors.equal_range(search_item);     pos.first != pos.second; ++pos.first){    cout &lt;&lt; pos.first-&gt;second &lt;&lt; endl; // 打印每本书}</code></pre><h3 id="2-4-删除元素"><a href="#2-4-删除元素" class="headerlink" title="2.4 删除元素"></a>2.4 删除元素</h3><p>使用erase</p><pre><code class="lang-cpp">// s为关联容器，可能为set/map/multiset/multimaps.erase(k); // 删除指定关键字的元素，返回删除的个数。s.erase(iter); // iter必须指向s中的一个真实元素，返回指向删除元素之后的元素的迭代器。s.erase(iter1, iter2); // 删除迭代器[iter1, iter2)，其中必须是真是的元素。iter2指向的元素不删除。返回iter2</code></pre><h2 id="3-无序容器特有操作"><a href="#3-无序容器特有操作" class="headerlink" title="3. 无序容器特有操作"></a>3. 无序容器特有操作</h2><h3 id="3-1-无序容器特点"><a href="#3-1-无序容器特点" class="headerlink" title="3.1 无序容器特点"></a>3.1 无序容器特点</h3><p>如果我们不关心容器中元素的次序，那么我们就可以使用无序容器。在无序容器中，元素没有明确的排列次序，当你迭代容器内的所有元素时，会发现他们的次序个有可能。我们唯一关心的是某个元素特定元素是否位于容器内。</p><p>无需容器，常常以Hash table实现出来，内部结构是一个类似于<code>vector&lt;list&gt;</code>的列表，列表的元素是链表<code>linked list</code>。通过某个hash函数的运算，确定元素落于这个列表的位置。</p><p>Hash函数的运算目标是让每个元素的落点（位置）有助于用户快速访问任何一个元素（前提则是哈希函数本身也必须够快）。</p><p>由于这样一个快速而完美的哈希函数不一定存在。抑或由于造成array耗费巨额内存而显得不切实际，因此退而求其次的哈希函数有可能让多个元素落于同一位置上，所以设计上就让vector的元素再被放进一个linked list中。如此一来，vector的每个位置就得以存放一个以上的元素。</p><p><img src="/2020/03/20/C-关联容器学习笔记/2020-03-20-18-01-59.png" srcset="/img/loading.gif" alt></p><p>无序容器的主要优点是，当你打算查找一个特定值的元素，其速度甚至可能快过有序关联式容器（时间复杂度O(1)）。前提是你有一个良好的哈希函数。然而这样的哈希函数可能需要许多内存。</p><p>以下所有操作需要添加头文件：</p><pre><code class="lang-cpp">#include &lt;unordered_map&gt;#include &lt;unordered_set&gt;</code></pre><p><code>unordered_map</code>/<code>unordered_multimap</code>/<code>unordered_set</code>/<code>unordered_multiset</code>的初始化、插入、查找和删除的方法与普通的<code>map</code>/<code>multimap</code>/<code>set</code>/<code>multiset</code>没有大的区别。注意以下主要区别要点：</p><ol><li>无序容器内部不含比较器，因此你也不能提供自定义比较器。</li><li>每次对无序容器的添加操作可能会引起无序容器次序的改变。即便是相同元素，在不同的电脑上也可能得到不同的次序。</li><li>删除元素虽然不会引起无序容器次序改变，但是删除之后的第一次插入必然会引发次序改变。</li></ol><p>次序究竟会不会变化、怎样变化取决于使用的rehashing策略，该策略可由程序员自定义，就像在有序关联容器里定义排序函数那样。</p><h3 id="3-2-管理桶"><a href="#3-2-管理桶" class="headerlink" title="3.2 管理桶"></a>3.2 管理桶</h3><p>1.桶接口</p><pre><code class="lang-cpp">m.bucket_count()        正在使用的桶的数目m.max_bucket_count()    容器能容纳的最多的桶的数量m.bucket_size(n)        第n个桶中有多少个元素m.bucket(k)             关键字为k的元素在哪个桶</code></pre><p>2.桶迭代</p><pre><code class="lang-cpp">local_iterator            可以用来访问桶中元素的迭代器类型const_local_iterator      桶迭代器的const版本m.begin(n)、m.end(n)      桶n的首元素迭代器和尾后迭代器（n是什么类型？）m.cbegin(n)、m.cend(n)    与前两个函数类似，但返回const_local_iterator</code></pre><p>3.哈希策略</p><pre><code class="lang-cpp">//每个桶的平均元素数量，返回float值m.load_factor() //m试图维护的平均桶大小，返回float值，要求创建的新桶的load_factor&lt;=max_load_factor         m.max_load_factor() //重新存储，使得bucket_count&gt;=n，且bucket_count&gt;size/max_load_factor         m.rehash(n)  //重新存储，使得m可以保存n个元素且不必rehash m.reserve(n)</code></pre><h3 id="3-3-自定义哈希函数和比较函数"><a href="#3-3-自定义哈希函数和比较函数" class="headerlink" title="3.3 自定义哈希函数和比较函数"></a>3.3 自定义哈希函数和比较函数</h3><p>默认情况下，无序容器使用<code>==</code>来判断两key是否相等，并使用系统内置的哈希函数生成哈希值。不同类型的key会应用到不同的哈希函数，如下都是STL内置的哈希函数对象：</p><pre><code class="lang-cpp">struct hash&lt;char*&gt;struct hash&lt;const char*&gt;struct hash&lt;char&gt; struct hash&lt;unsigned char&gt; struct hash&lt;signed char&gt;struct hash&lt;short&gt;struct hash&lt;unsigned short&gt; struct hash&lt;int&gt; struct hash&lt;unsigned int&gt;struct hash&lt;long&gt; struct hash&lt;unsigned long&gt;</code></pre><p>如果key使用的是以上类型中的一种，可以使用缺省的hash函数。当然你程序员可以定义自己的hash函数。对于自定义对象，只能自定义hash函数。</p><p>下面是《C++ Primer》的一个自定义哈希函数的一个例子：</p><pre><code class="lang-cpp">/* 定义哈希函数和判等器 */size_t hasher(const Sales_data &amp;sd){    // 对书籍对象的哈希    return hash&lt;string&gt;() (sd.isbn()); // 返回其isbn编号的哈希，调用内置的string哈希函数}bool eqOp(const Sales_data &amp;lhs, const Sales_data &amp;rhs){    // 如何判断两本书是否相等？    return lhs.isbn() == rhs.isbn(); // 判断两书的isbn编号是否相等}/* 使用哈希函数和判等器 */using SD_multiset = unordered_multiset&lt;Sales_data, decltype(hasher)*, decltype(eqOp)*&gt;; // 类型名太长了，将类型名保存成别的变量名SD_multiset bookstore(42, hasher, eqOp);</code></pre><h2 id="4-小结"><a href="#4-小结" class="headerlink" title="4. 小结"></a>4. 小结</h2><p>对C++ 关联容器的总结到此告一段落。本篇文章从开始着手写作到完成，不间断地工作了八个小时，期间不断重温已经遗忘的知识，查阅资料，其中很多还是自己曾经收藏过的资料。</p><p>写作时，我多次问自己：写一篇不会有人看的文章值得吗？我也多次想要像我以前很多文章、像CSDN大多数的文章那样，随便水水，记录一下，反正只有自己看。</p><p>但是这次我觉得，我要为自己负责，要为已经付出的精力和时间负责。我相信大家都会有这种迷茫的时候，怀疑自己手头上的工作有没有意义，甚至想放弃。</p><p>不要轻言放弃，尤其是当你怀疑它的意义的时候。因为这个时候你可能是在为自己的懒惰找借口。将一件事情的意义贬低，这种想法出现的太容易，又太能让自己解脱了。这是一种让人没有负罪感的放弃方式。但是回头看，很多好想法，明明只要坚持一下就可以实现。因为对意义的评价，近乎于预测未来，我们大多数平凡人是没有这种本事的。</p><p>Be a better man, 每天进步一点点。大家共勉！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-关联容器特点简介&quot;&gt;&lt;a href=&quot;#1-关联容器特点简介&quot; class=&quot;headerlink&quot; title=&quot;1. 关联容器特点简介&quot;&gt;&lt;/a&gt;1. 关联容器特点简介&lt;/h2&gt;&lt;p&gt;关联容器和顺序容器是两种适用范围不同的容器。许多C++程序员只用过顺序容
      
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="C++" scheme="https://superlova.github.io/tags/C/"/>
    
      <category term="map" scheme="https://superlova.github.io/tags/map/"/>
    
      <category term="set" scheme="https://superlova.github.io/tags/set/"/>
    
      <category term="STL" scheme="https://superlova.github.io/tags/STL/"/>
    
  </entry>
  
  <entry>
    <title>DeepStellar--Model-Based Quantitative Analysis of Stateful Deep Learning Systems 论文阅读笔记</title>
    <link href="https://superlova.github.io/2020/03/19/DeepStellar%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>https://superlova.github.io/2020/03/19/DeepStellar%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</id>
    <published>2020-03-19T07:38:00.000Z</published>
    <updated>2020-03-25T02:37:49.035Z</updated>
    
    <content type="html"><![CDATA[<p>DeepStellar 是少有的针对RNN进行测试的工具，它提出了RNN测试的新思路。<br>封面上的大神就是谢肖飞博士，他是DeepStellar一问的作者。<br><a id="more"></a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DeepStellar 是少有的针对RNN进行测试的工具，它提出了RNN测试的新思路。&lt;br&gt;封面上的大神就是谢肖飞博士，他是DeepStellar一问的作者。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://superlova.github.io/categories/paper/"/>
    
    
      <category term="RNN" scheme="https://superlova.github.io/tags/RNN/"/>
    
      <category term="testing" scheme="https://superlova.github.io/tags/testing/"/>
    
      <category term="DeepStellar" scheme="https://superlova.github.io/tags/DeepStellar/"/>
    
  </entry>
  
  <entry>
    <title>循环神经网络模型的覆盖率调研</title>
    <link href="https://superlova.github.io/2020/03/19/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A6%86%E7%9B%96%E7%8E%87%E8%B0%83%E7%A0%94/"/>
    <id>https://superlova.github.io/2020/03/19/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A6%86%E7%9B%96%E7%8E%87%E8%B0%83%E7%A0%94/</id>
    <published>2020-03-19T01:07:47.000Z</published>
    <updated>2020-03-20T01:12:40.514Z</updated>
    
    <content type="html"><![CDATA[<h1 id="循环神经网络模型的覆盖率调研"><a href="#循环神经网络模型的覆盖率调研" class="headerlink" title="循环神经网络模型的覆盖率调研"></a>循环神经网络模型的覆盖率调研</h1><h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><h3 id="1-1-循环神经网络"><a href="#1-1-循环神经网络" class="headerlink" title="1.1 循环神经网络"></a>1.1 循环神经网络</h3><p>循环神经网络（Recurrent neural network：RNN）是神经网络的一种。</p><p>单纯的RNN因为无法处理随着递归，权重指数级爆炸或梯度消失问题，难以捕捉长期时间关联；而结合不同的LSTM可以很好解决这个问题。循环神经网络可以描述动态时间行为。和前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。手写识别是最早成功利用RNN的研究结果。</p><p>为了更好地理解循环神经网络，首先需要介绍前馈神经网络。</p><h4 id="1-前馈神经网络"><a href="#1-前馈神经网络" class="headerlink" title="1) 前馈神经网络"></a>1) 前馈神经网络</h4><p>前馈网络通过在网络的每个节点上做出的一系列操作传递信息。前馈网络每次通过每个层直接向后传递信息。这与循环神经网络不同。一般而言，前馈网络接受一个输入并据此产生输出，这也是大多数监督学习的步骤，输出结果可能是一个分类结果。输出可以是以猫狗等作为标签的类别。我们常见的卷积神经网络（CNN）就是一类经典的前馈网络。</p><p>前馈网络是基于一系列预先标注过的数据训练的。训练阶段的目的是减少前馈网络猜类别时的误差。一旦训练完成，我们就可以用训练后的权重对新批次的数据进行分类。</p><p><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-18-22-02-26.png" srcset="/img/loading.gif" alt="img1"></p><p>在前馈网络中，在<strong>测试阶段</strong>无论展示给分类器的图像是什么，都不会改变权重，所以也不会影响第二个决策。这是前馈网络和循环网络之间一个非常大的不同。也就是说，前馈网络<strong>在测试时</strong>不会记得之前的输入数据。它们只会<strong>在训练阶段</strong>记得历史输入数据。</p><p>与前馈神经网络不同，循环网络不仅将当前的输入样例作为网络输入，还将它们之前感知到的一并作为输入。</p><h4 id="2-前馈网络到循环网络的转变"><a href="#2-前馈网络到循环网络的转变" class="headerlink" title="2) 前馈网络到循环网络的转变"></a>2) 前馈网络到循环网络的转变</h4><p>下图是一个多层感知机示意图，该图所示的模型只拥有一个隐藏层（Hidden Layer），其接受来自输入层经过ReLU处理后的信号，输出的信号再经过Softmax层，从而产生一个分类结果。</p><p><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-18-22-06-04.png" srcset="/img/loading.gif" alt="img2"></p><p>如果在上述示例中的层数增加了，并且我们令隐藏层也接收输入，那么第一个隐藏层将激活传递到第二个隐藏层上，以此类推，最后到达输出层，每一层都有自己的权重（W）、偏置项（B）和激活函数（F）。</p><p><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-18-22-09-56.png" srcset="/img/loading.gif" alt="img3"></p><p>我们令所有隐藏层的权重和偏置项替换成相同的值，从而能使得隐藏层在某种意义上“合并”，如下图所示（注意图中方框内部隐藏层的参数）：</p><p><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-18-22-12-56.png" srcset="/img/loading.gif" alt="img4"></p><p>现在我们就可以将所有层合并在一起了。所有的隐藏层都可以结合在一个循环层中，如下图：</p><p><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-18-22-13-27.png" srcset="/img/loading.gif" alt="img5"></p><p>我们在每一步都会向隐藏层提供输入。现在一个循环神经元存储了所有之前步的输入，并将这些信息和当前步的输入合并。因此，它还捕获到一些当前数据步和之前步的相关性信息。t-1 步的决策影响到第 t 步做的决策。</p><p>如果我们在向网络输入 7 个字母后试着找出第 8 个字母，隐藏层会经历 8 次迭代。如果展开网络的话就是一个 8 层的网络，每一层对应一个字母。所以一个普通的神经网络被重复了多次。展开的次数与它记得多久之前的数据是直接相关的。</p><h4 id="3-循环神经网络基本结构"><a href="#3-循环神经网络基本结构" class="headerlink" title="3) 循环神经网络基本结构"></a>3) 循环神经网络基本结构</h4><p>下图是一个简单的循环神经网络如，它由输入层、一个隐藏层和一个输出层组成：<br><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-18-23-31-00.png" srcset="/img/loading.gif" alt></p><ul><li>x是一个向量，它表示输入层的值；<br>s是一个向量，它表示隐藏层的值（你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；<br>U是输入层到隐藏层的权重矩阵；<br>o也是一个向量，它表示输出层的值；<br>V是隐藏层到输出层的权重矩阵；<br>W是隐藏层上一次的值作为这一次的输入的权重。</li></ul><p>把上面的图展开，循环神经网络也可以画成下面这个样子：<br><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-18-23-33-05.png" srcset="/img/loading.gif" alt></p><p>网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$。关键一点是，$s_t$的值不仅仅取决于$x_t$，还取决于$s_{t-1}$。</p><p>我们可以用下面的公式来表示循环神经网络的计算方法：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{o}_{t} &=g\left(V \mathbf{s}_{t}\right) \tag{1}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}\mathbf{s}_{t} &=f\left(U \mathbf{x}_{t}+W \mathbf{s}_{t-1}\right) \tag{2}\end{aligned}</script><p>(1)是输出层的计算公式，输出层是一个全连接层，它的每个节点都和隐藏层的各个节点相连。V是输出层的权重矩阵，g是激活函数。(2)是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一次的值作为这一次的输入的权重矩阵，f是激活函数。</p><p>从上面的公式我们可以看出，循环层和全连接层的区别就是循环层多了一个权重矩阵 W。</p><h4 id="4-长短期记忆网络"><a href="#4-长短期记忆网络" class="headerlink" title="4) 长短期记忆网络"></a>4) 长短期记忆网络</h4><p>长短期记忆（英语：Long Short-Term Memory，LSTM）是一种时间循环神经网络（RNN），论文首次发表于1997年。由于独特的设计结构，LSTM适合于处理和预测时间序列中间隔和延迟非常长的重要事件。</p><p>LSTM是一种含有LSTM区块（blocks）或其他的一种类神经网络，文献或其他资料中LSTM区块可能被描述成智能网络单元，因为它可以记忆不定时间长度的数值，区块中有一个gate能够决定input是否重要到能被记住及能不能被输出output。</p><p>下图中，底下是四个S函数单元，最左边的单元为input，右边三个gate决定input是否能传入下个；左边第二个为input gate，如果这里gate近似于零，将把这里的值挡住，不会进到下一层。左数第三个是forget gate，当这产生值近似于零，将把过去记住的值忘掉。第四个也就是最右边的input为output gate，他可以决定在区块记忆中的input是否能输出 。</p><p><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-18-23-51-41.png" srcset="/img/loading.gif" alt></p><p>LSTM有很多个版本，其中一个重要的版本是GRU（Gated Recurrent Unit），根据谷歌的测试表明，LSTM中最重要的是Forget gate，其次是Input gate，最次是Output gate。</p><h3 id="1-2-循环神经网络测试的机遇和挑战"><a href="#1-2-循环神经网络测试的机遇和挑战" class="headerlink" title="1.2 循环神经网络测试的机遇和挑战"></a>1.2 循环神经网络测试的机遇和挑战</h3><p>当前研究大多集中于针对前馈神经网络的测试，诸如DeepXplore、DeepGauge针对CNN的测试等，而对RNN鲜有研究。由于RNN的循环特性，适用于CNN的分析方法不能简单地迁移到RNN上。目前学者大多采用模糊测试的方法，通过随机干扰数据集产生对抗性样本，而后分析RNN内部状态信息，引导数据集的扰动方向的方法，快速生成能使得RNN模型判断错误的对抗测试用例集合，达到模型测试的目的。</p><p>根据此思路，一方面可以针对数据集添加干扰的方式进行优化，采用启发式搜索改善对测试样本的Mutation过程；另一方面则是针对RNN内部状态信息的分析，引导算法快速找到有效的、难以察觉变化的对抗样本，这个思路类似于模型攻击。</p><p>目前RNN的对抗性测试主要面临三方面的挑战：</p><ol><li>对于非分类模型而言，没有较好的标准识别对抗样本能不能让模型发生错误。</li></ol><blockquote><p>For the sequential outputs not then applied to classification, there is no standard to decide the outputs as wrong outputs with respect to the changing degree.</p></blockquote><ol><li>对于序列输入的Mutation来说，很难保证添加的扰动是最小的</li></ol><blockquote><p>Applying the perturbations to words in a discrete space always cannot obtain a legal input and the explicit modification is distinguishable for humans.</p></blockquote><ol><li>现有应用于CNN等的覆盖率指标没有考虑到RNN内部结构特性，因此不能直接应用到RNN上。</li></ol><h2 id="2-覆盖指标调研"><a href="#2-覆盖指标调研" class="headerlink" title="2. 覆盖指标调研"></a>2. 覆盖指标调研</h2><p>目前针对循环神经网络的测试 (testing) 和验证 (verification) 等工作的研究还十分有限，根据目前调研取得的结果，现有学者的研究思路大体分成两类：抽象替代模型法和门覆盖率法。下文分别对这两个模型进行简述。</p><h3 id="2-1-抽象替代模型法"><a href="#2-1-抽象替代模型法" class="headerlink" title="2.1 抽象替代模型法"></a>2.1 抽象替代模型法</h3><h4 id="2-1-1-代表论文："><a href="#2-1-1-代表论文：" class="headerlink" title="2.1.1 代表论文："></a>2.1.1 代表论文：</h4><blockquote><p>Du, X., Xie, X., Li, Y., Ma, L., Liu, Y., &amp; Zhao, J. (2019, August). <strong>Deepstellar: model-based quantitative analysis of stateful deep learning systems.</strong> In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 477-487).</p><p>Du, X., Xie, X., Li, Y., Ma, L., Liu, Y., &amp; Zhao, J. (2019, November). <strong>A Quantitative Analysis Framework for Recurrent Neural Network.</strong> In 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE) (pp. 1062-1065). IEEE.</p></blockquote><h4 id="2-1-2-关键方法"><a href="#2-1-2-关键方法" class="headerlink" title="2.1.2 关键方法"></a>2.1.2 关键方法</h4><p>由于直接分析RNN内部结构具有状态转移的特性，因此DeepStellar一文提出<strong>将RNN建模成马尔科夫链</strong>，来模拟其内部状态和动态行为特性。基于马尔科夫链的抽象，该文设计了两个相似度指标和五个覆盖率标准，来衡量输入测试用例差异和测试用例的测试充分性。</p><p><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-19-02-00-02.png" srcset="/img/loading.gif" alt></p><ul><li><p>抽象模型结构模块：输入训练好的RNN，通过Profiling，分析其内部行为。一系列的RNN状态向量叫做trace。每个输入序列会通过Profiling分析得到一个trace。profiling结束之后就可以得到一系列的trace，记载着RNN训练过程访问过的和经过的状态。</p></li><li><p>分析内部状态空间和被训练集激活的trace的过程计算量很大，因此进一步抽象模型来简化状态和trace。首先对状态向量采用主成分分析，以保留其前k个主成分，并将这k个主成分等分成m部分。在状态转换方面，根据抽象状态将具体的转换概括成为抽象的转换。并且根据每个状态向不同方向转换的频率，导出了训练RNN的离散时间Markov链（DTMC）模型。</p></li><li><p>设计了两个相似度指标，用于衡量不同输入下激活的两个trace的相似度。分别是state-based trace similarity和transition-based trace similarity，简写为SBTSIM和TBTSIM。</p></li><li><p>五个覆盖率包括basic state coverage/n-step state boundary coverage, weighted state coverage/ basic transition coverage/weighted transition coverage，简写为BSCov/n-SBCov/WSCov/BTCov/WTCov。</p></li><li><p>将两个指标和五个标准应用于对抗样本检测和覆盖引导的测试上，来缓解来自对抗样本的威胁。运用两个相似度，即可在运行时检测对抗样本（<strong>Monitor</strong>）；运用五个覆盖率，我们将其用于指导测试用例生成上，生成的测试用例以提升覆盖率和找到更多的未被发现的defects为目标。这两个应用互相补充。</p></li></ul><h4 id="2-1-3-实验效果及评价"><a href="#2-1-3-实验效果及评价" class="headerlink" title="2.1.3 实验效果及评价"></a>2.1.3 实验效果及评价</h4><p>DeepStellar在精心的调参下，通过trace相似度检测算法能够很好地检测出当前输入样本是否为对抗样本（音频），准确度达到了89%。</p><p>DeepStellar提出的测试方法本质上是对一个等价模型进行分析的方法，抽象掉了RNN模型的很多细节，只保留了主干部分。思路值得借鉴。</p><p>DeepStellar的缺点也很明显，其高识别率的背后是精心的调参，并且作者也提到，面对更复杂的模型，结果未必会这么好。</p><blockquote><p>With finer-grained model, the result is not necessarily better.</p></blockquote><h3 id="2-2-门覆盖率法"><a href="#2-2-门覆盖率法" class="headerlink" title="2.2 门覆盖率法"></a>2.2 门覆盖率法</h3><h4 id="2-2-1-代表论文："><a href="#2-2-1-代表论文：" class="headerlink" title="2.2.1 代表论文："></a>2.2.1 代表论文：</h4><blockquote><p>Huang, W., Sun, Y., Huang, X., &amp; Sharp, J. (2019). <strong>testRNN: Coverage-guided Testing on Recurrent Neural Networks.</strong> arXiv preprint arXiv:1906.08557.</p></blockquote><h4 id="2-2-2-关键方法"><a href="#2-2-2-关键方法" class="headerlink" title="2.2.2 关键方法"></a>2.2.2 关键方法</h4><p>testRNN关注LSTM和其鲁棒性，鲁棒性指对输入添加小的扰动并不影响LSTM的判断结果的特性。注意，该工具只针对LSTM及相似的网络结构进行分析，原因在于其算法依赖于内部门结构的实现，而这种门结构只存在于LSTM类型的网络中。</p><p>testRNN的特色在于，其直接分析RNN内部结构并加以分析的思路非常类似于其前辈DeepXplore分析前馈神经网络的思路。但由于RNN网络内部关于“层”和“节点”的概念不同于CNN，因此对如何实现CNN中覆盖率迁移到RNN的应用中，testRNN提出了自己的方法。</p><ul><li><p>Cell覆盖率。Cell（后文称之为<strong>单元</strong>）覆盖旨在覆盖每个时间步的隐藏状态发生的显著变化$\Delta\xi_t$。当单元值$\Delta\xi_t$大于用户定义的阈值参数$\alpha_h$时，该单元将被激活并被测试用例覆盖。然后使用覆盖率来衡量由生成的测试用例激活至少一次的单元的百分比。（单元的隐藏状态变化大了，超过了用户定义的某个阈值，就算激活，测试用例激活的单元个数占总个数的比例就是该测试用例的覆盖率）</p></li><li><p>Gate覆盖率。门的覆盖率类似于单元覆盖率，但是信息是从LSTM单元的门中筛选的。上文提到，Google的研究团队发现，LSTM的四种门中最重要的门是忘记门（forget gate），因此testRNN专注于统计忘记门的覆盖率。忘记门的值$Rt(f，x)$表示可以从最后一个单元继承多少信息。由于LSTM以其长期的存储能力而闻名，因此检查一个单元格是否丢弃了从先前输入中学习到的适当数量的信息非常有意义。（忘记们忘记的信息量用Rt函数表示，则Rt太大了就激活？）</p></li></ul><p>下图是testRNN的具体处理流程。</p><p><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-19-02-43-32.png" srcset="/img/loading.gif" alt></p><h4 id="2-2-3-实验效果"><a href="#2-2-3-实验效果" class="headerlink" title="2.2.3 实验效果"></a>2.2.3 实验效果</h4><p>testRNN着眼于门覆盖率，成功将分析CNN的那一套迁移了过来。但是testRNN的实验结果仅限于小数据集训练下的小网络，诸如MNIST分类数据集训练的双层LSTM网络等。因此能否将此方法推广到更加复杂的大型网络中，还有待探究。</p><h3 id="2-3-优化引导法"><a href="#2-3-优化引导法" class="headerlink" title="2.3 优化引导法"></a>2.3 优化引导法</h3><h4 id="2-3-1-代表论文："><a href="#2-3-1-代表论文：" class="headerlink" title="2.3.1 代表论文："></a>2.3.1 代表论文：</h4><blockquote><p>Guo, J., Zhao, Y., Han, X., Jiang, Y., &amp; Sun, J. (2019). <strong>RNN-Test: Adversarial Testing Framework for Recurrent Neural Network Systems.</strong> arXiv preprint arXiv:1911.06155.</p></blockquote><h4 id="2-3-2-关键方法"><a href="#2-3-2-关键方法" class="headerlink" title="2.3.2 关键方法"></a>2.3.2 关键方法</h4><p>RNN-Test一方面采用了和testRNN类似的“门覆盖率”方法来引导测试用例生成，另一方面采用了一种全新的优化函数思想，计算能同时使得扰动添加最小并且最有可能令模型发生判断错误的扰动方向。RNN-Test将二者结合起来，但正是因为该文仅仅是将这两种方法求得的偏移方向简单的加和，让人不禁怀疑其工作是否没有进行完全。</p><ul><li><p>状态不连续方向（State inconsistency orientation）：</p><script type="math/tex; mode=display">obj_{orient} = h_{t-1}^l + c_t^l - h_t^l</script><p>该优化函数的设计思想是，若一个样本能使得从隐状态t-1时刻输入的信息尽量大，而输出尽量小，那么这种样本更容易出现问题。这是因为t时刻$h_t^l$的值完全取决于$h_{t-1}^l $和$ c_t^l$，让这两部分产生大小差异更容易引发不确定行为。</p></li><li><p>损失函数优化方向（Cost orientation）:</p><script type="math/tex; mode=display">obj_{orient} = L_{seq}(y, \hat{y})</script><p>这一部分引用自FGSM的优化算法，该论文是对抗样本生成领域的开山之作。讲的是如何通过梯度上升算法求得添加扰动的方向，从而使得扰动最小的同时模型的变化最大。</p></li><li><p>决策边界方向（Decision boundary orientation）：</p><script type="math/tex; mode=display">obj_{orient} = (\sum_{i=0}^{k}\hat{y_{t_i}}) - \hat{y_t}</script><p>这一部分的灵感来自于RNN内部结构，由于每个时间步的隐状态事实上都会产生中间输出$y=o_t^l$，但一般我们认为只有最后阶段的输出向量才是有意义的。该优化函数将除了原来预测的最大值y之外前k个最大的y加起来，并减去原来的y。</p></li></ul><h4 id="2-3-3-实验效果"><a href="#2-3-3-实验效果" class="headerlink" title="2.3.3 实验效果"></a>2.3.3 实验效果</h4><p>该文章使用自己生成的对抗样本集合，对模型进行重新训练，模型的复杂度（Perplexity）有大约1.159%的降低。这说明模型更稳定了。</p><p>然而提升不大，运行效率却很低，计算代价较大。并且原文为了提升算法运行性能，Mutation过程采用了一遍Mutation，若不成功直接放弃的做法，原来的测试用例利用率较低、为了达到较高的突变利用率，算法不得不放宽添加干扰力度，这就导致生成太多无用的假测试用例。算法本身可以被优化。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;循环神经网络模型的覆盖率调研&quot;&gt;&lt;a href=&quot;#循环神经网络模型的覆盖率调研&quot; class=&quot;headerlink&quot; title=&quot;循环神经网络模型的覆盖率调研&quot;&gt;&lt;/a&gt;循环神经网络模型的覆盖率调研&lt;/h1&gt;&lt;h2 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1
      
    
    </summary>
    
    
      <category term="paper" scheme="https://superlova.github.io/categories/paper/"/>
    
    
      <category term="RNN" scheme="https://superlova.github.io/tags/RNN/"/>
    
      <category term="testing" scheme="https://superlova.github.io/tags/testing/"/>
    
  </entry>
  
  <entry>
    <title>如何使用VS Code编写github pages博客</title>
    <link href="https://superlova.github.io/2020/01/11/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8VS-Code%E7%BC%96%E5%86%99github-pages%E5%8D%9A%E5%AE%A2/"/>
    <id>https://superlova.github.io/2020/01/11/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8VS-Code%E7%BC%96%E5%86%99github-pages%E5%8D%9A%E5%AE%A2/</id>
    <published>2020-01-11T13:55:17.000Z</published>
    <updated>2020-03-19T01:42:00.352Z</updated>
    
    <content type="html"><![CDATA[<p>使用VS Code写博客，需要你按照我之前写的两篇博客，将github pages平台搭建起来。</p><p><a href="https://superlova.github.io/2019/04/14/%E9%85%8D%E7%BD%AEhexo+GitHub%20Pages%E7%BA%AA%E5%AE%9E/">配置hexo+GitHub Pages纪实</a><br><a href="https://superlova.github.io/2019/04/25/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/">hexo图片加载失败解决方案</a></p><p>之后我们安装VSCode。接下来介绍我一直使用的几个插件，和它们的配置小技巧。</p><p>第一个是<strong>Markdown Preview Enhanced</strong>，有了该插件，就可以提前预览markdown文件的渲染效果。方法是使用VSCode打开以md后缀名结尾的文件，右键点击<strong>Markdown Preview Enhanced： Open Preview To The Side</strong>，即可在侧边栏生成即时渲染的md效果文件。</p><p>第二个是<strong>Markdown PDF</strong>，该插件可以令写好的md文件打印成pdf格式。该插件需要安装chromium内核。</p><p>第三个是<strong><strong>Paste Image</strong></strong>插件，可以很方便地在md文章中粘贴位于剪切板的图片。</p><p>粘贴的快捷键是Ctrl+Alt+V。</p><p>在Paste Image插件的Path设置部分，改成如下所示：<br><img src="/2020/01/11/如何使用VS-Code编写github-pages博客/2020-01-11-23-28-36.png" srcset="/img/loading.gif" alt><br>这样图片粘贴的位置就变成了<strong>当前文章目录下，与该文章同名的文件夹内</strong>，方便我们进行进一步整理。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;使用VS Code写博客，需要你按照我之前写的两篇博客，将github pages平台搭建起来。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://superlova.github.io/2019/04/14/%E9%85%8D%E7%BD%AEhexo+GitHub%20P
      
    
    </summary>
    
    
      <category term="record" scheme="https://superlova.github.io/categories/record/"/>
    
    
      <category term="github pages" scheme="https://superlova.github.io/tags/github-pages/"/>
    
      <category term="hexo" scheme="https://superlova.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>总结论文中常用的Matplotlib和Seaborn绘图技术</title>
    <link href="https://superlova.github.io/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/"/>
    <id>https://superlova.github.io/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/</id>
    <published>2020-01-11T13:47:26.000Z</published>
    <updated>2020-03-19T01:45:18.572Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、使用matplotlib绘制图像"><a href="#一、使用matplotlib绘制图像" class="headerlink" title="一、使用matplotlib绘制图像"></a>一、使用matplotlib绘制图像</h1><p>matplotlib是一个Python的数据可视化2D图形库。matplotlib的特点是可以采用面向对象的方法，模仿MATLAB中的图形命令。matplotlib经常与numpy、pandas等库结合起来使用。<br>matplotlib可以采用MATLAB的命令风格使用，也可以采用面向对象的风格使用。</p><h2 id="matplotlib的图像中各组件名称"><a href="#matplotlib的图像中各组件名称" class="headerlink" title="matplotlib的图像中各组件名称"></a>matplotlib的图像中各组件名称</h2><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-11-23-42-39.png" srcset="/img/loading.gif" alt></p><h2 id="新建图像"><a href="#新建图像" class="headerlink" title="新建图像"></a>新建图像</h2><pre><code class="lang-python">fig, axes = plt.subplots(2,1,figsize=(5,10)) #两行一列组成一张图，图像大小宽5高10</code></pre><p>上面的语句创建了一个figure，由两个ax组成。把它想象成一张画布上面的两个贴画，会比较容易理解。</p><p>plt.figure()函数的前两个参数是设置figure是由几行几列的ax组成。figure(2,1)说明figure是由两行一列的ax一共两个ax组成。</p><p>后面的figsize参数设置画布的宽和高，单位为英寸。</p><h1 id="二、使用Seaborn绘制图像"><a href="#二、使用Seaborn绘制图像" class="headerlink" title="二、使用Seaborn绘制图像"></a>二、使用Seaborn绘制图像</h1><p>首先确定我们需要可视化的数据的结构。以iris鸢尾花数据集为例，</p><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-07-35.png" srcset="/img/loading.gif" alt></p><p>每一行代表一个数据对象，每一列代表数据对象的一个属性。但是现实生活的数据很多不长这样，只不过组织成一个表格的形式，内容大相径庭。因此在进行数据可视化时一定要保证你的数据也是<strong>用行代表数据对象，用列表示数据的属性</strong>。</p><h2 id="2-1-关联图"><a href="#2-1-关联图" class="headerlink" title="2.1 关联图"></a>2.1 关联图</h2><p>我们是用 <code>relplot</code>函数进行进一步绘制。实际上，<code>relplot</code> 可以看作是 <code>scatterplot</code> 和 <code>lineplot</code> 的结合版本。但是relplot包装层级更加高，这意味着它更适合快速应用，不适合自定义。如果你对它的效果不满意，恐怕还是得诉诸<code>scatterplot</code> 和 <code>lineplot</code>等与matplotlib结合更紧密的api，或者直接使用matplotlib。</p><pre><code class="lang-python">sns.relplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-10-26.png" srcset="/img/loading.gif" alt></p><p>x为花萼长度，y为花萼宽度。这样分x，y其实有一定道理，我们的目的是能够把不同类型的数据对象在图上区分开。因为同类花朵一般个头差不多，花萼的长度和宽度聚集在图的一部分区域。但是在上图我们是看不出来的。我们希望给不同类别添加不同颜色。</p><pre><code class="lang-python">sns.relplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, hue=&quot;species&quot;, data=iris)</code></pre><p>可以看到我们添加了<code>hue</code>字段，并要求按照<code>species</code>进行进一步分类。<code>hue</code>字段就是进行二次分类的参数。</p><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-15-48.png" srcset="/img/loading.gif" alt></p><p>如果是论文，则我们要使得读者在黑白打印的条件下也能发现区别。添加<code>stype</code>参数为<code>species</code>或许会有帮助。</p><pre><code class="lang-python">sns.relplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;,            hue=&quot;species&quot;, style=&quot;species&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-18-31.png" srcset="/img/loading.gif" alt></p><p>不只是散点图，该方法还支持线形图，只需要指定 <code>kind=&quot;line&quot;</code> 参数即可。</p><pre><code class="lang-python">sns.relplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;,            hue=&quot;species&quot;, style=&quot;species&quot;, kind=&quot;line&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-20-55.png" srcset="/img/loading.gif" alt></p><p>上图其实就是折线图，我们使用一个与matplotlib结合更紧密的api来探究花萼长度和花瓣长度之间的关系。</p><pre><code class="lang-python">sns.lineplot(x=&quot;sepal_length&quot;, y=&quot;petal_length&quot;,             hue=&quot;species&quot;, style=&quot;species&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-26-06.png" srcset="/img/loading.gif" alt></p><h2 id="2-2-类别图"><a href="#2-2-类别图" class="headerlink" title="2.2 类别图"></a>2.2 类别图</h2><p>懒人函数是<code>catplot</code>，<code>catplot</code>是下面几个底层函数的封装：</p><ul><li><p>分类散点图:</p><ul><li><a href="https://seaborn.pydata.org/generated/seaborn.stripplot.html" target="_blank" rel="noopener"><code>stripplot()</code></a> (<code>kind=&quot;strip&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.swarmplot.html" target="_blank" rel="noopener"><code>swarmplot()</code></a> (<code>kind=&quot;swarm&quot;</code>)</li></ul></li><li><p>分类分布图:</p><ul><li><a href="https://seaborn.pydata.org/generated/seaborn.boxplot.html" target="_blank" rel="noopener"><code>boxplot()</code></a> (<code>kind=&quot;box&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.violinplot.html" target="_blank" rel="noopener"><code>violinplot()</code></a> (<code>kind=&quot;violin&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.boxenplot.html" target="_blank" rel="noopener"><code>boxenplot()</code></a> (<code>kind=&quot;boxen&quot;</code>)</li></ul></li><li><p>分类估计图:</p><ul><li><a href="https://seaborn.pydata.org/generated/seaborn.pointplot.html" target="_blank" rel="noopener"><code>pointplot()</code></a> (<code>kind=&quot;point&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.barplot.html" target="_blank" rel="noopener"><code>barplot()</code></a> (<code>kind=&quot;bar&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.countplot.html" target="_blank" rel="noopener"><code>countplot()</code></a> (<code>kind=&quot;count&quot;</code>)</li></ul></li></ul><p>我们想知道不同类别下花萼长度的散点图。</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&#39;strip&#39;,data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-36-33.png" srcset="/img/loading.gif" alt></p><p><code>kind=&quot;swarm&quot;</code> 可以让散点按照 beeswarm 的方式防止重叠，可以更好地观测数据分布。</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;swarm&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-38-46.png" srcset="/img/loading.gif" alt></p><p>箱线图</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;box&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-41-06.png" srcset="/img/loading.gif" alt><br>变种箱线图</p><pre><code class="lang-python">sns.catplot(x=&quot;species&quot;, y=&quot;sepal_length&quot;, kind=&quot;boxen&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-41-24.png" srcset="/img/loading.gif" alt><br>提琴图</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;violin&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-41-16.png" srcset="/img/loading.gif" alt><br>点线图</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;point&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-41-36.png" srcset="/img/loading.gif" alt><br>柱状图</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;bar&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-41-48.png" srcset="/img/loading.gif" alt></p><h2 id="2-3-分布图"><a href="#2-3-分布图" class="headerlink" title="2.3 分布图"></a>2.3 分布图</h2><p>如果想看一个变量到底是正态分布、卡方分布还是指数分布，此时就要使用分布图进行可视化了。一维分布图比较常见，二维以上分布图不太直观。绘制分布图的函数有这几个：<code>jointplot</code> <code>pairplot</code> <code>distplot</code> <code>kdeplot</code>。</p><p><code>distplot</code>可以方便的查看单变量的分布图。</p><pre><code class="lang-python">sns.distplot(iris[&quot;sepal_length&quot;])</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-48-17.png" srcset="/img/loading.gif" alt><br>图上那条曲线是根据数据拟合出来的核密度估计kde曲线（原理有待学习）。如果不想要这条线，可以在参数中设置<code>kde=False</code>。更可以只要kde曲线，设置<code>hist=False</code>即可。</p><p><code>jointplot</code>绘制二元变量的分布图，比如花瓣长度和宽度的关系。</p><pre><code class="lang-python">sns.jointplot(x=&quot;petal_length&quot;, y=&quot;petal_width&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-55-40.png" srcset="/img/loading.gif" alt></p><p>kde估计图也可以在二元变量分布图中出现。还有蜂巢图<code>kind=&quot;hex&quot;</code>、回归图<code>kind=&quot;reg&quot;</code>等。</p><pre><code class="lang-python">sns.jointplot(x=&quot;petal_length&quot;, y=&quot;petal_width&quot;, data=iris, kind=&quot;kde&quot;)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-57-09.png" srcset="/img/loading.gif" alt></p><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-58-51.png" srcset="/img/loading.gif" alt></p><p>最后注意到我们的鸢尾花数据集含有四组属性。我们想探究这四组属性两两之间的关系，就需要用到<code>pairplot</code></p><pre><code class="lang-python">sns.pairplot(iris, hue=&quot;species&quot;)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-17-01-04.png" srcset="/img/loading.gif" alt></p><h2 id="2-4-回归图"><a href="#2-4-回归图" class="headerlink" title="2.4 回归图"></a>2.4 回归图</h2><p><code>regplot</code> 绘制回归图，只会绘制一组回归曲线。</p><pre><code class="lang-python">sns.regplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-17-02-28.png" srcset="/img/loading.gif" alt></p><p><code>lmplot</code> 可以引入<code>hue</code>变量，绘制不同类别数据的回归图</p><pre><code class="lang-python">sns.lmplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, hue=&quot;species&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-17-03-57.png" srcset="/img/loading.gif" alt></p><h2 id="2-5-矩阵图"><a href="#2-5-矩阵图" class="headerlink" title="2.5 矩阵图"></a>2.5 矩阵图</h2><p><code>heatmap</code>用来画热图，数据值大的格子颜色比较深。热力图在某些场景下非常实用，例如绘制出变量相关性系数热力图。<br><code>clustermap</code>用来画层次聚类结构图。对于iris数据集来说，这两类图没有用武之地。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一、使用matplotlib绘制图像&quot;&gt;&lt;a href=&quot;#一、使用matplotlib绘制图像&quot; class=&quot;headerlink&quot; title=&quot;一、使用matplotlib绘制图像&quot;&gt;&lt;/a&gt;一、使用matplotlib绘制图像&lt;/h1&gt;&lt;p&gt;matplo
      
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="matplotlib" scheme="https://superlova.github.io/tags/matplotlib/"/>
    
      <category term="seaborn" scheme="https://superlova.github.io/tags/seaborn/"/>
    
  </entry>
  
  <entry>
    <title>用于科学计算的GPU选购参考</title>
    <link href="https://superlova.github.io/2019/07/01/%E7%94%A8%E4%BA%8E%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E7%9A%84GPU%E9%80%89%E8%B4%AD%E5%8F%82%E8%80%83/"/>
    <id>https://superlova.github.io/2019/07/01/%E7%94%A8%E4%BA%8E%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E7%9A%84GPU%E9%80%89%E8%B4%AD%E5%8F%82%E8%80%83/</id>
    <published>2019-07-01T10:11:15.000Z</published>
    <updated>2020-03-19T01:44:37.971Z</updated>
    
    <content type="html"><![CDATA[<p>实验室最近要采购一批显卡，需要调研显卡的型号和价格。</p><h2 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h2><p>首先说一下需求：</p><ul><li>首先显卡的用途是科学计算，更具体一点是深度学习，有人做图像，有人做NLP；</li><li>其次预算有限，得买性价比最好的；</li><li>然后可能会有很多人要用服务器训练模型。</li></ul><p>然后这是戴尔服务器的售后人员发来的建议采购清单：<br><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-18-19-43.png" srcset="/img/loading.gif" alt></p><p>其中M10：19999￥ P100： 49999￥ V100：59999￥ P40：49999￥</p><p>值得一提的是，谷歌的Colab上面用的是这款：<br><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-18-21-48.png" srcset="/img/loading.gif" alt><br><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-18-21-58.png" srcset="/img/loading.gif" alt></p><p>但是一个从事深度学习研究的学长建议我买1080Ti。他好像提都没提过Tesla啊？难道显卡水这么深？</p><h2 id="GPU参数"><a href="#GPU参数" class="headerlink" title="GPU参数"></a>GPU参数</h2><p>GPU的性能主要由下面三个主要参数构成：</p><p><strong>计算能力</strong>。通常我们关心的是32位浮点计算能力。当然，对于高玩来说也可以考虑16位浮点用来训练，8位整数来预测。</p><p><strong>内存大小</strong>。神经网络越深，或者训练时批量大小越大，所需要的GPU内存就越多。</p><p><strong>内存带宽</strong>。内存带宽要足够才能发挥出所有计算能力。</p><p>此外，针对不同深度学习架构，GPU参数的选择优先级是不一样的，总体来说分两条路线：</p><p><strong>卷积网络和Transformer</strong>：张量核心&gt;FLOPs（每秒浮点运算次数）&gt;显存带宽&gt;16位浮点计算能力</p><p><strong>循环神经网络</strong>：显存带宽&gt;16位浮点计算能力&gt;张量核心&gt;FLOPs</p><p>这个排序背后有一套逻辑，下面将详细解释一下。</p><p>在说清楚哪个GPU参数对速度尤为重要之前，先看看两个最重要的张量运算：矩阵乘法和卷积。</p><p>举个栗子，以运算矩阵乘法A×B=C为例，将A、B复制到显存上比直接计算A×B更耗费资源。也就是说，如果你想用LSTM等处理大量小型矩阵乘法的循环神经网络，显存带宽是GPU最重要的属性。</p><p>矩阵乘法越小，内存带宽就越重要。</p><p>相反，卷积运算受计算速度的约束比较大。因此，要衡量GPU运行ResNets等卷积架构的性能，最佳指标就是FLOPs。张量核心可以明显增加FLOPs。</p><p>Transformer中用到的大型矩阵乘法介于卷积运算和RNN的小型矩阵乘法之间，16位存储、张量核心和TFLOPs都对大型矩阵乘法有好处，但它仍需要较大的显存带宽。</p><h2 id="性价比分析"><a href="#性价比分析" class="headerlink" title="性价比分析"></a>性价比分析</h2><p>下面总结了一张GPU和TPU的标准性能数据，值越高代表性能越好。RTX系列假定用了16位计算，Word RNN数值是指长度&lt;100的段序列的biLSTM性能。</p><p>这项基准测试是用PyTorch 1.0.1和CUDA 10完成的。</p><p><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-19-09-53.png" srcset="/img/loading.gif" alt="GPU和TPU的性能数据"></p><p>性价比可能是选择一张GPU最重要的考虑指标。</p><p>性价比可能是选择一张GPU最重要的考虑指标。在攻略中，小哥进行了如下运算测试各显卡的性能：</p><ul><li>用语言模型Transformer-XL和BERT进行Transformer性能的基准测试。</li><li>用最先进的biLSTM进行了单词和字符级RNN的基准测试。</li><li>上述两种测试是针对Titan Xp、Titan RTX和RTX 2080 Ti进行的，对于其他GPU则线性缩放了性能差异。</li><li>借用了现有的CNN基准测试。</li><li>用了亚马逊和eBay上显卡的平均售价作为GPU的参考成本。<br>最后，可以得出CNN、RNN和Transformer的归一化性能/成本比值，如下所示：</li></ul><p><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-19-11-20.png" srcset="/img/loading.gif" alt="CNN、RNN和Transformer的每美元性能"></p><p>在上面这张图中，数字越大代表每一美元能买到的性能越强。可以看出， RTX 2060比RTX 2070，RTX 2080或RTX 2080 Ti更具成本效益，<strong>甚至是Tesla V100性价比的5倍以上</strong>。</p><p>所以此轮的性价比之王已经确定，是RTX 2060无疑了。</p><p>下图是李沐老师画了900和1000系列里各个卡的32位浮点计算能力和价格的对比（价格是wikipedia的推荐价格，真实价格通常会有浮动）。</p><p><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-19-06-00.png" srcset="/img/loading.gif" alt></p><p>由于GPU的功耗，散热和体积，需要一些额外考虑。</p><ul><li>机箱体积<br>GPU尺寸较大，通常不考虑太小的机箱。而且机箱自带的风扇要好。</li><li>电源<br>购买GPU时需要查下GPU的功耗，50w到300w不等。因此买电源时需要功率足够的。</li><li>主板的PCIe卡槽<br>推荐使用PCIe 3.0 16x来保证足够的GPU到主内存带宽。如果是多卡的话，要仔细看主板说明，保证多卡一起使用时仍然是16x带宽。（有些主板插4卡时会降到8x甚至4x）</li></ul><h2 id="Tesla为什么那么贵？"><a href="#Tesla为什么那么贵？" class="headerlink" title="Tesla为什么那么贵？"></a>Tesla为什么那么贵？</h2><p>英伟达现在有一项非常坑爹的政策，如果在数据中心使用CUDA，那么只允许使用Tesla GPU而不能用GTX或RTX GPU。</p><p>由于担心法律问题，研究机构和大学经常被迫购买低性价比的Tesla GPU。<strong>然而，Tesla与GTX和RTX相比并没有真正的优势，价格却高出10倍。</strong></p><p>Nvidia卡有面向个人用户（例如GTX系列）和企业用户（例如Tesla系列）两种。企业用户卡通常使用被动散热和增加了内存校验从而更加适合数据中心。但计算能力上两者相当。<strong>企业卡通常要贵上10倍</strong>。</p><p>Tesla显卡那么贵，其实是贵在双精度浮点数运算能力上了，外加一个鸡肋的ECC校验功能，实在不值。</p><h2 id="总结建议："><a href="#总结建议：" class="headerlink" title="总结建议："></a>总结建议：</h2><p><strong>最佳GPU</strong>：RTX 2070</p><p><strong>避免的坑</strong>：所有Tesla、Quadro、创始人版（Founders Edition）的显卡，还有Titan RTX、Titan V、Titan XP</p><p><strong>高性价比</strong>：RTX 2070（高端），RTX 2060或GTX 1060 (6GB)（中低端）</p><p><strong>计算机视觉或机器翻译研究人员</strong>：采用鼓风设计的GTX 2080 Ti，如果训练非常大的网络，请选择RTX Titans</p><p><strong>NLP研究人员</strong>：RTX 2080 Ti</p><p><strong>已经开始研究深度学习</strong>：RTX 2070起步，以后按需添置更多RTX 2070</p><h2 id="其他配件要求："><a href="#其他配件要求：" class="headerlink" title="其他配件要求："></a>其他配件要求：</h2><p><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-19-19-14.png" srcset="/img/loading.gif" alt="各硬件性能要求"></p><ul><li><p>CPU:<br>因为主要使用显卡进行cuda计算，因此对CPU的要求并不是很高，频率越高、线程数越多越好，一般最低要求cpu核心数大于显卡个数。其中一个制约因素：cpu的最大PCI-E 通道数。每张显卡占用16条pcie通道才能达到最大性能，而单cpu最大支持40条pcie，也就是即使有4个pcie x16接口，只能最多达到2路x16加一路x8，插上的显卡不能发挥全部性能。不过，主板芯片组其实也可以扩充一部分pcie通道。（x99主板可以扩宽2.0的8lanes，z170可以扩充3.0的20lanes）</p></li><li><p>主板:<br>前面提到了cpu提供的pcie通道数的限制，如果要使用多块显卡，就需要主板提供额外的pcie通道，一般只有服务器级别的主板才会提供扩展pcie通道如x99、x299等主板，但是使用此类主板必须搭配具有该接口的服务器级cpu（xeon系列、i7 7900x以上、i9系列等），如果不需要三块以上的显卡，使用cpu提供的40lane pcie即可。</p></li><li><p>内存：<br>深度学习需要大量数据，中间计算过程也会临时储存大量数据，一般要求具有显存2~3倍的内存，32G或64G乃至更高。内存频率越高越好。<br>最低建议32G DDR4 3200MHz内存(16G*2)约2000元，预算宽裕可升级到64G（约4000元）</p></li><li><p>硬盘：<br>深度学习需要大量数据，和较快的访问速度，一般使用一个较大的固态硬盘作为系统盘和训练数据仓储盘，另外使用hdd机械硬盘作为仓储盘。<br>建议使用512G以上nVME固态硬盘（800元）搭配几TB(2TB约300元）Hdd作为储存空间</p></li><li><p>电源、机箱：电源其实还是要买个比较稳定的，因为要保证长期稳定运行会有“无休止”的training。一般使用大品牌的经过80PLUS金牌或铂金认证的电源。只搭配一张显卡700w即可，每多一张增加400w。4*titan V大概使用1600w电源。</p></li></ul><p>深度学习实验室共享服务器，7x24小时运行  2080ti或者4titan V ，预算充裕可以专门购置一台高性能多显卡深度学习服务器，24*7小时运行，其他用户可以在自己的笔记本电脑和台式机上编写和初步调试卷积神经网络，本地验证无误后，上传至服务器进行训练任务。这样做可以极大的节省设备开支，最大限度的利用计算资源，也避免了每个用户单独配置复杂的软件环境。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;实验室最近要采购一批显卡，需要调研显卡的型号和价格。&lt;/p&gt;
&lt;h2 id=&quot;需求分析&quot;&gt;&lt;a href=&quot;#需求分析&quot; class=&quot;headerlink&quot; title=&quot;需求分析&quot;&gt;&lt;/a&gt;需求分析&lt;/h2&gt;&lt;p&gt;首先说一下需求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先显卡的用
      
    
    </summary>
    
    
      <category term="record" scheme="https://superlova.github.io/categories/record/"/>
    
    
      <category term="deep learning" scheme="https://superlova.github.io/tags/deep-learning/"/>
    
      <category term="GPU" scheme="https://superlova.github.io/tags/GPU/"/>
    
  </entry>
  
  <entry>
    <title>NLP学习笔记4</title>
    <link href="https://superlova.github.io/2019/06/30/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04/"/>
    <id>https://superlova.github.io/2019/06/30/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04/</id>
    <published>2019-06-30T12:59:29.000Z</published>
    <updated>2020-03-19T01:30:44.788Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-利用朴素贝叶斯模型进行文本分类"><a href="#1-利用朴素贝叶斯模型进行文本分类" class="headerlink" title="1. 利用朴素贝叶斯模型进行文本分类"></a>1. 利用朴素贝叶斯模型进行文本分类</h2><p>朴素贝叶斯是一种构建分类器的简单方法。该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。它不是训练这种分类器的单一算法，而是一系列基于相同原理的算法：<strong>所有朴素贝叶斯分类器都假定样本每个特征与其他特征都不相关</strong>。</p><p>举个例子，如果一种水果其具有红，圆，直径大概3英寸等特征，该水果可以被判定为是苹果。尽管这些特征相互依赖或者有些特征由其他特征决定，然而朴素贝叶斯分类器认为这些属性在判定该水果是否为苹果的概率分布上独立的。</p><p>尽管是带着这些朴素思想和过于简单化的假设，但朴素贝叶斯分类器在很多复杂的现实情形中仍能够获取相当好的效果。2004年，一篇分析贝叶斯分类器问题的文章揭示了朴素贝叶斯分类器获取看上去不可思议的分类效果的若干理论上的原因。尽管如此，2006年有一篇文章详细比较了各种分类方法，发现更新的方法（如决策树和随机森林）的性能超过了贝叶斯分类器。</p><p>对于某些类型的概率模型，在监督式学习的样本集中能获取得非常好的分类效果。在许多实际应用中，朴素贝叶斯模型参数估计使用<strong>最大似然估计方法</strong>；换而言之，在不用到贝叶斯概率或者任何贝叶斯模型的情况下，朴素贝叶斯模型也能奏效。</p><p>朴素贝叶斯分类器的一个优势在于只需要根据少量的训练数据估计出必要的参数（变量的均值和方差）。由于变量独立假设，只需要估计各个变量的方法，而不需要确定整个协方差矩阵。</p><p>朴素贝叶斯分类器是与线性模型非常相似的一种分类器，但它的训练速度往往更快。这种高效率所付出的代价是，朴素贝叶斯模型的泛化能力要比线性分类器（如LogisticRegression 和LinearSVC）稍差。</p><p>朴素贝叶斯模型如此高效的原因在于，它通过单独查看每个特征来学习参数，并从每个特征中收集简单的类别统计数据。scikit-learn 中实现了三种朴素贝叶斯分类器：GaussianNB、BernoulliNB 和MultinomialNB。GaussianNB 可应用于任意连续数据， 而BernoulliNB 假定输入数据为二分类数据，MultinomialNB 假定输入数据为计数数据（即每个特征代表某个对象的整数计数，比如一个单词在句子里出现的次数）。BernoulliNB 和MultinomialNB 主要用于文本数据分类。</p><pre><code class="lang-python"># 从sklearn.datasets里导入20类新闻文本数据抓取器。from sklearn.datasets import fetch_20newsgroups# 从互联网上即时下载新闻样本,subset=&#39;all&#39;参数代表下载全部近2万条文本存储在变量news中。news = fetch_20newsgroups(subset=&#39;all&#39;)# 从sklearn.cross_validation导入train_test_split模块用于分割数据集。from sklearn.cross_validation import train_test_split# 对news中的数据data进行分割，25%的文本用作测试集；75%作为训练集。X_train, X_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25, random_state=33)# 从sklearn.feature_extraction.text里导入CountVectorizerfrom sklearn.feature_extraction.text import CountVectorizer# 采用默认的配置对CountVectorizer进行初始化（默认配置不去除英文停用词），并且赋值给变量count_vec。count_vec = CountVectorizer()# 只使用词频统计的方式将原始训练和测试文本转化为特征向量。#学习词汇的词典并返回文档矩阵。X_count_train = count_vec.fit_transform(X_train)#不进行学习直接转换文档document-term矩阵X_count_test = count_vec.transform(X_test)# 从sklearn.naive_bayes里导入朴素贝叶斯分类器。from sklearn.naive_bayes import MultinomialNB# 使用默认的配置对分类器进行初始化。mnb_count = MultinomialNB()# 使用朴素贝叶斯分类器，对CountVectorizer（不去除停用词）后的训练样本进行参数学习。mnb_count.fit(X_count_train, y_train)# 输出模型准确性结果。print (&#39;The accuracy of classifying 20newsgroups using Naive Bayes (CountVectorizer without filtering stopwords):&#39;, mnb_count.score(X_count_test, y_test))# 将分类预测的结果存储在变量y_count_predict中。y_count_predict = mnb_count.predict(X_count_test)# 从sklearn.metrics 导入 classification_report。from sklearn.metrics import classification_report# 输出更加详细的其他评价分类性能的指标。print (classification_report(y_test, y_count_predict, target_names = news.target_names))</code></pre><p><img src="/2019/06/30/NLP学习笔记4/2019-06-30-20-42-49.png" srcset="/img/loading.gif" alt></p><h2 id="2-利用SVM模型进行文本分类"><a href="#2-利用SVM模型进行文本分类" class="headerlink" title="2. 利用SVM模型进行文本分类"></a>2. 利用SVM模型进行文本分类</h2><h2 id="3-pLSA、共轭先验分布、LDA"><a href="#3-pLSA、共轭先验分布、LDA" class="headerlink" title="3. pLSA、共轭先验分布、LDA"></a>3. pLSA、共轭先验分布、LDA</h2><p>常用于文本数据的一种特殊技术是主题建模（topic modeling），这是描述将每个文档分配给一个或多个主题的任务（通常是无监督的）的概括性术语。这方面一个很好的例子是新闻数据，它们可以被分为“政治”“体育”“金融”等主题。如果为每个文档分配一个主题，那么这是一个文档聚类任务。如果每个文档可以有多个主题，那么这个任务与第3 章中的分解方法有关。我们学到的每个成分对应于一个主题，文档表示中的成分系数告诉我们这个文档与该主题的相关性强弱。通常来说，人们在谈论主题建模时，他们指的是一种叫作隐含狄利克雷分布（Latent Dirichlet Allocation，LDA）的特定分解方法</p><h3 id="隐含狄利克雷分布"><a href="#隐含狄利克雷分布" class="headerlink" title="隐含狄利克雷分布"></a>隐含狄利克雷分布</h3><p>从直观上来看，LDA 模型试图找出频繁共同出现的单词群组（即主题）。LDA 还要求，每个文档可以被理解为主题子集的“混合”。重要的是要理解，机器学习模型所谓的“主题”可能不是我们通常在日常对话中所说的主题，而是更类似于 PCA 或 NMF所提取的成分，它可能具有语义，也可能没有。即使 LDA“主题”具有语义，它可能也不是我们通常所说的主题。</p><p>举个自然语言处理的例子，我们可能有许多关于体育、政治和金融的文章，由两位作者所写。在一篇政治文章中，我们预计可能会看 到“州长”“投票”“党派”等词语，而在一篇体育文章中，我们预计可能会看到类似“队 伍”“得分”和“赛季”之类的词语。这两组词语可能会同时出现，而例如“队伍”和 “州长”就不太可能同时出现。但是，这并不是我们预计可能同时出现的唯一的单词群组。这两位记者可能偏爱不同的短语或者选择不同的单词。可能其中一人喜欢使用“划界”（demarcate）这个词，而另一人喜欢使用“两极分化”（polarize）这个词。其他“主题”可 能是“记者 A 常用的词语”和“记者 B 常用的词语”，虽然这并不是通常意义上的主题。</p><h2 id="4-使用LDA生成主题特征，在之前特征的基础上加入主题特征进行文本分类"><a href="#4-使用LDA生成主题特征，在之前特征的基础上加入主题特征进行文本分类" class="headerlink" title="4. 使用LDA生成主题特征，在之前特征的基础上加入主题特征进行文本分类"></a>4. 使用LDA生成主题特征，在之前特征的基础上加入主题特征进行文本分类</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-利用朴素贝叶斯模型进行文本分类&quot;&gt;&lt;a href=&quot;#1-利用朴素贝叶斯模型进行文本分类&quot; class=&quot;headerlink&quot; title=&quot;1. 利用朴素贝叶斯模型进行文本分类&quot;&gt;&lt;/a&gt;1. 利用朴素贝叶斯模型进行文本分类&lt;/h2&gt;&lt;p&gt;朴素贝叶斯是一种
      
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="NLP" scheme="https://superlova.github.io/tags/NLP/"/>
    
      <category term="deep learning" scheme="https://superlova.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>NLP学习笔记3</title>
    <link href="https://superlova.github.io/2019/06/27/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03/"/>
    <id>https://superlova.github.io/2019/06/27/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03/</id>
    <published>2019-06-27T12:51:24.000Z</published>
    <updated>2020-03-19T01:30:30.272Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-TF-IDF原理。"><a href="#1-TF-IDF原理。" class="headerlink" title="1. TF-IDF原理。"></a>1. TF-IDF原理。</h2><p>tf-idf（英语：term frequency–inverse document frequency）是一种用于信息检索与文本挖掘的常用加权技术。tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。tf-idf加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了tf-idf以外，互联网上的搜索引擎还会使用基于链接分析的评级方法，以确定文件在搜索结果中出现的顺序。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>在一份给定的文件里，词频（term frequency，tf）指的是某一个给定的词语在该文件中出现的频率。这个数字是对词数（term count）的归一化，以防止它偏向长的文件。（同一个词语在长文件里可能会比短文件有更高的词数，而不管该词语重要与否。）对于在某一特定文件里的词语 ${\displaystyle t_{i}!}$来说，它的重要性可表示为：</p><script type="math/tex; mode=display">{\displaystyle \mathrm {tf_{i,j}\!} ={\frac {n_{i,j}\!}{\sum _{k}n_{k,j}\!}\!}\!}</script><p>以上式子中 ${\displaystyle n_{i,j}!} n_{i,j}$是该词在文件 ${\displaystyle d_{j}!} d_{j}$中的出现次数，而分母则是在文件 ${\displaystyle d_{j}!} d_{j}$中所有字词的出现次数之和。</p><p>逆向文件频率（inverse document frequency，idf）是一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到：</p><script type="math/tex; mode=display">{\displaystyle \mathrm {idf_{i}\!} =\lg {\frac {|D|}{|\{j:t_{i}\in d_{j}\}|}\!}\!}</script><p>其中</p><p>$|D|$：语料库中的文件总数</p><script type="math/tex; mode=display">{\displaystyle |\{j:t_{i}\in d_{j}\}|} $$：包含词语 ${\displaystyle t_{i}\!} t_{i}$的文件数目（即 ${\displaystyle n_{i,j}\neq 0} n_{i,j}\neq 0$的文件数目）如果词语不在数据中，就导致分母为零，因此一般情况下使用 $${\displaystyle 1+|\{j:t_{i}\in d_{j}\}|}</script><p>然后</p><script type="math/tex; mode=display">{\displaystyle \mathrm {tf{}idf_{i,j}\!} =\mathrm {tf_{i,j}\!} \times \mathrm {idf_{i}\!} }</script><p>某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的tf-idf。因此，tf-idf倾向于过滤掉常见的词语，保留重要的词语。</p><h3 id="1-2-例子"><a href="#1-2-例子" class="headerlink" title="1.2 例子"></a>1.2 例子</h3><p>有很多不同的数学公式可以用来计算tf-idf。这边的例子以上述的数学公式来计算。词频（tf）是一词语出现的次数除以该文件的总词语数。假如一篇文件的总词语数是100个，而词语“母牛”出现了3次，那么“母牛”一词在该文件中的词频就是3/100=0.03。而计算文件频率（IDF）的方法是以文件集的文件总数，除以出现“母牛”一词的文件数。所以，如果“母牛”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是lg（10,000,000 / 1,000）=4。最后的tf-idf的分数为0.03 * 4=0.12。</p><h3 id="1-3-tf-idf的理论依据及不足"><a href="#1-3-tf-idf的理论依据及不足" class="headerlink" title="1.3 tf-idf的理论依据及不足"></a>1.3 tf-idf的理论依据及不足</h3><p>tf-idf算法是创建在这样一个假设之上的：对区别文档最有意义的词语应该是那些在文档中出现频率高，而在整个文档集合的其他文档中出现频率少的词语，所以如果特征空间坐标系取tf词频作为测度，就可以体现同类文本的特点。另外考虑到单词区别不同类别的能力，tf-idf法认为一个单词出现的文本频数越小，它区别不同类别文本的能力就越大。因此引入了逆文本频度idf的概念，以tf和idf的乘积作为特征空间坐标系的取值测度，并用它完成对权值tf的调整，调整权值的目的在于突出重要单词，抑制次要单词。但是在本质上idf是一种试图抑制噪声的加权，并且单纯地认为文本频率小的单词就越重要，文本频率大的单词就越无用，显然这并不是完全正确的。idf的简单结构并不能有效地反映单词的重要程度和特征词的分布情况，使其无法很好地完成对权值调整的功能，所以tf-idf法的精度并不是很高。</p><p>此外，在tf-idf算法中并没有体现出单词的位置信息，对于Web文档而言，权重的计算方法应该体现出HTML的结构特征。特征词在不同的标记符中对文章内容的反映程度不同，其权重的计算方法也应不同。因此应该对于处于网页不同位置的特征词分别赋予不同的系数，然后乘以特征词的词频，以提高文本表示的效果。</p><h2 id="2-文本矩阵化，使用词袋模型，以TF-IDF特征值为权重。"><a href="#2-文本矩阵化，使用词袋模型，以TF-IDF特征值为权重。" class="headerlink" title="2. 文本矩阵化，使用词袋模型，以TF-IDF特征值为权重。"></a>2. 文本矩阵化，使用词袋模型，以TF-IDF特征值为权重。</h2><p>TfidfVectorizer可以把原始文本转化为tf-idf的特征矩阵，从而为后续的文本相似度计算，主题模型(如LSI)，文本搜索排序等一系列应用奠定基础。基本应用如：</p><h3 id="第一步：分词"><a href="#第一步：分词" class="headerlink" title="第一步：分词"></a>第一步：分词</h3><p>采用著名的中文分词库jieba进行分词：</p><pre><code class="lang-python">import jiebatext = &quot;&quot;&quot;我是一条天狗呀！我把月来吞了，我把日来吞了，我把一切的星球来吞了，我把全宇宙来吞了。我便是我了！&quot;&quot;&quot;sentences = text.split()sent_words = [list(jieba.cut(sent0)) for sent0 in sentences]document = [&quot; &quot;.join(sent0) for sent0 in sent_words]print(document)</code></pre><h3 id="第二步：建模"><a href="#第二步：建模" class="headerlink" title="第二步：建模"></a>第二步：建模</h3><pre><code>理论上，现在得到的document的格式已经可以直接拿来训练了。让我们跑一下模型试试。</code></pre><pre><code class="lang-python">tfidf_model = TfidfVectorizer().fit(document)print(tfidf_model.vocabulary_)# {&#39;一条&#39;: 1, &#39;天狗&#39;: 4, &#39;日来&#39;: 5, &#39;一切&#39;: 0, &#39;星球&#39;: 6, &#39;全宇宙&#39;: 3, &#39;便是&#39;: 2}sparse_result = tfidf_model.transform(document)print(sparse_result)# (0, 4)    0.707106781187# (0, 1)    0.707106781187# (2, 5)    1.0# (3, 6)    0.707106781187# (3, 0)    0.707106781187# (4, 3)    1.0# (5, 2)    1.0</code></pre><h3 id="第三步：参数"><a href="#第三步：参数" class="headerlink" title="第三步：参数"></a>第三步：参数</h3><pre><code>查了一些资料以后，发现单字的问题是token_pattern这个参数搞的鬼。它的默认值只匹配长度≥2的单词，就像其实开头的例子中的&#39;I&#39;也被忽略了一样，一般来说，长度为1的单词在英文中一般是无足轻重的，但在中文里，就可能有一些很重要的单字词，所以修改如下：</code></pre><pre><code class="lang-python">tfidf_model2 = TfidfVectorizer(token_pattern=r&quot;(?u)\b\w+\b&quot;).fit(document)print(tfidf_model2.vocabulary_)# {&#39;我&#39;: 8, &#39;是&#39;: 12, &#39;一条&#39;: 1, &#39;天狗&#39;: 7, &#39;呀&#39;: 6, &#39;把&#39;: 9, &#39;月&#39;: 13, &#39;来&#39;: 14, &#39;吞&#39;: 5, &#39;了&#39;: 2, &#39;日来&#39;: 10, &#39;一切&#39;: 0, &#39;的&#39;: 15, &#39;星球&#39;: 11, &#39;全宇宙&#39;: 4, &#39;便是&#39;: 3}</code></pre><p>token_pattern这个参数使用正则表达式来分词，其默认参数为r”(?u)\b\w\w+\b”，其中的两个\w决定了其匹配长度至少为2的单词，所以这边减到1个。对这个参数进行更多修改，可以满足其他要求，比如这里依然没有得到标点符号，在此不详解了。</p><h2 id="3-互信息"><a href="#3-互信息" class="headerlink" title="3. 互信息"></a>3. 互信息</h2><p>在概率论和信息论中，两个随机变量的互信息（Mutual Information，简称MI）或转移信息（transinformation）是变量间相互依赖性的量度。不同于相关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布$ p(X,Y) $和分解的边缘分布的乘积$ p(X)p(Y) $的相似程度。互信息是点间互信息（PMI）的期望值。互信息最常用的单位是bit。</p><p>一般地，两个离散随机变量$ X $和$ Y $的互信息可以定义为：</p><script type="math/tex; mode=display">{\displaystyle I(X;Y)=\sum _{y\in Y}\sum _{x\in X}p(x,y)\log {\left({\frac {p(x,y)}{p(x)\,p(y)}\!}\right)},\,\!}</script><p>其中$ p(x,y) $是 $X $和 $Y $的联合概率分布函数，而 ${\displaystyle p(x)} $和$ {\displaystyle p(y)}  $分别是 X 和 Y 的边缘概率分布函数。</p><p>在连续随机变量的情形下，求和被替换成了二重定积分：</p><script type="math/tex; mode=display">{\displaystyle I(X;Y)=\int _{Y}\int _{X}p(x,y)\log {\left({\frac {p(x,y)}{p(x)\,p(y)}\!}\right)}\;dx\,dy,} $$,其中 $p(x,y)$ 当前是 X 和 Y 的联合概率密度函数，而 ${\displaystyle p(x)} $和$ {\displaystyle p(y)}$分别是$ X $和$ Y $的边缘概率密度函数。如果对数以 2 为基底，互信息的单位是bit。直观上，互信息度量 X 和 Y 共享的信息：它度量知道这两个变量其中一个，对另一个不确定度减少的程度。例如，如果 X 和 Y 相互独立，则知道 X 不对 Y 提供任何信息，反之亦然，所以它们的互信息为零。在另一个极端，如果 X 是 Y 的一个确定性函数，且 Y 也是 X 的一个确定性函数，那么传递的所有信息被 X 和 Y 共享：知道 X 决定 Y 的值，反之亦然。因此，在此情形互信息与 Y（或 X）单独包含的不确定度相同，称作 Y（或 X）的熵。而且，这个互信息与 X 的熵和 Y 的熵相同。（这种情形的一个非常特殊的情况是当 X 和 Y 为相同随机变量时。）互信息是 X 和 Y 的联合分布相对于假定 X 和 Y 独立情况下的联合分布之间的内在依赖性。 于是互信息以下面方式度量依赖性：$I(X; Y) = 0$ 当且仅当 X 和 Y 为独立随机变量。从一个方向很容易看出：当 X 和 Y 独立时，$p(x,y) = p(x) p(y)$，因此：$${\displaystyle \log {\left({\frac {p(x,y)}{p(x)\,p(y)} \!}\right)}=\log 1=0.\,\!}</script><p>此外，互信息是非负的（即 $I(X;Y) ≥ 0$; 见下文），而且是对称的（即 $I(X;Y) = I(Y;X)$）。</p><p>互信息又可以等价地表示成</p><script type="math/tex; mode=display">{\displaystyle {\begin{aligned}I(X;Y)&{}=H(X)-H(X|Y)\\&{}=H(Y)-H(Y|X)\\&{}=H(X)+H(Y)-H(X,Y)\\&{}=H(X,Y)-H(X|Y)-H(Y|X)\end{aligned}\!}\!}</script><p>其中$ {\displaystyle \ H(X)} $ 和$ {\displaystyle \ H(Y)}  $是边缘熵，$H(X|Y) $和$ H(Y|X) $是条件熵，而 $H(X,Y) $是 X 和 Y 的联合熵。</p><p><strong>互信息越小，两个来自不同事件空间的随机变量彼此之间的关系性越低; 互信息越高，关系性则越高。</strong></p><h2 id="4-对特征矩阵使用互信息进行特征筛选"><a href="#4-对特征矩阵使用互信息进行特征筛选" class="headerlink" title="4. 对特征矩阵使用互信息进行特征筛选"></a>4. 对特征矩阵使用互信息进行特征筛选</h2><p><code>sklearn.metrics.mutual_info_score</code></p><pre><code class="lang-python">from sklearn import datasetsfrom sklearn import metrics as mriris = datasets.load_iris()x = iris.datalabel = iris.targetx0 = x[:, 0]x1 = x[:, 1]x2 = x[:, 2]x3 = x[:, 3]# 计算各特征与label的互信息print(mr.mutual_info_score(x0, label))print(mr.mutual_info_score(x1, label))print(mr.mutual_info_score(x2, label))print(mr.mutual_info_score(x3, label))</code></pre><p><img src="/2019/06/27/NLP学习笔记3/2019-06-27-20-45-13.png" srcset="/img/loading.gif" alt><br><code>sklearn.feature_selection.mutual_info_classif</code></p><pre><code class="lang-python">from sklearn import datasetsfrom sklearn.feature_selection import mutual_info_classifiris = datasets.load_iris()x = iris.datalabel = iris.targetmutual_info = mutual_info_classif(x, label, discrete_features= False)print(mutual_info)</code></pre><p><img src="/2019/06/27/NLP学习笔记3/2019-06-27-20-45-28.png" srcset="/img/loading.gif" alt></p><blockquote><p>参考文献<br><a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/Tf-idf</a><br><a href="https://zh.wikipedia.org/wiki/%E4%BA%92%E4%BF%A1%E6%81%AF" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E4%BA%92%E4%BF%A1%E6%81%AF</a><br><a href="https://blog.csdn.net/yyy430/article/details/88249709" target="_blank" rel="noopener">https://blog.csdn.net/yyy430/article/details/88249709</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-TF-IDF原理。&quot;&gt;&lt;a href=&quot;#1-TF-IDF原理。&quot; class=&quot;headerlink&quot; title=&quot;1. TF-IDF原理。&quot;&gt;&lt;/a&gt;1. TF-IDF原理。&lt;/h2&gt;&lt;p&gt;tf-idf（英语：term frequency–inverse
      
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="NLP" scheme="https://superlova.github.io/tags/NLP/"/>
    
      <category term="deep learning" scheme="https://superlova.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习——集成方法</title>
    <link href="https://superlova.github.io/2019/06/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95/"/>
    <id>https://superlova.github.io/2019/06/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95/</id>
    <published>2019-06-27T08:04:10.000Z</published>
    <updated>2020-03-19T01:38:46.937Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-投票分类器"><a href="#1-投票分类器" class="headerlink" title="1. 投票分类器"></a>1. 投票分类器</h2><p>三个臭皮匠顶一个诸葛亮。即便是很多预测准确率仅强于随机的个体学习器的组合，经过一定的安排，也可以发挥令人惊讶的效果。在机器学习中，这种看起来没什么含金量的学习策略称之为<strong>集成学习</strong>。</p><p>集成学习首先需要一系列的个体学习器。之后采用某些策略结合它们的判断。</p><p>集成学习的要求：</p><ul><li>构成集成学习器的个体学习器，其性能不能太差，至少要为强于随机的<strong>弱学习器</strong>。</li></ul><p>当然强学习器更好。在最后的结果汇总阶段，也会更多听取强学习器的意见。</p><ul><li>个体学习器要具有一定的多样性。</li></ul><p>广泛吸收各种不同学习器的意见，做出的决策才有代表性。在机器学习中，体现出的要求就是模型之间的差别要尽可能的大。一方面可以通过划分不同的数据集，独立训练来得到差异性；另一方面我们也可以选取不同的训练模型，比如SVM、决策树、逻辑回归。</p><p>集成学习法的准确率比集成学习中表现最好的分类器准确率还高，这究竟是为什么？难道那一些不入流的臭鱼烂虾机器学习法，它们存在的意义就是提升集成学习中的准确率的吗？</p><p>我们来打个比方。假设我有一枚硬币，这枚硬币经过加工处理，正面朝上的可能性比背面要高那么一点点，51%的可能性是正面。问，如果我投掷1000次硬币，正面朝上次数大于背面朝上次数的可能性占比多少？一万次呢？</p><p>事实上，1000次投掷，最后正面次数比背面多的概率就达到了0.72，如果投掷10000次，那么就是0.94，几乎是必然事件。</p><p>将其类比到集成学习中来，如果相互独立的个体学习器足够多，那么我们得到正确结论的概率将大大提升。不过这里有一个最关键的点：<strong>模型之间相互独立</strong>。这个要求其实是蛮难达到的，因为即便是不同的机器学习模型，如果采用相同或者相似的数据集进行训练，那么他们之间必然存在某种相关性。更不用说连模型都是一模一样的情况了。</p><h2 id="2-Boosting算法"><a href="#2-Boosting算法" class="headerlink" title="2. Boosting算法"></a>2. Boosting算法</h2><p>Boosting算法的核心思想是分割训练集，用同一种机器学习算法得到差异化的一系列模型。</p><p>先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本的分布进行调整，是的先前基学习器做错的训练样本在后续收到更多地关注。然后基于调整后的样本分布来训练下一个基学习器。如此反复进行，训练T个基学习器，最终加权投票。</p><p><img src="/2019/06/27/机器学习——集成方法/2019-06-27-21-57-03.png" srcset="/img/loading.gif" alt></p><p><img src="/2019/06/27/机器学习——集成方法/2019-06-27-21-57-17.png" srcset="/img/loading.gif" alt></p><h2 id="3-Bagging算法"><a href="#3-Bagging算法" class="headerlink" title="3. Bagging算法"></a>3. Bagging算法</h2><p>有放回采样被称为Bagging。采用Bagging的方法我们可以得到很多的可能有重复样本的数据子集。我们在之前的文章中已经提到，对于每个选取的子集，平均下来只有63%的训练实例被采样，剩下的37%正好当做测试集。</p><p>随机森林就采用了Bagging采样方法来训练很多的决策树。如果你观察单一的决策树，重要的特征会出现在更靠近根部的位置，不重要的特征会经常出现在靠近叶子的位置。因此我们可以通过计算一个特征在森林的全部树中出现的平均深度来预测特征的重要性。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-投票分类器&quot;&gt;&lt;a href=&quot;#1-投票分类器&quot; class=&quot;headerlink&quot; title=&quot;1. 投票分类器&quot;&gt;&lt;/a&gt;1. 投票分类器&lt;/h2&gt;&lt;p&gt;三个臭皮匠顶一个诸葛亮。即便是很多预测准确率仅强于随机的个体学习器的组合，经过一定的安排，也可以
      
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="machine learning" scheme="https://superlova.github.io/tags/machine-learning/"/>
    
      <category term="boosting" scheme="https://superlova.github.io/tags/boosting/"/>
    
  </entry>
  
</feed>
