<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Superlova</title>
  
  <subtitle>Be a better man...</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://superlova.github.io/"/>
  <updated>2020-03-17T01:54:50.736Z</updated>
  <id>https://superlova.github.io/</id>
  
  <author>
    <name>Superlova</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>如何使用VS Code编写github pages博客</title>
    <link href="https://superlova.github.io/2020/01/11/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8VS-Code%E7%BC%96%E5%86%99github-pages%E5%8D%9A%E5%AE%A2/"/>
    <id>https://superlova.github.io/2020/01/11/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8VS-Code%E7%BC%96%E5%86%99github-pages%E5%8D%9A%E5%AE%A2/</id>
    <published>2020-01-11T13:55:17.000Z</published>
    <updated>2020-03-17T01:54:50.736Z</updated>
    
    <content type="html"><![CDATA[<p>使用VS Code写博客，需要你按照我之前写的两篇博客，将github pages平台搭建起来。</p><p><a href="https://superlova.github.io/2019/04/14/%E9%85%8D%E7%BD%AEhexo+GitHub%20Pages%E7%BA%AA%E5%AE%9E/">配置hexo+GitHub Pages纪实</a><br><a href="https://superlova.github.io/2019/04/25/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/">hexo图片加载失败解决方案</a></p><p>之后我们安装VSCode。接下来介绍我一直使用的几个插件，和它们的配置小技巧。</p><p>第一个是<strong>Markdown Preview Enhanced</strong>，有了该插件，就可以提前预览markdown文件的渲染效果。方法是使用VSCode打开以md后缀名结尾的文件，右键点击<strong>Markdown Preview Enhanced： Open Preview To The Side</strong>，即可在侧边栏生成即时渲染的md效果文件。</p><p>第二个是<strong>Markdown PDF</strong>，该插件可以令写好的md文件打印成pdf格式。该插件需要安装chromium内核。</p><p>第三个是<strong><strong>Paste Image</strong></strong>插件，可以很方便地在md文章中粘贴位于剪切板的图片。</p><p>粘贴的快捷键是Ctrl+Alt+V。</p><p>在Paste Image插件的Path设置部分，改成如下所示：<br><img src="/2020/01/11/如何使用VS-Code编写github-pages博客/2020-01-11-23-28-36.png" srcset="/img/loading.gif" alt><br>这样图片粘贴的位置就变成了<strong>当前文章目录下，与该文章同名的文件夹内</strong>，方便我们进行进一步整理。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;使用VS Code写博客，需要你按照我之前写的两篇博客，将github pages平台搭建起来。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://superlova.github.io/2019/04/14/%E9%85%8D%E7%BD%AEhexo+GitHub%20P
      
    
    </summary>
    
    
      <category term="经验教训" scheme="https://superlova.github.io/categories/%E7%BB%8F%E9%AA%8C%E6%95%99%E8%AE%AD/"/>
    
    
      <category term="GitHub Pages" scheme="https://superlova.github.io/tags/GitHub-Pages/"/>
    
      <category term="hexo" scheme="https://superlova.github.io/tags/hexo/"/>
    
      <category term="Visual Studio Code" scheme="https://superlova.github.io/tags/Visual-Studio-Code/"/>
    
      <category term="markdown" scheme="https://superlova.github.io/tags/markdown/"/>
    
  </entry>
  
  <entry>
    <title>总结论文中常用的Matplotlib和Seaborn绘图技术</title>
    <link href="https://superlova.github.io/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/"/>
    <id>https://superlova.github.io/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/</id>
    <published>2020-01-11T13:47:26.000Z</published>
    <updated>2020-03-17T01:54:50.739Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、使用matplotlib绘制图像"><a href="#一、使用matplotlib绘制图像" class="headerlink" title="一、使用matplotlib绘制图像"></a>一、使用matplotlib绘制图像</h1><p>matplotlib是一个Python的数据可视化2D图形库。matplotlib的特点是可以采用面向对象的方法，模仿MATLAB中的图形命令。matplotlib经常与numpy、pandas等库结合起来使用。<br>matplotlib可以采用MATLAB的命令风格使用，也可以采用面向对象的风格使用。</p><h2 id="matplotlib的图像中各组件名称"><a href="#matplotlib的图像中各组件名称" class="headerlink" title="matplotlib的图像中各组件名称"></a>matplotlib的图像中各组件名称</h2><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-11-23-42-39.png" srcset="/img/loading.gif" alt></p><h2 id="新建图像"><a href="#新建图像" class="headerlink" title="新建图像"></a>新建图像</h2><pre><code class="lang-python">fig, axes = plt.subplots(2,1,figsize=(5,10)) #两行一列组成一张图，图像大小宽5高10</code></pre><p>上面的语句创建了一个figure，由两个ax组成。把它想象成一张画布上面的两个贴画，会比较容易理解。</p><p>plt.figure()函数的前两个参数是设置figure是由几行几列的ax组成。figure(2,1)说明figure是由两行一列的ax一共两个ax组成。</p><p>后面的figsize参数设置画布的宽和高，单位为英寸。</p><h1 id="二、使用Seaborn绘制图像"><a href="#二、使用Seaborn绘制图像" class="headerlink" title="二、使用Seaborn绘制图像"></a>二、使用Seaborn绘制图像</h1><p>首先确定我们需要可视化的数据的结构。以iris鸢尾花数据集为例，</p><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-07-35.png" srcset="/img/loading.gif" alt></p><p>每一行代表一个数据对象，每一列代表数据对象的一个属性。但是现实生活的数据很多不长这样，只不过组织成一个表格的形式，内容大相径庭。因此在进行数据可视化时一定要保证你的数据也是<strong>用行代表数据对象，用列表示数据的属性</strong>。</p><h2 id="2-1-关联图"><a href="#2-1-关联图" class="headerlink" title="2.1 关联图"></a>2.1 关联图</h2><p>我们是用 <code>relplot</code>函数进行进一步绘制。实际上，<code>relplot</code> 可以看作是 <code>scatterplot</code> 和 <code>lineplot</code> 的结合版本。但是relplot包装层级更加高，这意味着它更适合快速应用，不适合自定义。如果你对它的效果不满意，恐怕还是得诉诸<code>scatterplot</code> 和 <code>lineplot</code>等与matplotlib结合更紧密的api，或者直接使用matplotlib。</p><pre><code class="lang-python">sns.relplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-10-26.png" srcset="/img/loading.gif" alt></p><p>x为花萼长度，y为花萼宽度。这样分x，y其实有一定道理，我们的目的是能够把不同类型的数据对象在图上区分开。因为同类花朵一般个头差不多，花萼的长度和宽度聚集在图的一部分区域。但是在上图我们是看不出来的。我们希望给不同类别添加不同颜色。</p><pre><code class="lang-python">sns.relplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, hue=&quot;species&quot;, data=iris)</code></pre><p>可以看到我们添加了<code>hue</code>字段，并要求按照<code>species</code>进行进一步分类。<code>hue</code>字段就是进行二次分类的参数。</p><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-15-48.png" srcset="/img/loading.gif" alt></p><p>如果是论文，则我们要使得读者在黑白打印的条件下也能发现区别。添加<code>stype</code>参数为<code>species</code>或许会有帮助。</p><pre><code class="lang-python">sns.relplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;,            hue=&quot;species&quot;, style=&quot;species&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-18-31.png" srcset="/img/loading.gif" alt></p><p>不只是散点图，该方法还支持线形图，只需要指定 <code>kind=&quot;line&quot;</code> 参数即可。</p><pre><code class="lang-python">sns.relplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;,            hue=&quot;species&quot;, style=&quot;species&quot;, kind=&quot;line&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-20-55.png" srcset="/img/loading.gif" alt></p><p>上图其实就是折线图，我们使用一个与matplotlib结合更紧密的api来探究花萼长度和花瓣长度之间的关系。</p><pre><code class="lang-python">sns.lineplot(x=&quot;sepal_length&quot;, y=&quot;petal_length&quot;,             hue=&quot;species&quot;, style=&quot;species&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-26-06.png" srcset="/img/loading.gif" alt></p><h2 id="2-2-类别图"><a href="#2-2-类别图" class="headerlink" title="2.2 类别图"></a>2.2 类别图</h2><p>懒人函数是<code>catplot</code>，<code>catplot</code>是下面几个底层函数的封装：</p><ul><li><p>分类散点图:</p><ul><li><a href="https://seaborn.pydata.org/generated/seaborn.stripplot.html" target="_blank" rel="noopener"><code>stripplot()</code></a> (<code>kind=&quot;strip&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.swarmplot.html" target="_blank" rel="noopener"><code>swarmplot()</code></a> (<code>kind=&quot;swarm&quot;</code>)</li></ul></li><li><p>分类分布图:</p><ul><li><a href="https://seaborn.pydata.org/generated/seaborn.boxplot.html" target="_blank" rel="noopener"><code>boxplot()</code></a> (<code>kind=&quot;box&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.violinplot.html" target="_blank" rel="noopener"><code>violinplot()</code></a> (<code>kind=&quot;violin&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.boxenplot.html" target="_blank" rel="noopener"><code>boxenplot()</code></a> (<code>kind=&quot;boxen&quot;</code>)</li></ul></li><li><p>分类估计图:</p><ul><li><a href="https://seaborn.pydata.org/generated/seaborn.pointplot.html" target="_blank" rel="noopener"><code>pointplot()</code></a> (<code>kind=&quot;point&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.barplot.html" target="_blank" rel="noopener"><code>barplot()</code></a> (<code>kind=&quot;bar&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.countplot.html" target="_blank" rel="noopener"><code>countplot()</code></a> (<code>kind=&quot;count&quot;</code>)</li></ul></li></ul><p>我们想知道不同类别下花萼长度的散点图。</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&#39;strip&#39;,data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-36-33.png" srcset="/img/loading.gif" alt></p><p><code>kind=&quot;swarm&quot;</code> 可以让散点按照 beeswarm 的方式防止重叠，可以更好地观测数据分布。</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;swarm&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-38-46.png" srcset="/img/loading.gif" alt></p><p>箱线图</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;box&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-41-06.png" srcset="/img/loading.gif" alt><br>变种箱线图</p><pre><code class="lang-python">sns.catplot(x=&quot;species&quot;, y=&quot;sepal_length&quot;, kind=&quot;boxen&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-41-24.png" srcset="/img/loading.gif" alt><br>提琴图</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;violin&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-41-16.png" srcset="/img/loading.gif" alt><br>点线图</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;point&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-41-36.png" srcset="/img/loading.gif" alt><br>柱状图</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;bar&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-41-48.png" srcset="/img/loading.gif" alt></p><h2 id="2-3-分布图"><a href="#2-3-分布图" class="headerlink" title="2.3 分布图"></a>2.3 分布图</h2><p>如果想看一个变量到底是正态分布、卡方分布还是指数分布，此时就要使用分布图进行可视化了。一维分布图比较常见，二维以上分布图不太直观。绘制分布图的函数有这几个：<code>jointplot</code> <code>pairplot</code> <code>distplot</code> <code>kdeplot</code>。</p><p><code>distplot</code>可以方便的查看单变量的分布图。</p><pre><code class="lang-python">sns.distplot(iris[&quot;sepal_length&quot;])</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-48-17.png" srcset="/img/loading.gif" alt><br>图上那条曲线是根据数据拟合出来的核密度估计kde曲线（原理有待学习）。如果不想要这条线，可以在参数中设置<code>kde=False</code>。更可以只要kde曲线，设置<code>hist=False</code>即可。</p><p><code>jointplot</code>绘制二元变量的分布图，比如花瓣长度和宽度的关系。</p><pre><code class="lang-python">sns.jointplot(x=&quot;petal_length&quot;, y=&quot;petal_width&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-55-40.png" srcset="/img/loading.gif" alt></p><p>kde估计图也可以在二元变量分布图中出现。还有蜂巢图<code>kind=&quot;hex&quot;</code>、回归图<code>kind=&quot;reg&quot;</code>等。</p><pre><code class="lang-python">sns.jointplot(x=&quot;petal_length&quot;, y=&quot;petal_width&quot;, data=iris, kind=&quot;kde&quot;)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-57-09.png" srcset="/img/loading.gif" alt></p><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-58-51.png" srcset="/img/loading.gif" alt></p><p>最后注意到我们的鸢尾花数据集含有四组属性。我们想探究这四组属性两两之间的关系，就需要用到<code>pairplot</code></p><pre><code class="lang-python">sns.pairplot(iris, hue=&quot;species&quot;)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-17-01-04.png" srcset="/img/loading.gif" alt></p><h2 id="2-4-回归图"><a href="#2-4-回归图" class="headerlink" title="2.4 回归图"></a>2.4 回归图</h2><p><code>regplot</code> 绘制回归图，只会绘制一组回归曲线。</p><pre><code class="lang-python">sns.regplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-17-02-28.png" srcset="/img/loading.gif" alt></p><p><code>lmplot</code> 可以引入<code>hue</code>变量，绘制不同类别数据的回归图</p><pre><code class="lang-python">sns.lmplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, hue=&quot;species&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-17-03-57.png" srcset="/img/loading.gif" alt></p><h2 id="2-5-矩阵图"><a href="#2-5-矩阵图" class="headerlink" title="2.5 矩阵图"></a>2.5 矩阵图</h2><p><code>heatmap</code>用来画热图，数据值大的格子颜色比较深。热力图在某些场景下非常实用，例如绘制出变量相关性系数热力图。<br><code>clustermap</code>用来画层次聚类结构图。对于iris数据集来说，这两类图没有用武之地。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一、使用matplotlib绘制图像&quot;&gt;&lt;a href=&quot;#一、使用matplotlib绘制图像&quot; class=&quot;headerlink&quot; title=&quot;一、使用matplotlib绘制图像&quot;&gt;&lt;/a&gt;一、使用matplotlib绘制图像&lt;/h1&gt;&lt;p&gt;matplo
      
    
    </summary>
    
    
      <category term="学习笔记" scheme="https://superlova.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="matplotlib" scheme="https://superlova.github.io/tags/matplotlib/"/>
    
      <category term="seaborn" scheme="https://superlova.github.io/tags/seaborn/"/>
    
      <category term="论文" scheme="https://superlova.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>用于科学计算的GPU选购参考</title>
    <link href="https://superlova.github.io/2019/07/01/%E7%94%A8%E4%BA%8E%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E7%9A%84GPU%E9%80%89%E8%B4%AD%E5%8F%82%E8%80%83/"/>
    <id>https://superlova.github.io/2019/07/01/%E7%94%A8%E4%BA%8E%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E7%9A%84GPU%E9%80%89%E8%B4%AD%E5%8F%82%E8%80%83/</id>
    <published>2019-07-01T10:11:15.000Z</published>
    <updated>2020-03-17T01:54:50.799Z</updated>
    
    <content type="html"><![CDATA[<p>实验室最近要采购一批显卡，需要调研显卡的型号和价格。</p><h2 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h2><p>首先说一下需求：</p><ul><li>首先显卡的用途是科学计算，更具体一点是深度学习，有人做图像，有人做NLP；</li><li>其次预算有限，得买性价比最好的；</li><li>然后可能会有很多人要用服务器训练模型。</li></ul><p>然后这是戴尔服务器的售后人员发来的建议采购清单：<br><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-18-19-43.png" srcset="/img/loading.gif" alt></p><p>其中M10：19999￥ P100： 49999￥ V100：59999￥ P40：49999￥</p><p>值得一提的是，谷歌的Colab上面用的是这款：<br><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-18-21-48.png" srcset="/img/loading.gif" alt><br><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-18-21-58.png" srcset="/img/loading.gif" alt></p><p>但是一个从事深度学习研究的学长建议我买1080Ti。他好像提都没提过Tesla啊？难道显卡水这么深？</p><h2 id="GPU参数"><a href="#GPU参数" class="headerlink" title="GPU参数"></a>GPU参数</h2><p>GPU的性能主要由下面三个主要参数构成：</p><p><strong>计算能力</strong>。通常我们关心的是32位浮点计算能力。当然，对于高玩来说也可以考虑16位浮点用来训练，8位整数来预测。</p><p><strong>内存大小</strong>。神经网络越深，或者训练时批量大小越大，所需要的GPU内存就越多。</p><p><strong>内存带宽</strong>。内存带宽要足够才能发挥出所有计算能力。</p><p>此外，针对不同深度学习架构，GPU参数的选择优先级是不一样的，总体来说分两条路线：</p><p><strong>卷积网络和Transformer</strong>：张量核心&gt;FLOPs（每秒浮点运算次数）&gt;显存带宽&gt;16位浮点计算能力</p><p><strong>循环神经网络</strong>：显存带宽&gt;16位浮点计算能力&gt;张量核心&gt;FLOPs</p><p>这个排序背后有一套逻辑，下面将详细解释一下。</p><p>在说清楚哪个GPU参数对速度尤为重要之前，先看看两个最重要的张量运算：矩阵乘法和卷积。</p><p>举个栗子，以运算矩阵乘法A×B=C为例，将A、B复制到显存上比直接计算A×B更耗费资源。也就是说，如果你想用LSTM等处理大量小型矩阵乘法的循环神经网络，显存带宽是GPU最重要的属性。</p><p>矩阵乘法越小，内存带宽就越重要。</p><p>相反，卷积运算受计算速度的约束比较大。因此，要衡量GPU运行ResNets等卷积架构的性能，最佳指标就是FLOPs。张量核心可以明显增加FLOPs。</p><p>Transformer中用到的大型矩阵乘法介于卷积运算和RNN的小型矩阵乘法之间，16位存储、张量核心和TFLOPs都对大型矩阵乘法有好处，但它仍需要较大的显存带宽。</p><h2 id="性价比分析"><a href="#性价比分析" class="headerlink" title="性价比分析"></a>性价比分析</h2><p>下面总结了一张GPU和TPU的标准性能数据，值越高代表性能越好。RTX系列假定用了16位计算，Word RNN数值是指长度&lt;100的段序列的biLSTM性能。</p><p>这项基准测试是用PyTorch 1.0.1和CUDA 10完成的。</p><p><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-19-09-53.png" srcset="/img/loading.gif" alt="GPU和TPU的性能数据"></p><p>性价比可能是选择一张GPU最重要的考虑指标。</p><p>性价比可能是选择一张GPU最重要的考虑指标。在攻略中，小哥进行了如下运算测试各显卡的性能：</p><ul><li>用语言模型Transformer-XL和BERT进行Transformer性能的基准测试。</li><li>用最先进的biLSTM进行了单词和字符级RNN的基准测试。</li><li>上述两种测试是针对Titan Xp、Titan RTX和RTX 2080 Ti进行的，对于其他GPU则线性缩放了性能差异。</li><li>借用了现有的CNN基准测试。</li><li>用了亚马逊和eBay上显卡的平均售价作为GPU的参考成本。<br>最后，可以得出CNN、RNN和Transformer的归一化性能/成本比值，如下所示：</li></ul><p><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-19-11-20.png" srcset="/img/loading.gif" alt="CNN、RNN和Transformer的每美元性能"></p><p>在上面这张图中，数字越大代表每一美元能买到的性能越强。可以看出， RTX 2060比RTX 2070，RTX 2080或RTX 2080 Ti更具成本效益，<strong>甚至是Tesla V100性价比的5倍以上</strong>。</p><p>所以此轮的性价比之王已经确定，是RTX 2060无疑了。</p><p>下图是李沐老师画了900和1000系列里各个卡的32位浮点计算能力和价格的对比（价格是wikipedia的推荐价格，真实价格通常会有浮动）。</p><p><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-19-06-00.png" srcset="/img/loading.gif" alt></p><p>由于GPU的功耗，散热和体积，需要一些额外考虑。</p><ul><li>机箱体积<br>GPU尺寸较大，通常不考虑太小的机箱。而且机箱自带的风扇要好。</li><li>电源<br>购买GPU时需要查下GPU的功耗，50w到300w不等。因此买电源时需要功率足够的。</li><li>主板的PCIe卡槽<br>推荐使用PCIe 3.0 16x来保证足够的GPU到主内存带宽。如果是多卡的话，要仔细看主板说明，保证多卡一起使用时仍然是16x带宽。（有些主板插4卡时会降到8x甚至4x）</li></ul><h2 id="Tesla为什么那么贵？"><a href="#Tesla为什么那么贵？" class="headerlink" title="Tesla为什么那么贵？"></a>Tesla为什么那么贵？</h2><p>英伟达现在有一项非常坑爹的政策，如果在数据中心使用CUDA，那么只允许使用Tesla GPU而不能用GTX或RTX GPU。</p><p>由于担心法律问题，研究机构和大学经常被迫购买低性价比的Tesla GPU。<strong>然而，Tesla与GTX和RTX相比并没有真正的优势，价格却高出10倍。</strong></p><p>Nvidia卡有面向个人用户（例如GTX系列）和企业用户（例如Tesla系列）两种。企业用户卡通常使用被动散热和增加了内存校验从而更加适合数据中心。但计算能力上两者相当。<strong>企业卡通常要贵上10倍</strong>。</p><p>Tesla显卡那么贵，其实是贵在双精度浮点数运算能力上了，外加一个鸡肋的ECC校验功能，实在不值。</p><h2 id="总结建议："><a href="#总结建议：" class="headerlink" title="总结建议："></a>总结建议：</h2><p><strong>最佳GPU</strong>：RTX 2070</p><p><strong>避免的坑</strong>：所有Tesla、Quadro、创始人版（Founders Edition）的显卡，还有Titan RTX、Titan V、Titan XP</p><p><strong>高性价比</strong>：RTX 2070（高端），RTX 2060或GTX 1060 (6GB)（中低端）</p><p><strong>计算机视觉或机器翻译研究人员</strong>：采用鼓风设计的GTX 2080 Ti，如果训练非常大的网络，请选择RTX Titans</p><p><strong>NLP研究人员</strong>：RTX 2080 Ti</p><p><strong>已经开始研究深度学习</strong>：RTX 2070起步，以后按需添置更多RTX 2070</p><h2 id="其他配件要求："><a href="#其他配件要求：" class="headerlink" title="其他配件要求："></a>其他配件要求：</h2><p><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-19-19-14.png" srcset="/img/loading.gif" alt="各硬件性能要求"></p><ul><li><p>CPU:<br>因为主要使用显卡进行cuda计算，因此对CPU的要求并不是很高，频率越高、线程数越多越好，一般最低要求cpu核心数大于显卡个数。其中一个制约因素：cpu的最大PCI-E 通道数。每张显卡占用16条pcie通道才能达到最大性能，而单cpu最大支持40条pcie，也就是即使有4个pcie x16接口，只能最多达到2路x16加一路x8，插上的显卡不能发挥全部性能。不过，主板芯片组其实也可以扩充一部分pcie通道。（x99主板可以扩宽2.0的8lanes，z170可以扩充3.0的20lanes）</p></li><li><p>主板:<br>前面提到了cpu提供的pcie通道数的限制，如果要使用多块显卡，就需要主板提供额外的pcie通道，一般只有服务器级别的主板才会提供扩展pcie通道如x99、x299等主板，但是使用此类主板必须搭配具有该接口的服务器级cpu（xeon系列、i7 7900x以上、i9系列等），如果不需要三块以上的显卡，使用cpu提供的40lane pcie即可。</p></li><li><p>内存：<br>深度学习需要大量数据，中间计算过程也会临时储存大量数据，一般要求具有显存2~3倍的内存，32G或64G乃至更高。内存频率越高越好。<br>最低建议32G DDR4 3200MHz内存(16G*2)约2000元，预算宽裕可升级到64G（约4000元）</p></li><li><p>硬盘：<br>深度学习需要大量数据，和较快的访问速度，一般使用一个较大的固态硬盘作为系统盘和训练数据仓储盘，另外使用hdd机械硬盘作为仓储盘。<br>建议使用512G以上nVME固态硬盘（800元）搭配几TB(2TB约300元）Hdd作为储存空间</p></li><li><p>电源、机箱：电源其实还是要买个比较稳定的，因为要保证长期稳定运行会有“无休止”的training。一般使用大品牌的经过80PLUS金牌或铂金认证的电源。只搭配一张显卡700w即可，每多一张增加400w。4*titan V大概使用1600w电源。</p></li></ul><p>深度学习实验室共享服务器，7x24小时运行  2080ti或者4titan V ，预算充裕可以专门购置一台高性能多显卡深度学习服务器，24*7小时运行，其他用户可以在自己的笔记本电脑和台式机上编写和初步调试卷积神经网络，本地验证无误后，上传至服务器进行训练任务。这样做可以极大的节省设备开支，最大限度的利用计算资源，也避免了每个用户单独配置复杂的软件环境。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;实验室最近要采购一批显卡，需要调研显卡的型号和价格。&lt;/p&gt;
&lt;h2 id=&quot;需求分析&quot;&gt;&lt;a href=&quot;#需求分析&quot; class=&quot;headerlink&quot; title=&quot;需求分析&quot;&gt;&lt;/a&gt;需求分析&lt;/h2&gt;&lt;p&gt;首先说一下需求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先显卡的用
      
    
    </summary>
    
    
      <category term="经验教训" scheme="https://superlova.github.io/categories/%E7%BB%8F%E9%AA%8C%E6%95%99%E8%AE%AD/"/>
    
    
      <category term="深度学习" scheme="https://superlova.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="GPU" scheme="https://superlova.github.io/tags/GPU/"/>
    
      <category term="采购" scheme="https://superlova.github.io/tags/%E9%87%87%E8%B4%AD/"/>
    
      <category term="Tesla" scheme="https://superlova.github.io/tags/Tesla/"/>
    
      <category term="1080Ti" scheme="https://superlova.github.io/tags/1080Ti/"/>
    
      <category term="Titan" scheme="https://superlova.github.io/tags/Titan/"/>
    
  </entry>
  
  <entry>
    <title>NLP学习笔记4</title>
    <link href="https://superlova.github.io/2019/06/30/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04/"/>
    <id>https://superlova.github.io/2019/06/30/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04/</id>
    <published>2019-06-30T12:59:29.000Z</published>
    <updated>2020-03-17T01:54:50.695Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-利用朴素贝叶斯模型进行文本分类"><a href="#1-利用朴素贝叶斯模型进行文本分类" class="headerlink" title="1. 利用朴素贝叶斯模型进行文本分类"></a>1. 利用朴素贝叶斯模型进行文本分类</h2><p>朴素贝叶斯是一种构建分类器的简单方法。该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。它不是训练这种分类器的单一算法，而是一系列基于相同原理的算法：<strong>所有朴素贝叶斯分类器都假定样本每个特征与其他特征都不相关</strong>。</p><p>举个例子，如果一种水果其具有红，圆，直径大概3英寸等特征，该水果可以被判定为是苹果。尽管这些特征相互依赖或者有些特征由其他特征决定，然而朴素贝叶斯分类器认为这些属性在判定该水果是否为苹果的概率分布上独立的。</p><p>尽管是带着这些朴素思想和过于简单化的假设，但朴素贝叶斯分类器在很多复杂的现实情形中仍能够获取相当好的效果。2004年，一篇分析贝叶斯分类器问题的文章揭示了朴素贝叶斯分类器获取看上去不可思议的分类效果的若干理论上的原因。尽管如此，2006年有一篇文章详细比较了各种分类方法，发现更新的方法（如决策树和随机森林）的性能超过了贝叶斯分类器。</p><p>对于某些类型的概率模型，在监督式学习的样本集中能获取得非常好的分类效果。在许多实际应用中，朴素贝叶斯模型参数估计使用<strong>最大似然估计方法</strong>；换而言之，在不用到贝叶斯概率或者任何贝叶斯模型的情况下，朴素贝叶斯模型也能奏效。</p><p>朴素贝叶斯分类器的一个优势在于只需要根据少量的训练数据估计出必要的参数（变量的均值和方差）。由于变量独立假设，只需要估计各个变量的方法，而不需要确定整个协方差矩阵。</p><p>朴素贝叶斯分类器是与线性模型非常相似的一种分类器，但它的训练速度往往更快。这种高效率所付出的代价是，朴素贝叶斯模型的泛化能力要比线性分类器（如LogisticRegression 和LinearSVC）稍差。</p><p>朴素贝叶斯模型如此高效的原因在于，它通过单独查看每个特征来学习参数，并从每个特征中收集简单的类别统计数据。scikit-learn 中实现了三种朴素贝叶斯分类器：GaussianNB、BernoulliNB 和MultinomialNB。GaussianNB 可应用于任意连续数据， 而BernoulliNB 假定输入数据为二分类数据，MultinomialNB 假定输入数据为计数数据（即每个特征代表某个对象的整数计数，比如一个单词在句子里出现的次数）。BernoulliNB 和MultinomialNB 主要用于文本数据分类。</p><pre><code class="lang-python"># 从sklearn.datasets里导入20类新闻文本数据抓取器。from sklearn.datasets import fetch_20newsgroups# 从互联网上即时下载新闻样本,subset=&#39;all&#39;参数代表下载全部近2万条文本存储在变量news中。news = fetch_20newsgroups(subset=&#39;all&#39;)# 从sklearn.cross_validation导入train_test_split模块用于分割数据集。from sklearn.cross_validation import train_test_split# 对news中的数据data进行分割，25%的文本用作测试集；75%作为训练集。X_train, X_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25, random_state=33)# 从sklearn.feature_extraction.text里导入CountVectorizerfrom sklearn.feature_extraction.text import CountVectorizer# 采用默认的配置对CountVectorizer进行初始化（默认配置不去除英文停用词），并且赋值给变量count_vec。count_vec = CountVectorizer()# 只使用词频统计的方式将原始训练和测试文本转化为特征向量。#学习词汇的词典并返回文档矩阵。X_count_train = count_vec.fit_transform(X_train)#不进行学习直接转换文档document-term矩阵X_count_test = count_vec.transform(X_test)# 从sklearn.naive_bayes里导入朴素贝叶斯分类器。from sklearn.naive_bayes import MultinomialNB# 使用默认的配置对分类器进行初始化。mnb_count = MultinomialNB()# 使用朴素贝叶斯分类器，对CountVectorizer（不去除停用词）后的训练样本进行参数学习。mnb_count.fit(X_count_train, y_train)# 输出模型准确性结果。print (&#39;The accuracy of classifying 20newsgroups using Naive Bayes (CountVectorizer without filtering stopwords):&#39;, mnb_count.score(X_count_test, y_test))# 将分类预测的结果存储在变量y_count_predict中。y_count_predict = mnb_count.predict(X_count_test)# 从sklearn.metrics 导入 classification_report。from sklearn.metrics import classification_report# 输出更加详细的其他评价分类性能的指标。print (classification_report(y_test, y_count_predict, target_names = news.target_names))</code></pre><p><img src="/2019/06/30/NLP学习笔记4/2019-06-30-20-42-49.png" srcset="/img/loading.gif" alt></p><h2 id="2-利用SVM模型进行文本分类"><a href="#2-利用SVM模型进行文本分类" class="headerlink" title="2. 利用SVM模型进行文本分类"></a>2. 利用SVM模型进行文本分类</h2><h2 id="3-pLSA、共轭先验分布、LDA"><a href="#3-pLSA、共轭先验分布、LDA" class="headerlink" title="3. pLSA、共轭先验分布、LDA"></a>3. pLSA、共轭先验分布、LDA</h2><p>常用于文本数据的一种特殊技术是主题建模（topic modeling），这是描述将每个文档分配给一个或多个主题的任务（通常是无监督的）的概括性术语。这方面一个很好的例子是新闻数据，它们可以被分为“政治”“体育”“金融”等主题。如果为每个文档分配一个主题，那么这是一个文档聚类任务。如果每个文档可以有多个主题，那么这个任务与第3 章中的分解方法有关。我们学到的每个成分对应于一个主题，文档表示中的成分系数告诉我们这个文档与该主题的相关性强弱。通常来说，人们在谈论主题建模时，他们指的是一种叫作隐含狄利克雷分布（Latent Dirichlet Allocation，LDA）的特定分解方法</p><h3 id="隐含狄利克雷分布"><a href="#隐含狄利克雷分布" class="headerlink" title="隐含狄利克雷分布"></a>隐含狄利克雷分布</h3><p>从直观上来看，LDA 模型试图找出频繁共同出现的单词群组（即主题）。LDA 还要求，每个文档可以被理解为主题子集的“混合”。重要的是要理解，机器学习模型所谓的“主题”可能不是我们通常在日常对话中所说的主题，而是更类似于 PCA 或 NMF所提取的成分，它可能具有语义，也可能没有。即使 LDA“主题”具有语义，它可能也不是我们通常所说的主题。</p><p>举个自然语言处理的例子，我们可能有许多关于体育、政治和金融的文章，由两位作者所写。在一篇政治文章中，我们预计可能会看 到“州长”“投票”“党派”等词语，而在一篇体育文章中，我们预计可能会看到类似“队 伍”“得分”和“赛季”之类的词语。这两组词语可能会同时出现，而例如“队伍”和 “州长”就不太可能同时出现。但是，这并不是我们预计可能同时出现的唯一的单词群组。这两位记者可能偏爱不同的短语或者选择不同的单词。可能其中一人喜欢使用“划界”（demarcate）这个词，而另一人喜欢使用“两极分化”（polarize）这个词。其他“主题”可 能是“记者 A 常用的词语”和“记者 B 常用的词语”，虽然这并不是通常意义上的主题。</p><h2 id="4-使用LDA生成主题特征，在之前特征的基础上加入主题特征进行文本分类"><a href="#4-使用LDA生成主题特征，在之前特征的基础上加入主题特征进行文本分类" class="headerlink" title="4. 使用LDA生成主题特征，在之前特征的基础上加入主题特征进行文本分类"></a>4. 使用LDA生成主题特征，在之前特征的基础上加入主题特征进行文本分类</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-利用朴素贝叶斯模型进行文本分类&quot;&gt;&lt;a href=&quot;#1-利用朴素贝叶斯模型进行文本分类&quot; class=&quot;headerlink&quot; title=&quot;1. 利用朴素贝叶斯模型进行文本分类&quot;&gt;&lt;/a&gt;1. 利用朴素贝叶斯模型进行文本分类&lt;/h2&gt;&lt;p&gt;朴素贝叶斯是一种
      
    
    </summary>
    
    
      <category term="学习笔记" scheme="https://superlova.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="https://superlova.github.io/tags/NLP/"/>
    
      <category term="深度学习" scheme="https://superlova.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="https://superlova.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>NLP学习笔记3</title>
    <link href="https://superlova.github.io/2019/06/27/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03/"/>
    <id>https://superlova.github.io/2019/06/27/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03/</id>
    <published>2019-06-27T12:51:24.000Z</published>
    <updated>2020-03-17T01:54:50.691Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-TF-IDF原理。"><a href="#1-TF-IDF原理。" class="headerlink" title="1. TF-IDF原理。"></a>1. TF-IDF原理。</h2><p>tf-idf（英语：term frequency–inverse document frequency）是一种用于信息检索与文本挖掘的常用加权技术。tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。tf-idf加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了tf-idf以外，互联网上的搜索引擎还会使用基于链接分析的评级方法，以确定文件在搜索结果中出现的顺序。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>在一份给定的文件里，词频（term frequency，tf）指的是某一个给定的词语在该文件中出现的频率。这个数字是对词数（term count）的归一化，以防止它偏向长的文件。（同一个词语在长文件里可能会比短文件有更高的词数，而不管该词语重要与否。）对于在某一特定文件里的词语 ${\displaystyle t_{i}!}$来说，它的重要性可表示为：</p><script type="math/tex; mode=display">{\displaystyle \mathrm {tf_{i,j}\!} ={\frac {n_{i,j}\!}{\sum _{k}n_{k,j}\!}\!}\!}</script><p>以上式子中 ${\displaystyle n_{i,j}!} n_{i,j}$是该词在文件 ${\displaystyle d_{j}!} d_{j}$中的出现次数，而分母则是在文件 ${\displaystyle d_{j}!} d_{j}$中所有字词的出现次数之和。</p><p>逆向文件频率（inverse document frequency，idf）是一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到：</p><script type="math/tex; mode=display">{\displaystyle \mathrm {idf_{i}\!} =\lg {\frac {|D|}{|\{j:t_{i}\in d_{j}\}|}\!}\!}</script><p>其中</p><p>$|D|$：语料库中的文件总数</p><script type="math/tex; mode=display">{\displaystyle |\{j:t_{i}\in d_{j}\}|} $$：包含词语 ${\displaystyle t_{i}\!} t_{i}$的文件数目（即 ${\displaystyle n_{i,j}\neq 0} n_{i,j}\neq 0$的文件数目）如果词语不在数据中，就导致分母为零，因此一般情况下使用 $${\displaystyle 1+|\{j:t_{i}\in d_{j}\}|}</script><p>然后</p><script type="math/tex; mode=display">{\displaystyle \mathrm {tf{}idf_{i,j}\!} =\mathrm {tf_{i,j}\!} \times \mathrm {idf_{i}\!} }</script><p>某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的tf-idf。因此，tf-idf倾向于过滤掉常见的词语，保留重要的词语。</p><h3 id="1-2-例子"><a href="#1-2-例子" class="headerlink" title="1.2 例子"></a>1.2 例子</h3><p>有很多不同的数学公式可以用来计算tf-idf。这边的例子以上述的数学公式来计算。词频（tf）是一词语出现的次数除以该文件的总词语数。假如一篇文件的总词语数是100个，而词语“母牛”出现了3次，那么“母牛”一词在该文件中的词频就是3/100=0.03。而计算文件频率（IDF）的方法是以文件集的文件总数，除以出现“母牛”一词的文件数。所以，如果“母牛”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是lg（10,000,000 / 1,000）=4。最后的tf-idf的分数为0.03 * 4=0.12。</p><h3 id="1-3-tf-idf的理论依据及不足"><a href="#1-3-tf-idf的理论依据及不足" class="headerlink" title="1.3 tf-idf的理论依据及不足"></a>1.3 tf-idf的理论依据及不足</h3><p>tf-idf算法是创建在这样一个假设之上的：对区别文档最有意义的词语应该是那些在文档中出现频率高，而在整个文档集合的其他文档中出现频率少的词语，所以如果特征空间坐标系取tf词频作为测度，就可以体现同类文本的特点。另外考虑到单词区别不同类别的能力，tf-idf法认为一个单词出现的文本频数越小，它区别不同类别文本的能力就越大。因此引入了逆文本频度idf的概念，以tf和idf的乘积作为特征空间坐标系的取值测度，并用它完成对权值tf的调整，调整权值的目的在于突出重要单词，抑制次要单词。但是在本质上idf是一种试图抑制噪声的加权，并且单纯地认为文本频率小的单词就越重要，文本频率大的单词就越无用，显然这并不是完全正确的。idf的简单结构并不能有效地反映单词的重要程度和特征词的分布情况，使其无法很好地完成对权值调整的功能，所以tf-idf法的精度并不是很高。</p><p>此外，在tf-idf算法中并没有体现出单词的位置信息，对于Web文档而言，权重的计算方法应该体现出HTML的结构特征。特征词在不同的标记符中对文章内容的反映程度不同，其权重的计算方法也应不同。因此应该对于处于网页不同位置的特征词分别赋予不同的系数，然后乘以特征词的词频，以提高文本表示的效果。</p><h2 id="2-文本矩阵化，使用词袋模型，以TF-IDF特征值为权重。"><a href="#2-文本矩阵化，使用词袋模型，以TF-IDF特征值为权重。" class="headerlink" title="2. 文本矩阵化，使用词袋模型，以TF-IDF特征值为权重。"></a>2. 文本矩阵化，使用词袋模型，以TF-IDF特征值为权重。</h2><p>TfidfVectorizer可以把原始文本转化为tf-idf的特征矩阵，从而为后续的文本相似度计算，主题模型(如LSI)，文本搜索排序等一系列应用奠定基础。基本应用如：</p><h3 id="第一步：分词"><a href="#第一步：分词" class="headerlink" title="第一步：分词"></a>第一步：分词</h3><p>采用著名的中文分词库jieba进行分词：</p><pre><code class="lang-python">import jiebatext = &quot;&quot;&quot;我是一条天狗呀！我把月来吞了，我把日来吞了，我把一切的星球来吞了，我把全宇宙来吞了。我便是我了！&quot;&quot;&quot;sentences = text.split()sent_words = [list(jieba.cut(sent0)) for sent0 in sentences]document = [&quot; &quot;.join(sent0) for sent0 in sent_words]print(document)</code></pre><h3 id="第二步：建模"><a href="#第二步：建模" class="headerlink" title="第二步：建模"></a>第二步：建模</h3><pre><code>理论上，现在得到的document的格式已经可以直接拿来训练了。让我们跑一下模型试试。</code></pre><pre><code class="lang-python">tfidf_model = TfidfVectorizer().fit(document)print(tfidf_model.vocabulary_)# {&#39;一条&#39;: 1, &#39;天狗&#39;: 4, &#39;日来&#39;: 5, &#39;一切&#39;: 0, &#39;星球&#39;: 6, &#39;全宇宙&#39;: 3, &#39;便是&#39;: 2}sparse_result = tfidf_model.transform(document)print(sparse_result)# (0, 4)    0.707106781187# (0, 1)    0.707106781187# (2, 5)    1.0# (3, 6)    0.707106781187# (3, 0)    0.707106781187# (4, 3)    1.0# (5, 2)    1.0</code></pre><h3 id="第三步：参数"><a href="#第三步：参数" class="headerlink" title="第三步：参数"></a>第三步：参数</h3><pre><code>查了一些资料以后，发现单字的问题是token_pattern这个参数搞的鬼。它的默认值只匹配长度≥2的单词，就像其实开头的例子中的&#39;I&#39;也被忽略了一样，一般来说，长度为1的单词在英文中一般是无足轻重的，但在中文里，就可能有一些很重要的单字词，所以修改如下：</code></pre><pre><code class="lang-python">tfidf_model2 = TfidfVectorizer(token_pattern=r&quot;(?u)\b\w+\b&quot;).fit(document)print(tfidf_model2.vocabulary_)# {&#39;我&#39;: 8, &#39;是&#39;: 12, &#39;一条&#39;: 1, &#39;天狗&#39;: 7, &#39;呀&#39;: 6, &#39;把&#39;: 9, &#39;月&#39;: 13, &#39;来&#39;: 14, &#39;吞&#39;: 5, &#39;了&#39;: 2, &#39;日来&#39;: 10, &#39;一切&#39;: 0, &#39;的&#39;: 15, &#39;星球&#39;: 11, &#39;全宇宙&#39;: 4, &#39;便是&#39;: 3}</code></pre><p>token_pattern这个参数使用正则表达式来分词，其默认参数为r”(?u)\b\w\w+\b”，其中的两个\w决定了其匹配长度至少为2的单词，所以这边减到1个。对这个参数进行更多修改，可以满足其他要求，比如这里依然没有得到标点符号，在此不详解了。</p><h2 id="3-互信息"><a href="#3-互信息" class="headerlink" title="3. 互信息"></a>3. 互信息</h2><p>在概率论和信息论中，两个随机变量的互信息（Mutual Information，简称MI）或转移信息（transinformation）是变量间相互依赖性的量度。不同于相关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布$ p(X,Y) $和分解的边缘分布的乘积$ p(X)p(Y) $的相似程度。互信息是点间互信息（PMI）的期望值。互信息最常用的单位是bit。</p><p>一般地，两个离散随机变量$ X $和$ Y $的互信息可以定义为：</p><script type="math/tex; mode=display">{\displaystyle I(X;Y)=\sum _{y\in Y}\sum _{x\in X}p(x,y)\log {\left({\frac {p(x,y)}{p(x)\,p(y)}\!}\right)},\,\!}</script><p>其中$ p(x,y) $是 $X $和 $Y $的联合概率分布函数，而 ${\displaystyle p(x)} $和$ {\displaystyle p(y)}  $分别是 X 和 Y 的边缘概率分布函数。</p><p>在连续随机变量的情形下，求和被替换成了二重定积分：</p><script type="math/tex; mode=display">{\displaystyle I(X;Y)=\int _{Y}\int _{X}p(x,y)\log {\left({\frac {p(x,y)}{p(x)\,p(y)}\!}\right)}\;dx\,dy,} $$,其中 $p(x,y)$ 当前是 X 和 Y 的联合概率密度函数，而 ${\displaystyle p(x)} $和$ {\displaystyle p(y)}$分别是$ X $和$ Y $的边缘概率密度函数。如果对数以 2 为基底，互信息的单位是bit。直观上，互信息度量 X 和 Y 共享的信息：它度量知道这两个变量其中一个，对另一个不确定度减少的程度。例如，如果 X 和 Y 相互独立，则知道 X 不对 Y 提供任何信息，反之亦然，所以它们的互信息为零。在另一个极端，如果 X 是 Y 的一个确定性函数，且 Y 也是 X 的一个确定性函数，那么传递的所有信息被 X 和 Y 共享：知道 X 决定 Y 的值，反之亦然。因此，在此情形互信息与 Y（或 X）单独包含的不确定度相同，称作 Y（或 X）的熵。而且，这个互信息与 X 的熵和 Y 的熵相同。（这种情形的一个非常特殊的情况是当 X 和 Y 为相同随机变量时。）互信息是 X 和 Y 的联合分布相对于假定 X 和 Y 独立情况下的联合分布之间的内在依赖性。 于是互信息以下面方式度量依赖性：$I(X; Y) = 0$ 当且仅当 X 和 Y 为独立随机变量。从一个方向很容易看出：当 X 和 Y 独立时，$p(x,y) = p(x) p(y)$，因此：$${\displaystyle \log {\left({\frac {p(x,y)}{p(x)\,p(y)} \!}\right)}=\log 1=0.\,\!}</script><p>此外，互信息是非负的（即 $I(X;Y) ≥ 0$; 见下文），而且是对称的（即 $I(X;Y) = I(Y;X)$）。</p><p>互信息又可以等价地表示成</p><script type="math/tex; mode=display">{\displaystyle {\begin{aligned}I(X;Y)&{}=H(X)-H(X|Y)\\&{}=H(Y)-H(Y|X)\\&{}=H(X)+H(Y)-H(X,Y)\\&{}=H(X,Y)-H(X|Y)-H(Y|X)\end{aligned}\!}\!}</script><p>其中$ {\displaystyle \ H(X)} $ 和$ {\displaystyle \ H(Y)}  $是边缘熵，$H(X|Y) $和$ H(Y|X) $是条件熵，而 $H(X,Y) $是 X 和 Y 的联合熵。</p><p><strong>互信息越小，两个来自不同事件空间的随机变量彼此之间的关系性越低; 互信息越高，关系性则越高。</strong></p><h2 id="4-对特征矩阵使用互信息进行特征筛选"><a href="#4-对特征矩阵使用互信息进行特征筛选" class="headerlink" title="4. 对特征矩阵使用互信息进行特征筛选"></a>4. 对特征矩阵使用互信息进行特征筛选</h2><p><code>sklearn.metrics.mutual_info_score</code></p><pre><code class="lang-python">from sklearn import datasetsfrom sklearn import metrics as mriris = datasets.load_iris()x = iris.datalabel = iris.targetx0 = x[:, 0]x1 = x[:, 1]x2 = x[:, 2]x3 = x[:, 3]# 计算各特征与label的互信息print(mr.mutual_info_score(x0, label))print(mr.mutual_info_score(x1, label))print(mr.mutual_info_score(x2, label))print(mr.mutual_info_score(x3, label))</code></pre><p><img src="/2019/06/27/NLP学习笔记3/2019-06-27-20-45-13.png" srcset="/img/loading.gif" alt><br><code>sklearn.feature_selection.mutual_info_classif</code></p><pre><code class="lang-python">from sklearn import datasetsfrom sklearn.feature_selection import mutual_info_classifiris = datasets.load_iris()x = iris.datalabel = iris.targetmutual_info = mutual_info_classif(x, label, discrete_features= False)print(mutual_info)</code></pre><p><img src="/2019/06/27/NLP学习笔记3/2019-06-27-20-45-28.png" srcset="/img/loading.gif" alt></p><blockquote><p>参考文献<br><a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/Tf-idf</a><br><a href="https://zh.wikipedia.org/wiki/%E4%BA%92%E4%BF%A1%E6%81%AF" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E4%BA%92%E4%BF%A1%E6%81%AF</a><br><a href="https://blog.csdn.net/yyy430/article/details/88249709" target="_blank" rel="noopener">https://blog.csdn.net/yyy430/article/details/88249709</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-TF-IDF原理。&quot;&gt;&lt;a href=&quot;#1-TF-IDF原理。&quot; class=&quot;headerlink&quot; title=&quot;1. TF-IDF原理。&quot;&gt;&lt;/a&gt;1. TF-IDF原理。&lt;/h2&gt;&lt;p&gt;tf-idf（英语：term frequency–inverse
      
    
    </summary>
    
    
      <category term="学习笔记" scheme="https://superlova.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="https://superlova.github.io/tags/NLP/"/>
    
      <category term="深度学习" scheme="https://superlova.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="https://superlova.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>机器学习——集成方法</title>
    <link href="https://superlova.github.io/2019/06/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95/"/>
    <id>https://superlova.github.io/2019/06/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95/</id>
    <published>2019-06-27T08:04:10.000Z</published>
    <updated>2020-03-17T01:54:50.782Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-投票分类器"><a href="#1-投票分类器" class="headerlink" title="1. 投票分类器"></a>1. 投票分类器</h2><p>三个臭皮匠顶一个诸葛亮。即便是很多预测准确率仅强于随机的个体学习器的组合，经过一定的安排，也可以发挥令人惊讶的效果。在机器学习中，这种看起来没什么含金量的学习策略称之为<strong>集成学习</strong>。</p><p>集成学习首先需要一系列的个体学习器。之后采用某些策略结合它们的判断。</p><p>集成学习的要求：</p><ul><li>构成集成学习器的个体学习器，其性能不能太差，至少要为强于随机的<strong>弱学习器</strong>。</li></ul><p>当然强学习器更好。在最后的结果汇总阶段，也会更多听取强学习器的意见。</p><ul><li>个体学习器要具有一定的多样性。</li></ul><p>广泛吸收各种不同学习器的意见，做出的决策才有代表性。在机器学习中，体现出的要求就是模型之间的差别要尽可能的大。一方面可以通过划分不同的数据集，独立训练来得到差异性；另一方面我们也可以选取不同的训练模型，比如SVM、决策树、逻辑回归。</p><p>集成学习法的准确率比集成学习中表现最好的分类器准确率还高，这究竟是为什么？难道那一些不入流的臭鱼烂虾机器学习法，它们存在的意义就是提升集成学习中的准确率的吗？</p><p>我们来打个比方。假设我有一枚硬币，这枚硬币经过加工处理，正面朝上的可能性比背面要高那么一点点，51%的可能性是正面。问，如果我投掷1000次硬币，正面朝上次数大于背面朝上次数的可能性占比多少？一万次呢？</p><p>事实上，1000次投掷，最后正面次数比背面多的概率就达到了0.72，如果投掷10000次，那么就是0.94，几乎是必然事件。</p><p>将其类比到集成学习中来，如果相互独立的个体学习器足够多，那么我们得到正确结论的概率将大大提升。不过这里有一个最关键的点：<strong>模型之间相互独立</strong>。这个要求其实是蛮难达到的，因为即便是不同的机器学习模型，如果采用相同或者相似的数据集进行训练，那么他们之间必然存在某种相关性。更不用说连模型都是一模一样的情况了。</p><h2 id="2-Boosting算法"><a href="#2-Boosting算法" class="headerlink" title="2. Boosting算法"></a>2. Boosting算法</h2><p>Boosting算法的核心思想是分割训练集，用同一种机器学习算法得到差异化的一系列模型。</p><p>先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本的分布进行调整，是的先前基学习器做错的训练样本在后续收到更多地关注。然后基于调整后的样本分布来训练下一个基学习器。如此反复进行，训练T个基学习器，最终加权投票。</p><p><img src="/2019/06/27/机器学习——集成方法/2019-06-27-21-57-03.png" srcset="/img/loading.gif" alt></p><p><img src="/2019/06/27/机器学习——集成方法/2019-06-27-21-57-17.png" srcset="/img/loading.gif" alt></p><h2 id="3-Bagging算法"><a href="#3-Bagging算法" class="headerlink" title="3. Bagging算法"></a>3. Bagging算法</h2><p>有放回采样被称为Bagging。采用Bagging的方法我们可以得到很多的可能有重复样本的数据子集。我们在之前的文章中已经提到，对于每个选取的子集，平均下来只有63%的训练实例被采样，剩下的37%正好当做测试集。</p><p>随机森林就采用了Bagging采样方法来训练很多的决策树。如果你观察单一的决策树，重要的特征会出现在更靠近根部的位置，不重要的特征会经常出现在靠近叶子的位置。因此我们可以通过计算一个特征在森林的全部树中出现的平均深度来预测特征的重要性。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-投票分类器&quot;&gt;&lt;a href=&quot;#1-投票分类器&quot; class=&quot;headerlink&quot; title=&quot;1. 投票分类器&quot;&gt;&lt;/a&gt;1. 投票分类器&lt;/h2&gt;&lt;p&gt;三个臭皮匠顶一个诸葛亮。即便是很多预测准确率仅强于随机的个体学习器的组合，经过一定的安排，也可以
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://superlova.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="学习笔记" scheme="https://superlova.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="https://superlova.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="西瓜书" scheme="https://superlova.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
      <category term="集成学习" scheme="https://superlova.github.io/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>NLP学习笔记2</title>
    <link href="https://superlova.github.io/2019/06/24/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/"/>
    <id>https://superlova.github.io/2019/06/24/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/</id>
    <published>2019-06-24T12:19:24.000Z</published>
    <updated>2020-03-17T01:54:50.679Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-分词的概念和实现细节"><a href="#1-分词的概念和实现细节" class="headerlink" title="1. 分词的概念和实现细节"></a>1. 分词的概念和实现细节</h2><p>NLP的底层任务可分为词法分析、句法分析和语义分析，分词是词法分析中最基本的任务。中文分词是在一个中文序列的词与此之间加上空格或者其他边界标志进行分割，从而方便接下来步骤的处理。</p><p>分词算法可分为两种，一种是基于词典的分词算法，另一种是基于字的分词算法。</p><h3 id="1-1-基于词典的分词算法："><a href="#1-1-基于词典的分词算法：" class="headerlink" title="1.1 基于词典的分词算法："></a>1.1 基于词典的分词算法：</h3><p><strong>最大匹配分词算法</strong>，有正向和反向两种。主要思路是将词典构造成一颗Trie树，也成为词典树。以“他说的确实在理”这句话为例，构造Trie树如图所示：</p><p><img src="/2019/06/24/NLP学习笔记2/2019-06-24-20-42-46.png" srcset="/img/loading.gif" alt></p><p>Trie树由词的公共前缀构成节点，降低了存储空间的同时提升查找效率。最大（正向）匹配分词将句子与Trie树进行匹配，在匹配到根结点时由下一个字重新开始进行查找。比如正向（从左至右）匹配“他说的确实在理”，得出的结果为“他／说／的确／实在／理”。如果进行反向最大匹配，则为“他／说／的／确实／在理”。</p><p>单独依仗这种方法达不到很好的分词效果，而且分词时间复杂度为O(N)，即随着字符串长度线性上升。</p><p><strong>最短路径分词算法</strong>，讲一句话中所有的词匹配出来构成<strong>词图</strong>，词图是一个有向无环图。之后分词问题转化为求开始节点和结束节点之间的最短路径的问题。有迪杰斯特拉算法以及其他算法。不一定只保存最短的路径，有可能保存前N短的路径。图的边上也有可能按照不同词汇出现的概率大小不同安排不同的权值。</p><p><img src="/2019/06/24/NLP学习笔记2/2019-06-24-20-47-26.png" srcset="/img/loading.gif" alt></p><p>如何构建不同权值的词图？有基于n-gram的分词算法。最后我们可以得到词的概率图。</p><p><img src="/2019/06/24/NLP学习笔记2/2019-06-24-20-49-14.png" srcset="/img/loading.gif" alt></p><h3 id="1-2-基于字的分词算法"><a href="#1-2-基于字的分词算法" class="headerlink" title="1.2 基于字的分词算法"></a>1.2 基于字的分词算法</h3><p>与基于词典的分词不同的是，基于字的分词事先不对句子进行词的匹配，而是将分词看成序列标注问题，把一个字标记成B(Begin), I(Inside), O(Outside), E(End), S(Single)。因此也可以看成是每个字的分类问题，输入为每个字及其前后字所构成的特征，输出为分类标记。对于分类问题，可以用统计机器学习或神经网络的方法求解。</p><p>在NLP中，最常用的神经网络为循环神经网络（RNN，Recurrent Neural Network），它在处理变长输入和序列输入问题中有着巨大的优势。LSTM为RNN变种的一种，在一定程度上解决了RNN在训练过程中梯度消失和梯度爆炸的问题。双向（Bidirectional）循环神经网络分别从句子的开头和结尾开始对输入进行处理，将上下文信息进行编码，提升预测效果。</p><h2 id="2-词、字符频率统计"><a href="#2-词、字符频率统计" class="headerlink" title="2. 词、字符频率统计"></a>2. 词、字符频率统计</h2><p>统计一篇文章中单词出现的次数，首先应该知道该文章中，有多少个单词（去重后），然后再统计单词在文章中的出现频率。这里使用最简单的方式来实现该功能。</p><pre><code class="lang-python">def statistics():    path = ...    with open(path, &#39;r&#39;, encoding=&#39;UTF-8&#39;) as text:        print(string.punctuation)        words = [raw_word.strip(string.punctuation).lower() for raw_word in text.read().split()]        words_index = set(words)        counts_dict = {index: words.count(index) for index in words_index}    for word in sorted(counts_dict, key=lambda x: counts_dict[x], reverse=True):        print(&#39;{}--{} times&#39;.format(word, counts_dict[word]))</code></pre><h2 id="3-语言模型中unigram、bigram、trigram的概念"><a href="#3-语言模型中unigram、bigram、trigram的概念" class="headerlink" title="3. 语言模型中unigram、bigram、trigram的概念"></a>3. 语言模型中unigram、bigram、trigram的概念</h2><p>简单地说，语言模型就是用来计算一个句子的概率的模型。为了解决參数空间过大的问题。引入了马尔科夫假设：随意一个词出现的概率只与它前面出现的有限的一个或者几个词有关。</p><p>如果一个词的出现与它周围的词是独立的，那么我们就称之为unigram也就是一元语言模型：</p><p><img src="/2019/06/24/NLP学习笔记2/2019-06-24-20-55-14.png" srcset="/img/loading.gif" alt></p><p>如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为bigram：</p><p><img src="/2019/06/24/NLP学习笔记2/2019-06-24-20-54-56.png" srcset="/img/loading.gif" alt></p><p>同理，trigram：</p><p><img src="/2019/06/24/NLP学习笔记2/2019-06-24-20-55-41.png" srcset="/img/loading.gif" alt></p><p>一般来说，N元模型就是假设当前词的出现概率只与它前面的N-1个词有关。在实践中用的最多的就是bigram和trigram了。</p><h2 id="4-文本矩阵化"><a href="#4-文本矩阵化" class="headerlink" title="4. 文本矩阵化"></a>4. 文本矩阵化</h2><pre><code class="lang-python">a =&quot;自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学&quot;b = &quot;因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。&quot;c =&quot;因而它是计算机科学的一部分。自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。&quot;import jiebaall_list= [&#39;  &#39;.join(jieba.cut(s,cut_all = False)) for s in [a,b,c]]print(all_list)</code></pre><pre><code class="lang-python">#从文件导入停用词表stpwrdpath =&quot;C:\\Users\\Administrator\Desktop\lect09_codes\lect09_proj\stop_words\\中文停用词库.txt&quot;with open(stpwrdpath, &#39;rb&#39;) as fp:    stopword = fp.read().decode(&#39;utf-8&#39;)  # 提用词提取#将停用词表转换为list  stpwrdlst = stopword.splitlines()# 从sklearn.feature_extraction.text里导入CountVectorizerfrom sklearn.feature_extraction.text import CountVectorizer# 对CountVectorizer进行初始化（去除中文停用词）count_vec=CountVectorizer(stop_words=stpwrdlst) #创建词袋数据结构X_count_train = count_vec.fit_transform(all_list[:2])  #&lt;class &#39;scipy.sparse.csr.csr_matrix&#39;&gt;# 将原始训练和测试文本转化为特征向量X_count_train= X_count_train.toarray()X_count_test = count_vec.transform(all_list[2]).toarray()print(X_count_train)#词汇表print(&#39;\nvocabulary list:\n\n&#39;,count_vec.get_feature_names())print( &#39;\nvocabulary dic :\n\n&#39;,count_vec.vocabulary_)print (&#39;vocabulary:\n\n&#39;)for key,value in count_vec.vocabulary_.items():    print(key,value)</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-分词的概念和实现细节&quot;&gt;&lt;a href=&quot;#1-分词的概念和实现细节&quot; class=&quot;headerlink&quot; title=&quot;1. 分词的概念和实现细节&quot;&gt;&lt;/a&gt;1. 分词的概念和实现细节&lt;/h2&gt;&lt;p&gt;NLP的底层任务可分为词法分析、句法分析和语义分析，分词
      
    
    </summary>
    
    
      <category term="学习笔记" scheme="https://superlova.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="https://superlova.github.io/tags/NLP/"/>
    
      <category term="深度学习" scheme="https://superlova.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="IMDB" scheme="https://superlova.github.io/tags/IMDB/"/>
    
  </entry>
  
  <entry>
    <title>NLP学习笔记1</title>
    <link href="https://superlova.github.io/2019/06/21/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/"/>
    <id>https://superlova.github.io/2019/06/21/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/</id>
    <published>2019-06-21T13:02:07.000Z</published>
    <updated>2020-03-17T01:54:50.678Z</updated>
    
    <content type="html"><![CDATA[<h1 id="IMDB数据集探索"><a href="#IMDB数据集探索" class="headerlink" title="IMDB数据集探索"></a>IMDB数据集探索</h1><p>实验是在Google Colab上面做的，机器也是用的谷歌云。</p><pre><code class="lang-python"># keras.datasets.imdb is broken in 1.13 and 1.14, by np 1.16.3!pip install tf_nightly</code></pre><p>安装tensorflow</p><pre><code class="lang-python">from __future__ import absolute_import, division, print_function, unicode_literalsimport tensorflow as tffrom tensorflow import kerasimport numpy as npprint(tf.__version__)</code></pre><p>导入相关的包</p><pre><code class="lang-python">imdb = keras.datasets.imdb(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)</code></pre><p>导入IMDB数据集，将其分成四部分，分别是训练集、训练集答案、测试集、测试集答案。</p><pre><code class="lang-python">print(&quot;Training entries: {}, labels: {}&quot;.format(len(train_data), len(train_labels)))print(train_data[0])len(train_data[0]), len(train_data[1])</code></pre><p>探索数据集。可以发现，训练集的每一个训练样本是一个由数字组成的列表。我还纳闷呢，这不应该是文本序列吗？</p><p>后来我发现，每个数字对应着不同的单词。比如1对应的是The，4对应的是film。之后只要根据字典mapping一下就好。</p><pre><code class="lang-python"># A dictionary mapping words to an integer indexword_index = imdb.get_word_index()# The first indices are reservedword_index = {k:(v+3) for k,v in word_index.items()}word_index[&quot;&lt;PAD&gt;&quot;] = 0word_index[&quot;&lt;START&gt;&quot;] = 1word_index[&quot;&lt;UNK&gt;&quot;] = 2  # unknownword_index[&quot;&lt;UNUSED&gt;&quot;] = 3reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])def decode_review(text):    return &#39; &#39;.join([reverse_word_index.get(i, &#39;?&#39;) for i in text])decode_review(train_data[0])</code></pre><p>提取字典。并尝试利用字典还原一个样本的本来面貌。</p><pre><code class="lang-python">train_data = keras.preprocessing.sequence.pad_sequences(train_data,                                                        value=word_index[&quot;&lt;PAD&gt;&quot;],                                                        padding=&#39;post&#39;,                                                        maxlen=256)test_data = keras.preprocessing.sequence.pad_sequences(test_data,                                                       value=word_index[&quot;&lt;PAD&gt;&quot;],                                                       padding=&#39;post&#39;,                                                       maxlen=256)len(train_data[0]), len(train_data[1])print(train_data[0])</code></pre><p>简化训练集和测试集，每个样本之提取最多256个单次，不够的就以0来凑。</p><pre><code class="lang-python"># input shape is the vocabulary count used for the movie reviews (10,000 words)vocab_size = 10000model = keras.Sequential()model.add(keras.layers.Embedding(vocab_size, 16))model.add(keras.layers.GlobalAveragePooling1D())model.add(keras.layers.Dense(16, activation=tf.nn.relu))model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))model.summary()</code></pre><p>安排网络，输入层、池化层、全连接、softmax层。</p><pre><code class="lang-python">model.compile(optimizer=&#39;adam&#39;,              loss=&#39;binary_crossentropy&#39;,              metrics=[&#39;acc&#39;])</code></pre><pre><code class="lang-python">x_val = train_data[:10000]partial_x_train = train_data[10000:]y_val = train_labels[:10000]partial_y_train = train_labels[10000:]history = model.fit(partial_x_train,                    partial_y_train,                    epochs=40,                    batch_size=512,                    validation_data=(x_val, y_val),                    verbose=1)</code></pre><p>训练！</p><pre><code class="lang-python">results = model.evaluate(test_data, test_labels)print(results)</code></pre><p>测试结果是准确率87%。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;IMDB数据集探索&quot;&gt;&lt;a href=&quot;#IMDB数据集探索&quot; class=&quot;headerlink&quot; title=&quot;IMDB数据集探索&quot;&gt;&lt;/a&gt;IMDB数据集探索&lt;/h1&gt;&lt;p&gt;实验是在Google Colab上面做的，机器也是用的谷歌云。&lt;/p&gt;
&lt;pre&gt;&lt;
      
    
    </summary>
    
    
      <category term="学习笔记" scheme="https://superlova.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="https://superlova.github.io/tags/NLP/"/>
    
      <category term="深度学习" scheme="https://superlova.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="IMDB" scheme="https://superlova.github.io/tags/IMDB/"/>
    
  </entry>
  
  <entry>
    <title>模型评估与选择——周志华《机器学习》CH2</title>
    <link href="https://superlova.github.io/2019/06/21/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9%E2%80%94%E2%80%94%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8BCH2/"/>
    <id>https://superlova.github.io/2019/06/21/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9%E2%80%94%E2%80%94%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8BCH2/</id>
    <published>2019-06-21T03:26:15.000Z</published>
    <updated>2020-03-17T01:54:50.787Z</updated>
    
    <content type="html"><![CDATA[<p>模型如何评估，选择标准是什么？<br>先让我们了解一下常见的衡量标准</p><p>错误率+精度=1</p><h3 id="误差："><a href="#误差：" class="headerlink" title="误差："></a>误差：</h3><p><strong>训练误差/经验误差</strong> training/empirical error<br><strong>泛化误差</strong> generalization error</p><p>训练误差低，泛化误差不一定低。这其中牵扯到过拟合和欠拟合的问题。</p><p><strong>过拟合</strong>：过分学习，将训练样本中不属于规律的的噪声也一并学习的现象。<br>防止过拟合，一般采用将数据集分成训练集和测试集，利用训练集训练模型，利用测试集拟合泛化误差的办法。</p><p><strong>测试误差</strong> testing error</p><h3 id="划分数据集的方法"><a href="#划分数据集的方法" class="headerlink" title="划分数据集的方法"></a>划分数据集的方法</h3><h4 id="样本划分之留出法-hold-out"><a href="#样本划分之留出法-hold-out" class="headerlink" title="样本划分之留出法 hold-out"></a>样本划分之留出法 hold-out</h4><p>将样本分成互斥的两部分S,T<br>用S训练，用T测试。分割比例自己确定。<br>需要注意的是，必须保证S、T同分布，建议采用<strong>分层采样</strong>stratified sampling，即数据集中的每个类别雨露均沾。<br>另外可以随机划分若干次，防止单次划分出现极端采样结果。随机次数越高，结果的<strong>保真性</strong>fidelity越高。</p><h4 id="样本划分之交叉验证-cross-validation"><a href="#样本划分之交叉验证-cross-validation" class="headerlink" title="样本划分之交叉验证 cross validation"></a>样本划分之交叉验证 cross validation</h4><p>将数据集采用分层划分，分成若干个互斥的小数据集。每个小数据集都当一次测试集，其他数据集组成新训练集。如果分割成k个小数据集，则这种验证方法会做k个不同的划分。因此又称为k-fold 交叉验证。</p><p>极端情况是留一法 leave one out，即k=|D|。</p><h4 id="样本划分之自助法-bootstrapping"><a href="#样本划分之自助法-bootstrapping" class="headerlink" title="样本划分之自助法 bootstrapping"></a>样本划分之自助法 bootstrapping</h4><p>从D中进行m次<strong>放回抽样</strong>，形成新的小数据集D’,理所应当地，新数据集内可能有重复元素，即<script type="math/tex">|unique(D')|\leq m</script></p><p>大数据集中的一个元素x不被选中的概率是<script type="math/tex">\mathbb{P}\{x\in \mathit{D}\cap x\notin \mathit{D}'\}=(1-\frac{1}{m})^m</script>，此时如果m取得越多，概率就越趋近于<script type="math/tex">\lim_{m\rightarrow\infty}(1-\frac{1}{m})^m=\frac{1}{e}</script>。</p><p>最后可以利用D’训练，用D/D’测试。此种抽样方法又称为<strong>包外估计</strong> out of bag estimate，适用于|D|很小的情况。</p><h3 id="如何判断模型的好坏？"><a href="#如何判断模型的好坏？" class="headerlink" title="如何判断模型的好坏？"></a>如何判断模型的好坏？</h3><p>为防止专有名词混淆，此处主要采用英文术语。<br>对于二分类模型，有以下评价模型的标准：</p><p>accuracy：模型结果与真实值相同的比率。中文称之为<strong>精度</strong>。</p><p>precision：模型所得结果中正例比率。中文称之为<strong>准确率</strong>。若想准确率提升，直观的方法是只挑选自己十分确定的样本。所谓不打无准备之仗。不过这样肯定会放过很多原本是正例的样本。</p><p>recall：正例中模型结果占比。中文称为<strong>查全率</strong>、<strong>召回率</strong>。想提高查全率，就要把所有疑似样本全都收集进来，所谓宁杀一千不放一个。这样显然也会提高误杀率。</p><p><strong>PR曲线</strong>：即准确率-查全率曲线。对于预测模型来说，对未知样本的预测，准确率和查全率往往不可兼得。呈现一个这种曲线：</p><p><img src="/2019/06/21/模型评估与选择——周志华《机器学习》CH2/2019-06-21-11-58-57.png" srcset="/img/loading.gif" alt></p><p>模型对每个样本会给出自己的判断，并且还会有自己的置信度。我们可以按照置信度排序，就可以做出PR曲线。</p><p>只有PR曲线，我们可以说模型C最差，因为这条曲线完全被A或B模型的曲线所包围。但是不好判断A红线与B黑线的性能，因为二者有交叉。这种情况有三种度量：</p><p>求<strong>曲线下面积</strong>是一种思路，不过有比较高的计算成本，更喜欢采用的是<strong>平衡点</strong>Break-Even point，可以看到我们挑了三个小红点。靠外的模型好。另外可以采用F1度量。计算公式是<script type="math/tex">F_1=\frac{2PR}{P+R}</script>这个公式就是准确率和查全率的调和平均<script type="math/tex">\frac{1}{F_1}=\frac{1}{2}(\frac{1}{P}+\frac{1}{R})</script>。之所以取调和平均，是因为调和平均在四种平均中最小，因此更重视较小值。</p><p><img src="/2019/06/21/模型评估与选择——周志华《机器学习》CH2/2019-06-21-12-46-48.png" srcset="/img/loading.gif" alt="四大基本不等式"></p><p>对于实际问题，准确率和查全率的意义不一样。超市小偷识别系统更害怕冤枉好人，因此可能更注重准确率；而地铁检查系统可能抱着“宁查一千不放一个”的态度，更追求查全率。因此对F1评价稍作修改，我们就得到了F_beta度量指标：<script type="math/tex">F_{\beta}=\frac{(1+\beta^2)PR}{\beta^2P+R}</script>，其实这个公式就是加了权重后的调和平均<script type="math/tex">\frac{1}{F_{\beta}}=\frac{1}{1+\beta^2}(\frac{1}{P}+\frac{\beta^2}{R})</script>。$\beta&gt;1$则更注重查全率R，$\beta&lt;1$则更注重查准率P。</p><p><strong>ROC曲线</strong>是另外一个思路的评价标准，横坐标是假正例，纵坐标是真正例。其绘制方法也是将样本按照置信度排序，如果样本是真正例，则垂直向y轴正方向绘制一个单位；如果样本是假正例，则水平向x轴正方向绘制一个单位。</p><p>理想状态下，模型将所有正例排在反例前面，因此曲线应该一直往上升，升到m个正例穷尽之后，再水平到|D|-m个反例。但是往往模型会判断失误，于是就出现了这种图像：</p><p><img src="/2019/06/21/模型评估与选择——周志华《机器学习》CH2/2019-06-21-12-59-40.png" srcset="/img/loading.gif" alt></p><p>左图是无限样例下，才可能达到的光滑ROC曲线。右图是实际可能的ROC曲线。ROC曲线围成的面积称之为<strong>AUC</strong>。ROC越丰满，AUC越大，模型的判别效果越好。</p><p><strong>AUC</strong>同样也可判断模型的好坏。而且AUC实际上可以通过数值方法来近似求解，有如下公式：<script type="math/tex">AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)(y_i+y_{i+1})</script></p><p>若预测正误代价不同，可参考西瓜书“<strong>代价曲线</strong>”，此处不再赘述。</p><p><strong>假设检验</strong>模块，数学味道太浓，写成博客实用度不高。而且西瓜书上讲的也不甚明了，真正想了解假设检验的朋友，可参考《统计推断》一书或其他的数理统计教材。</p><h3 id="偏差方差分解-bias-variance-decomposition"><a href="#偏差方差分解-bias-variance-decomposition" class="headerlink" title="偏差方差分解 bias variance decomposition"></a>偏差方差分解 bias variance decomposition</h3><p>这种分解可以解释泛化性能为什么会下降到一定程度后上升，越训练越差。</p><p>偏差 bias<br>方差 var</p><p><script type="math/tex">E(f) = bias^2(x)+var(x)+\epsilon^2</script>，我们想令Ef最小。事实上训练越充分，偏差bias越小，但是方差var会提高。所以并不是训练越多越好。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;模型如何评估，选择标准是什么？&lt;br&gt;先让我们了解一下常见的衡量标准&lt;/p&gt;
&lt;p&gt;错误率+精度=1&lt;/p&gt;
&lt;h3 id=&quot;误差：&quot;&gt;&lt;a href=&quot;#误差：&quot; class=&quot;headerlink&quot; title=&quot;误差：&quot;&gt;&lt;/a&gt;误差：&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://superlova.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="学习笔记" scheme="https://superlova.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="https://superlova.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="西瓜书" scheme="https://superlova.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
      <category term="模型评估与选择" scheme="https://superlova.github.io/tags/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/"/>
    
  </entry>
  
  <entry>
    <title>三星笔记本升级硬盘实录</title>
    <link href="https://superlova.github.io/2019/06/16/%E4%B8%89%E6%98%9F%E7%AC%94%E8%AE%B0%E6%9C%AC%E5%8D%87%E7%BA%A7%E7%A1%AC%E7%9B%98%E5%AE%9E%E5%BD%95/"/>
    <id>https://superlova.github.io/2019/06/16/%E4%B8%89%E6%98%9F%E7%AC%94%E8%AE%B0%E6%9C%AC%E5%8D%87%E7%BA%A7%E7%A1%AC%E7%9B%98%E5%AE%9E%E5%BD%95/</id>
    <published>2019-06-16T08:21:15.000Z</published>
    <updated>2020-03-17T01:54:50.709Z</updated>
    
    <content type="html"><![CDATA[<p>这是一篇装机实录，主要内容有：</p><ul><li>给老式（2014年左右）无光驱笔记本电脑安装固态硬盘、拆卸机械硬盘；</li><li>重装系统的坑</li><li>固态硬盘体验</li></ul><h1 id="旧本盼望新生"><a href="#旧本盼望新生" class="headerlink" title="旧本盼望新生"></a>旧本盼望新生</h1><p>很早之前就想为自己的古董笔记本提升一下性能了。我的笔记本型号是NP370R5V-S02CN，属于2014年那会儿产的机型。i5的CPU、8G的内存（其中我额外购置了4G内存条），再加上5年的使用习惯，让我对这台机器还比较满意。限制笔记本电脑性能的主要瓶颈就是硬盘了。</p><p>我是一个等等党，平时也不会在自己的电脑上运行特别复杂的程序，大部分都在服务器或者公有云上跑了。所以对笔记本电脑的需求不是那么急切。固态2块钱1G的时候我没有心动，1块钱1G的时候我还是没有心动，但是我看到西数蓝盘500G的固态只卖379元，而且5年质保时，我真的憋不住了。</p><p>之前我一直用的是windows 7的操作系统，一直没敢上windows 10，因为听说windows 10挺吃性能的。现在我打算把原来笔记本电脑上的数据备份一下之后，直接上windows 10。因此我不需要进行系统迁移，直接重装系统。</p><h1 id="部件难堪重负"><a href="#部件难堪重负" class="headerlink" title="部件难堪重负"></a>部件难堪重负</h1><h2 id="1-制作启动U盘"><a href="#1-制作启动U盘" class="headerlink" title="1) 制作启动U盘"></a>1) 制作启动U盘</h2><p>的过程，我完全按照网上说的，先到校园网下载win10专业版镜像，利用我手头上的16G空U盘和Rufus 3.5软件，将ISO镜像拷入U盘，格式化为启动盘。</p><p>此处U盘的大小一般8G以上为宜。现在是2019年，市面上8G的U盘算容量不大的了。不知道以后会不会淘汰掉。Rufus是免费软件，相同功能的软件还有UltraISO等。</p><p>需要注意的是，不要使用老毛桃等PE，虽然傻瓜式安装，但是系统不纯净。而且装系统的过程并不复杂，多踩踩坑就熟悉了。</p><h2 id="2-拆卸更换硬盘"><a href="#2-拆卸更换硬盘" class="headerlink" title="2) 拆卸更换硬盘"></a>2) 拆卸更换硬盘</h2><p>我的笔记本似乎在设计的时候就料到了用户会添加内存条和更换硬盘，因此包裹硬盘和内存区域的外壳与主板是分开的，只需拆卸一颗螺丝，即可将硬盘位和内存位暴露出来。这给我的拆卸过程减少了很多麻烦。</p><p>关机，拆后盖，连电池都不用拆卸，直接把硬盘固定位的四颗螺丝拧下来，换上固态，美滋滋。</p><p>将拆卸下来的机械硬盘放在我买的硬盘盒里面（25元），大小刚刚合适（2.5寸）。沉甸甸的，里面存了不少数据【doge】。</p><p>开始重装系统。现在的固态硬盘是空的，里面没有系统。插入U盘，按下电源键后按F2进入BIOS模式。这里开始就有坑了。</p><h2 id="3-更新操作系统"><a href="#3-更新操作系统" class="headerlink" title="3) 更新操作系统"></a>3) 更新操作系统</h2><p>首先我准备安装的是win10，原来安装的是win7。win7的磁盘引导是MBR方式，而最新的引导方式是UEFI，这两种磁盘格式化方式是不一样的，也只有当磁盘里没有任何数据的一开始，我敢将磁盘的引导方式修改一下，换作原来我真的害怕稍微操作数据就没了。所以我进行了如下设置：</p><ul><li>将Security Boot关闭</li><li>将AHCI引导换成UEFI启动</li><li>Boot Mode换成UEFI only<br>一顿操作，win10安装完成了，哈哈哈。令我没想到的事情来了！</li></ul><h2 id="4-电脑受不了Windows-10"><a href="#4-电脑受不了Windows-10" class="headerlink" title="4) 电脑受不了Windows 10"></a>4) 电脑受不了Windows 10</h2><p>安装win10后，固态硬盘的好处显现出来了，任何东西都是秒开，延迟大大的降低了。就在我享受各种操作的时候，突然电脑卡住了。</p><p>就是卡死在一瞬间，鼠标都不能动了。等了好久也没用，我长按电源键强制关机后再启动，没用，过一会儿还是死机。</p><p>我怀疑是不是我的电脑其他部件太过古老了？当时的风扇吹出来的风都烫手。但是当我放凉之后再次开机，还是会毫无征兆地死机。</p><p>我怀疑是不是我安装的软件里面有不兼容的驱动？有可能是显卡驱动之类的。因为三星官网关于我这个机型的驱动，最多支持到win7。</p><p>各种原因都查了个遍，一下午过去了，一晚上过去了，我还是没能解决这个问题。总不能是固态的问题吧？</p><p>我认真的思考了一下，西部数据蓝盘，打折出售，也有可能是卖给我了劣质盘。但是我现在没办法安装磁盘检测软件，一开电脑就死机。</p><h2 id="5-悲伤地换回windows-7"><a href="#5-悲伤地换回windows-7" class="headerlink" title="5) 悲伤地换回windows 7"></a>5) 悲伤地换回windows 7</h2><p>于是我决定，重新安装回win7，如果能用就不折腾了。</p><p>下载win7镜像，使用rufus制作启动盘，此时的分区格式就选择mbr好了。我妥协了！不折腾了。</p><p>一如既往地修改BIOS，BootMode改成Legacy。win7一会儿就安装好了。没有其他的问题。</p><p>真是邪了门了，而且用win7发热的现象也缓解了很多！</p><p>看来真的是，给老电脑更换一个新固态，好比是给老年人更换年轻人的心脏，心有余而力不足啊！</p><h1 id="体验固态硬盘"><a href="#体验固态硬盘" class="headerlink" title="体验固态硬盘"></a>体验固态硬盘</h1><p>上次有这种性能大幅度稳定提升一个档次的感觉的时候，是这台电脑安装新的内存条之后。读写速度真的是限制目前性能的一大瓶颈。</p><p>由于我的内部接口是SATA 2，所以无法发挥SATA 3接口的西数蓝盘的全部威力。不过这也在我的计算之中了，因为相对于HDD，SSD带来的提升实在是天壤之别。</p><p><img src="/2019/06/16/三星笔记本升级硬盘实录/2019-06-19-19-01-03.png" srcset="/img/loading.gif" alt></p><p>等到老娘今后有钱了，这个硬盘还能拆下来重复利用，岂不是美滋滋。</p><p>本来寻思给这篇文章加点图，算啦，反正也不是什么成功经验。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这是一篇装机实录，主要内容有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;给老式（2014年左右）无光驱笔记本电脑安装固态硬盘、拆卸机械硬盘；&lt;/li&gt;
&lt;li&gt;重装系统的坑&lt;/li&gt;
&lt;li&gt;固态硬盘体验&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;旧本盼望新生&quot;&gt;&lt;a href=&quot;#旧本盼望
      
    
    </summary>
    
    
      <category term="经验教训" scheme="https://superlova.github.io/categories/%E7%BB%8F%E9%AA%8C%E6%95%99%E8%AE%AD/"/>
    
    
      <category term="装机实录" scheme="https://superlova.github.io/tags/%E8%A3%85%E6%9C%BA%E5%AE%9E%E5%BD%95/"/>
    
      <category term="笔记本电脑" scheme="https://superlova.github.io/tags/%E7%AC%94%E8%AE%B0%E6%9C%AC%E7%94%B5%E8%84%91/"/>
    
      <category term="固态硬盘" scheme="https://superlova.github.io/tags/%E5%9B%BA%E6%80%81%E7%A1%AC%E7%9B%98/"/>
    
  </entry>
  
  <entry>
    <title>高级操作系统——分布式系统——课程设计与实现</title>
    <link href="https://superlova.github.io/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"/>
    <id>https://superlova.github.io/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/</id>
    <published>2019-06-15T12:15:05.000Z</published>
    <updated>2020-03-17T01:54:50.830Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实验二"><a href="#实验二" class="headerlink" title="实验二"></a>实验二</h1><h2 id="一、实验目的"><a href="#一、实验目的" class="headerlink" title="一、实验目的"></a>一、实验目的</h2><p>尝试实现一个无连接的数据报Socket进程间通信（UDP）</p><h2 id="二、实验内容"><a href="#二、实验内容" class="headerlink" title="二、实验内容"></a>二、实验内容</h2><ol><li>创建两个进程，使用无链接的数据报Socket实现交换一个字符串。一个叫做Sender.java ，用于向一个名为Receiver.java的进程发送一个字符串。</li><li>Sender.java需要一个命令行参数，用于表示传递消息的端口号.</li><li>Receiver.java 需要一个命令行参数用于表示接受消息的端口号。</li><li>接受方需要阻塞直到从发送方收到一个消息。如果发送放在接受运行之前发送出消息，消息会丢失。这种情况在此实验中是允许的。</li><li>消息缓冲可以是定长的。如果发送的消息比消息缓冲长，接受方就看不到完整的消息。这种情况在此实验中是允许的。例如，消息缓冲的长度是5，而发送方发送一个消息 “123456789”，则接受方只能看到“12345”。</li></ol><h2 id="三、实验设计"><a href="#三、实验设计" class="headerlink" title="三、实验设计"></a>三、实验设计</h2><h3 id="实验背景："><a href="#实验背景：" class="headerlink" title="实验背景："></a>实验背景：</h3><p><strong>UDP简介</strong></p><p><a href="https://blog.csdn.net/qq_23473123/article/details/51464272" target="_blank" rel="noopener">JAVA Socket 实现 UDP 编程</a></p><p>UDP协议全称是用户数据报协议，在网络中它与TCP协议一样用于处理数据包，是一种无连接的协议。在OSI模型中，在第四层——传输层，处于IP协议的上一层。UDP有不提供数据包分组、组装和不能对数据包进行排序的缺点，也就是说，当报文发送之后，是无法得知其是否安全完整到达的。UDP用来支持那些需要在计算机之间传输数据的网络应用。包括网络视频会议系统在内的众多的客户/服务器模式的网络应用都需要使用UDP协议。</p><p>UDP协议的主要作用是将网络数据流量压缩成数据包的形式。一个典型的数据包就是一个二进制数据的传输单位。每一个数据包的前8个字节用来包含报头信息，剩余字节则用来包含具体的传输数据。</p><p><strong>什么时候应该使用UDP：</strong><br>当对网络通讯质量要求不高的时候，要求网络通讯速度能尽量的快，这时就可以使用UDP。 </p><p>比如，日常生活中，常见使用UDP协议的应用如下：</p><p>QQ语音、QQ视频……</p><p><strong>Java中的UDP</strong></p><p>在Java中，实现UDP连接和数据传输的类主要是DatagramPacket和DatagramSocket。</p><p>DatagramSocket类表示用来发送和接收数据报包的套接字。</p><p>数据报套接字是包投递服务的发送或接收点。每个在数据报套接字上发送或接收的包都是单独编址和路由的。从一台机器发送到另一台机器的多个包可能选择不同的路由，也可能按不同的顺序到达。</p><p>在 DatagramSocket 上总是启用 UDP 广播发送。在接收端（Receiver），DatagramSocket只需输入空闲端口即可初始化对象。在消息发送端（Sender），DatagramSocket不需要任何参数初始化，直接新建对象。</p><p>DatagramSocket对象拥有send()和receive()方法，参数是DatagramPacket对象，即发送和接收数据包。</p><p>DatagramPacket类表示数据报包。</p><p>数据报包用来实现无连接包投递服务。每条报文仅根据该包中包含的信息从一台机器路由到另一台机器。从一台机器发送到另一台机器的多个包可能选择不同的路由，也可能按不同的顺序到达。不对包投递做出保证。</p><p>发送方（sender）要负责在数据报包上面贴好标签，注明收信人（地址和端口），这样才能够准确地被收信人（receiver）收到。</p><p>在DatagramPacket包中的函数 int getLength()返回实际接受的字节数，<br>byte[] getData()返回接受到的数据。</p><p>要想接受端给发送端回信息，就需要知道发送端的IP地址InetAddress getAddress()和发送端进程所绑定的端口号int getPort()。</p><p>数据报套接字发送成功之后，就相当于建立了一个虚连接，双方可以发送数据。</p><h3 id="实验环境："><a href="#实验环境：" class="headerlink" title="实验环境："></a>实验环境：</h3><p>Intellij IDEA, JDK 1.8, Windows 10</p><h3 id="实验思路："><a href="#实验思路：" class="headerlink" title="实验思路："></a>实验思路：</h3><ol><li>新建两个类，Sender和Receiver。由于逻辑简单，具体代码可直接在main中实现；</li><li>Sender和Receiver类需要接收命令行参数，可利用Intellij IDEA的类运行设置来输入参数，利用main函数的args参数来传递命令行参数，避免了操作命令行界面的不便；<br><img src="/2019/06/15/高级操作系统——分布式系统——课程设计与实现/2019-06-15-16-36-58.png" srcset="/img/loading.gif" alt><br><img src="/2019/06/15/高级操作系统——分布式系统——课程设计与实现/2019-06-15-16-37-07.png" srcset="/img/loading.gif" alt></li><li>Sender：定义地址、端口号、数据；创建数据报包，包含发送的数据信息；创建DatagramSocket对象；向服务器发送数据报包。</li><li>Receiver：创建服务器端DatagramSocket，指定端口；创建数据报，用于接收客户端发送的数据；接收客户端发送的数据；读取数据。</li></ol><h3 id="相关代码："><a href="#相关代码：" class="headerlink" title="相关代码："></a>相关代码：</h3><p><code>Sender.java</code></p><pre><code class="lang-java">public class Sender {    public static void main(String[] args) throws IOException {        /*         * 向Receiver发送数据         * java Sender 192.168.1.101 12 hello         */        // 1.定义服务器的地址、端口号、数据        if (args.length != 3) {            System.out.println(&quot;参数个数不正确！&quot; + args.length);            return;        }        System.out.println(args[0]);        System.out.println(args[1]);        System.out.println(args[2]);        InetAddress address = InetAddress.getByName(args[0]);        //InetAddress address = InetAddress.getByName(&quot;localhost&quot;);        int port = Integer.parseInt(args[1]);        byte[] data = args[2].getBytes();        // 2.创建数据报，包含发送的数据信息        DatagramPacket packet = new DatagramPacket(data, data.length, address, port);        // 3.创建DatagramSocket对象        DatagramSocket socket = new DatagramSocket();        // 4.向服务器端发送数据报        socket.send(packet);    }}</code></pre><p><code>Receiver.java</code></p><pre><code class="lang-java">public class Receiver {    /*     * 接收Sender发送的数据     * java Receiver 12     */    public static void main(String[] args) throws IOException {        // 设置缓冲区大小        int bufferLength = 5;        // 设置参数格式        if (args.length != 1) {            System.out.println(&quot;参数个数不正确！&quot; + args.length);            return;        }        // 1.创建服务器端DatagramSocket，指定端口        DatagramSocket socket = new DatagramSocket(Integer.parseInt(args[0]));        // 2.创建数据报，用于接收客户端发送的数据        byte[] data = new byte[bufferLength];        DatagramPacket packet = new DatagramPacket(data, data.length);        // 3.接收客户端发送的数据        System.out.println(&quot;****服务器端已经启动，等待客户端发送数据&quot;);        socket.receive(packet);// 此方法在接收到数据报之前会一直阻塞        // 4.读取数据        String info = new String(data, 0, packet.getLength());        System.out.println(&quot;我是服务器，客户端说：&quot; + info);    }}</code></pre><h2 id="四、实验结果与分析"><a href="#四、实验结果与分析" class="headerlink" title="四、实验结果与分析"></a>四、实验结果与分析</h2><p>编译过程略去。</p><ol><li>运行Reciever<br><img src="/2019/06/15/高级操作系统——分布式系统——课程设计与实现/2019-06-15-16-53-22.png" srcset="/img/loading.gif" alt></li><li>运行Sender<br><img src="/2019/06/15/高级操作系统——分布式系统——课程设计与实现/2019-06-15-16-54-43.png" srcset="/img/loading.gif" alt></li><li>此时的Receiver<br><img src="/2019/06/15/高级操作系统——分布式系统——课程设计与实现/2019-06-15-16-55-10.png" srcset="/img/loading.gif" alt></li></ol><h3 id="实验分析："><a href="#实验分析：" class="headerlink" title="实验分析："></a>实验分析：</h3><p>缓冲区大小可以修改。修改后的缓冲区大小为5，可以看到，Sender中的消息<code>Hello!</code>发送给Receiver后，尾部的感叹号被削去。</p><p>通过本次实验，我了解了UDP连接的原理与优缺点，掌握了在Java中建立UDP连接的方法。</p><hr><h1 id="实验三"><a href="#实验三" class="headerlink" title="实验三"></a>实验三</h1><h2 id="一、实验目的-1"><a href="#一、实验目的-1" class="headerlink" title="一、实验目的"></a>一、实验目的</h2><p>尝试通过面向流模式的socket实现通信。</p><h2 id="二、实验内容-1"><a href="#二、实验内容-1" class="headerlink" title="二、实验内容"></a>二、实验内容</h2><ol><li>创建一个名为Acceptor.java的程序。此程序可以接受一个连接并用流模式socket接受一个消息。创建一个名为 Requestor.java 的程序。此程序可以请求一个连接，并使用流模式socket。</li><li>Acceptor.java 有2个命令行参数，分别用于表示本进程使用的服务器socket的端口号，以及要发送的消息。</li><li>Requestor.java 有2个命令行参数，分别表示连接acceptor的主机名和连接acceptor的端口号。</li></ol><h2 id="三、实验设计-1"><a href="#三、实验设计-1" class="headerlink" title="三、实验设计"></a>三、实验设计</h2><h3 id="实验背景：-1"><a href="#实验背景：-1" class="headerlink" title="实验背景："></a>实验背景：</h3><p><a href="https://blog.csdn.net/qq_23473123/article/details/51461894" target="_blank" rel="noopener">Java 通过 Socket 实现 TCP 编程</a></p><p><strong>TCP简介</strong></p><p>TCP（Transmission Control Protocol 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议。</p><p><strong>Java Socket简介</strong></p><p>所谓socket 通常也称作”套接字“，用于描述IP地址和端口，是一个通信链的句柄。应用程序通常通过”套接字”向网络发出请求或者应答网络请求。</p><p>ServerSocket用于服务器端，Socket是建立网络连接时使用的。在连接成功时，应用程序两端都会产生一个Socket实例，操作这个实例，完成所需的会话。对于一个网络连接来说，套接字是平等的，并没有差别，不因为在服务器端或在客户端而产生不同级别。不管是Socket还是ServerSocket它们的工作都是通过SocketImpl类及其子类完成的。</p><p><strong>Java Socket常用方法</strong></p><p>. Accept方法用于产生”阻塞”，直到接受到一个连接，并且返回一个客户端的Socket对象实例。”阻塞”是一个术语，它使程序运行暂时”停留”在这个地方，直到一个会话产生，然后程序继续；通常”阻塞”是由循环产生的。</p><p>. getInputStream方法获得网络连接输入，同时返回一个InputStream对象实例。<br>. getOutputStream方法连接的另一端将得到输入，同时返回一个OutputStream对象实例。</p><p>注意：其中getInputStream和getOutputStream方法均会产生一个IOException，它必须被捕获，因为它们返回的流对象，通常都会被另一个流对象使用。</p><p><strong>Java 操作流对象</strong></p><p>建立Socket连接后，两台机器之间便以流模式进行通信。在Java API中，可以从其中读入一个字节序列的对象称做输入流，而可以向其中写入一个字节序列的对象称做输出流。这些字节序列的来源地和目的地可以是文件，而且通常都是文件，但是也可以是网络连接，甚至是内存块。抽象类InputStream和OutputStream构成了输入/输出（I/O)类层次结构的基础。</p><p>本次实验我选择利用Scanner作为读取流的手段，用PrintWriter作为写入流的手段。</p><h3 id="实验环境：-1"><a href="#实验环境：-1" class="headerlink" title="实验环境："></a>实验环境：</h3><p>Intellij IDEA, JDK 1.8, Windows 10</p><h3 id="实验思路：-1"><a href="#实验思路：-1" class="headerlink" title="实验思路："></a>实验思路：</h3><ol><li>新建两个类，ConnectionAcceptor和ConnectionRequestor。由于逻辑简单，具体代码可直接在main中实现；</li><li>ConnectionAcceptor和ConnectionRequestor类需要接收命令行参数，可利用Intellij IDEA的类运行设置来输入参数，利用main函数的args参数来传递命令行参数，避免了操作命令行界面的不便；</li><li>ConnectionRequestor：建立连接；接收数据；发送数据；关闭连接。</li><li>ConnectionAcceptor：接收请求；发送数据；接收数据；断开连接。</li></ol><h3 id="相关代码：-1"><a href="#相关代码：-1" class="headerlink" title="相关代码："></a>相关代码：</h3><p><code>ConnectionRequestor.java</code></p><pre><code class="lang-java">public class ConnectionRequestor {    /*     * java ConnectionRequestor 192.168.1.101 12     */    public static void main(String[] args) throws IOException {        // 检查参数个数        if (args.length != 2) {            System.out.println(&quot;Wrong parameters!&quot;);            return;        }        // 1. 建立连接        String host = args[0];        int port = Integer.parseInt(args[1]);        Socket socket = new Socket(host, port);        // 2. 接收数据        InputStream inputStream = socket.getInputStream();        Scanner scanner = new Scanner(inputStream);        String line = scanner.nextLine();        System.out.println(&quot;收到来自发送方的消息：&quot; + line);        // 3. 发送数据        OutputStream outputStream = socket.getOutputStream();        PrintWriter printWriter = new PrintWriter(outputStream, true);        printWriter.println(&quot;我已收到消息，内容是&quot; + line);        // 4. 关闭连接        socket.close();    }}</code></pre><p><code>ConnectionAcceptor.java</code></p><pre><code class="lang-java">public class ConnectionAcceptor {    /*     * java ConnectionAcceptor 12 Hello     */    public static void main(String[] args) throws IOException {        // 检查参数个数        if (args.length != 2) {            System.out.println(&quot;Wrong parameters!&quot;);            return;        }        // 1. 接受请求        int port = Integer.parseInt(args[0]);        ServerSocket serverSocket = new ServerSocket(port);        Socket socket = serverSocket.accept();        // 2. 发送数据        OutputStream outputStream = socket.getOutputStream();        PrintWriter printWriter = new PrintWriter(outputStream, true);        printWriter.println(args[1]);        // 3. 接收数据        InputStream inputStream = socket.getInputStream();        Scanner scanner = new Scanner(inputStream);        String line = scanner.nextLine();        System.out.println(&quot;服务器应答：&quot; + line);        // 4. 断开连接        socket.close();    }}</code></pre><h2 id="四、实验结果与分析-1"><a href="#四、实验结果与分析-1" class="headerlink" title="四、实验结果与分析"></a>四、实验结果与分析</h2><p>编译过程略去。</p><ol><li>运行ConnectionAcceptor<br><img src="/2019/06/15/高级操作系统——分布式系统——课程设计与实现/2019-06-15-17-27-34.png" srcset="/img/loading.gif" alt></li><li>运行ConnectionRequestor<br><img src="/2019/06/15/高级操作系统——分布式系统——课程设计与实现/2019-06-15-17-28-48.png" srcset="/img/loading.gif" alt></li><li>此时的ConnectionAcceptor<br><img src="/2019/06/15/高级操作系统——分布式系统——课程设计与实现/2019-06-15-17-29-13.png" srcset="/img/loading.gif" alt></li></ol><h3 id="实验分析：-1"><a href="#实验分析：-1" class="headerlink" title="实验分析："></a>实验分析：</h3><p>通过本次实验，我了解了TCP连接的原理与优缺点，掌握了TCP和UDP的主要不同之处，掌握了在Java中建立TCP Socket连接的方法，掌握了流处理的基本方法。</p><hr><h1 id="实验四"><a href="#实验四" class="headerlink" title="实验四"></a>实验四</h1><h2 id="一、实验目的-2"><a href="#一、实验目的-2" class="headerlink" title="一、实验目的"></a>一、实验目的</h2><p>创建进程之间的多播。</p><h2 id="二、实验内容-2"><a href="#二、实验内容-2" class="headerlink" title="二、实验内容"></a>二、实验内容</h2><ol><li>MulticastSender.java用于发送多播消息给多播接收程序。多播IP地址是239.1.2.3 端口号为1234。</li><li>MulticastReceiver.java 用于接收多播消息并显示消息。</li><li>实验最终效果要求：至少开启两个以上的MulticastReceiver进程，MulticastSender发送的消息，均可被MulticastReceiver收到。</li></ol><h2 id="三、实验设计-2"><a href="#三、实验设计-2" class="headerlink" title="三、实验设计"></a>三、实验设计</h2><h3 id="实验背景：-2"><a href="#实验背景：-2" class="headerlink" title="实验背景："></a>实验背景：</h3><p><a href="https://blog.csdn.net/zhouzixin053/article/details/22823521" target="_blank" rel="noopener">java————多播编程——-MulticastSocket</a></p><p><strong>单播：</strong></p><p>一个单个的发送者和一个接受者之间通过网络进行的通信。</p><p>1、服务器及时响应客户机的请求</p><p>2、服务器针对每个客户不同的请求发送不同的数据，容易实现个性化服务。</p><p><strong>多播：</strong></p><p>一个发送者和多个接受者之间的通信。</p><p>广播特点：主机之间“一对所有”的通讯模式，网络对其中每一台主机发出的信号都进行无条件复制并转发，所有主机都可以接收到所有信息（不管你是否需要）。</p><p>1、网络设备简单，维护简单，布网成本低廉。</p><p>2、由于服务器不用向每个客户机单独发送数据，所以服务器流量负载极低。</p><p>多播的地址是特定的，D类地址用于多播。D类IP地址就是多播IP地址，即224.0.0.0至239.255.255.255之间的IP地址。</p><p><strong>多播程序设计的框架</strong></p><p>要进行多播的编程，需要遵从一定的编程框架。多播程序框架主要包含套接字初始化、设置多播超时时间、加入多播组、发送数据、接收数据以及从多播组中离开几个方面。其步骤如下：</p><p>（1）建立一个socket。</p><p>（2）然后设置多播的参数，例如超时时间TTL、本地回环许可LOOP等。</p><p>（3）加入多播组。</p><p>（4）发送和接收数据。</p><p>（5）从多播组离开。</p><p><strong>Java MulticastSocket</strong></p><p>多播通过多播数据报套接MulticastSocket类来实现</p><p>重要的构造方法：</p><pre><code class="lang-java">MulticastSocket()//创建多播套接字MulticastSocket(int port)//创建多播套接字并将其绑定到特定端口MulticastSocket(SocketAddress bindaddr)//创建绑定到指定套接字地址的MulticastSocket</code></pre><p>常用的方法：</p><pre><code class="lang-java">void joinGroup(InetAddress meastaddr)//加入多播组void leaveGroup(InetAddress meastaddr)//离开多播组void send(DatagramPacket p)//从此套接字发送数据包public void receive(DatagramPacket p)//从此套接字接收数据包</code></pre><h3 id="实验环境：-2"><a href="#实验环境：-2" class="headerlink" title="实验环境："></a>实验环境：</h3><p>Intellij IDEA, JDK 1.8, Windows 10</p><h3 id="实验思路：-2"><a href="#实验思路：-2" class="headerlink" title="实验思路："></a>实验思路：</h3><ol><li>新建两个类，MulticastSender和MulticastReciever。由于逻辑简单，具体代码可直接在main中实现；</li><li>为了体现多播的广播特性，MulticastReciever的进程要多几个，随着MulticastSender的启动，全部收到信息才行。</li><li>MulticastSender：新建多播连接；构造数据包；广播数据；关闭连接。</li><li>MulticastReciever：新建多播连接；构造数据包；接收数据；关闭连接。</li></ol><h3 id="相关代码：-2"><a href="#相关代码：-2" class="headerlink" title="相关代码："></a>相关代码：</h3><p><code>MulticastSender.java</code></p><pre><code class="lang-java">public class MulticastSender {    public static void main(String[] args) throws IOException {        // 新建多播连接        int port = 1234;        String address = &quot;239.1.2.3&quot;;        MulticastSocket multicastSocket = new MulticastSocket(port);        InetAddress groupAddress = InetAddress.getByName(address);        multicastSocket.joinGroup(groupAddress);        // 构造数据包        byte[] message = &quot;Hello!&quot;.getBytes();        DatagramPacket datagramPacket = new DatagramPacket(message, message.length, InetAddress.getByName(address), port);        // 广播数据        multicastSocket.send(datagramPacket);        // 关闭连接        multicastSocket.close();    }}</code></pre><p><code>MulticastReciever.java</code></p><pre><code class="lang-java">public class MulticastReciever {    public static void main(String[] args) throws IOException {        // 新建多播连接        int port = 1234;        String address = &quot;239.1.2.3&quot;;        MulticastSocket multicastSocket = new MulticastSocket(port);        InetAddress groupAddress = InetAddress.getByName(address);        multicastSocket.joinGroup(groupAddress);        // 构造数据包        byte[] message = new byte[1024];        DatagramPacket datagramPacket = new DatagramPacket(message, message.length, InetAddress.getByName(address), port);        // 接收数据        multicastSocket.receive(datagramPacket);        System.out.println(&quot;接收到了广播消息：&quot; + new String(message));        // 关闭连接        multicastSocket.close();    }}</code></pre><h2 id="四、实验结果与分析-2"><a href="#四、实验结果与分析-2" class="headerlink" title="四、实验结果与分析"></a>四、实验结果与分析</h2><p>编译过程略去。</p><ol><li>运行MulticastReciever（6个）<br><img src="/2019/06/15/高级操作系统——分布式系统——课程设计与实现/2019-06-15-17-45-12.png" srcset="/img/loading.gif" alt></li><li>运行MulticastSender<br><img src="/2019/06/15/高级操作系统——分布式系统——课程设计与实现/2019-06-15-17-46-17.png" srcset="/img/loading.gif" alt></li><li>此时的MulticastReciever<br><img src="/2019/06/15/高级操作系统——分布式系统——课程设计与实现/2019-06-15-17-46-34.png" srcset="/img/loading.gif" alt><br><img src="/2019/06/15/高级操作系统——分布式系统——课程设计与实现/2019-06-15-17-46-48.png" srcset="/img/loading.gif" alt><br><img src="/2019/06/15/高级操作系统——分布式系统——课程设计与实现/2019-06-15-17-47-02.png" srcset="/img/loading.gif" alt><br>剩下三个略去不表，结果相同。</li></ol><h3 id="实验分析：-2"><a href="#实验分析：-2" class="headerlink" title="实验分析："></a>实验分析：</h3><p>通过本次实验，我了解了多播的原理和相对于单播的优点，掌握了在Java中建立多播MulticastSocket连接的方法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;实验二&quot;&gt;&lt;a href=&quot;#实验二&quot; class=&quot;headerlink&quot; title=&quot;实验二&quot;&gt;&lt;/a&gt;实验二&lt;/h1&gt;&lt;h2 id=&quot;一、实验目的&quot;&gt;&lt;a href=&quot;#一、实验目的&quot; class=&quot;headerlink&quot; title=&quot;一、实验目的&quot;&gt;&lt;
      
    
    </summary>
    
    
      <category term="学校课程" scheme="https://superlova.github.io/categories/%E5%AD%A6%E6%A0%A1%E8%AF%BE%E7%A8%8B/"/>
    
      <category term="分布式系统" scheme="https://superlova.github.io/categories/%E5%AD%A6%E6%A0%A1%E8%AF%BE%E7%A8%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="分布式系统" scheme="https://superlova.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="操作系统" scheme="https://superlova.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="课程设计" scheme="https://superlova.github.io/tags/%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1/"/>
    
      <category term="Java" scheme="https://superlova.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>高级操作系统——分布式系统——课程作业与解答</title>
    <link href="https://superlova.github.io/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A%E4%B8%8E%E8%A7%A3%E7%AD%94/"/>
    <id>https://superlova.github.io/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A%E4%B8%8E%E8%A7%A3%E7%AD%94/</id>
    <published>2019-06-15T12:14:38.000Z</published>
    <updated>2020-03-17T01:54:50.823Z</updated>
    
    <content type="html"><![CDATA[<h2 id="第一章作业"><a href="#第一章作业" class="headerlink" title="第一章作业"></a>第一章作业</h2><h3 id="1-什么是分布式系统？请举例说明分布式系统的特点。"><a href="#1-什么是分布式系统？请举例说明分布式系统的特点。" class="headerlink" title="1. 什么是分布式系统？请举例说明分布式系统的特点。"></a>1. 什么是分布式系统？请举例说明分布式系统的特点。</h3><p>定义1：分布式系统是多个独立计算机的集合，该系统用户认为它是一个单独的一致的系统<br>定义2：由在通过消息进行通信和协作操作的网络计算机上的软件硬件部件组成的任何系统<br>分布式系统的特点：<br>（1）隐藏性，隐藏了计算机之间的不同，隐藏了计算机之间的通信过程。<br>（2）统一性和一致性，无论何时何地，用户都采用统一和一致的方法访问分布式系统。<br>（3）可扩展性，隐藏一个独立的计算机如何参与系统的运作；在系统的一部分不能工作的情况下整个系统仍然能持续提供服务；当系统的某些部件被替换或被修改或是系统提供了新的服务时，不应让用户注意到这些改变。<br>（4）并发性，在共享资源的情况下，协调不同的程序并发执行。<br>（5）没有全局时钟，很难在网络中精确地同步所有的时钟，更重要的是发现动作发生的先后顺序。<br>举例部分略。</p><h3 id="2-有一个简单的C-S系统，此系统只用一台服务器来响应所有客户的请求。"><a href="#2-有一个简单的C-S系统，此系统只用一台服务器来响应所有客户的请求。" class="headerlink" title="2. 有一个简单的C/S系统，此系统只用一台服务器来响应所有客户的请求。"></a>2. 有一个简单的C/S系统，此系统只用一台服务器来响应所有客户的请求。</h3><p>（1）请解释为什么在这种情况下不能对服务器的响应时间进行限制（例如，这个限制可能是要求服务器一定要在10ms内对客户的请求进行响应）；<br>因为服务器过于简单，处理能力差，对于多线程高并发情况，当服务器发生堵塞时造成的服务等待问题会造成服务器响应不及时的情况。并且一台服务器性能有限，不能保证服务端高效性和高的容错性。因此在这种情况下不能对服务器的响应时间进行限制。</p><p>（2）如果我们想限制服务器的响应时间，应该如何修改设计？<br>可以通过增加服务器的数量或提高服务器的处理能力。</p><p>（3）在现实世界中使用的系统中，这种限制是否是必须的？为什么？<br>是必须的。因为如果不加限制，那么就会导致用户使用系统的体验降低，从而放弃这个系统。</p><h3 id="3-当年12306订票系统在2011年第一次上线的时候出现了很多的问题。如果你是12306的设计师，请问应该注意哪几个方面的内容？"><a href="#3-当年12306订票系统在2011年第一次上线的时候出现了很多的问题。如果你是12306的设计师，请问应该注意哪几个方面的内容？" class="headerlink" title="3. 当年12306订票系统在2011年第一次上线的时候出现了很多的问题。如果你是12306的设计师，请问应该注意哪几个方面的内容？"></a>3. 当年12306订票系统在2011年第一次上线的时候出现了很多的问题。如果你是12306的设计师，请问应该注意哪几个方面的内容？</h3><p>（答案略）</p><h2 id="第二章作业"><a href="#第二章作业" class="headerlink" title="第二章作业"></a>第二章作业</h2><h3 id="1-网络协议分层有什么好处？"><a href="#1-网络协议分层有什么好处？" class="headerlink" title="1.    网络协议分层有什么好处？"></a>1.    网络协议分层有什么好处？</h3><p>每层协议可以单独设计，只依赖于底层协议工作，增加了灵活性。</p><h3 id="2-一个客户向服务器发出RPC。客户花5ms时间计算每个请求的参数，服务器花10ms处理每个请求。本地操作系统每次发送和接收的时间是5ms，网络传递或者应答消息的时间是3ms。编码、解码每个消息需要0-5ms。计算一个-RPC实现所需要的时间。"><a href="#2-一个客户向服务器发出RPC。客户花5ms时间计算每个请求的参数，服务器花10ms处理每个请求。本地操作系统每次发送和接收的时间是5ms，网络传递或者应答消息的时间是3ms。编码、解码每个消息需要0-5ms。计算一个-RPC实现所需要的时间。" class="headerlink" title="2.    一个客户向服务器发出RPC。客户花5ms时间计算每个请求的参数，服务器花10ms处理每个请求。本地操作系统每次发送和接收的时间是5ms，网络传递或者应答消息的时间是3ms。编码、解码每个消息需要0.5ms。计算一个 RPC实现所需要的时间。"></a>2.    一个客户向服务器发出RPC。客户花5ms时间计算每个请求的参数，服务器花10ms处理每个请求。本地操作系统每次发送和接收的时间是5ms，网络传递或者应答消息的时间是3ms。编码、解码每个消息需要0.5ms。计算一个 RPC实现所需要的时间。</h3><p>答：<br>5ms 准备参数<br>0.5ms 打包参数<br>5ms 发送<br>3ms 网络传送<br>5ms 接收<br>0.5ms 解包参数<br>10ms 处理请求<br>0.5ms 结果打包<br>5ms 发送<br>3ms 网络传送<br>5ms 接收<br>0.5ms 解包结果<br>共43ms</p><h3 id="3-举例说明RPC在异构系统中实现时进行参数传递会遇到什么问题？"><a href="#3-举例说明RPC在异构系统中实现时进行参数传递会遇到什么问题？" class="headerlink" title="3.    举例说明RPC在异构系统中实现时进行参数传递会遇到什么问题？"></a>3.    举例说明RPC在异构系统中实现时进行参数传递会遇到什么问题？</h3><p>（例子见PPT，略）</p><h3 id="4-暂时通信和永久通信有什么区别？"><a href="#4-暂时通信和永久通信有什么区别？" class="headerlink" title="4.    暂时通信和永久通信有什么区别？"></a>4.    暂时通信和永久通信有什么区别？</h3><p>(答案见PPT)</p><h3 id="5-什么是复杂流？如何实现复杂流同步？"><a href="#5-什么是复杂流？如何实现复杂流同步？" class="headerlink" title="5.    什么是复杂流？如何实现复杂流同步？"></a>5.    什么是复杂流？如何实现复杂流同步？</h3><p>(答案见PPT)</p><h2 id="第三章作业"><a href="#第三章作业" class="headerlink" title="第三章作业"></a>第三章作业</h2><h3 id="1-设计一个并发服务器，它为每个到来的请求创建一个服务器进程。请分析这种设计与多线程服务器之间的利弊。"><a href="#1-设计一个并发服务器，它为每个到来的请求创建一个服务器进程。请分析这种设计与多线程服务器之间的利弊。" class="headerlink" title="1. 设计一个并发服务器，它为每个到来的请求创建一个服务器进程。请分析这种设计与多线程服务器之间的利弊。"></a>1. 设计一个并发服务器，它为每个到来的请求创建一个服务器进程。请分析这种设计与多线程服务器之间的利弊。</h3><p>多线程服务器实现了并发性并可以充分利用服务器端的资源。虽然采用多进程的方式可以获得这些好处，但是进程的创建和消亡需要更大的开销，进程之间的通信也比一个进程内部多个线程之间的开销大。</p><h3 id="2-假设一个单线程服务器需要花6ms的时间接受一个请求，并使用20ms的时间计算出请求结果并将结果返回。请问，对于一个拥有10个CPU的服务器，若同时到来4个请求，则"><a href="#2-假设一个单线程服务器需要花6ms的时间接受一个请求，并使用20ms的时间计算出请求结果并将结果返回。请问，对于一个拥有10个CPU的服务器，若同时到来4个请求，则" class="headerlink" title="2. 假设一个单线程服务器需要花6ms的时间接受一个请求，并使用20ms的时间计算出请求结果并将结果返回。请问，对于一个拥有10个CPU的服务器，若同时到来4个请求，则"></a>2. 假设一个单线程服务器需要花6ms的时间接受一个请求，并使用20ms的时间计算出请求结果并将结果返回。请问，对于一个拥有10个CPU的服务器，若同时到来4个请求，则</h3><p>（1）如果使用单线程服务器，需要多少时间完成这4个请求？（2）如果设计一个dispatcher/worker形式的多线程服务器，需要dispatcher线程在接受请求后多耗费1ms进行任务的分配工作。使用此多线程服务器，一个CPU专门负责执行dispatcher线程，最短需要多少时间能够将这些请求处理完？（线程创建的开销和线程之间传递数据的时间忽略不计）<br>答:<br>(1)    因为只有一个线程，所以四个请求串行完成，一共需要（6ms+20ms）<em>4=26ms</em>4=104ms<br>(2)    因为有10个CPU，所以可以dispatcher占用一个CPU，余下的四个worker线程各占用一个CPU。所以一共需要：（6ms+1ms）*4+20ms=28ms+20ms=48ms</p><h3 id="3-迭代服务器和并发服务器有何区别？"><a href="#3-迭代服务器和并发服务器有何区别？" class="headerlink" title="3. 迭代服务器和并发服务器有何区别？"></a>3. 迭代服务器和并发服务器有何区别？</h3><p>迭代服务器负责处理请求并将结果返回给客户端。并发服务器本身并不处理客户端的请求，它将请求发送给一个线程或者进程，并等待下一个请求。</p><h3 id="4-如果要设计一个购物网站，在进行服务器设计的时候应该采用有状态服务器还是无状态服务器？说明你做出此种的选择的原因。"><a href="#4-如果要设计一个购物网站，在进行服务器设计的时候应该采用有状态服务器还是无状态服务器？说明你做出此种的选择的原因。" class="headerlink" title="4. 如果要设计一个购物网站，在进行服务器设计的时候应该采用有状态服务器还是无状态服务器？说明你做出此种的选择的原因。"></a>4. 如果要设计一个购物网站，在进行服务器设计的时候应该采用有状态服务器还是无状态服务器？说明你做出此种的选择的原因。</h3><p>采用有状态服务器。因为有状态服务器维护客户信息，如果服务器失败，其上的所有状态必须恢复。在购物时，服务器需要记录客户很多的相关信息。</p><h3 id="5-什么是软件代理？若要设计一个个人秘书系统，希望根据用户的需求搜集用户感兴趣的信息，可以选择怎样的软件代理实现什么样的功能（至少设计出3个功能）"><a href="#5-什么是软件代理？若要设计一个个人秘书系统，希望根据用户的需求搜集用户感兴趣的信息，可以选择怎样的软件代理实现什么样的功能（至少设计出3个功能）" class="headerlink" title="5. 什么是软件代理？若要设计一个个人秘书系统，希望根据用户的需求搜集用户感兴趣的信息，可以选择怎样的软件代理实现什么样的功能（至少设计出3个功能）"></a>5. 什么是软件代理？若要设计一个个人秘书系统，希望根据用户的需求搜集用户感兴趣的信息，可以选择怎样的软件代理实现什么样的功能（至少设计出3个功能）</h3><p>软件代理是一个自治的进程，能够在其所在环境中创建并初始化变化，并可以与用户或者其他代理进行协作。<br>可以选择的代理，比如接口代理，可以提供用户使用方便舒适的接口；或者选择数据代理，可以为用户提供数据的分类和过滤等等。</p><h3 id="6-8分-假设处理器P有一个新任务T，可以有下面的三个算法来实现处理器任务分配"><a href="#6-8分-假设处理器P有一个新任务T，可以有下面的三个算法来实现处理器任务分配" class="headerlink" title="6. (8分)假设处理器P有一个新任务T，可以有下面的三个算法来实现处理器任务分配"></a>6. (8分)假设处理器P有一个新任务T，可以有下面的三个算法来实现处理器任务分配</h3><p>算法A：<br>（i）搜集所有处理器（包括P自己）的负载信息；<br>（ii）选择负载最低的处理器，并将任务分配给负载最低的处理器；<br>（iii）算法结束。<br>算法B:<br>（i）随机选择一个处理器Q，询问其负载情况;<br>（ii）若Q欠载，则将T分配给Q，算法结束；<br>（iii）否则继续执行（i）<br>算法C:<br>（i）随机选择一个处理器Q，询问其负载情况;<br>（ii）若Q欠载，则将T分配给Q，算法结束；<br>（iii）否则，如果已经执行（i）K次，则将T留在本地执行，否则继续执行（i）<br>请问：<br>（1）（2分）上面三个算法哪个是确定性算法？哪个是启发式算法？<br>算法A是确定性算法，B和C是启发式算法。<br>（2）（4分）算法B和算法C很相似，请问哪个算法可行性更强？为什么？<br>算法B有可能不能在有限步骤内结束，因此是个错误的算法。而算法C则可以在有限步骤内结束，可行性强。<br>（3）（2分）算法C是否还可以优化？<br>可以有各种优化方式。比如，只有在本地机器过载的情况下才开始询问其他机器；或者用多播机制发送K个消息询问K台机器的负载情况，然后选择负载最小的机器执行进程。</p><h2 id="第四章作业"><a href="#第四章作业" class="headerlink" title="第四章作业"></a>第四章作业</h2><h3 id="1-在分布式系统中，扁平式命名方式（flat-naming）和名字空间的命名方式（name-space）的主要区别是什么？"><a href="#1-在分布式系统中，扁平式命名方式（flat-naming）和名字空间的命名方式（name-space）的主要区别是什么？" class="headerlink" title="1. 在分布式系统中，扁平式命名方式（flat naming）和名字空间的命名方式（name space）的主要区别是什么？"></a>1. 在分布式系统中，扁平式命名方式（flat naming）和名字空间的命名方式（name space）的主要区别是什么？</h3><p>扁平式命名方式使用ID唯一表示实体，没有结构，不包含任何关于如何定位实体访问点的任何信息。<br>名字空间是一个有标签的有向图，它是结构化的名字。</p><h3 id="2-在扁平式命名方式中使用的层次化方法来定位一个实体的时候，为什么其插入和查找可以体现局部性？"><a href="#2-在扁平式命名方式中使用的层次化方法来定位一个实体的时候，为什么其插入和查找可以体现局部性？" class="headerlink" title="2. 在扁平式命名方式中使用的层次化方法来定位一个实体的时候，为什么其插入和查找可以体现局部性？"></a>2. 在扁平式命名方式中使用的层次化方法来定位一个实体的时候，为什么其插入和查找可以体现局部性？</h3><p>因为在层次化方法中，网络被分为了若干个域，顶层与扩展整个网络，每个域切分为多个更小的域，每个域D中的每个实体E都在dir(D)中有位置记录。因此在修改或者查找的时候，所有的操作都可以在饱含了操作发起点和目标点的最小域里解决，因此体现了局部性。</p><h3 id="3-在图中给出的名字空间中，请为标记为A的实体给出一个绝对路径名和任意一个相对路径名。"><a href="#3-在图中给出的名字空间中，请为标记为A的实体给出一个绝对路径名和任意一个相对路径名。" class="headerlink" title="3. 在图中给出的名字空间中，请为标记为A的实体给出一个绝对路径名和任意一个相对路径名。"></a>3. 在图中给出的名字空间中，请为标记为A的实体给出一个绝对路径名和任意一个相对路径名。</h3><p><img src="/2019/06/15/高级操作系统——分布式系统——课程作业与解答/2019-06-21-13-31-47.png" srcset="/img/loading.gif" alt><br>绝对路径名：k: /vu/home/steen/mbox<br>相对路径名：n0: /home/steen/mbox</p><h3 id="4-为什么缓存可以提升名字服务的可用性？"><a href="#4-为什么缓存可以提升名字服务的可用性？" class="headerlink" title="4. 为什么缓存可以提升名字服务的可用性？"></a>4. 为什么缓存可以提升名字服务的可用性？</h3><p>名字解析过程中，很多步骤的结果都是不变化的，因此使用缓存可以大大提升名字服务的可用性。</p><h3 id="5-为什么要删除不被引用的实体？请解释纯粹采用引用图的方式来解决这个问题时会遇到什么问题？"><a href="#5-为什么要删除不被引用的实体？请解释纯粹采用引用图的方式来解决这个问题时会遇到什么问题？" class="headerlink" title="5. 为什么要删除不被引用的实体？请解释纯粹采用引用图的方式来解决这个问题时会遇到什么问题？"></a>5. 为什么要删除不被引用的实体？请解释纯粹采用引用图的方式来解决这个问题时会遇到什么问题？</h3><p>因为删除无引用实体可以节省资源。<br>当纯粹采用引用图方式解决问题时，很难维护一个全局的引用图，并且需要确定根集，由根集负责发起算法，这些集中式的特点不适用于分布式系统的扩展性。</p><h3 id="6-什么是幂等操作？为什么在删除不被引用的实体的时候采用引用列表方式来可以利用到幂等操作的特性，而采用简单引用计数的方法就不可以？"><a href="#6-什么是幂等操作？为什么在删除不被引用的实体的时候采用引用列表方式来可以利用到幂等操作的特性，而采用简单引用计数的方法就不可以？" class="headerlink" title="6. 什么是幂等操作？为什么在删除不被引用的实体的时候采用引用列表方式来可以利用到幂等操作的特性，而采用简单引用计数的方法就不可以？"></a>6. 什么是幂等操作？为什么在删除不被引用的实体的时候采用引用列表方式来可以利用到幂等操作的特性，而采用简单引用计数的方法就不可以？</h3><p>一个操作多次执行的结果和一次执行的结果是一样的，不会因为多次执行而产生副作用。<br>使用简单计数法时计数增加和减少的操作都不是幂等操作；而采用引用列表时插入和删除操作都是幂等操作。因此在分布式系统中因为通信的不可靠性的存在，幂等操作的存在可以非常适应。</p><h3 id="7-在使用标记不可达对象的跟踪方案删除不被引用的实体时，可以使用两个方案：一个是从根集出发，标记所有可达实体，从而删除所有不可达实体；一个是采用组内跟踪方案，先在一个进程组中标记所有外部可达实体，删除垃圾，然后再扩大组的范围。请解释这两种方案各有何利弊。"><a href="#7-在使用标记不可达对象的跟踪方案删除不被引用的实体时，可以使用两个方案：一个是从根集出发，标记所有可达实体，从而删除所有不可达实体；一个是采用组内跟踪方案，先在一个进程组中标记所有外部可达实体，删除垃圾，然后再扩大组的范围。请解释这两种方案各有何利弊。" class="headerlink" title="7. 在使用标记不可达对象的跟踪方案删除不被引用的实体时，可以使用两个方案：一个是从根集出发，标记所有可达实体，从而删除所有不可达实体；一个是采用组内跟踪方案，先在一个进程组中标记所有外部可达实体，删除垃圾，然后再扩大组的范围。请解释这两种方案各有何利弊。"></a>7. 在使用标记不可达对象的跟踪方案删除不被引用的实体时，可以使用两个方案：一个是从根集出发，标记所有可达实体，从而删除所有不可达实体；一个是采用组内跟踪方案，先在一个进程组中标记所有外部可达实体，删除垃圾，然后再扩大组的范围。请解释这两种方案各有何利弊。</h3><p>第一种方法伸缩性很差，因为需要跟踪分布式系统中的所有对象，是个集中式算法。<br>第二种方法可以适应于分布式系统，可以解决伸缩性问题</p><h2 id="第五章作业"><a href="#第五章作业" class="headerlink" title="第五章作业"></a>第五章作业</h2><h3 id="1-Cristian算法和Berkeley算法哪一个算法可以保证系统中所有的时钟可以与UTC一致？"><a href="#1-Cristian算法和Berkeley算法哪一个算法可以保证系统中所有的时钟可以与UTC一致？" class="headerlink" title="1. Cristian算法和Berkeley算法哪一个算法可以保证系统中所有的时钟可以与UTC一致？"></a>1. Cristian算法和Berkeley算法哪一个算法可以保证系统中所有的时钟可以与UTC一致？</h3><p>答：只有Cristian算法维持了一个拥有WWV接收器的Time server，让所有其他的机器与Time server一致。而Berkeley算法只是维持所有机器时间一致。</p><h3 id="2-在下图中，有三个进程，每个进程上的圆点表示这个进程上发生的事件，事件之间的有箭头的线表示一个消息的发送和接收，请在括号中写出下面四对事件之间的关系【用a-gt-b表示事件a与事件b有happens-before关系，否则用a-x-b表示事件a和b之间没有happens-before关系】"><a href="#2-在下图中，有三个进程，每个进程上的圆点表示这个进程上发生的事件，事件之间的有箭头的线表示一个消息的发送和接收，请在括号中写出下面四对事件之间的关系【用a-gt-b表示事件a与事件b有happens-before关系，否则用a-x-b表示事件a和b之间没有happens-before关系】" class="headerlink" title="2. 在下图中，有三个进程，每个进程上的圆点表示这个进程上发生的事件，事件之间的有箭头的线表示一个消息的发送和接收，请在括号中写出下面四对事件之间的关系【用a -&gt; b表示事件a与事件b有happens before关系，否则用a x b表示事件a和b之间没有happens before关系】"></a>2. 在下图中，有三个进程，每个进程上的圆点表示这个进程上发生的事件，事件之间的有箭头的线表示一个消息的发送和接收，请在括号中写出下面四对事件之间的关系【用a -&gt; b表示事件a与事件b有happens before关系，否则用a x b表示事件a和b之间没有happens before关系】</h3><p><img src="/2019/06/15/高级操作系统——分布式系统——课程作业与解答/2019-06-21-13-33-55.png" srcset="/img/loading.gif" alt><br>（1）P1和q1    【    p1 x q1           】<br>（2）p4和q4    【    q4 -&gt; p4           】<br>（3）p1和r4     【    p1 -&gt; r4           】<br>（4）P4和r1    【    p4 x r1           】</p><h3 id="3-（分）下面的两个全局快照的切口是否是一致的切口？"><a href="#3-（分）下面的两个全局快照的切口是否是一致的切口？" class="headerlink" title="3. （分）下面的两个全局快照的切口是否是一致的切口？"></a>3. （分）下面的两个全局快照的切口是否是一致的切口？</h3><p> <img src="/2019/06/15/高级操作系统——分布式系统——课程作业与解答/2019-06-21-13-34-07.png" srcset="/img/loading.gif" alt><br>               （a）                                            （b）<br>（a）是一致的切口<br>（m1）p3已发送给p1的消息，p1还未接收到；（m2）p2已发送给p3的消息，p3还未收到；（m3）代表的事件还未发生。<br>（b）是不一致的切口<br>（m1）p3已发送给p1的消息，p1还未接收到，此事件没有问题；（m2）p2已发送给p3的消息，p3还未接收到，此事件也没有问题；（m3）p2已接收到p1给其发送的消息，而p1还未发送该消息，不符合事情发展的顺序，因此图（b）不是一致的切口。</p><h3 id="4-（分）假设有一个事务用于在一个银行的两个账户A和B之间进行转账100元钱。写法如下："><a href="#4-（分）假设有一个事务用于在一个银行的两个账户A和B之间进行转账100元钱。写法如下：" class="headerlink" title="4. （分）假设有一个事务用于在一个银行的两个账户A和B之间进行转账100元钱。写法如下："></a>4. （分）假设有一个事务用于在一个银行的两个账户A和B之间进行转账100元钱。写法如下：</h3><p>Begin-Transaction<br>A=A-100;<br>B=B+100;<br>End-Transaction<br>请使用这个例子来说明一个事务的ACID特性。</p><p>事物的ACID特性为：原子性（A）、一致性（C）、独立性（I）、永久性（D）<br>（1）原子性：Begin-Transaction和End-Transaction之间的操作要么全部正常执行完毕，要么不执行，若中间该事物出现错误需要全部回滚到Begin-Transaction之前的状态<br>（2）一致性：一个事物不能破坏系统的一致性，事务发生之前、之后系统应该维持一个稳定的平衡状态，在此例中，银行的总资产可看作是这个稳定的平衡状态（系统的一致性），即A+B的和维持不变<br>（3）独立性：例如与此例一同并发的还有另一个事务：<br>Begin-Transaction<br>A=A+B<em>0.1;<br>C=C-B</em>0.1;<br>End-Transaction<br>两个事务并发，第一个事务的第二条语句执行之前和执行之后B的值是不一样的，这样就导致了最终第二个事务执行之前和之后A+C的值不一样。这就导致一个事务的执行影响了另一个事务的执行结果，独立性没有了。<br>（4）永久性：事务中的操作成功执行完毕后事务便会提交，提交所带来的变化是永久性的。</p><h3 id="5-为实现事务回滚，请分析Private-Workspace方法和Writeahead-Log方法分别应该在怎样的环境下使用。"><a href="#5-为实现事务回滚，请分析Private-Workspace方法和Writeahead-Log方法分别应该在怎样的环境下使用。" class="headerlink" title="5. 为实现事务回滚，请分析Private Workspace方法和Writeahead Log方法分别应该在怎样的环境下使用。"></a>5. 为实现事务回滚，请分析Private Workspace方法和Writeahead Log方法分别应该在怎样的环境下使用。</h3><p>Private workspace应该在事务冲突很多发生的时候使用，而writeahead log应该在事务冲突发生很少的时候使用。</p><h3 id="6-什么是串行性？"><a href="#6-什么是串行性？" class="headerlink" title="6. 什么是串行性？"></a>6. 什么是串行性？</h3><p>如果多个事务的并发执行结果与所有事务按照某个顺序串行执行是一样的，那么就是串行一致的。并发事务的调度要符合串行性就是正确的调度。</p><h3 id="7-请举例说明为何2PL方法会导致死锁？"><a href="#7-请举例说明为何2PL方法会导致死锁？" class="headerlink" title="7. 请举例说明为何2PL方法会导致死锁？"></a>7. 请举例说明为何2PL方法会导致死锁？</h3><p>如果有两个事务用相反的顺序申请两个共享数据A和B，采用2PL的方法，如果第一个事务先访问了A，于是给A加锁；然后第二个事务访问了B，于是给B加锁。再后来两个事务就都不能执行下去，因为都申请不到下一个数据的锁，导致死锁。</p><h3 id="8-为什么会出现假死锁？请举例说明。"><a href="#8-为什么会出现假死锁？请举例说明。" class="headerlink" title="8.为什么会出现假死锁？请举例说明。"></a>8.为什么会出现假死锁？请举例说明。</h3><p>在用一台机器（协调者）维护整个系统的资源图，来描述进程和资源关系时，协调者检测出循环则认为是死锁。机器之间的消息传递有延时，可能会造成先发送的消息后收到，如课件中的例子：首先B想要归还资源R，随后发生B申请资源T，但后一条消息首先到达Coordinator，Coordinator增加了一条B-&gt; T的边，此时本应该没有R-&gt;B，由于还未收到该消息，Coordinator判断资源图有循环，呈现假死锁状态。</p><h2 id="第六章作业"><a href="#第六章作业" class="headerlink" title="第六章作业"></a>第六章作业</h2><h3 id="1-在分布式系统中为什么要对一些实体建立副本？"><a href="#1-在分布式系统中为什么要对一些实体建立副本？" class="headerlink" title="1. 在分布式系统中为什么要对一些实体建立副本？"></a>1. 在分布式系统中为什么要对一些实体建立副本？</h3><p>可靠性：分布在各个机器上的备份可以防止其中某几台机器文件的损坏甚至宕机带来的影响；<br>性能：建立副本性能上得以提升，获取资源可以从地域近的、不繁忙的机器上获取需要的数据，减少通信的开销，实现负载均衡；<br>容错：由于其可大大降低单机文件损坏的影响，因此其容错性高</p><h3 id="2-为何严格的一致性模型在分布式系统中很难实现？"><a href="#2-为何严格的一致性模型在分布式系统中很难实现？" class="headerlink" title="2. 为何严格的一致性模型在分布式系统中很难实现？"></a>2. 为何严格的一致性模型在分布式系统中很难实现？</h3><p>由于严格一致性需要每个读操作都返回最新的写操作的结果，因此需要存在全局的时钟来确定哪个操作时最新的写操作。这在分布式系统中是很难实现的。</p><h3 id="3-假设在一个分布式系统中有下面四个进程："><a href="#3-假设在一个分布式系统中有下面四个进程：" class="headerlink" title="3. 假设在一个分布式系统中有下面四个进程："></a>3. 假设在一个分布式系统中有下面四个进程：</h3><p>P1：a=5；a=6；<br>P2：a=7；<br>P3：printf(a); printf(a); printf(a);<br>P4: printf(a); printf(a); printf(a);<br>最后P3和P4的运行结果都是6 5 7。<br>请问这个运行结果在顺序一致性模型中是可以接受的吗？为什么？<br>顺序一致性不能够接受这个结果，因为这个结果不符合P1的程序中语句的执行次序。</p><h3 id="4-请写出在维护副本一致性的时候进行修改传播的三种方法。"><a href="#4-请写出在维护副本一致性的时候进行修改传播的三种方法。" class="headerlink" title="4.请写出在维护副本一致性的时候进行修改传播的三种方法。"></a>4.请写出在维护副本一致性的时候进行修改传播的三种方法。</h3><p>三种方法是：<br>（1）传播无效性消息<br>（2）传播修改了的数据<br>（3）传播修改操作</p><h3 id="5-为了实现副本的一致性可以采用push的协议，也可以使用pull的协议。请问两种协议都适用于哪种类型的系统"><a href="#5-为了实现副本的一致性可以采用push的协议，也可以使用pull的协议。请问两种协议都适用于哪种类型的系统" class="headerlink" title="5.为了实现副本的一致性可以采用push的协议，也可以使用pull的协议。请问两种协议都适用于哪种类型的系统"></a>5.为了实现副本的一致性可以采用push的协议，也可以使用pull的协议。请问两种协议都适用于哪种类型的系统</h3><p>push的协议会把修改积极推送到各个副本上。在这种情况下，服务器需维护副本的信息，在读写频率高时性能较好，免去了多个副本不断询问的开销；<br>pull的协议副本向服务器轮询是否有修改，当读写频率低时使用较好，不用维护副本信息表，服务器开销小。</p><h3 id="6-（8分）假设在网络中为了提升数据的有效性，为一个数据保存了10个副本。若使用基于法定数量的数据一致性策略，学生A、B、C采用了不同的方案。学生A的方案为读法定数量为1，写法定数量为9；学生B的方案为读法定数量为6，写法定数量为5；学生C的方案为读法定数量为2，写法定数量为9。请回答：（1）这三个方案中哪个方案是正确的？"><a href="#6-（8分）假设在网络中为了提升数据的有效性，为一个数据保存了10个副本。若使用基于法定数量的数据一致性策略，学生A、B、C采用了不同的方案。学生A的方案为读法定数量为1，写法定数量为9；学生B的方案为读法定数量为6，写法定数量为5；学生C的方案为读法定数量为2，写法定数量为9。请回答：（1）这三个方案中哪个方案是正确的？" class="headerlink" title="6.（8分）假设在网络中为了提升数据的有效性，为一个数据保存了10个副本。若使用基于法定数量的数据一致性策略，学生A、B、C采用了不同的方案。学生A的方案为读法定数量为1，写法定数量为9；学生B的方案为读法定数量为6，写法定数量为5；学生C的方案为读法定数量为2，写法定数量为9。请回答：（1）这三个方案中哪个方案是正确的？"></a>6.（8分）假设在网络中为了提升数据的有效性，为一个数据保存了10个副本。若使用基于法定数量的数据一致性策略，学生A、B、C采用了不同的方案。学生A的方案为读法定数量为1，写法定数量为9；学生B的方案为读法定数量为6，写法定数量为5；学生C的方案为读法定数量为2，写法定数量为9。请回答：（1）这三个方案中哪个方案是正确的？</h3><p>（2）对于每种错误的方案给出一个例子说明错误发生的原因。<br>（1）三个方案中C的是正确的<br>（2）如果按照A的方案，读法定数量为1，写法定数量为9，读者会读到一个旧版本的数据；如果按照C的方案，读法定数量为6，写法定数量为5，读者永远读不到6个版本一致的数据。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;第一章作业&quot;&gt;&lt;a href=&quot;#第一章作业&quot; class=&quot;headerlink&quot; title=&quot;第一章作业&quot;&gt;&lt;/a&gt;第一章作业&lt;/h2&gt;&lt;h3 id=&quot;1-什么是分布式系统？请举例说明分布式系统的特点。&quot;&gt;&lt;a href=&quot;#1-什么是分布式系统？请举例说明
      
    
    </summary>
    
    
      <category term="分布式系统" scheme="https://superlova.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="学校课程" scheme="https://superlova.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/%E5%AD%A6%E6%A0%A1%E8%AF%BE%E7%A8%8B/"/>
    
    
      <category term="分布式系统" scheme="https://superlova.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="操作系统" scheme="https://superlova.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="作业" scheme="https://superlova.github.io/tags/%E4%BD%9C%E4%B8%9A/"/>
    
  </entry>
  
  <entry>
    <title>贝叶斯分类</title>
    <link href="https://superlova.github.io/2019/06/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB/"/>
    <id>https://superlova.github.io/2019/06/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB/</id>
    <published>2019-06-13T13:11:08.000Z</published>
    <updated>2020-03-17T01:54:50.816Z</updated>
    
    <content type="html"><![CDATA[<h2 id="朴素贝叶斯法（Naive-Bayes）"><a href="#朴素贝叶斯法（Naive-Bayes）" class="headerlink" title="朴素贝叶斯法（Naive Bayes）"></a>朴素贝叶斯法（Naive Bayes）</h2><p>在高斯判别分析（GDA）方法中，特征向量 $x$ 是连续的，值为实数的向量。下面我们要讲的是当 $x_i$ 是离散值的时候来使用的另外一种学习算法。</p><p>下面就来继续看一个之前见过的样例，来尝试建立一个邮件筛选器，使用机器学习的方法。这回咱们要来对邮件信息进行分类，来判断是否为商业广告邮件（就是垃圾邮件），还是非垃圾邮件。在学会了怎么实现之后，我们就可以让邮件阅读器能够自动对垃圾信息进行过滤，或者单独把这些垃圾邮件放进一个单独的文件夹中。对邮件进行分类是一个案例，属于文本分类这一更广泛问题集合。</p><p>假设我们有了一个训练集（也就是一堆已经标好了是否为垃圾邮件的邮件）。要构建垃圾邮件分选器，咱们先要开始确定用来描述一封邮件的特征$x_i$有哪些。</p><p>我们将用一个特征向量来表示一封邮件，这个向量的长度等于字典中单词的个数。如果邮件中包含了字典中的第 $i$ 个单词，那么就令 $x_i = 1$；反之则$x_i = 0$。例如下面这个向量：</p><script type="math/tex; mode=display">x=\begin{bmatrix}1\\0\\0\\\vdots \\1\\ \vdots \\0\end{bmatrix} \begin{matrix}\text{a}\\ \text{aardvark}\\ \text{aardwolf}\\ \vdots\\ \text{buy}\\ \vdots\\ \text{zygmurgy}\\ \end{matrix}</script><p>就用来表示一个邮件，其中包含了两个单词 “a” 和 “buy”，但没有单词 “aardvark”， “aardwolf” 或者 “zymurgy”  。这个单词集合编码整理成的特征向量也成为<strong>词汇表（vocabulary,），</strong> 所以特征向量 $x$ 的维度就等于词汇表的长度。</p><blockquote><p>注：实际应用中并不需要遍历整个英语词典来组成所有英语单词的列表，实践中更常用的方法是遍历一下训练集，然后把出现过一次以上的单词才编码成特征向量。这样做除了能够降低模型中单词表的长度之外，还能够降低运算量和空间占用，此外还有一个好处就是能够包含一些你的邮件中出现了而词典中没有的单词，比如本课程的缩写CS229。有时候（比如在作业里面），还要排除一些特别高频率的词汇，比如像冠词the，介词of 和and 等等；这些高频率但是没有具体意义的虚词也叫做stop words，因为很多文档中都要有这些词，用它们也基本不能用来判定一个邮件是否为垃圾邮件。</p></blockquote><p>选好了特征向量了，接下来就是建立一个生成模型（generative model）。所以我们必须对$p(x|y)$进行建模。但是，假如我们的单词有五万个词，则特征向量$x \in  \{0, 1\}^{50000}$ （即 $x$是一个 $50000$ 维的向量，其值是$0$或者$1$），如果我们要对这样的 $x$进行多项式分布的建模，那么就可能有$2^{50000}$ 种可能的输出，然后就要用一个 $(2^{50000}-1)$维的参数向量。这样参数明显太多了。</p><p>要给$p(x|y)$建模，先来做一个非常强的假设。我们<strong>假设特征向量$x_i$ 对于给定的 $y$ 是独立的。</strong> 这个假设也叫做<strong>朴素贝叶斯假设（Naive Bayes ，NB assumption），</strong> 基于此假设衍生的算法也就叫做<strong>朴素贝叶斯分类器（Naive Bayes classifier）。</strong> 例如，如果 $y = 1$ 意味着一个邮件是垃圾邮件；然后其中”buy” 是第$2087$个单词，而 “price”是第$39831$个单词；那么接下来我们就假设，如果我告诉你 $y = 1$，也就是说某一个特定的邮件是垃圾邮件，那么对于$x_{2087}$ （也就是单词 buy 是否出现在邮件里）的了解并不会影响你对$x_{39831}$ （单词price出现的位置）的采信值。更正规一点，可以写成 $p(x_{2087}|y) = p(x_{2087}|y, x_{39831})$。（要注意这个并不是说$x_{2087}$ 和 $x_{39831}$这两个特征是独立的，那样就变成了$p(x_{2087}) = p(x_{2087}|x_{39831})$，我们这里是说在给定了 $y$ 的这样一个条件下，二者才是有条件的独立。）</p><p>然后我们就得到了等式：</p><script type="math/tex; mode=display">\begin{aligned}p(x_1, ..., x_{50000}|y) & = p(x_1|y)p(x_2|y,x_1)p(x_3|y,x_1,x_2) ... p(x_{50000}|y,x_1,x_2,...,x_{49999})\\& = p(x_1|y)p(x_2|y)p(x_3|y) ... p(x_{50000}|y)\\& = \prod^n_{i=1}p(x_i|y)\\\end{aligned}</script><p>第一行的等式就是简单地来自概率的基本性质，第二个等式则使用了朴素贝叶斯假设。这里要注意，朴素贝叶斯假设也是一个很强的假设，产生的这个算法可以适用于很多种问题。</p><p>我们这个模型的参数为 $\phi_{i|y=1} = p (x_i = 1|y = 1), \phi_{i|y=0} = p (x_i = 1|y = 0)$, 而 $\phi_y = p (y = 1)$。和以往一样，给定一个训练集$\{(x^{(i)},y^{(i)}); i = 1, …, m\}$，就可以写出下面的联合似然函数：</p><script type="math/tex; mode=display">\mathcal{L}(\phi_y,\phi_{j|y=0},\phi_{j|y=1})=\prod^m_{i=1}p(x^{(i)},y^{(i)})</script><p>找到使联合似然函数取得最大值的对应参数组合 $\phi_y , \phi_{i|y=0} 和 \phi_{i|y=1}$ 就给出了最大似然估计：</p><script type="math/tex; mode=display">\begin{aligned}\phi_{j|y=1} &=\frac{\sum^m_{i=1}1\{x_j^{(i)} =1 \wedge y^{(i)} =1\} }{\sum^m_{i=1}1\{y^{(i)} =1\}} \\\phi_{j|y=0} &= \frac{\sum^m_{i=1}1\{x_j^{(i)} =1 \wedge y^{(i)} =0\} }{\sum^m_{i=1}1\{y^{(i)} =0\}} \\\phi_{y} &= \frac{\sum^m_{i=1}1\{y^{(i)} =1\}}{m}\\\end{aligned}</script><p>在上面的等式中，”$\wedge$(and)”这个符号的意思是逻辑”和”。这些参数有一个非常自然的解释。例如 $\phi_{j|y=1}$ 正是单词 $j$ 出现的邮件中垃圾邮件所占 $(y = 1)$ 的比例。</p><p>拟合好了全部这些参数之后，要对一个新样本的特征向量 $x$ 进行预测，只要进行如下的简单地计算：</p><script type="math/tex; mode=display">\begin{aligned}p(y=1|x)&=  \frac{p(x|y=1)p(y=1)}{p(x)}\\&= \frac{(\prod^n_{i=1}p(x_i|y=1))p(y=1)}{(\prod^n_{i=1}p(x_i|y=1))p(y=1)+  (\prod^n_{i=1}p(x_i|y=0))p(y=0)}  \\\end{aligned}</script><p>然后选择有最高后验概率的概率。</p><p>最后我们要注意，刚刚我们对朴素贝叶斯算法的使用中，特征向量 $x_i$ 都是二值化的，其实特征向量也可以是多个离散值，比如$\{1, 2, …, k_i\}$这样也都是可以的。这时候只需要把对$p(x_i|y)$ 的建模从伯努利分布改成多项式分布。实际上，即便一些原始的输入值是连续值（比如我们第一个案例中的房屋面积），也可以转换成一个小规模的离散值的集合，然后再使用朴素贝叶斯方法。例如，如果我们用特征向量 $x_i$ 来表示住房面积，那么就可以按照下面所示的方法来对这一变量进行离散化：</p><div class="table-container"><table><thead><tr><th style="text-align:center">居住面积</th><th style="text-align:center">$&lt;400$</th><th style="text-align:center">$400-800$</th><th style="text-align:center">$800-1200$</th><th style="text-align:center">$1200-1600$</th><th style="text-align:center">$&gt;1600$</th></tr></thead><tbody><tr><td style="text-align:center">离散值 $x_i$</td><td style="text-align:center">$1$</td><td style="text-align:center">$2$</td><td style="text-align:center">$3$</td><td style="text-align:center">$4$</td><td style="text-align:center">$5$</td></tr></tbody></table></div><p>这样，对于一个面积为 $890$ 平方英尺的房屋，就可以根据上面这个集合中对应的值来把特征向量的这一项的$x_i$值设置为$3$。然后就可以用朴素贝叶斯算法，并且将$p(x_i|y)$作为多项式分布来进行建模，就都跟前面讲过的内容一样了。当原生的连续值的属性不太容易用一个多元正态分布来进行建模的时候，将其特征向量离散化然后使用朴素贝叶斯法（NB）来替代高斯判别分析法（GDA），通常能形成一个更好的分类器。</p><h3 id="1-拉普拉斯平滑（Laplace-smoothing）"><a href="#1-拉普拉斯平滑（Laplace-smoothing）" class="headerlink" title="1 拉普拉斯平滑（Laplace smoothing）"></a>1 拉普拉斯平滑（Laplace smoothing）</h3><p>刚刚讲过的朴素贝叶斯算法能够解决很多问题了，但还能对这种方法进行一点小调整来进一步提高效果，尤其是应对文本分类的情况。我们来简要讨论一下一个算法当前状态的一个问题，然后在讲一下如何解决这个问题。</p><p>还是考虑垃圾邮件分类的过程，设想你学完了CS229的课程，然后做了很棒的研究项目，之后你决定在$2003$年$6$月把自己的作品投稿到NIPS会议，这个NIPS是机器学习领域的一个顶级会议，递交论文的截止日期一般是六月末到七月初。你通过邮件来对这个会议进行了讨论，然后你也开始收到带有 nips 四个字母的信息。但这个是你第一个NIPS论文，而在此之前，你从来没有接到过任何带有 nips 这个单词的邮件；尤其重要的是，nips 这个单词就从来都没有出现在你的垃圾/正常邮件训练集里面。加入这个 nips 是你字典中的第$35000$个单词那么你的朴素贝叶斯垃圾邮件筛选器就要对参数$\phi_{35000|y}$ 进行最大似然估计，如下所示：</p><script type="math/tex; mode=display">\begin{aligned}\phi_{35000|y=1} &=  \frac{\sum^m_{i=1}1\{x^{(i)}_{35000}=1 \wedge y^{(i)}=1  \}}{\sum^m_{i=1}1\{y^{(i)}=0\}}  &=0 \\\phi_{35000|y=0} &=  \frac{\sum^m_{i=1}1\{x^{(i)}_{35000}=1 \wedge y^{(i)}=0  \}}{\sum^m_{i=1}1\{y^{(i)}=0\}}  &=0 \\\end{aligned}</script><p>也就是说，因为之前程序从来没有在别的垃圾邮件或者正常邮件的训练样本中看到过 nips 这个词，所以它就认为看到这个词出现在这两种邮件中的概率都是$0$。因此当要决定一个包含 nips 这个单词的邮件是否为垃圾邮件的时候，他就检验这个类的后验概率，然后得到了：</p><script type="math/tex; mode=display">\begin{aligned}p(y=1|x) &= \frac{ \prod^n_{i=1} p(x_i|y=1)p(y=1) }   {\prod^n_{i=1} p(x_i|y=1)p(y=1) +\prod^n_{i=1} p(x_i|y=1)p(y=0)    }\\&= \frac00\\\end{aligned}</script><p>这是因为对于”  $\prod^n_{i=1} p(x_i|y)$”中包含了$p(x_{35000}|y) = 0$的都加了起来，也就还是$0$。所以我们的算法得到的就是 $\frac00$，也就是不知道该做出怎么样的预测了。</p><p>然后进一步拓展一下这个问题，统计学上来说，只因为你在自己以前的有限的训练数据集中没见到过一件事，就估计这个事件的概率为零，这明显不是个好主意。假设问题是估计一个多项式随机变量 $z$ ，其取值范围在$\{1,…, k\}$之内。接下来就可以用$\phi_i = p (z = i)$ 来作为多项式参数。给定一个 $m$ 个独立观测$\{z^{(1)}, …, z^{(m)}\}$ 组成的集合，然后最大似然估计的形式如下：</p><script type="math/tex; mode=display">\phi_j=\frac{\sum^m_{i=1}1\{z^{(i)}=j\}}m</script><p>正如咱们之前见到的，如果我们用这些最大似然估计，那么一些$\phi_j$可能最终就是零了，这就是个问题了。要避免这个情况，咱们就可以引入<strong>拉普拉斯平滑（Laplace smoothing），</strong> 这种方法把上面的估计替换成：</p><script type="math/tex; mode=display">\phi_j=\frac{\sum^m_{i=1}1\{z^{(i)}=j\}+1}{m+k}</script><p>这里首先是对分子加$1$，然后对分母加$k$，要注意$\sum^k_{j=1} \phi_j = 1$依然成立（自己检验一下），这是一个必须有的性质，因为$\phi_j$ 是对概率的估计，然后所有的概率加到一起必然等于$1$。另外对于所有的 $j$ 值，都有$\phi_j \neq 0$，这就解决了刚刚的概率估计为零的问题了。在某些特定的条件下（相当强的假设条件下，arguably quite strong），可以发现拉普拉斯平滑还真能给出对参数$\phi_j$ 的最佳估计（optimal estimator）。</p><p>回到我们的朴素贝叶斯分选器问题上，使用了拉普拉斯平滑之后，对参数的估计就写成了下面的形式：</p><script type="math/tex; mode=display">\begin{aligned}\phi_{j|y=1} & =\frac{\sum^m_{i=1}1\{x_j^{(i)}=1\wedge y ^{(i)}=1\}+1}{\sum^m_{i=1}1{\{y^{(i)}=1\}}+2}\\\phi_{j|y=0} & =\frac{\sum^m_{i=1}1\{x_j^{(i)}=1\wedge y ^{(i)}=10\}+1}{\sum^m_{i=1}1{\{y^{(i)}=0\}}+2}\\\end{aligned}</script><p>（在实际应用中，通常是否对$\phi_y$ 使用拉普拉斯并没有太大影响，因为通常我们会对每个垃圾邮件和非垃圾邮件都有一个合适的划分比例，所以$\phi_y$ 会是对$p(y = 1)$ 的一个合理估计，无论如何都会与零点有一定距离。）</p><h3 id="2-针对文本分类的事件模型（Event-models-for-text-classification）"><a href="#2-针对文本分类的事件模型（Event-models-for-text-classification）" class="headerlink" title="2 针对文本分类的事件模型（Event models for text classification）"></a>2 针对文本分类的事件模型（Event models for text classification）</h3><p>到这里就要给咱们关于生成学习算法的讨论进行一下收尾了，所以就接着讲一点关于文本分类方面的另一个模型。我们刚已经演示过的朴素贝叶斯方法能够解决很多分类问题了，不过还有另一个相关的算法，在针对文本的分类效果还要更好。</p><p>在针对文本进行分类的特定背景下，咱们上面讲的朴素贝叶斯方法使用的是一种叫做<strong>多元伯努利事件模型（Multi-Variate Bernoulli event model）。</strong> 在这个模型里面，我们假设邮件发送的方式，是随机确定的（根据先验类<em>class priors</em>， $p(y)$），无论是不是垃圾邮件发送者，他是否给你发下一封邮件都是随机决定的。那么发件人就会遍历词典，决定在邮件中是否包含某个单词 $i$，各个单词之间互相独立，并且服从概率分布$p(x_i=1|y)=\phi_{i|y}$。因此，一条消息的概率为：$p(y)\prod^n_{i-1}p(x_i|y)$</p><p> 然后还有另外一个模型，叫做<strong>多项式事件模型（Multinomial event model）。</strong> 要描述这个模型，我们需要使用一个不同的记号和特征集来表征各种邮件。设 $x_i$ 表示单词中的第$i$个单词。因此，$x_i$现在就是一个整数，取值范围为$\{1,…,|V|\}$，这里的$|V|$是词汇列表（即字典）的长度。这样一个有 $n$ 个单词的邮件就可以表征为一个长度为 $n$ 的向量$(x_1,x_2,…,x_n)$；这里要注意，不同的邮件内容，$n$ 的取值可以是不同的。例如，如果一个邮件的开头是”A NIPS . . .” ，那么$x_1 = 1$ (“a” 是词典中的第一个)，而$x_2 = 35000$ (这是假设 “nips”是词典中的第35000个)。</p><p>在多项式事件模型中，我们假设邮件的生成是通过一个随机过程的，而是否为垃圾邮件是首先决定的（根据$p(y)$），这个和之前的模型假设一样。然后邮件的发送者写邮件首先是要生成  从对单词$(p(x_1|y))$ 的某种多项式分布中生成 $x_1$。然后第二步是独立于 $x_1$ 来生成 $x_2$，但也是从相同的多项式分布中来选取，然后是 $x_3$,$x_4$  等等，以此类推，直到生成了整个邮件中的所有的词。因此，一个邮件的总体概率就是$p(y)\prod^n_{i=1}p(x_i|y)$。要注意这个方程看着和我们之前那个多元伯努利事件模型里面的邮件概率很相似，但实际上这里面的意义完全不同了。尤其是这里的$x_i|y$现在是一个多项式分布了，而不是伯努利分布了。</p><p>我们新模型的参数还是$\phi_y = p(y)$，这个跟以前一样，然后还有$\phi_{k|y=1} = p(x_j =k|y=1)$ (对任何 $j$)以及 $\phi_{i|y=0} =p(x_j =k|y=0)$。要注意这里我们已经假设了对于任何$j$ 的值，$p(x_j|y)$这个概率都是相等的，也就是意味着在这个哪个词汇生成的这个分布不依赖这个词在邮件中的位置$j$。</p><p>如果给定一个训练集$\{(x^{(i)},y^{(i)}); i = 1, …, m\}$，其中 $x^{(i)}  = ( x^{(i)}_{1} , x^{(i)}_{2} ,…, x^{(i)}_{n_i})$（这里的$n$是在第$i$个训练样本中的单词数目），那么这个数据的似然函数如下所示：</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}(\phi,\phi_{k|y=0},\phi_{k|y=1})& = \prod^m_{i=1}p( x^{(i)},y^{(i)})\\& = \prod^m_{i=1}(\prod^{n_i}_{j=1}p(x_j^{(i)}|y;\phi_{k|y=0},\phi_{k|y=1}))p( y^{(i)};\phi_y)\\\end{aligned}</script><p>让上面的这个函数最大化就可以产生对参数的最大似然估计：</p><script type="math/tex; mode=display">\begin{aligned}\phi_{k|y=1}&=  \frac{\sum^m_{i=1}\sum^{n_i}_{j=1}1\{x_j^{(i)}=k\wedge y^{(i)}=1\}}{\sum^m_{i=1}1\{y^{(i)}=1\}n_i} \\\phi_{k|y=0}&=  \frac{\sum^m_{i=1}\sum^{n_i}_{j=1}1\{x_j^{(i)}=k\wedge y^{(i)}=0\}}{\sum^m_{i=1}1\{y^{(i)}=0\}n_i} \\\phi_y&=   \frac{\sum^m_{i=1}1\{y^{(i)}=1\}}{m}\\\end{aligned}</script><p>如果使用拉普拉斯平滑（实践中会用这个方法来提高性能）来估计$\phi_{k|y=0}$ 和 $\phi_{k|y=1}$，就在分子上加1，然后分母上加$|V|$，就得到了下面的等式：</p><script type="math/tex; mode=display">\begin{aligned}\phi_{k|y=1}&=  \frac{\sum^m_{i=1}\sum^{n_i}_{j=1}1\{x_j^{(i)}=k\wedge y^{(i)}=1\}+1}{\sum^m_{i=1}1\{y^{(i)}=1\}n_i+|V|} \\\phi_{k|y=0}&=  \frac{\sum^m_{i=1}\sum^{n_i}_{j=1}1\{x_j^{(i)}=k\wedge y^{(i)}=0\}+1}{\sum^m_{i=1}1\{y^{(i)}=0\}n_i+|V|} \\\end{aligned}</script><p>当然了，这个并不见得就是一个最好的分类算法，不过朴素贝叶斯分选器通常用起来还都出乎意料地那么好。所以这个方法就是一个很好的”首发选择”，因为它很简单又很好实现。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;朴素贝叶斯法（Naive-Bayes）&quot;&gt;&lt;a href=&quot;#朴素贝叶斯法（Naive-Bayes）&quot; class=&quot;headerlink&quot; title=&quot;朴素贝叶斯法（Naive Bayes）&quot;&gt;&lt;/a&gt;朴素贝叶斯法（Naive Bayes）&lt;/h2&gt;&lt;p&gt;在高
      
    
    </summary>
    
    
      <category term="学习笔记" scheme="https://superlova.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="机器学习" scheme="https://superlova.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="转载" scheme="https://superlova.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%BD%AC%E8%BD%BD/"/>
    
    
      <category term="机器学习" scheme="https://superlova.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="贝叶斯" scheme="https://superlova.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
  </entry>
  
  <entry>
    <title>牛奶可乐经济学note2</title>
    <link href="https://superlova.github.io/2019/06/08/%E7%89%9B%E5%A5%B6%E5%8F%AF%E4%B9%90%E7%BB%8F%E6%B5%8E%E5%AD%A6note2/"/>
    <id>https://superlova.github.io/2019/06/08/%E7%89%9B%E5%A5%B6%E5%8F%AF%E4%B9%90%E7%BB%8F%E6%B5%8E%E5%AD%A6note2/</id>
    <published>2019-06-08T07:49:56.000Z</published>
    <updated>2020-03-17T01:54:50.797Z</updated>
    
    <content type="html"><![CDATA[<p>回顾一下上一节的概念：</p><ol><li><strong>机会成本</strong>：</li></ol><p>一件事情的机会成本，是指为了从事这件事而放弃的其他事情的价值。</p><p>假设小金抽中了大众点评霸王餐，价值200元的A餐馆的晚饭，必须今晚吃。但是今晚小金最爱的林俊杰今晚开演唱会，演唱会门票400元。小金之前想的是演唱会门票超过500元就不去，但是霸王餐的消息突如其来，小金有点犹豫了。</p><p>那么请问小金去吃霸王餐的机会成本是多少？</p><p>答案是100元。吃霸王餐的价值与本题无关，因为小金放弃的是林俊杰的演唱会。林俊杰演唱会在小金心中值500，但他要花费400元买票，因此只要霸王餐在小金的心目中的价值超过100元，小金就应该去吃霸王餐。</p><p><strong>在经济学理论里，成本如果没有特殊说明就是指机会成本。</strong></p><ol><li><strong>成本效益原则</strong>：</li></ol><p>唯有当行动带来的收益比成本高时，你才应该这么做。</p><p>设想一下，马上618购物节了，你想买空调和衣服。已知你有一张打折优惠券。买空调价格2000元，打折后能省100块钱；买衣服200元，打折后能省90元。空调打折了衣服就不能打折，衣服打折了空调就不能打折。你应该在买什么的时候使用打折优惠券？</p><p>答案显而易见，该在买空调时用，多省10元。</p><ol><li><strong>有例外，才能证明规律存在</strong>：</li></ol><p>个人理解，就是找出命题的逆否命题，并证明逆否命题成立。</p><p>比如动物中很多雄性的体型比雌性大，这是因为体型大的雄性有机会与更多雌性交配。这算是规律吗？还是巧合呢？按照“有例外，才能证明规律存在”这一标准，我们只需找到雄性不会和很多雌性交配的情况，并看看他们是不是雄性体型小于等于雌性即可。</p><p>大多一夫一妻制的鸟类，雄鸟和雌鸟体型相似。很多昆虫中的雄性并不会通过体型来和其他雄性竞争交配权，所以昆虫的雄性甚至比雌性还小。</p><ol><li><strong>以叙述形式表达的信息最易被吸收</strong>：</li></ol><p>这就是为什么<em>码农有道</em>公众号的文章受很多程序员喜爱的原因。他每次科普都会讲一个故事，虽然十分老套但是效果很神奇，很简单就理解了（当然记不记得住、能不能应用就是另一回事了）。</p><ol><li><strong>学习最好的方式之一，就是把它写下来</strong>：</li></ol><p>这就是众多大佬，比如《暗时间》一书的作者刘未鹏提倡写博客的缘故吧。</p><hr><p>开始这一讲的话题：</p><h1 id="CH1-产品设计中的经济学"><a href="#CH1-产品设计中的经济学" class="headerlink" title="CH1 产品设计中的经济学"></a>CH1 产品设计中的经济学</h1><ol><li><strong>除非改动带来的收益大于改动的成本，否则生产商不会给产品研发新功能</strong></li></ol><p>商业环境中，产品设计、科学研发要符合成本效益原则。即产品设计既要包含最符合消费者心意的功能，又要满足卖方保持低价、便于竞争的需求。产品设计必须在两者之间保持平衡。</p><p>举个例子，为什么某些公司从“技工贸”转向“贸工技”（即从技术最重要转向贸易最重要）？这也许是公司领导层认为投入在技术研发上面的成本超过了所能带来的收入。</p><h1 id="CH2-供求关系实践"><a href="#CH2-供求关系实践" class="headerlink" title="CH2 供求关系实践"></a>CH2 供求关系实践</h1><ol><li><strong>商业公司的最大价值，不在于它能提高多少生产率，而在于它能创造多少利润</strong></li></ol><p>从长远角度看，新技术所借省下来的成本并不会给生产者带来更高的利润，而是降低了产品的价格，使消费者受惠。</p><ol><li><strong>没有免费的午餐</strong></li></ol><p>提防那些太过美好的机遇。赚取财富的唯一方式仍然是天赋、勤俭、幸运，再加上艰苦的劳动。</p><p>然而，成千上万的人似乎认为自己能够轻松致富。他们亲眼见别人这么做过：有人把手里的钱全买了一飞冲天的高科技股票，比如阿里巴巴、新浪等，就这样发了大财；还有人拼命借贷，买下大大超过个人负担能力的房地产，一夜暴富。</p><p>“没有免费午餐”定律成功解释了2008年的金融危机。不了解的朋友请看《大空头》这部电影，电影描述了几个经济学家在所有人都陷入房价狂欢的时候，率先意识到股市和房市会崩盘，因此大量做空房市，最后大赚一笔。电影中写了几个细节，说就连夜场舞女、地痞流氓等低信用人士都借贷购买了大量房产，一旦金融危机爆发，他们根本无力还款。所有人都陷入免费午餐中的时候，你就要警惕了。</p><ol><li><strong>一价定律</strong></li></ol><p>两座城市之间商品的价格，不会超过两地之间的运输成本。“一价定律”尤其适合于竞争激烈的市场。</p><p>“一价定律”指出，任何试图利用富人多花钱的想法的供应商，都会给竞争对手创造出直接获利的机会。</p><p>在沙漠里向马云和另外一个穷人卖水，马云可能会给出更高的价格。但是在竞争激烈的市场，水的价格都是统一的。这个价格就是人们心中认为它的价值。</p><ol><li><strong>边际成本</strong></li></ol><p>生产的商品数目越多，<strong>总成本</strong>越高，这是显然的。但是总成本与生产数目的关系并不是线性的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;回顾一下上一节的概念：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;机会成本&lt;/strong&gt;：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一件事情的机会成本，是指为了从事这件事而放弃的其他事情的价值。&lt;/p&gt;
&lt;p&gt;假设小金抽中了大众点评霸王餐，价值200元的A餐馆的晚饭，必须今晚吃。但
      
    
    </summary>
    
    
      <category term="人文社科" scheme="https://superlova.github.io/categories/%E4%BA%BA%E6%96%87%E7%A4%BE%E7%A7%91/"/>
    
      <category term="学习笔记" scheme="https://superlova.github.io/categories/%E4%BA%BA%E6%96%87%E7%A4%BE%E7%A7%91/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习方法" scheme="https://superlova.github.io/tags/%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
      <category term="经济学" scheme="https://superlova.github.io/tags/%E7%BB%8F%E6%B5%8E%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>神经网络学习笔记note1</title>
    <link href="https://superlova.github.io/2019/06/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0note1/"/>
    <id>https://superlova.github.io/2019/06/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0note1/</id>
    <published>2019-06-06T13:38:03.000Z</published>
    <updated>2020-03-17T01:54:50.815Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实现神经网络"><a href="#实现神经网络" class="headerlink" title="实现神经网络"></a>实现神经网络</h1><p>简单的神经网络实现。包含随机梯度下降算法、反向传播算法。</p><pre><code class="lang-python">class Network(object):    def __init__(self, sizes):        self.num_layers = len(sizes)        self.sizes = sizes        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]        self.weights = [np.random.randn(y, x)                        for x, y in zip(sizes[:-1], sizes[1:])]    def feedforward(self, a):        for b, w in zip(self.biases, self.weights):            a = sigmoid(np.dot(w, a)+b)        return a    def SGD(self, training_data, epochs, mini_batch_size, eta,            test_data=None):        training_data = list(training_data)        n = len(training_data)        if test_data:            test_data = list(test_data)            n_test = len(test_data)        for j in range(epochs):            random.shuffle(training_data)            mini_batches = [                training_data[k:k+mini_batch_size]                for k in range(0, n, mini_batch_size)]            for mini_batch in mini_batches:                self.update_mini_batch(mini_batch, eta)            if test_data:                print(&quot;Epoch {} : {} / {}&quot;.format(j,self.evaluate(test_data),n_test));            else:                print(&quot;Epoch {} complete&quot;.format(j))    def update_mini_batch(self, mini_batch, eta):        nabla_b = [np.zeros(b.shape) for b in self.biases]        nabla_w = [np.zeros(w.shape) for w in self.weights]        for x, y in mini_batch:            delta_nabla_b, delta_nabla_w = self.backprop(x, y)            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]        self.weights = [w-(eta/len(mini_batch))*nw                        for w, nw in zip(self.weights, nabla_w)]        self.biases = [b-(eta/len(mini_batch))*nb                       for b, nb in zip(self.biases, nabla_b)]    def backprop(self, x, y):        nabla_b = [np.zeros(b.shape) for b in self.biases]        nabla_w = [np.zeros(w.shape) for w in self.weights]        # feedforward        activation = x        activations = [x] # list to store all the activations, layer by layer        zs = [] # list to store all the z vectors, layer by layer        for b, w in zip(self.biases, self.weights):            z = np.dot(w, activation)+b            zs.append(z)            activation = sigmoid(z)            activations.append(activation)        # backward pass        delta = self.cost_derivative(activations[-1], y) * \            sigmoid_prime(zs[-1])        nabla_b[-1] = delta        nabla_w[-1] = np.dot(delta, activations[-2].transpose())        for l in range(2, self.num_layers):            z = zs[-l]            sp = sigmoid_prime(z)            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp            nabla_b[-l] = delta            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())        return (nabla_b, nabla_w)    def evaluate(self, test_data):        test_results = [(np.argmax(self.feedforward(x)), y)                        for (x, y) in test_data]        return sum(int(x == y) for (x, y) in test_results)    def cost_derivative(self, output_activations, y):        return (output_activations-y)#### Miscellaneous functionsdef sigmoid(z):    &quot;&quot;&quot;The sigmoid function.&quot;&quot;&quot;    return 1.0/(1.0+np.exp(-z))def sigmoid_prime(z):    &quot;&quot;&quot;Derivative of the sigmoid function.&quot;&quot;&quot;    return sigmoid(z)*(1-sigmoid(z))</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;实现神经网络&quot;&gt;&lt;a href=&quot;#实现神经网络&quot; class=&quot;headerlink&quot; title=&quot;实现神经网络&quot;&gt;&lt;/a&gt;实现神经网络&lt;/h1&gt;&lt;p&gt;简单的神经网络实现。包含随机梯度下降算法、反向传播算法。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;lan
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://superlova.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://superlova.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="学习笔记" scheme="https://superlova.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="转载" scheme="https://superlova.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%BD%AC%E8%BD%BD/"/>
    
    
      <category term="机器学习" scheme="https://superlova.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://superlova.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://superlova.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机笔记</title>
    <link href="https://superlova.github.io/2019/05/30/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AC%94%E8%AE%B0/"/>
    <id>https://superlova.github.io/2019/05/30/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AC%94%E8%AE%B0/</id>
    <published>2019-05-30T09:09:20.000Z</published>
    <updated>2020-03-17T01:54:50.772Z</updated>
    
    <content type="html"><![CDATA[<h1 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h1><h2 id="1-思想"><a href="#1-思想" class="headerlink" title="1. 思想"></a>1. 思想</h2><p>SVM的思想，即对于平面上的二分类问题，找到一条直线，不但能够将两类数据恰好分开，而且要分的越开越好。即最大化不同类别的（关键）样本点之间的距离。</p><p><strong>我们的目的是找到一条河，让这条河的河岸恰恰经过正负样本的同时，尽可能宽。</strong></p><p><img src="/2019/05/30/支持向量机笔记/2019-05-30-20-27-32.png" srcset="/img/loading.gif" alt></p><p>上图蓝线是最终确定的超平面，红线则是正例和反例的“楚河汉界”。这“楚河汉界”与超平面平行，并且距超平面相同距离。</p><p><strong>我们不妨定义这段距离为1。</strong></p><h2 id="2-最大化距离的方法"><a href="#2-最大化距离的方法" class="headerlink" title="2. 最大化距离的方法"></a>2. 最大化距离的方法</h2><p>假设已经找到了这样一条超平面，它的法向量为$\vec{w}$.</p><p>对于任意样本$\vec{u}$，如果$f(u)=w\cdot u+b &gt; 0$则把样本u归为正例，反之为负例。</p><p>现在想要找到一条河，让这条河的河岸恰恰经过正负样本的同时，尽可能宽。这样我们对正例和负例的划分做出了更高的要求：不仅仅大于0，更要大于河的宽度（的一半），即大于1。</p><script type="math/tex; mode=display">f\left(\mathbf{x}_{+}\right)=\mathbf{w} \cdot \mathbf{x}_{+}+b \geq 1</script><script type="math/tex; mode=display">f\left(\mathbf{x}_{-}\right)=\mathbf{w} \cdot \mathbf{x}_{-}+b \leq-1</script><p>设想一下，我们有两个样本，一个小红，一个小蓝，他们站在河的对岸相望。</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{w} \cdot \mathbf{x}_{1}+b &=+1 \\ \mathbf{w} \cdot \mathbf{x}_{2}+b &=-1 \end{aligned}</script><p><img src="/2019/05/30/支持向量机笔记/2019-05-30-20-45-50.png" srcset="/img/loading.gif" alt></p><p>那我现在问你，求河的长度。WTF？这样就能求出河的长度了吗？是的，想一下，从小红指向小蓝的那一条向量$x_1-x_2$，向超平面的法向量$w$方向投影，然后得到的向量长度不就是河的长度了吗？</p><p>河的长度：$ \mathbf{w} \cdot\left(\mathbf{x}_{1}-\mathbf{x}_{2}\right)=2 $<br>。还记得两个向量投影就是求点积吗？蛤蛤。</p><p>忘了，法向量也是有长度的，为了把法向量标准化，我们需要除以法向量的长度：</p><script type="math/tex; mode=display">\frac{\mathbf{w}}{\|\mathbf{w}\|} \cdot\left(\mathbf{x}_{1}-\mathbf{x}_{2}\right)=\frac{2}{\|\mathbf{w}\|}</script><p>问题就转化为将$\frac{2}{||\mathbf{w}||}$求最大。这就是一个最优化问题，按理说利用拉格朗日乘子法，对分量挨个求导即可求出极值点。但是现在这副模样连求导都做不到。我们想办法变形一下：</p><p>常数项肯定就不要了，问题就变成了最小化$\frac{1}{2}|\mathbf{w}|^{2}$。这一切都是为了方便数学处理呀。</p><p>不要忘了，还有一个（其实是i个）限制条件：</p><script type="math/tex; mode=display">y_{i}\left(\mathbf{w} \cdot \mathbf{x}_{i}+b\right) \geq 1</script><p>y取1或-1，用来标记样本类别。</p><h2 id="3-二次优化问题"><a href="#3-二次优化问题" class="headerlink" title="3. 二次优化问题"></a>3. 二次优化问题</h2><p>所谓拉格朗日，就是在限制条件前面加个参数，然后附加在要求的优化方程式中，从而将问题转化成无约束优化问题。被拉格朗日之后，我们加了i个参数，组成参数向量a</p><script type="math/tex; mode=display">L=\frac{1}{2}\|\mathbf{w}\|^{2}-\sum_{i=1}^{l} a_{i}\left(y_{i}\left(\mathbf{x}_{i} \cdot \mathbf{w}+b\right)-1\right)</script><p>接下来要对向量求导了。害怕吗？有如下公式：$\frac{\partial|\mathbf{w}|^{2}}{\partial \mathbf{w}}=2 \mathbf{w}$和$\frac{\partial \mathbf{x} \cdot \mathbf{w}}{\partial \mathbf{w}}=\mathbf{x}$</p><p>我们分别对w和b求导，看看拉格朗日函数对w和b的变化分别有什么反应：</p><script type="math/tex; mode=display">\begin{aligned} \frac{\partial L}{\partial \mathbf{w}} &=\mathbf{w}-\sum_{i=1}^{l} a_{i} y_{i} \mathbf{x}_{i}=0 \\ \frac{\partial L}{\partial b} &=\sum_{i=1}^{l} a_{i} y_{i}=0 \end{aligned}</script><p>现在合适的 $ w^*=\sum_{i=1}^{l} a_{i} y_{i} \mathbf{x}_{i} $ 也都找到了，将他们回代到拉格朗日函数中去，看看他的最小点长什么样子。</p><script type="math/tex; mode=display">L=\sum_{i=1}^{l} a_{i}-\frac{1}{2} \sum_{i, j=1}^{l} a_{i} a_{j} y_{i} y_{j} \mathbf{x}_{i} \cdot \mathbf{x}_{j}</script><p>我们可以看到啊，这个L的大小，取决于样本之间的点积，也就是我们选的两个小红和小蓝，他们站的位置。他们的位置要是不好，那距离就会不够大，那我们的拉格朗日同学就要重新选人。因此呢，最终被选择的样本，那一定是具有代表性的，我们称它们为<strong>支持向量</strong>。这就是支持向量机一词的由来。</p><h2 id="4-核方法的运用"><a href="#4-核方法的运用" class="headerlink" title="4. 核方法的运用"></a>4. 核方法的运用</h2><p>有时候我们会碰到一些样本，他们本来就不是线性可分的，比如</p><p><img src="/2019/05/30/支持向量机笔记/2019-05-30-21-14-02.png" srcset="/img/loading.gif" alt></p><p>但我们还是将他们利用支持向量机分开了。怎么做到的？怎么“以直为曲”？那是因为有一个道理，<strong>高维空间比低维空间更广阔的</strong>，更难出现线性不可分的状况。所以很自然的，我们人为添加一些分量，比如将二维点添加z轴分量到三维啊，甚至无限维。之后我们可以找到一个超平面，将他们分开就是轻而易举的事情啦。</p><p>我们需要一种神秘变换$\Phi$，接受一个样本向量，输出提升维度之后的样本向量。</p><p><img src="/2019/05/30/支持向量机笔记/2019-05-30-21-17-55.png" srcset="/img/loading.gif" alt></p><p>$\Phi$很难找，但是我们不必找。因为根据前面的结论，拉格朗日函数根我们选取的关键向量的<strong>内积</strong>有关，直接表示出$ \Phi\left(\mathbf{x}_{1}\right) \cdot \Phi\left(\mathbf{x}_{2}\right) $，一步到位哦。</p><p>正好有这种专门表示两个向量运算的函数，叫做核函数。</p><script type="math/tex; mode=display">\Phi\left(\mathbf{x}_{1}\right) \cdot \Phi\left(\mathbf{x}_{2}\right)=K\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right)</script><p>举个例子，线性核函数：</p><script type="math/tex; mode=display">K(x, y) = x\cdot y</script><p>多项式核函数：</p><script type="math/tex; mode=display">K\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right)=\left(\mathbf{x}_{1} \cdot \mathbf{x}_{2}+1\right)^{n}</script><p>高斯核函数：</p><script type="math/tex; mode=display">K\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right)=e^{\frac{-|| x_{1}+x_{2} ||^{2}}{2 \sigma^{2}}}</script><p>有的时候我们会碰到一些样本，他们很难缠，甚至根本线性不可分。类似于红军打入了蓝军内部，根本无法找到一条直线将他们分开。这时候我们就需要放低要求。</p><p>引入松弛变量，出现的错误样本在一定程度范围内越小越好。</p><script type="math/tex; mode=display">\xi_i =  1-y_{i}\left(w^{T} x_{i}+b\right)</script><p>则有下式</p><script type="math/tex; mode=display">y_{i}\left(w^{T} x_{i}+b\right) \geq 1-\xi_{i}, \quad i=1, \dots, n</script><p>最终的优化问题就变成了</p><script type="math/tex; mode=display">\begin{array}{l}{\min \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{n} \xi_{i}} \\ {\text {s.t.}, y_{i}\left(w^{T} x_{i}+b\right) \geq 1-\xi_{i}, i=1, \ldots, n} \\ {\xi_{i} \geq 0, i=1, \ldots, n}\end{array}</script><p>C为经验参数，调参侠必备。您想让误差少点，C就大点。</p><p>接下来的套路就和之前一样了，拉格朗日啊之类的，略去不表。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;支持向量机&quot;&gt;&lt;a href=&quot;#支持向量机&quot; class=&quot;headerlink&quot; title=&quot;支持向量机&quot;&gt;&lt;/a&gt;支持向量机&lt;/h1&gt;&lt;h2 id=&quot;1-思想&quot;&gt;&lt;a href=&quot;#1-思想&quot; class=&quot;headerlink&quot; title=&quot;1. 思想
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://superlova.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="学习笔记" scheme="https://superlova.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="https://superlova.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="支持向量机" scheme="https://superlova.github.io/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>华为的命运与中国的反应</title>
    <link href="https://superlova.github.io/2019/05/25/%E5%8D%8E%E4%B8%BA%E7%9A%84%E5%91%BD%E8%BF%90%E4%B8%8E%E4%B8%AD%E5%9B%BD%E7%9A%84%E5%8F%8D%E5%BA%94/"/>
    <id>https://superlova.github.io/2019/05/25/%E5%8D%8E%E4%B8%BA%E7%9A%84%E5%91%BD%E8%BF%90%E4%B8%8E%E4%B8%AD%E5%9B%BD%E7%9A%84%E5%8F%8D%E5%BA%94/</id>
    <published>2019-05-25T12:27:35.000Z</published>
    <updated>2020-03-17T01:54:50.722Z</updated>
    
    <content type="html"><![CDATA[<h1 id="美国将华为列入了黑名单，实施一系列与针对中兴类似的、外科手术式的精准打击。华为在劫难逃。那么华为或中国可以做什么来应对呢？中国会采取哪些手段？"><a href="#美国将华为列入了黑名单，实施一系列与针对中兴类似的、外科手术式的精准打击。华为在劫难逃。那么华为或中国可以做什么来应对呢？中国会采取哪些手段？" class="headerlink" title="美国将华为列入了黑名单，实施一系列与针对中兴类似的、外科手术式的精准打击。华为在劫难逃。那么华为或中国可以做什么来应对呢？中国会采取哪些手段？"></a>美国将华为列入了黑名单，实施一系列与针对中兴类似的、外科手术式的精准打击。华为在劫难逃。那么华为或中国可以做什么来应对呢？中国会采取哪些手段？</h1><p>华为被列为美国“实体清单”无疑是当下最受关注的话题。显然，美国是想采取一系列精准打击，妄图将华为推入和中兴一样的境地。因此，无论是国内还是国外，线上还是线下，华为和中国的反应无疑是最受人关注的。鉴于国内新闻平台罕有客观分析的文章，我通过Quora论坛（又称作美国版知乎）摘录并翻译了Janus Dongye Qimeng对于该问题的精彩回答。他回答于5月22日，截至目前（25日）收获了5.9k个赞同。让我们来欣赏Janus对于该问题的精彩回答吧。</p><p>链接：<a href="https://www.quora.com/US-blacklisted-Huawei-in-what-is-evidently-ZTE-style-surgical-strike-It-appears-Huawei-is-pretty-much-dead-What-can-Huawei-or-China-do-to-overcome-this-or-to-retaliate-What-will-China-do" target="_blank" rel="noopener">US blacklisted Huawei in what is evidently ZTE style surgical strike. It appears Huawei is pretty much dead. What can Huawei or China do to overcome this or to retaliate? What will China do?</a></p><p>Janus Dongye Qimeng：</p><p>不谈观点，只说事实。</p><p>下面是华为2019年最畅销的P30手机的供应链：</p><p><img src="/2019/05/25/华为的命运与中国的反应/芯片.png" srcset="/img/loading.gif" alt></p><p>华为P30手机的“大脑”是由海思公司设计的麒麟980系统芯片（System-on-Chip，SOC）。而海思是华为旗下的子公司。为什么称它为系统芯片？因为该芯片集成了许多由世界其他地方设计的组件。</p><p>该系统芯片背后是什么？</p><p><strong>指令集架构</strong>（Instruction set architecture）：海思购买了<strong>英国</strong>剑桥ARM的CPU和GPU架构许可证。通过许可，海思可以使用ARM指令集（armv8）并开发自己的64位CPU架构。AMBA等总线标准也是ARM授权的。</p><p><strong>CPU，GPU</strong>：海思在中国深圳拥有数百名员工，负责设计CPU内核、加速器和IP组件。为了设计自己的CPU，他们需要使用Synopsis，Cadence和Xilinx的电子设计自动化（EDA）工具。这些EDA公司都是<strong>美国</strong>加利福尼亚州的公司。海思需要支付许可费才能使用他们的工具来设计和模拟自己的CPU。</p><blockquote><p>译者注：电子设计自动化（英语：Electronic design automation，缩写：EDA）是指利用计算机辅助设计（CAD）软件，来完成超大规模集成电路（VLSI）芯片的功能设计、综合、验证、物理设计（包括布局、布线、版图、设计规则检查等）等流程的设计方式。</p></blockquote><p>同时，海思还可以集成ARM设计的现有软核，如强大的核心Cortex A76和高效的核心Cortex A55。 两者都在同一芯片中。大核心在<strong>美国</strong>德克萨斯州奥斯汀市设计，小核心在<strong>英国</strong>剑桥设计。一些低端CPU核心也从中国台湾联发科购买。同时，海思还可以从ARM购买其他知识产权，包括Mali T830 GPU和互连子系统。Mali GPU设计在<strong>英国</strong>剑桥的ARM总部。</p><p><strong>内存</strong>：海思在存储器控制器和SRAM系统中设计了自己的逻辑电路。SRAM和DRAM单元由<strong>韩国</strong>三星授权。未来的7nm 3D堆叠RAM也将由三星设计，但在中国大连制造。</p><p><strong>DSP和相机</strong>：海思购买了德国徕卡相机的相机镜头设计知识产权和控制系统，其中大部分系统都是在<strong>德国</strong>韦茨拉尔设计的。实际镜头由中国台湾的大立光电（Largan Precision）和中国大陆的舜宇光学科技（Sunny Optical Technology）制造。用于驱动相机改变焦点的电动马达由三美电机（Mitsumi）在日本Tsurumaki制造。为了将光转换为信号，光敏胶片由中国深圳的欧菲光（O-film）设计（也是iPhone X的供应商）。海思从<strong>美国</strong>亚利桑那州凤凰城的安森美（ON Semiconductors）购买了用于自动聚焦和图像稳定的硬件解决方案。高清视频处理芯片由<strong>日本</strong>索尼授权。海思设计了自己的图像处理硬件加速器（ISP），从<strong>美国</strong>加利福尼亚州的CEVA购买了许多DSP专利，并从中国北京购买了来自寒武纪科技（Cambricon Technologies）的AI芯片。</p><p><strong>通信模块</strong>（Baseband）：海思购买了许可证，使用来自<strong>美国</strong>加利福尼亚州圣何塞的博通（Broadcom）的WIFI、GPS和蓝牙。对于3G支持，海思必须向<strong>美国</strong>加利福尼亚州圣地亚哥高通公司持有的专利支付许可使用费。对于后来的4G LTE和5G，海思拥有自己的专利和通信模块处理器，称为Balong，由中国数百人设计。海思还从中国科学院购买了北斗导航系统。请注意，一些芯片验证任务由<strong>印度</strong>海得拉巴的工程师执行。</p><p><strong>射频模块</strong>（Radio frequency，缩写为RF）：要在各种通信信号之间进行多路复用并将模拟信号放大到不同的无线频率，它们需要射频集成电路（RFIC）。RFIC的大多数专利都是由<strong>美国</strong>北卡罗来纳州的RF Micro Devices公司持有，现在在与TriQuint合并后成为Qorvo。RFIC芯片需要一些功率放大器，这由<strong>日本</strong>京都的Murata Manufacturing制造的高端电容器提供。 还需要TST台湾和Microgate在深圳设计和制造的表面声波（SAW）传感器。还需要一些由<strong>美国</strong>Skyworks Solutions设计并由Skyworks在中国制造的绝缘硅绝缘体开关。对于天线组件，它们由深圳的Sunway公司和位于中国上海的Rosenberger（<strong>美国</strong>）工厂设计和制造。在5G时代，华为模拟设备也必须使用来自<strong>美国</strong>，<strong>日本</strong>和中国的这些设备。</p><p><strong>NFC和触控设备</strong>：<strong>荷兰</strong>的恩智浦半导体为华为提供NFC解决方案。该芯片由英飞凌在<strong>德国</strong>西门子开发。深圳Goodix Co提供指纹传感器。USB Type-C解决方案由深圳Everwin Precision提供。</p><p><strong>芯片装配</strong>：在海思将所有知识产权和零件集成到一个系统芯片SOC之后，该设计被送到台湾半导体制造公司（TSMC）进行物理布局和制造。SOC芯片的制造过程是一项非常复杂的任务。对于最重要的步骤，TSMC需要导入由<strong>荷兰</strong>ASML设计的掩模对齐系统（MAS）。他们还需要使用来自<strong>日本</strong>的Shin-Etsu，来自<strong>德国</strong>的Siltronic AG和来自<strong>日本</strong>Minato的SUMCO Corporation的大量晶圆化学品。</p><p><strong>源材料</strong>：但是，大多数化学产品和半成品都是从中国进口的。最具代表性的是中国的稀土金属。对于包括玻璃和钢材在内的其他材料，比亚迪在深圳负责制造手机梯度框架和高密度眼镜。盛意电子为手机生产所有PCB板。</p><p><strong>屏幕</strong>：华为P30采用三星OLED硬屏，但P30 Pro采用京东方科技在中国设计的OLED软屏。有些屏幕也由<strong>韩国</strong>LG在中国广州制造。现在，韩国和中国公司都在屏幕市场占据主导地位。</p><p><strong>组装</strong>：华为随后从每个服务提供商处订购所有组件，并将组件运送到中国郑州的富士康公司。富士康的工人将所有组件组装成一个完整的手机。</p><p><img src="/2019/05/25/华为的命运与中国的反应/p30.jpg" srcset="/img/loading.gif" alt></p><p>这是华为单个手机的供应链。华为的主要产品不只是手机，但他们的手机仍然成功击败苹果，成为未进入美国市场的第二大智能手机公司。华为的主要优势在于其通信基础设施和解决方案。因为这方面我并不熟悉，所以我并不打算写。</p><p>现在请计算这些供应商来自美国，中国，日本和韩国的数量。对于上面列出的每家公司，请访问他们自己的网站，查看他们的产品实际销售到华为或中国市场的份额，以及他们的材料从中国进口的数量。你会惊讶地发现，华为通常是他们最大的客户，他们再也不能离开中国了。</p><p>这意味着如果你杀死了华为，那么大多数供应商也会受到很大的伤害。有些公司会陪葬。他们中的大多数是韩国和日本唯一的高价值公司。他们会遭受40％及以上的市场损失。这将对韩国和日本经济造成巨大打击。</p><p>很显然，特朗普背后的智囊并不了解半导体行业的现状。我想Quora的大多数人都不知道。</p><p><img src="/2019/05/25/华为的命运与中国的反应/trump.jpg" srcset="/img/loading.gif" alt></p><p>华为已死？</p><p>当然没有。十年前，华为已经启动了美国政府的各种情景备胎计划。他们甚至有针对当整个中国被阻止使用x86指令集场景的极端备份计划。</p><h3 id="中国会采取什么手段？"><a href="#中国会采取什么手段？" class="headerlink" title="中国会采取什么手段？"></a>中国会采取什么手段？</h3><p>让我们来看看中国采取的最新的应对措施：</p><p><a href="https://www.scmp.com/tech/enterprises/article/2139699/china-cuts-taxes-chip-makers-promote-industry-development" target="_blank" rel="noopener">由于贸易紧张局势加剧，中国削减了芯片制造商的税收</a></p><p>中国宣布所有国内半导体供应商公司将获得五年免税。他们五年不需要纳税！这意味着他们可以降低运营成本并击败其外国竞争对手。这是美国一直在抱怨中国的地方。现在中国一直在做，不管美国有什么话要说。</p><p>如果华为找不到美国的供应商，那么他们会找到替代品，主要是中国的国内供应商。这些供应商在中国免税。这将为国内公司带来巨大的推动力，因为它们可以同时降低成本和获得客户。</p><p>我一直在与中国半导体行业不同领域的许多中国学者交谈。他们说，华为之所以从这么多来源购买知识产权，并不是说华为没有这项技术，更重要的是，对于他们最重要的领域，他们并不打算<strong>重新发明轮子</strong>（reinvent the wheels），他们希望与世界共享利益。</p><p>确实有一些中国仍然落后的关键技术，如制造工艺和射频芯片。但我们应该明白。中国能达到目前为止的技术水平，正是由于巴黎多边出口管制协调委员会的技术封锁和制裁，中国被禁止使用所有高端技术。多亏了他们，中国有机会自己“重新发明轮子”。</p><p>同时，由于采用了华为技术，北京的地铁已经安装并覆盖了5G信号。</p><p><img src="/2019/05/25/华为的命运与中国的反应/5g.jpg" srcset="/img/loading.gif" alt></p><p>但现在，我坐在伦敦的地铁上。我的手机没有任何信号。因此，我必须离线阅读关于一篇精彩的英国脱欧文章的帖子。我周围的人都在看华为手机上的离线小说。</p><p>有关软件方面“供应链”的分析，请参考我的答案：</p><p><a href="https://www.quora.com/How-badly-will-Huaweis-smartphone-business-be-affected-by-Googles-response-to-US-placing-Huawei-on-Entity-list-Huawei-loses-access-to-Google-proprietary-apps-and-services-but-is-still-be-able-to-run-the-Android-Open/answer/Janus-Dongye-Qimeng" target="_blank" rel="noopener">Janus Dongye Qimeng’s answer to How badly will Huawei’s smartphone business be affected by Google’s response to US placing Huawei on “Entity” list? (Huawei loses access to Google proprietary apps and services but is still be able to run the Android Open Source license (AOSP).</a></p><p>我希望你能通过本文了解更多关于现状的信息。</p><blockquote><p>Talk is cheap. Show me the code.<br>— Linus Torvalds<br>空谈误国，实干兴邦。</p></blockquote><p>本文由<strong>Superlova</strong>翻译自Quora平台Janus Dongye Qimeng对<a href="https://www.quora.com/US-blacklisted-Huawei-in-what-is-evidently-ZTE-style-surgical-strike-It-appears-Huawei-is-pretty-much-dead-What-can-Huawei-or-China-do-to-overcome-this-or-to-retaliate-What-will-China-do" target="_blank" rel="noopener">US blacklisted Huawei in what is evidently ZTE style surgical strike. It appears Huawei is pretty much dead. What can Huawei or China do to overcome this or to retaliate? What will China do?</a>的回答，翻译并传播的初心是用作英语翻译习作和思想交流。郑重声明：本文的观点并不代表译者的观点。转载本译文请注明本译文出处！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;美国将华为列入了黑名单，实施一系列与针对中兴类似的、外科手术式的精准打击。华为在劫难逃。那么华为或中国可以做什么来应对呢？中国会采取哪些手段？&quot;&gt;&lt;a href=&quot;#美国将华为列入了黑名单，实施一系列与针对中兴类似的、外科手术式的精准打击。华为在劫难逃。那么华为或
      
    
    </summary>
    
    
      <category term="翻译" scheme="https://superlova.github.io/categories/%E7%BF%BB%E8%AF%91/"/>
    
      <category term="转载" scheme="https://superlova.github.io/categories/%E7%BF%BB%E8%AF%91/%E8%BD%AC%E8%BD%BD/"/>
    
    
      <category term="国际关系" scheme="https://superlova.github.io/tags/%E5%9B%BD%E9%99%85%E5%85%B3%E7%B3%BB/"/>
    
  </entry>
  
  <entry>
    <title>机器学习——决策树——note1</title>
    <link href="https://superlova.github.io/2019/05/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%86%B3%E7%AD%96%E6%A0%91%E2%80%94%E2%80%94note1/"/>
    <id>https://superlova.github.io/2019/05/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%86%B3%E7%AD%96%E6%A0%91%E2%80%94%E2%80%94note1/</id>
    <published>2019-05-22T07:16:55.000Z</published>
    <updated>2020-03-17T01:54:50.779Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-树的划分流程：原理与思想"><a href="#1-树的划分流程：原理与思想" class="headerlink" title="1. 树的划分流程：原理与思想"></a>1. 树的划分流程：原理与思想</h2><h3 id="1-1-决策树的工作原理"><a href="#1-1-决策树的工作原理" class="headerlink" title="1.1 决策树的工作原理"></a>1.1 决策树的工作原理</h3><p>决策树是广泛用于分类和回归任务的模型。本质上，它从一层层的if/else 问题中进行学习，并得出结论。</p><p><img src="/2019/05/22/机器学习——决策树——note1/2019-05-22-21-00-49.png" srcset="/img/loading.gif" alt="区分几种动物的决策树"></p><p>在这张图中，树的每个结点代表一个问题或一个包含答案的终结点（也叫叶结点）。树的边将问题的答案与将问的下一个问题连接起来。</p><p>用机器学习的语言来说就是，为了区分四类动物（鹰、企鹅、海豚和熊），我们利用三个特征（“有没有羽毛”“会不会飞”和“有没有鳍”）来构建一个模型。我们可以利用监督学习从数据中学习模型，而无需人为构建模型。</p><h3 id="1-2-决策树的思想"><a href="#1-2-决策树的思想" class="headerlink" title="1.2 决策树的思想"></a>1.2 决策树的思想</h3><p>原则上讲，对于给定的属性集，可以构造的决策树的数目达指数级。尽管某些决策树比其他决策树更准确，但是由于搜索空间是指数规模的，找出最佳决策树在计算上是不可行的。尽管如此，人们还是开发出了一写有效的算法，这些算法通常基于贪心策略，在选择划分数据的属性时，采取一系列局部最优决策来构造决策树。下面介绍Hunt算法，Hunt算法是许多决策树算法的基础</p><p>1) Hunt算法</p><p>在Hunt算法中，通过将训练记录相继划分成较纯的子集，以递归方式建立决策树。设D_t是与节点t相关联的训练记录集，而y={y_1, y_2,\cdots,y_c}是类标号，则Hunt算法的递归定义如下：</p><p>（1）如果D_t中所有记录同属于一个类y_t，则t是叶节点，用y_t标记；<br>（2）如果D_t中包含属于多个类的纪录，则选择一个<strong>属性测试条件</strong>，将记录划分成较小的子集。对于测试条件的每个输出，创建一个子女节点，并根据测试结果将D_t中的记录分布到子女节点中。然后，对于每个子女节点，递归地调用该算法.</p><p>如果属性值的每种组合都在训练数据中出现，并且每种组合都具有唯一的类标号，则Hunt算法是有效的。但是对于大多数情况，这些假设太苛刻了。因此，需要附加的条件来处理以下的情况：</p><ul><li>算法的第二部所创建的子女节点可能为空，即不存在与这些节点相关联的记录。这时，该节点成为叶节点，类标号为其父节点上训练记录中的多数类。</li><li>在第二步中，如果与D_t相关联的所有记录都具有相同的属性值（目标属性除外），则不可能进一步划分这些记录。在这种情况下，该节点为叶节点，其标号为与该节点相关联的训练记录中的多数类。</li></ul><p>2) 决策树归纳的设计问题<br>决策树归纳的学习算法必须解决下面两个问题。<br>（1）如何分裂训练记录？树增长过程的每个递归步都必须选择1个属性测试条件，将记录划分成较小的子集。为了实现这个步骤，算法必须提供为不同类型的属性指定测试条件的方法并提供评估毎种测试条件的客观度量。<br>（2）如何停止分裂过程？需要有结束条件，以终止决策树的生长过程。一个可能的策略是分裂结点，直到所有的记录都属于同一个类。或者所有的记录都具有相同的属性值。</p><p>3) 属性的表示</p><p>决策树归纳算法必须为不间类的属性提供表示属性测试条件和其对应输出的方法。不同属性按类别可分为<strong>二元属性</strong>、<strong>标称属性</strong>、<strong>序列属性</strong>、<strong>连续属性</strong>。如果是二元属性，我们可以简单的将树分成两个分支；但是对于其他属性，特别是非离散属性，我们要考虑的问题就更多了。后面会谈到离散属性连续化，以及应用于回归分析的算法。</p><h2 id="2-树如何进行划分：信息增益、增益率、基尼指数"><a href="#2-树如何进行划分：信息增益、增益率、基尼指数" class="headerlink" title="2. 树如何进行划分：信息增益、增益率、基尼指数"></a>2. 树如何进行划分：信息增益、增益率、基尼指数</h2><p>决策树最关键的是选择合适的划分方法，来使得问题的规模最大程度的简化。一般是根据划分后子女结点不纯性的程度。</p><p>不纯性的度量：</p><script type="math/tex; mode=display">Entropy(t)=-\Sigma_{i=0}^{c-1}p(i|t)\log_2{p(i|t)}</script><script type="math/tex; mode=display">Gini(t)=1-\Sigma_{i=0}^{c-1}[p(i|t)]^2</script><script type="math/tex; mode=display">Classification\_Error(t)=1-\max_i{p(i|t)}</script><p>增益率：</p><p>为解决熵和Gini指标等不纯性度量趋于有利于具有大量不同值的属性，CART使用二元划分属性，C4.5使用增益率。</p><script type="math/tex; mode=display">Gain\_ratio=\frac{\Delta_{info}}{Split\_info}</script><h2 id="3-对抗过拟合的手段：预剪枝、后剪枝"><a href="#3-对抗过拟合的手段：预剪枝、后剪枝" class="headerlink" title="3. 对抗过拟合的手段：预剪枝、后剪枝"></a>3. 对抗过拟合的手段：预剪枝、后剪枝</h2><p>先剪枝（提前终于规则）：</p><p>在这种方法中，树增长算法在产生完全拟合整个训练数据集的完全增长的决策树之前就停止决策树的生长。具体实现方式可以当观察到的不纯性度量的增益低于某个确定的阈值时就停止扩展叶结点。这种方法的优点在于避免产生过分拟合训练数据的过于复杂的子树。但是阈值的选取是一个比较复杂的问题。</p><p>后剪枝：</p><p>初始决策树按照最大规模生长，然后进行剪枝的步骤，按照自底向上的方式修剪完全增长然后修剪。</p><p>修剪有两种做法：</p><p>（1）用新的叶子节点替换子树，该叶结点的类标号由子树下记录中的多数类决定。子树替换</p><p>（2）用子树中最常使用的分支代替子树。当模型不能再改进时终止剪枝步骤。子树提升</p><h2 id="4-如何处理连续、缺失值"><a href="#4-如何处理连续、缺失值" class="headerlink" title="4. 如何处理连续、缺失值"></a>4. 如何处理连续、缺失值</h2><p>1) 连续值如何划分？</p><p>C4.5：Information Gain （Ratio） based Threshold<br>CART：遍历所有输入变量j 和切分点s，根据最小化平方误差准则选取；</p><p>2) 是否能够处理Missing值？ 如果能， 是如何处理的？</p><ul><li>插值法（Imputation）： QUEST, CRUISE</li><li>替代法（Alternate/Surrogate Splits）：CART， CRUISE</li><li>缺失值单独分支（Missing value branch）：CHAID， GUIDE</li><li>概率权重（Probability weights）： C4.5</li></ul><p>3) 决策树是如何处理不完整数据的？</p><ul><li>采用抛弃缺失值<br>抛弃极少量的缺失值的样本对决策树的创建影响不是太大。但是如果属性缺失值较多或是关键属性值缺失,创建的决策树将是不完全的,同时可能给用户造成知识上的大量错误信息,所以抛弃缺失值一般不采用。只有在数据库具有极少量的缺失值同时缺失值不是关键的属性值时,且为了加快创建决策树的速度,才采用抛弃属性缺失值的方式创建决策树。</li><li>补充缺失值<br>缺失值较少时按照我们上面的补充规则是可行的。但如果数据库的数据较大,缺失值较多(当然,这样获取的数据库在现实中使用的意义已不大,同时在信息获取方面基本不会出现这样的数据库),这样根据填充后的数据库创建的决策树可能和根据正确值创建的决策树有很大变化。</li><li>概率化缺失值<br>对缺失值的样本赋予该属性所有属性值的概率分布,即将缺失值按照其所在属性已知值的相对概率分布来创建决策树。用系数F进行合理的修正计算的信息量,F=数据库中缺失值所在的属性值样本数量去掉缺失值样本数量/数据库中样本数量的总和,即F表示所给属性具有已知值样本的概率。</li></ul><h2 id="5-各种决策树之间的比较：ID3-C4-5-CART"><a href="#5-各种决策树之间的比较：ID3-C4-5-CART" class="headerlink" title="5. 各种决策树之间的比较：ID3/C4.5/CART"></a>5. 各种决策树之间的比较：ID3/C4.5/CART</h2><h3 id="5-1-ID3决策树"><a href="#5-1-ID3决策树" class="headerlink" title="5.1 ID3决策树"></a>5.1 ID3决策树</h3><p>信息熵是度量样本集合纯度最常用的一种指标。假设样本集合D中第k类样本所占的比重为pk，那么信息熵的计算则为下面的计算方式</p><script type="math/tex; mode=display">Entropy(t)=-\Sigma_{i=0}^{c-1}p(i|t)\log_2{p(i|t)}</script><p>当这个Entropy(D)的值越小，说明样本集合D的纯度就越高</p><p>有了信息熵，当我选择用样本的某一个属性a来划分样本集合D时，就可以得出用属性a对样本D进行划分所带来的“信息增益”</p><script type="math/tex; mode=display">\operatorname{Gain}(D, a)=\operatorname{Entropy}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Entropy}\left(D^{v}\right)</script><p>一般来讲，信息增益越大，说明如果用属性a来划分样本集合D，那么纯度会提升，因为我们分别对样本的所有属性计算增益情况，选择最大的来作为决策树的一个结点，或者可以说那些信息增益大的属性往往离根结点越近，因为我们会优先用能区分度大的也就是信息增益大的属性来进行划分。当一个属性已经作为划分的依据，在下面就不在参与竞选了，我们刚才说过根结点代表全部样本，而经过根结点下面属性各个取值后样本又可以按照相应属性值进行划分，并且在当前的样本下利用剩下的属性再次计算信息增益来进一步选择划分的结点，ID3决策树就是这样建立起来的。</p><h3 id="5-2-C4-5决策树"><a href="#5-2-C4-5决策树" class="headerlink" title="5.2 C4.5决策树"></a>5.2 C4.5决策树</h3><p>C4.5决策树的提出完全是为了解决ID3决策树的一个缺点，当一个属性的可取值数目较多时，那么可能在这个属性对应的可取值下的样本只有一个或者是很少个，那么这个时候它的信息增益是非常高的，这个时候纯度很高，ID3决策树会认为这个属性很适合划分，但是较多取值的属性来进行划分带来的问题是它的泛化能力比较弱，不能够对新样本进行有效的预测。</p><p>而C4.5决策树则不直接使用信息增益来作为划分样本的主要依据，而提出了另外一个概念，增益率</p><script type="math/tex; mode=display">Gain\_ratio=\frac{\Delta_{info}}{Split\_info}</script><script type="math/tex; mode=display">Split\_info(a)=-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \log _{2} \frac{\left|D^{v}\right|}{|D|}</script><p>但是同样的这个增益率对可取值数目较少的属性有所偏好，因此C4.5决策树先从候选划分属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的。</p><h3 id="5-3-CART决策树"><a href="#5-3-CART决策树" class="headerlink" title="5.3 CART决策树"></a>5.3 CART决策树</h3><p>CART决策树的全称为Classification and Regression Tree,可以应用于分类和回归。</p><p>采用基尼系数来划分属性</p><p>基尼值</p><script type="math/tex; mode=display">Gini(t)=1-\Sigma_{i=0}^{c-1}[p(i|t)]^2</script><p>基尼系数</p><script type="math/tex; mode=display">\operatorname{Gini\_index}(D, a)=\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Gini}\left(D^{v}\right)</script><p>因此在候选属性中选择基尼系数最小的属性作为最优划分属性。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-树的划分流程：原理与思想&quot;&gt;&lt;a href=&quot;#1-树的划分流程：原理与思想&quot; class=&quot;headerlink&quot; title=&quot;1. 树的划分流程：原理与思想&quot;&gt;&lt;/a&gt;1. 树的划分流程：原理与思想&lt;/h2&gt;&lt;h3 id=&quot;1-1-决策树的工作原理&quot;&gt;&lt;
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://superlova.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="学习笔记" scheme="https://superlova.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="https://superlova.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="决策树" scheme="https://superlova.github.io/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
      <category term="西瓜书" scheme="https://superlova.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>牛奶可乐经济学读书笔记（不定期更新）</title>
    <link href="https://superlova.github.io/2019/05/18/%E7%89%9B%E5%A5%B6%E5%8F%AF%E4%B9%90%E7%BB%8F%E6%B5%8E%E5%AD%A6%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%8D%E5%AE%9A%E6%9C%9F%E6%9B%B4%E6%96%B0%EF%BC%89/"/>
    <id>https://superlova.github.io/2019/05/18/%E7%89%9B%E5%A5%B6%E5%8F%AF%E4%B9%90%E7%BB%8F%E6%B5%8E%E5%AD%A6%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%8D%E5%AE%9A%E6%9C%9F%E6%9B%B4%E6%96%B0%EF%BC%89/</id>
    <published>2019-05-18T08:27:31.000Z</published>
    <updated>2020-03-17T01:54:50.798Z</updated>
    
    <content type="html"><![CDATA[<p>2019年5月16日</p><p>我对经济学比较感兴趣，毕竟我本身就是个穷人，我也希望能够高效利用我身上有限的资源。学习经济学能够对我的思维方式有很大改善。</p><p>我认为阅读一本能够给自己带来很多知识的书时，应当一步一步不要贪多。每次阅读都是美妙的经历，就好像在和读者对话一样。</p><p>这本《牛奶可乐经济学》一开始让我感到有点啰嗦，作者一直在讲他的授课经历，以及学生的反映。似乎欧美的书籍都会有这样的通病。我误以为这又是一本充满废话和鸡汤的科普书籍，便一页一页翻过去，翻到后来却感觉什么也没看进去。于是我从头沉下心来，一点一点阅读。</p><p>终于在睡前阅读完毕了第一章。我强忍着想要阅读第二章的好奇心，闭上眼睛总结学到的东西。我一直有这样的读书习惯。</p><p>对我来说，第一章教给我的并不是“沉没成本”，而是以下两点：</p><ol><li><p>学习一门知识，如果能把知识点穿成串，最好形成绘声绘色的故事，辅以生动的图画，那么这样的知识算是“易消化吸收”的知识。也就是说，知识叙述化、图像化，更利于知识的吸收（和分享）。由此可见，初学者就不要费力去找什么机器学习精华笔记、思维导图了同理，大牛们也别指望将自己的所谓极简笔记造福新手了，你的笔记越简短，越难被别人理解。如果在这个领域没有基础，根本不能被其他人吸收。</p></li><li><p>学习一个领域的知识，要靠不断地、创新性地应用于各种场景。重复是重要的，但是机械化的重复是没必要的，只有创新性的（即换一种形式）应用，才能代表你（现在）真正的掌握了。当然现在掌握了将来也要不断重复，要不就忘记了。</p></li></ol><p>突然对欧美的行文风格产生好感了:)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;2019年5月16日&lt;/p&gt;
&lt;p&gt;我对经济学比较感兴趣，毕竟我本身就是个穷人，我也希望能够高效利用我身上有限的资源。学习经济学能够对我的思维方式有很大改善。&lt;/p&gt;
&lt;p&gt;我认为阅读一本能够给自己带来很多知识的书时，应当一步一步不要贪多。每次阅读都是美妙的经历，就好像在和
      
    
    </summary>
    
    
      <category term="学习笔记" scheme="https://superlova.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="人文社科" scheme="https://superlova.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%BA%BA%E6%96%87%E7%A4%BE%E7%A7%91/"/>
    
    
      <category term="学习方法" scheme="https://superlova.github.io/tags/%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
      <category term="经济学" scheme="https://superlova.github.io/tags/%E7%BB%8F%E6%B5%8E%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>Python3中enumerate/zip等函数使用方法</title>
    <link href="https://superlova.github.io/2019/05/18/Python3%E4%B8%ADenumerate-zip%E7%AD%89%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>https://superlova.github.io/2019/05/18/Python3%E4%B8%ADenumerate-zip%E7%AD%89%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</id>
    <published>2019-05-18T07:24:47.000Z</published>
    <updated>2020-03-17T01:54:50.697Z</updated>
    
    <content type="html"><![CDATA[<p>最近正在研读《Python机器学习基础教程》（Introduction to Machine Learning with Python）这本书。书中的Python3代码、对于numpy、pandas、matplotlib以及scikit-learn库的使用都让人叹为观止。作为Python初学者，这本书不仅可以让人入门机器学习，更可以让人的Python技巧得到提升。</p><p>下面的代码使用sklearn自带数据集moon以及sklearn的随机森林模型构建由5棵树组成的随机森林，并利用matplotlib库可视化。</p><pre><code class="lang-python">import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport mglearn # 需要额外下载from sklearn.ensemble import RandomForestClassifierfrom sklearn.datasets import make_moonsfrom sklearn.model_selection import train_test_splitX, y = make_moons(n_samples=100, noise=0.25, random_state=3)X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,                                                     random_state=42)# 创建5棵树组成的随机森林forest = RandomForestClassifier(n_estimators=5, random_state=2)# 对训练集进行拟合forest.fit(X_train, y_train)# 生成两行三列的六张图，宽20高10fig, axes = plt.subplots(2, 3, figsize=(20, 10))</code></pre><p><code>forest.estimators_</code>是一个列表，保存五棵树的信息</p><pre><code class="lang-python">print(len(forest.estimators_))print(type(forest.estimators_[0]))5&lt;class &#39;sklearn.tree.tree.DecisionTreeClassifier&#39;&gt;</code></pre><p>下面的for遍历用法是我之前很少接触的，尤其是对于enumerate与zip的使用，在此记录下来。</p><pre><code class="lang-python">for i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):    ax.set_title(&quot;Tree {}&quot;.format(i))    mglearn.plots.plot_tree_partition(X_train, y_train, tree, ax=ax)mglearn.plots.plot_2d_separator(forest, X_train, fill=True, ax=axes[-1, -1],                               alpha=.4)axes[-1, -1].set_title(&quot;Random Forest&quot;)mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)</code></pre><p>首先zip函数就像大型相亲现场，接受两个可迭代集合：一波男性和一波女性，将前一个集合中的元素与后一个集合中的元素一一配对，返回两两结合的对象，即返回一大群元素组成的集合，集合中元素都是一男一女配对。配对方法就是粗暴的第i个男生-第i个女生，如果有男生or女生多了咋办？zip不管配不上对的元素，只挑选配对成功的组合。</p><p>经过zip函数处理，axes里面的六张图配上了五棵树，最后一张图我们留到最后处理。</p><p>接下来是enumerate，enumerate接受可迭代对象，不仅仅输出对象元素，还附带输出该元素所在的位置。enumerate本身就是“枚举”的意思嘛。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近正在研读《Python机器学习基础教程》（Introduction to Machine Learning with Python）这本书。书中的Python3代码、对于numpy、pandas、matplotlib以及scikit-learn库的使用都让人叹为观止。作
      
    
    </summary>
    
    
      <category term="编程语言" scheme="https://superlova.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
  </entry>
  
</feed>
