<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Superlova</title>
  
  <subtitle>Welcome...</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://superlova.github.io/"/>
  <updated>2020-08-22T15:43:49.737Z</updated>
  <id>https://superlova.github.io/</id>
  
  <author>
    <name>Superlova</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【竞赛打卡】数据挖掘实践——时间序列模型</title>
    <link href="https://superlova.github.io/2020/08/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/"/>
    <id>https://superlova.github.io/2020/08/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-08-22T15:40:04.000Z</published>
    <updated>2020-08-22T15:43:49.737Z</updated>
    
    <content type="html"><![CDATA[<p>不要停下来啊！<br><a id="more"></a><br>AR模型是自回归模型，AutoRegression的简称。是一种较为朴素的时间序列数据处理方法，利用同一变量的前n期来预测本期的变量的数值，并且假定为线性关系。这种分析方法中，自变量不是其他的影响因素，而是变量本身的历史数据，利用xt-n来预测xt，因此被称为自回归。</p><p>MA模型：移动平均模型将序列{xt}表示为白噪声的线性加权。</p><p>在一个平稳的随机过程中，如果既有自回归的特性，又有移动平均过程的特性，则需要对两个模型进行混合使用，也即是较为普遍的ARMA模型，一般记为ARMA(p,q)。</p><p>ARIMA(p,d,q)是差分自回归移动平均模型，是运用最为广泛的一种时间序列分析模型。p,q的意义不变，其中d的含义是将时间序列化为平稳时间序列所做的差分次数。</p><p>在建立时间序列的模型后，我们要对时间序列数据进行多重检验，以确定该数据符合我们的统计学上的分析准则。主要的检验要观察数据的自相关性，阶数识别和单位根检验（ADF检验）。对于ARMA模型来说，最难的步骤是进行阶数的识别。ADF检验是时间序列中最为重要的检验之一，帮助我们准确判断时间序列数据是否平稳，是为后期的Johansen 检验、Granger 检验等的基础。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不要停下来啊！&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="Data Mining" scheme="https://superlova.github.io/tags/Data-Mining/"/>
    
      <category term="Time Series Analysis" scheme="https://superlova.github.io/tags/Time-Series-Analysis/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】数据挖掘实践——时间序列规则</title>
    <link href="https://superlova.github.io/2020/08/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E8%A7%84%E5%88%99/"/>
    <id>https://superlova.github.io/2020/08/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E8%A7%84%E5%88%99/</id>
    <published>2020-08-22T15:38:15.000Z</published>
    <updated>2020-08-22T15:43:47.259Z</updated>
    
    <content type="html"><![CDATA[<p>不要停下来啊！<br><a id="more"></a></p><p>时间序列基本规则法<br>提取时间序列的周期性特征进行预测，参考：时间序列规则法快速入门<br>观察序列，当序列存在周期性时，可以用线性回归-利用时间特征做线性回归做为baseline</p><p>时间序列是指将同一统计指标的数值按其发生的时间先后顺序排列而成的数列。</p><p>时间序列分析的主要目的是根据已有的历史数据对未来进行预测。与面板数据不同，面板数据侧重于同一时间点不同样本的数值，而时间序列侧重于同一统计指标在时间的不同点的数值。时间序列有两个重要指标，一个是资料所属的时间，另一个是时间上的统计指标数值。时间序列可以描述社会经济现象在不同时间的发展状态和过程，也可以根据历史数据进行合理的未来推测。</p><p>一般地，我们认为一个随机游走的变量会服从正态分布。</p><p>提取时间的周期性特点做为特征，此时训练集每条样本为”时间特征-&gt;目标值”，时间序列的依赖关系被剔除，不需要严格依赖滑窗截取训练样本。常见是将时间用0-1哑变量表达，有以下若干种特征：</p><ul><li>将星期转化为了0-1变量，从周一至周天，独热编码共7个变量</li><li>将节假日转化为0-1变量，视具体节假日数目，可简单分为两类，”有假日”-“无假日”，独热编码共2个变量；或赋予不同编码值，如区分国庆、春节、劳动节等使用1、2、3表示</li><li>将月初转化为0-1变量，简单分两类表示为”是月初”-“非月初”，共2个特征</li><li>类似的月中、月初可以转化为0-1变量</li><li>控制时间粒度，区分是weekday or weekend</li></ul><p>按列提取中位数是一种简单而有效的提取周期因子的方法。中位数十分鲁棒，不受极端值的影响。但中位数损失了很多信息。实践中，可以在此基础上进一步优化。比如可以提取一个均值和一个中位数，然后将均值和中位数融合。融合的比例按照测试集的表现来确定。也可以根据与预测周的时间距离来赋予不同的权重。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不要停下来啊！&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="Data Mining" scheme="https://superlova.github.io/tags/Data-Mining/"/>
    
      <category term="Time Series Analysis" scheme="https://superlova.github.io/tags/Time-Series-Analysis/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】数据挖掘实践——数据探索和分析</title>
    <link href="https://superlova.github.io/2020/08/20/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%86%E6%9E%90/"/>
    <id>https://superlova.github.io/2020/08/20/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%86%E6%9E%90/</id>
    <published>2020-08-20T15:40:07.000Z</published>
    <updated>2020-08-20T15:49:38.181Z</updated>
    
    <content type="html"><![CDATA[<p>我也是加把劲骑士！<br><a id="more"></a></p><pre><code class="lang-py">file_names = glob.glob(&#39;Purchase Redemption Data/*.csv&#39;)file_names[&#39;Purchase Redemption Data/mfd_bank_shibor.csv&#39;, &#39;Purchase Redemption Data/comp_predict_table.csv&#39;, &#39;Purchase Redemption Data/user_profile_table.csv&#39;, &#39;Purchase Redemption Data/mfd_day_share_interest.csv&#39;, &#39;Purchase Redemption Data/user_balance_table.csv&#39;]</code></pre><h2 id="1-用户信息表-user-profile-table"><a href="#1-用户信息表-user-profile-table" class="headerlink" title="1. 用户信息表 user_profile_table"></a>1. 用户信息表 user_profile_table</h2><p>我们总共随机抽取了约 3 万用户。</p><p>其中部分用户在 2014 年 9 月份第一次出现，这部分用户只在测试数据中（真的太鉴了这个）。</p><p>因此用户信息表是约 2.8 万 个用户的基本数据，在原始数据的基础上处理后，主要包含了用户的性别、城市和星座。</p><p><img src="/2020/08/20/【竞赛打卡】数据挖掘实践——数据探索和分析/2020-08-20-23-43-55.png" srcset="/img/loading.gif" alt></p><h2 id="2-用户申购赎回数据表-user-balance-table"><a href="#2-用户申购赎回数据表-user-balance-table" class="headerlink" title="2. 用户申购赎回数据表 user_balance_table"></a>2. 用户申购赎回数据表 user_balance_table</h2><p>里面有 20130701 至 20140831 申购和赎回信息、以及所有的子类目信息。</p><p>数据经过脱敏处理。脱敏之后的数据，基本保持了原数据趋势。</p><p>数据主要包括用户操作时间和操作记录，其中操作记录包括申购和赎回两个部分。</p><p>金额的单位是分，即 0.01 元人民币。 如果用户今日消费总量为0，即consume_amt=0，则四个字类目为空。</p><div class="table-container"><table><thead><tr><th>字段名称</th><th>含义</th></tr></thead><tbody><tr><td>report_date</td><td>日期</td></tr><tr><td>tBalance</td><td>今日余额</td></tr><tr><td>yBalance</td><td>昨日余额</td></tr><tr><td>total_purchase_amt</td><td>今日总购买量 = 直接购买 + 收益</td></tr><tr><td>direct_purchase_amt</td><td>今日直接购买量</td></tr><tr><td>purchase_bal_amt</td><td>今日支付宝余额购买量</td></tr><tr><td>purchase_bank_amt</td><td>今日银行卡购买量</td></tr><tr><td>total_redeem_amt</td><td>今日总赎回量 = 消费 + 转出</td></tr><tr><td>consume_amt</td><td>今日消费总量</td></tr><tr><td>transfer_amt</td><td>今日转出总量</td></tr><tr><td>tftobal_amt</td><td>今日转出到支付宝余额总量</td></tr><tr><td>tftocard_amt</td><td>今日转出到银行卡总量</td></tr><tr><td>share_amt</td><td>今日收益</td></tr><tr><td>category1</td><td>今日类目 1 消费总额</td></tr><tr><td>category2</td><td>今日类目 2 消费总额</td></tr><tr><td>category3</td><td>今日类目 3 消费总额</td></tr><tr><td>category4</td><td>今日类目 4 消费总额</td></tr></tbody></table></div><p><img src="/2020/08/20/【竞赛打卡】数据挖掘实践——数据探索和分析/2020-08-20-23-47-13.png" srcset="/img/loading.gif" alt></p><p>注 1 ：上述的数据都是经过脱敏处理的，收益为重新计算得到的，计算方法按照简化后的计算方式处理，具体计算方式在下节余额宝收益计算方式中描述。</p><p>注 2 ：脱敏后的数据保证了今日余额 = 昨日余额 + 今日申购 - 今日赎回，不会出现负值。</p><h2 id="3-收益率表-mfd-day-share-interest"><a href="#3-收益率表-mfd-day-share-interest" class="headerlink" title="3. 收益率表 mfd_day_share_interest"></a>3. 收益率表 mfd_day_share_interest</h2><p>收益表为余额宝在 14 个月内的收益率表 。</p><p><img src="/2020/08/20/【竞赛打卡】数据挖掘实践——数据探索和分析/2020-08-20-23-47-55.png" srcset="/img/loading.gif" alt></p><div class="table-container"><table><thead><tr><th>列名</th><th>含义</th></tr></thead><tbody><tr><td>mfd_date</td><td>日期</td></tr><tr><td>mfd_daily_yield</td><td>万份收益，即 1 万块钱的收益。</td></tr><tr><td>mfd_7daily_yield</td><td>七日年化收益率（ % ）</td></tr></tbody></table></div><h2 id="4-上海银行间同业拆放利率（Shibor）表-df-mfd-bank-shibor"><a href="#4-上海银行间同业拆放利率（Shibor）表-df-mfd-bank-shibor" class="headerlink" title="4. 上海银行间同业拆放利率（Shibor）表 df_mfd_bank_shibor"></a>4. 上海银行间同业拆放利率（Shibor）表 df_mfd_bank_shibor</h2><p>银行间拆借利率表是 14 个月期间银行之间的拆借利率（皆为年化利率）</p><p><img src="/2020/08/20/【竞赛打卡】数据挖掘实践——数据探索和分析/2020-08-20-23-48-31.png" srcset="/img/loading.gif" alt></p><div class="table-container"><table><thead><tr><th>列名</th><th>类型</th><th>含义</th><th>示例</th></tr></thead><tbody><tr><td>mfd_date</td><td>String</td><td>日期</td><td>20140102</td></tr><tr><td>Interest_O_N</td><td>Double</td><td>隔夜利率（%）</td><td>2.8</td></tr><tr><td>Interest_1_W</td><td>Double</td><td>1周利率（%）</td><td>4.25</td></tr><tr><td>Interest_2_W</td><td>Double</td><td>2周利率（%）</td><td>4.9</td></tr><tr><td>Interest_1_M</td><td>Double</td><td>1个月利率（%）</td><td>5.04</td></tr><tr><td>Interest_3_M</td><td>Double</td><td>3个月利率（%）</td><td>4.91</td></tr><tr><td>Interest_6_M</td><td>Double</td><td>6个月利率（%）</td><td>4.79</td></tr><tr><td>Interest_9_M</td><td>Double</td><td>9个月利率（%）</td><td>4.76</td></tr><tr><td>Interest_1_Y</td><td>Double</td><td>1年利率（%）</td><td>4.78</td></tr></tbody></table></div><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><h2 id="收益计算方式"><a href="#收益计算方式" class="headerlink" title="收益计算方式"></a>收益计算方式</h2><p>本赛题的余额宝收益方式，主要基于实际余额宝收益计算方法，但是进行了一定的简化，此处计算简化的地方如下：</p><p>首先，收益计算的时间不再是会计日，而是自然日，以 0 点为分隔，如果是 0 点之前转入或者转出的金额算作昨天的，如果是 0 点以后转入或者转出的金额则算作今天的。</p><p>然后，收益的显示时间，即实际将第一份收益打入用户账户的时间为如下表格，以周一转入周三显示为例，如果用户在周一存入 10000 元，即 1000000 分，那么这笔金额是周一确认，周二是开始产生收益，用户的余额还是 10000 元，在周三将周二产生的收益打入到用户的账户中，此时用户的账户中显示的是 10001.1 元，即 1000110 分。其他时间的计算按照表格中的时间来计算得到。</p><div class="table-container"><table><thead><tr><th>转入时间</th><th>首次显示收益时间</th></tr></thead><tbody><tr><td>周一</td><td>周三</td></tr><tr><td>周二</td><td>周四</td></tr><tr><td>周三</td><td>周五</td></tr><tr><td>周四</td><td>周六</td></tr><tr><td>周五</td><td>下周二</td></tr><tr><td>周六</td><td>下周三</td></tr><tr><td>周天</td><td>下周三</td></tr></tbody></table></div><h2 id="提交格式"><a href="#提交格式" class="headerlink" title="提交格式"></a>提交格式</h2><div class="table-container"><table><thead><tr><th>字段</th><th>类型</th><th>含义</th><th>示例</th></tr></thead><tbody><tr><td>report_date</td><td>bigint</td><td>日期</td><td>20140901</td></tr><tr><td>purchase</td><td>bigint</td><td>申购总额</td><td>40000000</td></tr><tr><td>redeem</td><td>bigint</td><td>赎回总额</td><td>30000000</td></tr></tbody></table></div><p><img src="/2020/08/20/【竞赛打卡】数据挖掘实践——数据探索和分析/2020-08-20-23-49-17.png" srcset="/img/loading.gif" alt></p><p>每一行数据是一天对申购、赎回总额的预测值， 2014 年 9 月每天一行数据，共 30 行数据。 Purchase 和 redeem 都是金额数据，精确到分，而不是精确到元。</p><p>评分数据格式要求与“选手结果数据样例文件”一致，结果表命名为：tc_comp_predict_table， 字段之间以逗号为分隔符</p><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><p>评估指标的设计主要期望选手对未来 30 天内每一天申购和赎回的总量数据预测的越准越好，同时考虑到可能存在的多种情况。</p><p>譬如有些选手在 30 天中 29 天预测都是非常精准的但是某一天预测的结果可能误差很大，而有些选手在 30 天中每天的预测都不是很精准误差较大，如果采用绝对误差则可能导致前者的成绩比后者差，而在实际业务中可能更倾向于前者。</p><p>所以最终选用积分式的计算方法：每天的误差选用相对误差来计算，然后根据用户预测申购和赎回的相对误差，通过得分函数映射得到一个每天预测结果的得分，将 30 天内的得分汇总，然后结合实际业务的倾向，对申购赎回总量预测的得分情况进行加权求和，得到最终评分。具体的操作如下：</p><p>1) 计算所有用户在测试集上每天的申购及赎回总额与实际情况总额的误差。</p><p><img src="https://gtms02.alicdn.com/tps/i2/TB1UMhaHVXXXXbWXpXXmP_sPXXX-372-168.png" srcset="/img/loading.gif" alt></p><p>2) 申购预测得分与 Purchasei 相关，赎回预测得分与 Redeemi 相关 , 误差与得分之间的计算公式不公布，但保证该计算公式为单调递减的，即误差越小，得分越高，误差与大，得分越低。当第 i 天的申购误差 Purchasei =0 ，这一天的得分为 10 分；当 Purchasei &gt; 0.3 ，其得分为 0 。</p><p>3) 最后公布总积分 = 申购预测得分 <em>45%+ 赎回预测得分 </em>55% 。 </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我也是加把劲骑士！&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="Data Mining" scheme="https://superlova.github.io/tags/Data-Mining/"/>
    
      <category term="Time Series Analysis" scheme="https://superlova.github.io/tags/Time-Series-Analysis/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】Fuzzing学习笔记3——灰盒Fuzzing</title>
    <link href="https://superlova.github.io/2020/08/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03%E2%80%94%E2%80%94%E7%81%B0%E7%9B%92Fuzzing/"/>
    <id>https://superlova.github.io/2020/08/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03%E2%80%94%E2%80%94%E7%81%B0%E7%9B%92Fuzzing/</id>
    <published>2020-08-19T17:01:27.000Z</published>
    <updated>2020-08-19T17:04:57.391Z</updated>
    
    <content type="html"><![CDATA[<p>灰盒变异的模糊测试。<br><a id="more"></a></p><p>普通fuzzing：从0开始构造测试用例<br>突变fuzzing：从seed开始构造测试用例<br>灰盒fuzzing：有引导的突变fuzzing</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>上回书说到，AFL是突变fuzzer，通过把种子字符串进行些微修改，得到变异体；此外AFL还会将一个种子的前半部分与另一个种子的后半部分连接，形成变异体。</p><p>AFL也是个灰盒fuzzer，这是由于AFL需要使用程序内部信息（即覆盖率）。AFL不是白盒，因为AFL没有对程序进行约束求解、程序分析之类的，只是简单获取了一个覆盖率。如果生成的样本能够提升覆盖率，那么就将这个样本添加进种子队列以供下次突变使用（这就意味着突变体有重复突变的可能）。</p><p>AFL计算覆盖率的方法，是通过在每个分支的跳转指令后执行一段标记代码。这样就可以做到，监控每个输入导致的激活分支，以及每个分支被激活的大概频率。注入代码这个环节通常在编译时完成。对于Python，可以在未经处理的情况下执行覆盖率信息收集。</p><h2 id="突变算法和种子"><a href="#突变算法和种子" class="headerlink" title="突变算法和种子"></a>突变算法和种子</h2><p>引入Mutator类。Mutator类是个过程类，包装了对输入inp的突变方法。</p><pre><code class="lang-py">class Mutator(object):    def __init__(self):        self.mutators = [            self.delete_random_character,            self.insert_random_character,            self.flip_random_character    def insert_random_character(self,s):        &quot;&quot;&quot;Returns s with a random character inserted&quot;&quot;&quot;        pos = random.randint(0, len(s))        random_character = chr(random.randrange(32, 127))        return s[:pos] + random_character + s[pos:]    def delete_random_character(self,s):        &quot;&quot;&quot;Returns s with a random character deleted&quot;&quot;&quot;        if s == &quot;&quot;:            return self.insert_random_character(s)        pos = random.randint(0, len(s) - 1)        return s[:pos] + s[pos + 1:]    def flip_random_character(self,s):        &quot;&quot;&quot;Returns s with a random bit flipped in a random position&quot;&quot;&quot;        if s == &quot;&quot;:            return self.insert_random_character(s)        pos = random.randint(0, len(s) - 1)        c = s[pos]        bit = 1 &lt;&lt; random.randint(0, 6)        new_c = chr(ord(c) ^ bit)        return s[:pos] + new_c + s[pos + 1:]    def mutate(self, inp):        &quot;&quot;&quot;Return s with a random mutation applied&quot;&quot;&quot;        mutator = random.choice(self.mutators)        return mutator(inp)</code></pre><p>使用Mutator是只需实例化Mutator，然后调用mutate()方法即可。</p><pre><code class="lang-py">Mutator().mutate(&quot;good&quot;)&#39;cood&#39;</code></pre><h2 id="精力分配（Power-Schedule）"><a href="#精力分配（Power-Schedule）" class="headerlink" title="精力分配（Power Schedule）"></a>精力分配（Power Schedule）</h2><p>模糊测试是一种执行很慢的测试方法。既然并不是每个测试用例种子都值得分配同样的精力，那么试图发现那些更令人感兴趣的种子就是理所当然的选择了。</p><p>我们把一个种子从种群中被选中的可能性称为种子的能量（energy）。我们希望优先突变和执行那些更有希望发现待测程序错误的种子，不希望在无进步的种子身上浪费精力。</p><p>决定种子能量分配的算法称为“功率表”（Power Schedule）。AFL的功率表会将更多的能量分配给那些长度较短、执行速度较快、覆盖率增加较多的种子。</p><p>由此，每个种子需要额外维护其能量。构建Seed类如下：</p><pre><code class="lang-py">class Seed(object):        def __init__(self, data):        &quot;&quot;&quot;Set seed data&quot;&quot;&quot;        self.data = data    def __str__(self):        &quot;&quot;&quot;Returns data as string representation of the seed&quot;&quot;&quot;        return self.data    __repr__ = __str__</code></pre><p>下面是功率表PowerSchedule类的定义：</p><pre><code class="lang-py">class PowerSchedule(object):        def assignEnergy(self, population):        &quot;&quot;&quot;Assigns each seed the same energy&quot;&quot;&quot;        for seed in population:            seed.energy = 1    def normalizedEnergy(self, population):        &quot;&quot;&quot;Normalize energy&quot;&quot;&quot;        energy = list(map(lambda seed: seed.energy, population))        sum_energy = sum(energy)  # Add up all values in energy        norm_energy = list(map(lambda nrg: nrg/sum_energy, energy))        return norm_energy    def choose(self, population):        &quot;&quot;&quot;Choose weighted by normalized energy.&quot;&quot;&quot;        import numpy as np        self.assignEnergy(population)        norm_energy = self.normalizedEnergy(population)        seed = np.random.choice(population, p=norm_energy)        return seed</code></pre><h2 id="灰盒fuzzing与黑盒fuzzing的比较"><a href="#灰盒fuzzing与黑盒fuzzing的比较" class="headerlink" title="灰盒fuzzing与黑盒fuzzing的比较"></a>灰盒fuzzing与黑盒fuzzing的比较</h2><p>首先定义不使用coverage的黑盒fuzzer，MutationFuzzer 类：</p><pre><code class="lang-py">class MutationFuzzer(Fuzzer):    def __init__(self, seeds, mutator, schedule):        self.seeds = seeds        self.mutator = mutator        self.schedule = schedule        self.inputs = []        self.reset()    def reset(self):        &quot;&quot;&quot;Reset the initial population and seed index&quot;&quot;&quot;        self.population = list(map(lambda x: Seed(x), self.seeds))        self.seed_index = 0    def create_candidate(self):        &quot;&quot;&quot;Returns an input generated by fuzzing a seed in the population&quot;&quot;&quot;        seed = self.schedule.choose(self.population)        # Stacking: Apply multiple mutations to generate the candidate        candidate = seed.data        trials = min(len(candidate), 1 &lt;&lt; random.randint(1,5))        for i in range(trials):            candidate = self.mutator.mutate(candidate)        return candidate    def fuzz(self):        &quot;&quot;&quot;Returns first each seed once and then generates new inputs&quot;&quot;&quot;        if self.seed_index &lt; len(self.seeds):            # Still seeding            self.inp = self.seeds[self.seed_index]            self.seed_index += 1        else:            # Mutating            self.inp = self.create_candidate()        self.inputs.append(self.inp)        return self.inp</code></pre><p>MutationFuzzer 是由一组初始种子、一个突变器和一个功率表构成的。在整个模糊化过程中，它维护着一个名为population的种子语料库。create_candidate对某个种子执行多次突变，fuzz先试图返回正常种子，随后返回突变种子。</p><p>population_coverage 是预先定义好的覆盖率计算库，返回（all_coverage，cumulative_coverage）。其中all_coverage是所有输入所覆盖的语句集，cumulative_coverage是随着执行输入数量的增加而覆盖的语句数量。</p><p>下面是GrayBox fuzzing的实现：</p><pre><code class="lang-py">class GreyboxFuzzer(MutationFuzzer):        def reset(self):        &quot;&quot;&quot;Reset the initial population, seed index, coverage information&quot;&quot;&quot;        super().reset()        self.coverages_seen = set()        self.population = [] # population is filled during greybox fuzzing    def run(self, runner):        &quot;&quot;&quot;Run function(inp) while tracking coverage.           If we reach new coverage,           add inp to population and its coverage to population_coverage        &quot;&quot;&quot;        result, outcome = super().run(runner)        new_coverage = frozenset(runner.coverage())        if new_coverage not in self.coverages_seen:            # We have new coverage            seed = Seed(self.inp)            seed.coverage = runner.coverage()            self.coverages_seen.add(new_coverage)            self.population.append(seed)        return (result, outcome)</code></pre><p>经过计算，分别得到覆盖率变化趋势blackbox_coverage和greybox_coverage，可视化如下：</p><p><img src="/2020/08/20/【学习笔记】Fuzzing学习笔记3——灰盒Fuzzing/2020-08-20-00-42-27.png" srcset="/img/loading.gif" alt></p><p>可以看到，灰盒fuzzing的覆盖率增长明显比黑盒要好。</p><h2 id="增强后的灰盒fuzzer"><a href="#增强后的灰盒fuzzer" class="headerlink" title="增强后的灰盒fuzzer"></a>增强后的灰盒fuzzer</h2><p>通过修改功率表PowerSchedule，使那些能激活不寻常的path的input具有更高的energy。不寻常的path指的是激活次数比较小。</p><p>有多种方法计算一个种子的能量。上述的要求形式化为具体定义即为</p><script type="math/tex; mode=display">e(s)=\frac{1}{f(p(s))^a}</script><p>其中$s$是种子<br>$p(s)$为$s$激活的path<br>$f(p)$返回path激活的次数<br>$a$是给定的超参数<br>$e(s)$是种子$s$被分配的能量</p><p>下面是按照此思想设置的PowerSchedule：</p><pre><code class="lang-py">class AFLFastSchedule(PowerSchedule):     def __init__(self, exponent):        self.exponent = exponent    def assignEnergy(self, population):        &quot;&quot;&quot;Assign exponential energy inversely proportional to path frequency&quot;&quot;&quot;        for seed in population:            seed.energy = 1 / (self.path_frequency[getPathID(seed.coverage)] ** self.exponent)</code></pre><p>改进的灰盒Fuzzer：</p><pre><code class="lang-py">class CountingGreyboxFuzzer(GreyboxFuzzer):    def reset(self):        &quot;&quot;&quot;Reset path frequency&quot;&quot;&quot;        super().reset()        self.schedule.path_frequency = {}    def run(self, runner):        &quot;&quot;&quot;Inform scheduler about path frequency&quot;&quot;&quot;        result, outcome = super().run(runner)        path_id = getPathID(runner.coverage())        if not path_id in self.schedule.path_frequency:            self.schedule.path_frequency[path_id] = 1        else:            self.schedule.path_frequency[path_id] += 1        return(result, outcome)</code></pre><p>覆盖率变化如图所示</p><p><img src="/2020/08/20/【学习笔记】Fuzzing学习笔记3——灰盒Fuzzing/2020-08-20-00-52-08.png" srcset="/img/loading.gif" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>三篇Fuzzing文章到此为止。</p><p>Fuzzing 的方法是通过大量生成input，来找出被测程序的错误的方法。</p><p>Fuzzing的关键点之一在于input生成方法，其二在于input的排序方法，其三在于软件内部信息的获取和应用。</p><p>如果input完全是自己构建的，那么这种方法称之为generational fuzzing</p><p>如果input是通过原始种子略微修改后得到的，那么这种fuzzing为Mutational fuzzing。</p><p>如果Mutator可以经过一定的程序信息的引导，那么这叫做GrayBox Fuzzing，比如覆盖率引导的模糊测试</p><p>如果Seed经过Power Schedule的精力分配，随后Mutator根据Seed的精力大小排序，那么这种方法称之为Boosted GrayBox Fuzzing</p><p>使用到的类：</p><p>Runner：待测程序的基类</p><p>Fuzzer：模糊测试器的基类</p><p>Seed：测试用例种子的基类</p><p>PowerSchedule：功率表的基类</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;灰盒变异的模糊测试。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Fuzzing" scheme="https://superlova.github.io/tags/Fuzzing/"/>
    
      <category term="Testing" scheme="https://superlova.github.io/tags/Testing/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】Fuzzing学习笔记2——基于变异的Fuzzing</title>
    <link href="https://superlova.github.io/2020/08/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E5%8F%98%E5%BC%82%E7%9A%84Fuzzing/"/>
    <id>https://superlova.github.io/2020/08/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E5%8F%98%E5%BC%82%E7%9A%84Fuzzing/</id>
    <published>2020-08-19T15:48:14.000Z</published>
    <updated>2020-08-19T15:50:49.438Z</updated>
    
    <content type="html"><![CDATA[<p>基于变异的模糊测试。<br><a id="more"></a></p><p><a href="https://www.fuzzingbook.org/html/MutationFuzzer.html" target="_blank" rel="noopener">https://www.fuzzingbook.org/html/MutationFuzzer.html</a></p><p>大多数随机生成的输入在语法上都是无效的，程序很快就会检测并拒绝这些输入，这样便达不到深入测试Runner内部的目的。因此我们必须试图生成有效的输入。</p><p>本节我们介绍Mutation Fuzzing，这种基于变异的方法对现有输入进行小的更改，这些更改可能仍使输入保持有效，但仍会表现出新的行为。 </p><p>要对字符串形式的输入进行变异（Mutate），具体来说，就是执行随机插入字符、删除字符、修改字符等操作。Mutational fuzzing的特点是基于一个有效的原始输入，与之前凭空捏造出来一个input的generational fuzzing不同。</p><p>随机删除</p><pre><code class="lang-py">def delete_random_character(s):    &quot;&quot;&quot;Returns s with a random character deleted&quot;&quot;&quot;    if s == &quot;&quot;:        return s    pos = random.randint(0, len(s) - 1)    # print(&quot;Deleting&quot;, repr(s[pos]), &quot;at&quot;, pos)    return s[:pos] + s[pos + 1:]seed_input = &quot;A quick brown fox&quot;for i in range(10):    x = delete_random_character(seed_input)    print(repr(x))&#39;A uick brown fox&#39;&#39;A quic brown fox&#39;&#39;A quick brown fo&#39;&#39;A quic brown fox&#39;&#39;A quick bown fox&#39;&#39;A quick bown fox&#39;&#39;A quick brown fx&#39;&#39;A quick brown ox&#39;&#39;A quick brow fox&#39;&#39;A quic brown fox&#39;</code></pre><p>随机插入</p><pre><code class="lang-py">def insert_random_character(s):    &quot;&quot;&quot;Returns s with a random character inserted&quot;&quot;&quot;    pos = random.randint(0, len(s))    random_character = chr(random.randrange(32, 127))    # print(&quot;Inserting&quot;, repr(random_character), &quot;at&quot;, pos)    return s[:pos] + random_character + s[pos:]for i in range(10):    print(repr(insert_random_character(seed_input)))&#39;A quick brvown fox&#39;&#39;A quwick brown fox&#39;&#39;A qBuick brown fox&#39;&#39;A quick broSwn fox&#39;&#39;A quick brown fvox&#39;&#39;A quick brown 3fox&#39;&#39;A quick brNown fox&#39;&#39;A quick brow4n fox&#39;&#39;A quick brown fox8&#39;&#39;A equick brown fox&#39;</code></pre><p>随机替换</p><pre><code class="lang-py">def flip_random_character(s):    &quot;&quot;&quot;Returns s with a random bit flipped in a random position&quot;&quot;&quot;    if s == &quot;&quot;:        return s    pos = random.randint(0, len(s) - 1)    c = s[pos]    bit = 1 &lt;&lt; random.randint(0, 6)    new_c = chr(ord(c) ^ bit)    # print(&quot;Flipping&quot;, bit, &quot;in&quot;, repr(c) + &quot;, giving&quot;, repr(new_c))    return s[:pos] + new_c + s[pos + 1:]for i in range(10):    print(repr(flip_random_character(seed_input)))&#39;A quick bRown fox&#39;&#39;A quici brown fox&#39;&#39;A&quot;quick brown fox&#39;&#39;A quick brown$fox&#39;&#39;A quick bpown fox&#39;&#39;A quick brown!fox&#39;&#39;A 1uick brown fox&#39;&#39;@ quick brown fox&#39;&#39;A quic+ brown fox&#39;&#39;A quick bsown fox&#39;</code></pre><p>只要我们有一些原始输入，这些输入是有效的，那么我们基于原始输入的变异也应该是有效的。</p><p>多重变异</p><p>假设我们这里有个方法mutate()，能对字符串执行变异操作。那么连续变异50次，输入会变成什么样子？</p><pre><code class="lang-py">seed_input = &quot;http://www.google.com/search?q=fuzzing&quot;mutations = 50inp = seed_inputfor i in range(mutations):    if i % 5 == 0:        print(i, &quot;mutations:&quot;, repr(inp))    inp = mutate(inp)0 mutations: &#39;http://www.google.com/search?q=fuzzing&#39;5 mutations: &#39;http:/L/www.googlej.com/seaRchq=fuz:ing&#39;10 mutations: &#39;http:/L/www.ggoWglej.com/seaRchqfu:in&#39;15 mutations: &#39;http:/L/wwggoWglej.com/seaR3hqf,u:in&#39;20 mutations: &#39;htt://wwggoVgle&quot;j.som/seaR3hqf,u:in&#39;25 mutations: &#39;htt://fwggoVgle&quot;j.som/eaRd3hqf,u^:in&#39;30 mutations: &#39;htv://&gt;fwggoVgle&quot;j.qom/ea0Rd3hqf,u^:i&#39;35 mutations: &#39;htv://&gt;fwggozVle&quot;Bj.qom/eapRd[3hqf,u^:i&#39;40 mutations: &#39;htv://&gt;fwgeo6zTle&quot;Bj.\&#39;qom/eapRd[3hqf,tu^:i&#39;45 mutations: &#39;htv://&gt;fwgeo]6zTle&quot;BjM.\&#39;qom/eaR[3hqf,tu^:i&#39;</code></pre><p>可以看到变异体已经几乎无法识别了。我们通过多次变异，获得了更加多样的输入。</p><p>MutationFuzzer的实现</p><pre><code class="lang-py">class MutationFuzzer(Fuzzer):    def __init__(self, seed, min_mutations=2, max_mutations=10):        self.seed = seed        self.min_mutations = min_mutations        self.max_mutations = max_mutations        self.reset()    def reset(self):        self.population = self.seed        self.seed_index = 0    def mutate(self, inp):        return mutate(inp)    def create_candidate(self):        candidate = random.choice(self.population)        trials = random.randint(self.min_mutations, self.max_mutations)        for i in range(trials):            candidate = self.mutate(candidate)        return candidate    def fuzz(self):        if self.seed_index &lt; len(self.seed):            # Still seeding            self.inp = self.seed[self.seed_index]            self.seed_index += 1        else:            # Mutating            self.inp = self.create_candidate()        return self.inp</code></pre><p><code>create_candidate()</code>随机选取种子<code>candidate</code>，然后将这个种子随机突变<code>trials</code>次，返回经过多次突变的<code>candidate</code>。</p><p><code>fuzz()</code>方法一开始返回的是未经突变的种子样本，当种子挑选完毕后，返回突变样本。这样可以确保每次调用fuzz()，得到的输出是不一样的。</p><p>Mutational Fuzzing成功的关键在于引导这些突变的方法—即保留那些特别有价值的样本。</p><p>覆盖率引导</p><p>我们可以利用被测程序来引导测试用例生成。以前我们只是收集程序执行成功或者失败的信息，现在我们可以收集多点信息，比如运行时代码覆盖率。</p><p>利用覆盖率引导变异的Fuzzing，最成功的实践是<a href="http://lcamtuf.coredump.cx/afl/" target="_blank" rel="noopener">American fuzzy loop</a>，即AFL。</p><p>AFL会生成“成功”的测试用例。AFL认为，所谓“成功”是指找到了一条新的程序执行路径。AFL不断地突变新路径的输入，如果产生了新的路径，输入会保留下来。</p><p>为了获得程序运行时的覆盖率信息，我们需要重新定义Runner。FunctionRunner类负责包装一个被测函数。</p><pre><code class="lang-py">class FunctionRunner(Runner):    def __init__(self, function):        &quot;&quot;&quot;Initialize.  `function` is a function to be executed&quot;&quot;&quot;        self.function = function    def run_function(self, inp):        return self.function(inp)    def run(self, inp):        try:            result = self.run_function(inp)            outcome = self.PASS        except Exception:            result = None            outcome = self.FAIL        return result, outcome</code></pre><p>而FunctionCoverageRunner在此基础上增加了覆盖率计算模块<code>Coverage</code>。</p><pre><code class="lang-py">class FunctionCoverageRunner(FunctionRunner):    def run_function(self, inp):        with Coverage() as cov:            try:                result = super().run_function(inp)            except Exception as exc:                self._coverage = cov.coverage()                raise exc        self._coverage = cov.coverage()        return result    def coverage(self):        return self._coverage</code></pre><p>下面改写Fuzzer类。</p><pre><code class="lang-py">class MutationCoverageFuzzer(MutationFuzzer):    def reset(self):        super().reset()        self.coverages_seen = set()        # Now empty; we fill this with seed in the first fuzz runs        self.population = []    def run(self, runner):        &quot;&quot;&quot;Run function(inp) while tracking coverage.           If we reach new coverage,           add inp to population and its coverage to population_coverage        &quot;&quot;&quot;        result, outcome = super().run(runner)        new_coverage = frozenset(runner.coverage())        if outcome == Runner.PASS and new_coverage not in self.coverages_seen:            # We have new coverage            self.population.append(self.inp)            self.coverages_seen.add(new_coverage)        return result</code></pre><p><code>MutationCoverageFuzzer</code>内部保存测试用例队列<code>population</code>和覆盖率队列<code>coverages_seen</code>。如果fuzz的input产生了新的coverage，则将该input添加到population中，并将该coverage添加到coverage_seen中。</p><p>由此，我们得到的population中的每个input都能够使得程序产生不同的coverage，这背后可能是程序的不同执行路径，也就增加了inputs的多样性。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;基于变异的模糊测试。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Fuzzing" scheme="https://superlova.github.io/tags/Fuzzing/"/>
    
      <category term="Testing" scheme="https://superlova.github.io/tags/Testing/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】Fuzzing学习笔记1——认识Fuzzing的基本单元</title>
    <link href="https://superlova.github.io/2020/08/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E8%AE%A4%E8%AF%86Fuzzing%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8D%95%E5%85%83/"/>
    <id>https://superlova.github.io/2020/08/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E8%AE%A4%E8%AF%86Fuzzing%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8D%95%E5%85%83/</id>
    <published>2020-08-19T12:10:10.000Z</published>
    <updated>2020-08-19T15:50:15.044Z</updated>
    
    <content type="html"><![CDATA[<p>Fuzzing又称模糊测试。<br><a id="more"></a></p><p><a href="https://www.fuzzingbook.org/html/Fuzzer.html" target="_blank" rel="noopener">https://www.fuzzingbook.org/html/Fuzzer.html</a></p><h2 id="Fuzzing测试是什么？"><a href="#Fuzzing测试是什么？" class="headerlink" title="Fuzzing测试是什么？"></a>Fuzzing测试是什么？</h2><blockquote><p>Create random inputs, and see if they break things.</p></blockquote><p>说得简单、纯粹点，Fuzzing是一种软件测试方法，通过不断生成不同的输入，使被测程序崩溃、出错，以此改进程序本身的软件测试方法。由于Fuzzing的核心在于生成软件测试用例，因此这种方法又被称为<strong>生成软件测试</strong>。</p><h2 id="构建第一个fuzzer"><a href="#构建第一个fuzzer" class="headerlink" title="构建第一个fuzzer"></a>构建第一个fuzzer</h2><pre><code class="lang-py">import randomdef fuzzer(max_length=100, char_start=32, char_range=32):    &quot;&quot;&quot;A string of up to `max_length` characters       in the range [`char_start`, `char_start` + `char_range`]&quot;&quot;&quot;    string_length = random.randrange(0, max_length + 1)    out = &quot;&quot;    for i in range(0, string_length):        out += chr(random.randrange(char_start, char_start + char_range))    return outfuzzer()4$)&gt;(,-&amp;!$25;&gt;6=27= 5)9?300(.466&amp;(&#39;$*,,1:8&#39; ,$/99&gt;&#39;*=(</code></pre><p>此fuzzer的作用是生成一堆随机字符。要想只生成26个字母，那么</p><pre><code class="lang-py">fuzzer(100, ord(&#39;a&#39;), 26)ueffzgwltwmspvmowihhtjmgsixofnvntnqmr</code></pre><h2 id="Fuzzing关键的两个部件："><a href="#Fuzzing关键的两个部件：" class="headerlink" title="Fuzzing关键的两个部件："></a>Fuzzing关键的两个部件：</h2><h3 id="Fuzzer类"><a href="#Fuzzer类" class="headerlink" title="Fuzzer类"></a>Fuzzer类</h3><p><code>Fuzzer</code>，是所有<code>fuzzer</code>的基类，<code>RandomFuzzer</code> 是其简单实现。<code>Fuzzer</code> 的<code>fuzz()</code>接口返回一个字符串，字符串内容是根据不同实现逻辑而构造出来的。</p><p>比如Fuzzer的实现RandomFuzzer，其<code>fuzz()</code>就是随机生成的字符串。</p><pre><code class="lang-py">&gt;&gt;&gt; random_fuzzer = RandomFuzzer(min_length=10, max_length=20, char_start=65, char_range=26)&gt;&gt;&gt; random_fuzzer.fuzz()&#39;XGZVDDPZOOW&#39;</code></pre><p>Fuzzer的run()接口负责运行一个Runner对象。</p><p>下面是Fuzzer的代码架构：</p><pre><code class="lang-py">class Fuzzer(object):    def __init__(self):        pass    def fuzz(self):        &quot;&quot;&quot;Return fuzz input&quot;&quot;&quot;        return &quot;&quot;    def run(self, runner=Runner()):        &quot;&quot;&quot;Run `runner` with fuzz input&quot;&quot;&quot;        return runner.run(self.fuzz())    def runs(self, runner=PrintRunner(), trials=10):        &quot;&quot;&quot;Run `runner` with fuzz input, `trials` times&quot;&quot;&quot;        # Note: the list comprehension below does not invoke self.run() for subclasses        # return [self.run(runner) for i in range(trials)]        outcomes = []        for i in range(trials):            outcomes.append(self.run(runner))        return outcomes</code></pre><p>此时Fuzzer基类的fuzz()接口还没有功能。派生类RandomFuzzer则实现了fuzz()：</p><pre><code class="lang-py">class RandomFuzzer(Fuzzer):    def __init__(self, min_length=10, max_length=100,                 char_start=32, char_range=32):        &quot;&quot;&quot;Produce strings of `min_length` to `max_length` characters           in the range [`char_start`, `char_start` + `char_range`]&quot;&quot;&quot;        self.min_length = min_length        self.max_length = max_length        self.char_start = char_start        self.char_range = char_range    def fuzz(self):        string_length = random.randrange(self.min_length, self.max_length + 1)        out = &quot;&quot;        for i in range(0, string_length):            out += chr(random.randrange(self.char_start,                                        self.char_start + self.char_range))        return out</code></pre><p>有了RandomFuzzer，我们可以生成一些随机的字符串了。</p><pre><code class="lang-py">random_fuzzer = RandomFuzzer(min_length=20, max_length=20)for i in range(10):    print(random_fuzzer.fuzz())&#39;&gt;23&gt;33)(&amp;&quot;09.377.*3*+:5 ? (?1$4&lt;&gt;!?3&gt;.&#39;4+3/(3 (0%!&gt;!(+9%,#$/51$2964&gt;;)2417&lt;9&quot;2&amp;907.. !7:&amp;--&quot;=$7&#39;,7*(5=5&#39;.!*+&amp;&gt;&quot;)6%9)=,/?:&amp;5) &quot;;.0!=6&gt;3+&gt;)=,6&amp;,?:!#2))- ?:)=63&#39;-,)9#839%)?&amp;(0&lt;6(&quot;*;)4?!(49+8=-&#39;&amp;499%?&lt; &#39;</code></pre><h3 id="Runner类"><a href="#Runner类" class="headerlink" title="Runner类"></a>Runner类</h3><p><code>Runner</code>，是所有待测程序的基类。一个Fuzzer 与一个Runner搭配。</p><p>Runner类含有run(input)接口，负责接收input并执行，返回 (result, outcome)，result是Runner在运行时的信息和细节，而outcom代表着这次运行的结果。</p><p>运行结果为枚举对象 outcome，含义为程序运行结果，有(PASS, FAIL, or UNRESOLVED)三种可能。</p><ul><li>Runner.PASS：测试通过，run()输出正确。</li><li>Runner.FAIL：测试失败，结果错误。</li><li>Runner.UNRESOLVED：没有输出，这一般代表runner无法应对输入而崩溃。</li></ul><p>Runner的大体架构如下：</p><pre><code class="lang-py">class Runner(object):    # Test outcomes    PASS = &quot;PASS&quot;    FAIL = &quot;FAIL&quot;    UNRESOLVED = &quot;UNRESOLVED&quot;    def __init__(self):        &quot;&quot;&quot;Initialize&quot;&quot;&quot;        pass    def run(self, inp):        &quot;&quot;&quot;Run the runner with the given input&quot;&quot;&quot;        return (inp, Runner.UNRESOLVED)</code></pre><p>想要实现其他Runner，只需继承Runner即可。</p><pre><code class="lang-py">class PrintRunner(Runner):    def run(self, inp):        &quot;&quot;&quot;Print the given input&quot;&quot;&quot;        print(inp)        return (inp, Runner.UNRESOLVED)</code></pre><pre><code>p = PrintRunner()(result, outcome) = p.run(&quot;Some input&quot;)Some input</code></pre><p>对于PrintRunner，我们无法验证其结果，因此通通返回UNRESOLVED。</p><p>下面是一个Runner的派生类ProgramRunner的代码，此架构代表了大多数程序。</p><pre><code class="lang-py">class ProgramRunner(Runner):    def __init__(self, program):        &quot;&quot;&quot;Initialize.  `program` is a program spec as passed to `subprocess.run()`&quot;&quot;&quot;        self.program = program    def run_process(self, inp=&quot;&quot;):        &quot;&quot;&quot;Run the program with `inp` as input.  Return result of `subprocess.run()`.&quot;&quot;&quot;        return subprocess.run(self.program,                              input=inp,                              stdout=subprocess.PIPE,                              stderr=subprocess.PIPE,                              universal_newlines=True)    def run(self, inp=&quot;&quot;):        &quot;&quot;&quot;Run the program with `inp` as input.  Return test outcome based on result of `subprocess.run()`.&quot;&quot;&quot;        result = self.run_process(inp)        if result.returncode == 0:            outcome = self.PASS        elif result.returncode &lt; 0:            outcome = self.FAIL        else:            outcome = self.UNRESOLVED        return (result, outcome)</code></pre><h2 id="Fuzzing-实例"><a href="#Fuzzing-实例" class="headerlink" title="Fuzzing 实例"></a>Fuzzing 实例</h2><pre><code class="lang-py">cat = ProgramRunner(program=&quot;cat&quot;)cat.run(&quot;hello&quot;)random_fuzzer = RandomFuzzer(min_length=20, max_length=20)random_fuzzer.runs(cat, 10)[(CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&#39;3976%%&amp;+%6=(1)3&amp;3:&lt;9&#39;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&#39;33$#42$ 11=*%$20=&lt;.-&#39;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&#39;&quot;?&lt;\&#39;#8 &lt;/:*%9.--\&#39;97!&#39;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&quot;/0-#(03/!#60&#39;+6&gt;&amp;&amp;72&quot;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&quot;=,+:,6&#39;5:950+&gt;&lt;3(*()&quot;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&quot; 379+0?&#39;%3137=2:4605&quot;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&quot;02&gt;!$&lt;/&#39;*81.#&lt;/22&gt;+:&quot;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&quot;=-&lt;&#39;3-#88*%&amp;*9&lt; +1&amp;&amp;&quot;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&#39;2;;0=3&amp;6=8&amp;30&amp;&lt;-;?*;&#39;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&#39;/#05=*3($&gt;::#7!0=12+&#39;, stderr=&#39;&#39;),  &#39;PASS&#39;)]</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Fuzzing又称模糊测试。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Fuzzing" scheme="https://superlova.github.io/tags/Fuzzing/"/>
    
      <category term="Testing" scheme="https://superlova.github.io/tags/Testing/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】leetcode打卡：分治算法</title>
    <link href="https://superlova.github.io/2020/08/19/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91leetcode%E6%89%93%E5%8D%A1%EF%BC%9A%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95/"/>
    <id>https://superlova.github.io/2020/08/19/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91leetcode%E6%89%93%E5%8D%A1%EF%BC%9A%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95/</id>
    <published>2020-08-19T10:21:29.000Z</published>
    <updated>2020-08-19T16:06:25.728Z</updated>
    
    <content type="html"><![CDATA[<p>见多了优秀的文章，再写博客的时候就会感叹自己的学识浅薄。<br><a id="more"></a></p><h2 id="leetcode-50-Pow-x-n"><a href="#leetcode-50-Pow-x-n" class="headerlink" title="leetcode 50 Pow(x,n)"></a>leetcode 50 Pow(x,n)</h2><p><strong>题目描述</strong></p><p>实现 pow(x, n) ，即计算 x 的 n 次幂函数。</p><p><strong>算法描述</strong>：</p><p>Pow算法有快速幂实现方法。</p><p>快速幂，二进制取幂（Binary Exponentiation，也称平方法），是一个在 $O(\log(n))$ 的时间内计算 $a^n$ 的小技巧，而暴力的计算需要 $O(n)$ 的时间。而这个技巧也常常用在非计算的场景，因为它可以应用在任何具有结合律的运算中。其中显然的是它可以应用于模意义下取幂、矩阵幂等运算。</p><p>计算a的n次方表示将n个a连乘在一起。然而当a和n太大的时候，这种方法就不太适用了。</p><p>不过我们知道，$a^{b+c}=a^b\cdot a^c$，$a^{2b}=(a^b)^2$。</p><p>快速幂的想法是，我们将取幂的任务按照指数的 <strong>二进制表示</strong> 来分割成更小的任务。</p><p>我们将 n 表示为 2 进制，举一个例子：</p><p>$3^{13}=3^{(1101)_2}=3^8\cdot 3^4\cdot 3^1$</p><p>因此只需把n转化成二进制，然后分解成对应的权值即可简化计算。</p><p>为什么这样能简化计算？因为n的二进制形式长度最长只有$O(\log(n))$。原问题被我们转化成了形式相同的子问题的乘积。</p><p><strong>实现</strong>：</p><pre><code class="lang-cpp">class Solution {public:    double myPow(double x, int n) {        if (n &lt; 0) {            return 1 / myPow(x, -n);        }        double base = x;        double res = 1.0;        for (; n != 0; n &gt;&gt;= 1) {            if (n &amp; 0x1) res *= base;            base *= base;        }        return res;    }};</code></pre><p>上面的代码在循环的过程中将二进制位为 1 时对应的幂累乘到答案中。</p><blockquote><p><a href="https://oi-wiki.org/" target="_blank" rel="noopener">https://oi-wiki.org/</a></p></blockquote><h2 id="leetcode-53-最大子序和"><a href="#leetcode-53-最大子序和" class="headerlink" title="leetcode 53 最大子序和"></a>leetcode 53 最大子序和</h2><p><strong>题目描述</strong></p><p>给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。</p><p><strong>算法描述</strong>：</p><p>给定一个数组，下标从start到end，即[start,end]。要求这其中的最大连续子数组之和。</p><p>分解成如下子问题：查找[start,mid]中的最大连续子数组之和，查找[mid,end]中的最大连续子数组之和，最后比较二者哪个更大。</p><p>但是最大连续子数组可能是跨越mid的数组，所以递归的时候要额外计算mid及其周围元素之和的最大值，用此值与前面两个区间的值比较。</p><p><strong>实现</strong></p><pre><code class="lang-cpp">class Solution {public:    int maxSubArray(vector&lt;int&gt;&amp; nums) {        return find(nums, 0, nums.size()-1);    }    int find(vector&lt;int&gt;&amp; nums, int start, int end) {        if (start == end) return nums[start];        if (start &gt; end) return INT_MIN;        int mid = start + (end - start) / 2;        int left_max = 0, right_max = 0, ml = 0, mr = 0;        left_max = find(nums, start, mid-1);        right_max = find(nums, mid+1, end);        for (int i = mid-1, sum = 0; i &gt;= start; --i) {            sum += nums[i];            if (sum &gt; ml) ml = sum;        }        for (int i = mid+1, sum = 0; i &lt;= end; ++i) {            sum += nums[i];            if (sum &gt; mr) mr = sum;        }        return max(max(left_max, right_max), ml + mr + nums[mid]);    }};</code></pre><p>在代码中，<code>left_max</code>为[start,mid)区间内的最大连续子数组和，<code>right_max</code>为(mid,end]区间内的最大连续子数组和。</p><p>而跨越中心mid的计算方法，则是通过两个for循环，从mid开始一个往前遍历得到最大值<code>ml</code>，一个往后遍历得到<code>mr</code>，最后得到<code>ml + mr + nums[mid]</code>即可。</p><p>结果为三者的最大值。</p><blockquote><p><a href="https://www.bilibili.com/video/BV19t411k7jR" target="_blank" rel="noopener">https://www.bilibili.com/video/BV19t411k7jR</a></p></blockquote><h2 id="leetcode-169-多数元素"><a href="#leetcode-169-多数元素" class="headerlink" title="leetcode 169 多数元素"></a>leetcode 169 多数元素</h2><p><strong>题目描述</strong></p><p>给定一个大小为 n 的数组，找到其中的多数元素。多数元素是指在数组中出现次数大于 ⌊ n/2 ⌋ 的元素。</p><p>你可以假设数组是非空的，并且给定的数组总是存在多数元素。</p><p><strong>算法描述</strong>：</p><p>如果数 a 是数组 nums 的众数，如果我们将 nums 分成两部分，那么 a 必定是至少一部分的众数。</p><p>这样一来，我们就可以使用分治法解决这个问题：将数组分成左右两部分，分别求出左半部分的众数 a1 以及右半部分的众数 a2，随后在 a1 和 a2 中选出正确的众数。</p><p><strong>实现</strong></p><p>遍历法：</p><pre><code class="lang-cpp">class Solution {public:    int majorityElement(vector&lt;int&gt;&amp; nums) {        int res = -1;        int count = 0;        for (auto c : nums) {            if (!count) {                res = c;            }            if (res == c) {                ++count;            } else {                --count;            }        }        return res;    }};</code></pre><p>分治法：</p><pre><code class="lang-cpp">class Solution {public:    int majorityElement(vector&lt;int&gt;&amp; nums) {        return majorityElement_inrange(nums, 0, nums.size()-1);    }private:    int majorityElement_inrange(vector&lt;int&gt;&amp; nums, int lo, int hi) {        if (lo == hi) return nums[lo];        if (lo &gt; hi) return -1;        int mid = lo + (hi - lo) / 2;        int left_maj = majorityElement_inrange(nums, lo, mid);        int right_maj = majorityElement_inrange(nums, mid+1, hi);        return (count_in_range(nums, lo, hi, left_maj) &gt; count_in_range(nums, lo, hi, right_maj)) ? left_maj : right_maj;    }    int count_in_range(vector&lt;int&gt;&amp; nums, int lo, int hi, int val) {        int count = 0;        for (int i = lo; i &lt;= hi; ++i) {            if (nums[i] == val) count++;        }        return count;    }};</code></pre><blockquote><p><a href="https://leetcode-cn.com/problems/majority-element/solution/duo-shu-yuan-su-by-leetcode-solution/" target="_blank" rel="noopener">https://leetcode-cn.com/problems/majority-element/solution/duo-shu-yuan-su-by-leetcode-solution/</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;见多了优秀的文章，再写博客的时候就会感叹自己的学识浅薄。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="algorithm" scheme="https://superlova.github.io/tags/algorithm/"/>
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="leetcode" scheme="https://superlova.github.io/tags/leetcode/"/>
    
      <category term="divide-and-conquer" scheme="https://superlova.github.io/tags/divide-and-conquer/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】使用tf.data对预处理过程优化</title>
    <link href="https://superlova.github.io/2020/08/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8tf-data%E5%AF%B9%E9%A2%84%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E4%BC%98%E5%8C%96/"/>
    <id>https://superlova.github.io/2020/08/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8tf-data%E5%AF%B9%E9%A2%84%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E4%BC%98%E5%8C%96/</id>
    <published>2020-08-15T15:18:43.000Z</published>
    <updated>2020-08-15T16:12:20.962Z</updated>
    
    <content type="html"><![CDATA[<p>本文是关于 <code>tf.data</code> 介绍的第二篇，主要介绍一些数据预处理方面的优化方法，诸如并行化预处理映射函数、使用缓存等。<br><a id="more"></a></p><p>构建一个机器学习模型时，由于数据预处理过程不能使用GPU进行加速，因此格外耗时。背后的原因可能是CPU、网络或者缓存等复杂的因素。因此要研究如何提升数据预处理的效率，首先需要控制实验的变量。想实现这一点，构造一个虚假的数据集比较可行。</p><p>通过构建一个虚假的数据集，从<code>tf.data.Dataset</code>继承的类，称为<code>ArtificialDataset</code>。该数据集模拟三件事：</p><ol><li>生成<code>num_samples</code>样本（默认为3）</li><li>在第一个模拟打开文件的项目之前睡眠一段时间</li><li>在产生每个项目以模拟从文件读取数据之前先休眠一段时间</li></ol><pre><code class="lang-py">class ArtificialDataset(tf.data.Dataset):    def _generator(num_samples):        # Opening the file        time.sleep(0.03)        for sample_idx in range(num_samples):            # Reading data (line, record) from the file            time.sleep(0.015)            yield (sample_idx,)    def __new__(cls, num_samples=3):        return tf.data.Dataset.from_generator(            cls._generator,            output_types=tf.dtypes.int64,            output_shapes=(1,),            args=(num_samples,)        )</code></pre><p>构建 <code>benchmark</code> ，通过模拟训练的方式，计算该数据预处理模式的耗时：</p><pre><code class="lang-py">def benchmark(dataset, num_epochs=2):    start_time = time.perf_counter()    for epoch_num in range(num_epochs):        for sample in dataset:            # Performing a training step            time.sleep(0.01)    tf.print(&quot;Execution time:&quot;, time.perf_counter() - start_time)</code></pre><p>我们先来不加任何优化地运行一次benchmark：</p><pre><code class="lang-py">benchmark(ArtificialDataset())Execution time: 0.33306735700000445</code></pre><p>此时模型的执行时间图如图所示：<br><img src="https://www.tensorflow.org/guide/images/data_performance/naive.svg" srcset="/img/loading.gif" alt="Naive"></p><p>时间消耗是这样的：先是打开文件，然后从文件中获取数据项，然后使用数据进行训练。这种执行方式，当数据进行预处理，模型就空闲；当模型开始训练，管道又空闲下来了。预处理和训练这两部分明显可以重叠。</p><p><code>tf.data</code> API提供了<code>tf.data.Dataset.prefetch</code>转换。它可以用于将数据生成时间与数据消耗时间分开。转换使用后台线程和内部缓冲区预取元素。要预取的元素数量应等于（或可能大于）单个训练步骤消耗的批次数量。将预取的元素数量设置为<code>tf.data.experimental.AUTOTUNE</code> ，这将提示<code>tf.data</code>运行时在运行时动态调整值。</p><pre><code class="lang-py">benchmark(    ArtificialDataset()    .prefetch(tf.data.experimental.AUTOTUNE))Execution time: 0.20504431599999862</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/prefetched.svg" srcset="/img/loading.gif" alt="Prefetched"></p><p>时间有了明显优化，因为数据的生产和消费有了些许重叠。</p><p>在实际工作中，输入数据可以远程存储在其他计算机上。在本地和远程存储之间存在以下差异：</p><ol><li>到达第一个字节的时间：从远程存储读取文件的第一个字节所花费的时间要比从本地存储中读取文件的时间长几个数量级。</li><li>读取吞吐量：虽然远程存储通常提供较大的聚合带宽，但是读取单个文件可能只能使用此带宽的一小部分。</li></ol><p>此外，一旦将原始字节加载到内存中，可能还需要对数据进行反序列化和/或解密，这需要进行额外的计算。不管数据是本地存储还是远程存储，都存在这种开销，但是<strong>如果数据没有有效地预取，则在远程情况下会更糟</strong>。</p><p>可以使用<code>tf.data.Dataset.interleave</code>转换来<strong>并行化数据加载步骤</strong>， <code>cycle_length</code> 表明可以一起处理的数据集数量， <code>num_parallel_calls</code> 则是并行度。</p><pre><code class="lang-py">benchmark(    tf.data.Dataset.range(2)    .interleave(        ArtificialDataset,        num_parallel_calls=tf.data.experimental.AUTOTUNE    )Execution time: 0.18243273299958673</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/parallel_interleave.svg" srcset="/img/loading.gif" alt="Parallel interleave"></p><p>该图可以显示<code>interleave</code>变换的行为，从两个可用的数据集中获取样本。这次，两个数据集的读取并行进行，从而减少了全局数据处理时间</p><h2 id="并行预处理操作"><a href="#并行预处理操作" class="headerlink" title="并行预处理操作"></a>并行预处理操作</h2><p>在准备数据时，可能需要对输入元素进行预处理。可以使用<code>tf.data.Dataset.map(f)</code>转换，其含义为将某个转换<code>f</code>作用于数据集<code>Dataset</code>中的每个元素。这里有个很重要的前提条件，由于输入元素彼此独立，因此预处理可以跨多个CPU内核并行化。因此<code>map</code>转换也提供<code>num_parallel_calls</code>参数来指定并行度。关于并行度的选择上，<code>map</code>转换支持<code>tf.data.experimental.AUTOTUNE</code>，而不必人工定义。</p><p>首先定义伪操作：</p><pre><code class="lang-py">def mapped_function(s):    # Do some hard pre-processing    tf.py_function(lambda: time.sleep(0.03), [], ())    return s</code></pre><p>我们来测试伪操作，此时没有任何并行优化：</p><pre><code class="lang-py">benchmark(    ArtificialDataset()    .map(mapped_function))Execution time: 0.4592052289999913</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/sequential_map.svg" srcset="/img/loading.gif" alt="Sequential mapping"></p><p>现在，使用相同的预处理功能，但将其并行应用于多个样本。</p><pre><code class="lang-py">benchmark(    ArtificialDataset()    .map(        mapped_function,        num_parallel_calls=tf.data.experimental.AUTOTUNE    ))Execution time: 0.3045882669994171</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/parallel_map.svg" srcset="/img/loading.gif" alt="Parallel mapping"></p><p>可以在图上看到预处理步骤重叠，从而减少了单次迭代的总时间。</p><p><code>tf.data.Dataset.cache</code>转换可以在内存中或本地存储上缓存数据集。这样可以避免在每个epoch执行某些重复性操作（例如打开文件和读取数据）。</p><pre><code class="lang-py">benchmark(    ArtificialDataset()    .map(  # Apply time consuming operations before cache        mapped_function    ).cache(    ),    5)Execution time: 0.3795637040002475</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/cached_dataset.svg" srcset="/img/loading.gif" alt="Cached dataset"></p><p>第一个epoch执行一次cache之前的转换（例如文件打开和数据读取）。下一个epoch将重用cache转换所缓存的数据。</p><p>这里涉及到一个<code>map</code>和<code>cache</code>操作谁先谁后的问题。有一个原则，如果<code>map</code>操作很复杂、昂贵，那么先<code>map</code>再<code>cache</code>，下次不用<code>map</code>了。如果<code>cache</code>过大而无法放入缓冲区，则先<code>cache</code>后<code>map</code>，或者试图采用一些数据预处理方法以减少资源使用。</p><h2 id="向量化数据预处理操作"><a href="#向量化数据预处理操作" class="headerlink" title="向量化数据预处理操作"></a>向量化数据预处理操作</h2><p>所谓向量化，即使得<code>mapping</code>操作能够一次处理一<code>batch</code>数据。这样做肯定可以加速，因为避免了繁杂的数据读取时间。对用户定义的函数进行向量化处理，并且对数据集应用<code>batch</code>转换再进入<code>mapping</code>。在某种情况下，这个做法非常有用。</p><p>首先定义一个数据集操作<code>increment</code>，负责把每个元素的值+1。另外之前的例子里面使用了毫秒级别的<code>sleep</code>操作，这会掩盖我们优化的结果。这次我们把它拿掉。</p><p>下面是未经向量化优化的<code>increment</code>操作耗时：</p><pre><code class="lang-py">fast_dataset = tf.data.Dataset.range(10000)def fast_benchmark(dataset, num_epochs=2):    start_time = time.perf_counter()    for _ in tf.data.Dataset.range(num_epochs):        for _ in dataset:            pass    tf.print(&quot;Execution time:&quot;, time.perf_counter() - start_time)def increment(x):    return x+1fast_benchmark(    fast_dataset    # Apply function one item at a time    .map(increment)    # Batch    .batch(256))Execution time: 0.7625284370005829</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/scalar_map.svg" srcset="/img/loading.gif" alt="Scalar map"></p><p>与之对比，经过向量化后，耗时明显减少：</p><pre><code class="lang-py">fast_benchmark(    fast_dataset    .batch(256)    # Apply function on a batch of items    # The tf.Tensor.__add__ method already handle batches    .map(increment))Execution time: 0.04735958700075571</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/vectorized_map.svg" srcset="/img/loading.gif" alt="Vectorized map"></p><h2 id="减少内存占用"><a href="#减少内存占用" class="headerlink" title="减少内存占用"></a>减少内存占用</h2><p>许多转换（包括interleave ， prefetch和shuffle ）各自维护内部缓冲区。如果传递给map转换的用户定义函数更改了元素的大小，则映射转换的顺序以及缓冲元素的转换会影响内存使用。</p><p>通常，我们建议选择导致内存占用减少的顺序，除非需要不同的顺序才能提高性能。</p><p>对于缓存，我们建议除非转换后的数据难以保存到缓冲区，否则一律先<code>map</code>再<code>cache</code>。如果你有两个<code>map</code>，其中一个比较耗时<code>time_consuming_mapping</code>，另一个比较耗内存<code>memory_consuming_mapping</code>，那么其实你可以将其拆分成两部分；</p><pre><code class="lang-py">dataset.map(time_consuming_mapping).cache().map(memory_consuming_mapping)</code></pre><p>这样，耗时部分仅在第一个epoch执行，并且避免了使用过多的缓存空间。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>使用<code>tf.data</code>，并采用合理的优化手段，就能让你的数据预处理过程节约很多时间。这些手段有：</p><ul><li>使用<code>prefetch</code>转换可以使生产者和消费者的工作重叠。</li><li>使用<code>interleave</code>变换并行化数据读取变换。</li><li>通过设置<code>num_parallel_calls</code>参数来并行化<code>map</code>转换 。</li><li>在第一个epoch使用<code>cache</code>转换将数据缓存在内存中</li><li>向量化传递给<code>map</code>转换的用户定义函数</li><li>应用<code>interleave</code> ， <code>prefetch</code>和<code>shuffle</code>转换时， 逐渐减少内存使用 。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是关于 &lt;code&gt;tf.data&lt;/code&gt; 介绍的第二篇，主要介绍一些数据预处理方面的优化方法，诸如并行化预处理映射函数、使用缓存等。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="preprocessing" scheme="https://superlova.github.io/tags/preprocessing/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】tf.data基本使用</title>
    <link href="https://superlova.github.io/2020/08/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91tf-data%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"/>
    <id>https://superlova.github.io/2020/08/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91tf-data%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</id>
    <published>2020-08-15T08:01:09.000Z</published>
    <updated>2020-08-15T15:20:36.629Z</updated>
    
    <content type="html"><![CDATA[<p>使用 <code>tf.data</code> API 可以轻松处理大量数据，支持多样化的数据格式，还可以方便执行复杂的转换。本文介绍了不同类别源数据转化为 <code>tf.data.Dataset</code> 的方法，以及 <code>Dataset</code> 常见的预处理方法。<br><a id="more"></a></p><h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>最近几年我们在机器学习的加速计算领域取得了一些突破。虽然我们进行指标运算和矩阵运算所需的时间大大减少了，但是提供数据加速的CPU却没能跟上相应的步伐，这就成为了预处理中的瓶颈。我们本以为可以通过构建更复杂的模型来减少对硬件的需求，但是CPU的效率还是取决于他们拥有多少RAM。</p><p><img src="/2020/08/15/【学习笔记】tf-data基本使用/2020-08-15-22-34-14.png" srcset="/img/loading.gif" alt></p><p>对于一些数据集很大的问题，解决问题的方法除了提升CPU的性能之外，还有对数据集进行分批次处理，但是分批预处理需要不断进行训练和合并操作，这对预处理增加了难度。<code>tf.data</code> 可以帮助解决数据集过大造成的预处理瓶颈问题。</p><p><code>tf.data</code> 是tensorflow的数据预处理框架。它速度快、灵活且易于使用。</p><p>假设你的数据以TFRecord格式存在磁盘，需要将这些数据读取、处理并训练模型，可以先通过<code>TFRecordDataset</code> 开始处理这些数据：</p><pre><code class="lang-py">dataset = tf.data.TFRecordDataset(&#39;.../*.tfrecord&#39;)</code></pre><p>然后执行一些数据预处理操作，这个过程可能很消耗资源：</p><pre><code class="lang-py">dataset = dataset.map(expensive_preprocess)</code></pre><p>随后你需要打乱数据，以降低模型训练过程中过拟合的可能性：</p><pre><code class="lang-py">dataset = dataset.shuffle(buffer_size=1024)</code></pre><p>然后我们需要分批次，以令模型加速计算</p><pre><code class="lang-py">dataset = dataset.batch(batch_size=32)</code></pre><p>最终要完成pipeline搭建，这样可以保证模型在运行一个batch的数据时，另一批数据进行预处理以提升效率。</p><pre><code class="lang-py">dataset = dataset.prefetch()</code></pre><p>将数据输入到模型，我们可以开始训练了。</p><pre><code class="lang-py">model = tf.keras.Model(...)model.fit(dataset)</code></pre><p>上面就是数据从读取到处理到训练的全部流程，称之为管道（pipeline）。</p><p>处理大量原始数据，要经过多次函数变换，这些函数变换都是可重用的。使用<code>tf.data</code>将这些变换整理成管道，一方面可以简化复杂输入的预处理过程，另一方面，由于<code>Dataset</code> 对象可迭代，可以执行分批处理。使用<code>tf.data.Dataset</code>可以方便地整合操作、构造数据集。</p><p>有两种方法构造可供训练使用的<code>Dataset</code>数据集：</p><ol><li>从文件、内存中直接构建<code>Dataset</code></li><li>从其他<code>Dataset</code>中转化</li></ol><p>如果打算从内存中读取数据构建 <code>Dataset</code> ，有 <code>tf.data.Dataset.from_tensors()</code> 和 <code>tf.data.Dataset.from_tensor_slices()</code> 可供选择；如果打算从 <code>TFRecord</code> 格式的文件中读取数据，可以调用 <code>tf.data.TFRecordDataset()</code> 。</p><p>当 <code>Dataset</code> 对象构建好了之后，通过使用 <code>Dataset.map()</code> 为其中每个元素施加变换、使用 <code>Dataset.batch()</code> 为整批元素添加变换等等对数据进行预处理。</p><h2 id="从内存或文件中构造Dataset"><a href="#从内存或文件中构造Dataset" class="headerlink" title="从内存或文件中构造Dataset"></a>从内存或文件中构造<code>Dataset</code></h2><h3 id="从内存中的array构造Dataset"><a href="#从内存中的array构造Dataset" class="headerlink" title="从内存中的array构造Dataset"></a>从内存中的array构造<code>Dataset</code></h3><p>如果你的所有数据都在内存中，那么最简单构造 <code>Dataset</code> 的方式就是，先将其利用 <code>tf.Tensor</code> 转成tensor，后使用<code>Dataset.from_tensor_slices()</code>。</p><pre><code class="lang-python">train, test = tf.keras.datasets.fashion_mnist.load_data()images, labels = trainimages = images/255dataset = tf.data.Dataset.from_tensor_slices((images, labels))dataset</code></pre><h3 id="从生成器构造Dataset"><a href="#从生成器构造Dataset" class="headerlink" title="从生成器构造Dataset"></a>从生成器构造<code>Dataset</code></h3><p>你也可以利用 <code>Dataset.from_generator</code> 从Python的生成器来构造 <code>Dataset</code> ，比如从 <code>preprocessing.image.ImageDataGenerator</code> 构造 <code>Dataset</code>。但这种方法受制于Python的GIL，因此效率不会太高。</p><p>首先下载花朵图片数据集，一共3670张花朵图片，分成五个类别。</p><pre><code class="lang-python">flowers = tf.keras.utils.get_file(    &#39;flower_photos&#39;,    &#39;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&#39;,    untar=True)</code></pre><p>利用 <code>preprocessing.image.ImageDataGenerator</code> 定义数据增强操作，然后将其套用到花朵数据集上。</p><pre><code class="lang-python">img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)images, labels = next(img_gen.flow_from_directory(flowers))</code></pre><p>最后构造 <code>Dataset</code> 。</p><pre><code class="lang-python">ds = tf.data.Dataset.from_generator(    img_gen.flow_from_directory, args=[flowers],     output_types=(tf.float32, tf.float32),     output_shapes=([32,256,256,3], [32,5]))ds&lt;FlatMapDataset shapes: ((32, 256, 256, 3), (32, 5)), types: (tf.float32, tf.float32)&gt;</code></pre><h3 id="从TFRecord格式文件构造Dataset"><a href="#从TFRecord格式文件构造Dataset" class="headerlink" title="从TFRecord格式文件构造Dataset"></a>从<code>TFRecord</code>格式文件构造<code>Dataset</code></h3><p>有些时候数据不在内存中，而是以特定格式存在磁盘上，比如 <code>TFRecord</code> 格式。这种情况我们可以使用 <code>tf.data.TFRecordDataset</code> 作为数据管道的一部分。</p><pre><code class="lang-python"># Creates a dataset that reads all of the examples from two files.fsns_test_file = tf.keras.utils.get_file(&quot;fsns.tfrec&quot;, &quot;https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001&quot;)dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file])</code></pre><p><code>tf.data.TFRecordDataset</code> API中，<code>filenames</code> 的输入很灵活，既可以是字符串，表明一个文件；也可以是字符串列表，表明多个文件。</p><h3 id="从-txt格式文件构造Dataset"><a href="#从-txt格式文件构造Dataset" class="headerlink" title="从.txt格式文件构造Dataset"></a>从<code>.txt</code>格式文件构造<code>Dataset</code></h3><p>如果是 <code>.txt</code> 格式，那么采用 <code>tf.data.TextLineDataset</code> 也可转成 <code>Dataset</code>。</p><pre><code class="lang-python">directory_url = &#39;https://storage.googleapis.com/download.tensorflow.org/data/illiad/&#39;file_names = [&#39;cowper.txt&#39;, &#39;derby.txt&#39;, &#39;butler.txt&#39;]file_paths = [    &#39;cowper.txt&#39;,&#39;derby.txt&#39;,&#39;butler.txt&#39;]dataset = tf.data.TextLineDataset(file_paths)</code></pre><p>看一下第一个文件的前5行：</p><pre><code class="lang-python">for line in dataset.take(5):  print(line.numpy())b&quot;\xef\xbb\xbfAchilles sing, O Goddess! Peleus&#39; son;&quot;b&#39;His wrath pernicious, who ten thousand woes&#39;b&quot;Caused to Achaia&#39;s host, sent many a soul&quot;b&#39;Illustrious into Ades premature,&#39;b&#39;And Heroes gave (so stood the will of Jove)&#39;</code></pre><p>前五行都是 <code>cowper.txt</code> 中的。如果我们希望生成的 <code>Dataset</code> 能够轮流选取三个文件中的元素，可以在构造之初，使用 <code>Dataset.interleave</code> ，并设置 <code>cycle_length</code>：</p><pre><code class="lang-python">files_ds = tf.data.Dataset.from_tensor_slices(file_paths)lines_ds = files_ds.interleave(tf.data.TextLineDataset, cycle_length=3)for i, line in enumerate(lines_ds.take(9)):  if i % 3 == 0:    print()  print(i, line.numpy())0 b&quot;\xef\xbb\xbfAchilles sing, O Goddess! Peleus&#39; son;&quot;1 b&quot;\xef\xbb\xbfOf Peleus&#39; son, Achilles, sing, O Muse,&quot;2 b&#39;\xef\xbb\xbfSing, O goddess, the anger of Achilles son of Peleus, that brought&#39;3 b&#39;His wrath pernicious, who ten thousand woes&#39;4 b&#39;The vengeance, deep and deadly; whence to Greece&#39;5 b&#39;countless ills upon the Achaeans. Many a brave soul did it send&#39;6 b&quot;Caused to Achaia&#39;s host, sent many a soul&quot;7 b&#39;Unnumbered ills arose; which many a soul&#39;8 b&#39;hurrying down to Hades, and many a hero did it yield a prey to dogs and&#39;</code></pre><p>有的时候我们不希望录入文件的第一行，或者只要文件中满足要求的特定行，可以分别使用 <code>Dataset.skip()</code> 和 <code>Dataset.filter()</code> 。比如下面的泰坦尼克数据集，去掉第一行后，筛选生存下来的人。</p><pre><code class="lang-python">titanic_file = tf.keras.utils.get_file(&quot;train.csv&quot;, &quot;https://storage.googleapis.com/tf-datasets/titanic/train.csv&quot;)titanic_lines = tf.data.TextLineDataset(titanic_file)for line in titanic_lines.take(10):  print(line.numpy())b&#39;survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone&#39;b&#39;0,male,22.0,1,0,7.25,Third,unknown,Southampton,n&#39;b&#39;1,female,38.0,1,0,71.2833,First,C,Cherbourg,n&#39;b&#39;1,female,26.0,0,0,7.925,Third,unknown,Southampton,y&#39;b&#39;1,female,35.0,1,0,53.1,First,C,Southampton,n&#39;b&#39;0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y&#39;b&#39;0,male,2.0,3,1,21.075,Third,unknown,Southampton,n&#39;b&#39;1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n&#39;b&#39;1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n&#39;b&#39;1,female,4.0,1,1,16.7,Third,G,Southampton,n&#39;def survived(line):  return tf.not_equal(tf.strings.substr(line, 0, 1), &quot;0&quot;)survivors = titanic_lines.skip(1).filter(survived)for line in survivors.take(10):  print(line.numpy())b&#39;1,female,38.0,1,0,71.2833,First,C,Cherbourg,n&#39;b&#39;1,female,26.0,0,0,7.925,Third,unknown,Southampton,y&#39;b&#39;1,female,35.0,1,0,53.1,First,C,Southampton,n&#39;b&#39;1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n&#39;b&#39;1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n&#39;b&#39;1,female,4.0,1,1,16.7,Third,G,Southampton,n&#39;b&#39;1,male,28.0,0,0,13.0,Second,unknown,Southampton,y&#39;b&#39;1,female,28.0,0,0,7.225,Third,unknown,Cherbourg,y&#39;b&#39;1,male,28.0,0,0,35.5,First,A,Southampton,y&#39;b&#39;1,female,38.0,1,5,31.3875,Third,unknown,Southampton,n&#39;</code></pre><h3 id="从csv格式构造Dataset"><a href="#从csv格式构造Dataset" class="headerlink" title="从csv格式构造Dataset"></a>从<code>csv</code>格式构造<code>Dataset</code></h3><p>除了<code>TFRecord</code>和<code>txt</code>格式，还有<code>csv</code>格式也很流行。<code>csv</code>格式能够以纯文本保存表格数据。<code>pandas</code>的<code>to_csv</code>是将<code>csv</code>搬运到内存的良好工具。</p><pre><code class="lang-python">titanic_file = tf.keras.utils.get_file(&quot;train.csv&quot;, &quot;https://storage.googleapis.com/tf-datasets/titanic/train.csv&quot;)df = pd.read_csv(titanic_file, index_col=None)df.head()titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))for feature_batch in titanic_slices.take(1):  for key, value in feature_batch.items():    print(&quot;  {!r:20s}: {}&quot;.format(key, value))</code></pre><p>当然<code>tf.data</code>强大之处在于可以处理<code>pandas</code>处理不了的文件大小。<code>experimental.make_csv_dataset</code>函数是用于读取csv文件集的高层接口，它可以自动推导每个<code>column</code>的文件类型。</p><pre><code class="lang-python">titanic_batches = tf.data.experimental.make_csv_dataset(    titanic_file, batch_size=4,    label_name=&quot;survived&quot;)</code></pre><p>查看第一个<code>batch</code>的内容。</p><pre><code class="lang-python">for feature_batch, label_batch in titanic_batches.take(1):  print(&quot;&#39;survived&#39;: {}&quot;.format(label_batch))  print(&quot;features:&quot;)  for key, value in feature_batch.items():    print(&quot;  {!r:20s}: {}&quot;.format(key, value))</code></pre><p>如果只需要<code>csv</code>的某一列，那么可以使用<code>select_columns</code>参数。</p><pre><code class="lang-python">titanic_batches = tf.data.experimental.make_csv_dataset(    titanic_file, batch_size=4,    label_name=&quot;survived&quot;, select_columns=[&#39;class&#39;, &#39;fare&#39;, &#39;survived&#39;])for feature_batch, label_batch in titanic_batches.take(1):  print(&quot;&#39;survived&#39;: {}&quot;.format(label_batch))  for key, value in feature_batch.items():    print(&quot;  {!r:20s}: {}&quot;.format(key, value))&#39;survived&#39;: [1 1 1 0]features:  &#39;sex&#39;               : [b&#39;female&#39; b&#39;female&#39; b&#39;male&#39; b&#39;female&#39;]  &#39;age&#39;               : [35. 31. 45. 28.]  &#39;n_siblings_spouses&#39;: [0 1 0 8]  &#39;parch&#39;             : [0 1 0 2]  &#39;fare&#39;              : [512.3292  20.525    8.05    69.55  ]  &#39;class&#39;             : [b&#39;First&#39; b&#39;Third&#39; b&#39;Third&#39; b&#39;Third&#39;]  &#39;deck&#39;              : [b&#39;unknown&#39; b&#39;unknown&#39; b&#39;unknown&#39; b&#39;unknown&#39;]  &#39;embark_town&#39;       : [b&#39;Cherbourg&#39; b&#39;Southampton&#39; b&#39;Southampton&#39; b&#39;Southampton&#39;]  &#39;alone&#39;             : [b&#39;y&#39; b&#39;n&#39; b&#39;y&#39; b&#39;n&#39;]</code></pre><p>还有一个底层的<code>experimental.CsvDataset</code>类，它可以更精细的控制读取<code>csv</code>的过程。不支持列类型推断。</p><pre><code class="lang-python">titanic_types  = [tf.int32, tf.string, tf.float32, tf.int32, tf.int32, tf.float32, tf.string, tf.string, tf.string, tf.string] dataset = tf.data.experimental.CsvDataset(titanic_file, titanic_types , header=True)for line in dataset.take(10):  print([item.numpy() for item in line])[0, b&#39;male&#39;, 22.0, 1, 0, 7.25, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 38.0, 1, 0, 71.2833, b&#39;First&#39;, b&#39;C&#39;, b&#39;Cherbourg&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 26.0, 0, 0, 7.925, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;y&#39;][1, b&#39;female&#39;, 35.0, 1, 0, 53.1, b&#39;First&#39;, b&#39;C&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][0, b&#39;male&#39;, 28.0, 0, 0, 8.4583, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Queenstown&#39;, b&#39;y&#39;][0, b&#39;male&#39;, 2.0, 3, 1, 21.075, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 27.0, 0, 2, 11.1333, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 14.0, 1, 0, 30.0708, b&#39;Second&#39;, b&#39;unknown&#39;, b&#39;Cherbourg&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 4.0, 1, 1, 16.7, b&#39;Third&#39;, b&#39;G&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][0, b&#39;male&#39;, 20.0, 0, 0, 8.05, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;y&#39;]</code></pre><p><code>CsvDataset</code>还可以指定每列的默认值，供元素为空时填充。</p><p>在Colab中，直接书写<code>csv</code>文件：</p><pre><code class="lang-bash">%%writefile missing.csv1,2,3,4,2,3,41,,3,41,2,,41,2,3,,,,</code></pre><p>设置每列默认值：</p><pre><code class="lang-python"># Creates a dataset that reads all of the records from two CSV files, each with# four float columns which may have missing values.record_defaults = [999,999,999,999]dataset = tf.data.experimental.CsvDataset(&quot;missing.csv&quot;, record_defaults)dataset = dataset.map(lambda *items: tf.stack(items))for line in dataset:  print(line.numpy())[1 2 3 4][999   2   3   4][  1 999   3   4][  1   2 999   4][  1   2   3 999][999 999 999 999]</code></pre><p>你也可以选择删除<code>header</code>，或者指定某列输出</p><pre><code class="lang-python"># Creates a dataset that reads all of the records from two CSV files with# headers, extracting float data from columns 2 and 4.record_defaults = [999, 999] # Only provide defaults for the selected columnsdataset = tf.data.experimental.CsvDataset(&quot;missing.csv&quot;, record_defaults, select_cols=[1, 3])dataset = dataset.map(lambda *items: tf.stack(items))for line in dataset:  print(line.numpy())[2 4][2 4][999   4][2 4][  2 999][999 999]</code></pre><h3 id="从文件夹中的每个文件构造Dataset"><a href="#从文件夹中的每个文件构造Dataset" class="headerlink" title="从文件夹中的每个文件构造Dataset"></a>从文件夹中的每个文件构造<code>Dataset</code></h3><p>如果每个单独的文件都是一个数据项（比如图片数据集），这样的数据集如何整理？</p><pre><code class="lang-python">flowers_root = tf.keras.utils.get_file(    &#39;flower_photos&#39;,    &#39;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&#39;,    untar=True)flowers_root = pathlib.Path(flowers_root)list_ds = tf.data.Dataset.list_files(str(flowers_root/&#39;*/*&#39;))for f in list_ds.take(5):  print(f.numpy())b&#39;/root/.keras/datasets/flower_photos/dandelion/8720503800_cab5c62a34.jpg&#39;b&#39;/root/.keras/datasets/flower_photos/dandelion/16510864164_3afa8ac37f.jpg&#39;b&#39;/root/.keras/datasets/flower_photos/tulips/7136973281_b2a935ce20.jpg&#39;b&#39;/root/.keras/datasets/flower_photos/sunflowers/14623719696_1bb7970208_n.jpg&#39;b&#39;/root/.keras/datasets/flower_photos/dandelion/4560663938_3557a1f831.jpg&#39;</code></pre><p>通过使用<code>tf.io.read_file</code>读取数据，并从路径中提取<code>label</code>，返回<code>(image, label)</code>数据对。</p><pre><code class="lang-python">def process_path(file_path):  label = tf.strings.split(file_path, os.sep)[-2]  return tf.io.read_file(file_path), labellabeled_ds = list_ds.map(process_path)for image_raw, label_text in labeled_ds.take(1):  print(repr(image_raw.numpy()[:100]))  print()  print(label_text.numpy())b&#39;\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x01\x03|\x03|\x00\x00\xff\xe2\x0cXICC_PROFILE\x00\x01\x01\x00\x00\x0cHLino\x02\x10\x00\x00mntrRGB XYZ \x07\xce\x00\x02\x00\t\x00\x06\x001\x00\x00acspMSFT\x00\x00\x00\x00IEC sRGB\x00\x00\x00\x00\x00\x00&#39;b&#39;sunflowers&#39;</code></pre><h2 id="批处理数据集元素"><a href="#批处理数据集元素" class="headerlink" title="批处理数据集元素"></a>批处理数据集元素</h2><p>批处理的最简单形式是将数据集的<code>n</code>个连续元素堆叠为单个元素。</p><p>batched_dataset = dataset.batch(4)</p><batchdataset shapes: ((none,), (none,)), types: (tf.int64, tf.int64)><p>最后一个批次可能未满，使用<code>drop_remainder</code>参数忽略最后一批，使得<code>shape</code>完整：</p><p>batched_dataset = dataset.batch(7, drop_remainder=True)<br>batched_dataset</p><batchdataset shapes: ((7,), (7,)), types: (tf.int64, tf.int64)><p>许多模型（例如序列模型）都可以使用大小可变（例如长度不同的序列）的输入数据。通过<code>Dataset.padded_batch</code>可以将不同长度的<code>tensor</code>转换成一个<code>batch</code>。</p><p>在多个<code>epochs</code>的情况下，需要重复迭代数据集，最简单的方法是将数据集重复<code>epochs</code>遍。可以使用 <code>Dataset.repeat()</code>完成。</p><p>原有的<code>titanic_lines</code>数据集中的数据数量为 628</p><pre><code class="lang-python">count = 0for data in titanic_lines:    count += 1    # print(data.numpy())print(count)</code></pre><p>titanic_lines数据集经过<code>repeat</code>之后数目变为原来的两倍，1256</p><pre><code class="lang-python">count = 0for data in titanic_lines.repeat(2):    count += 1    # print(data.numpy())print(count)</code></pre><p>将数据集打散的方法 <code>Dataset.shuffle()</code> 通过维护一个固定大小的缓冲区来实现。</p><pre><code class="lang-py">dataset = tf.data.Dataset.zip((counter, lines))dataset = dataset.shuffle(buffer_size=100)dataset = dataset.batch(20)</code></pre><p>由于<code>buffer_size</code>为100，而批大小为20，因此第一批不包含索引大于120的元素。</p><p>在实际使用中，<code>repeat</code>操作、<code>batch</code>操作和<code>shuffle</code>操作经常一起混用，但是一定要注意操作的先后顺序。</p><h2 id="预处理数据"><a href="#预处理数据" class="headerlink" title="预处理数据"></a>预处理数据</h2><p><code>Dataset.map(f)</code> 通过函数 <code>f</code> 对数据集执行变换，<code>f</code> 必须以单个Tensor为输入，单个Tensor为输出（这里指的单个tensor，其意思是由原始数据和标签组成的数据对）。</p><p>假设我们定义了单个图像变换函数 <code>parse_image</code> ，只需 <code>images_ds = list_ds.map(parse_image)</code> 即可对 <code>image_ds</code> 数据集中的所有图片执行变换了。</p><p>假设我们需要将照片随机旋转，可以定义函数，然后使用map将其应用于数据集的所有图片上。</p><pre><code class="lang-py">import scipy.ndimage as ndimagedef random_rotate_image(image):  image = ndimage.rotate(image, np.random.uniform(-30, 30), reshape=False)  return imageimage, label = next(iter(images_ds))image = random_rotate_image(image)show(image, label)</code></pre><p><img src="https://www.tensorflow.org/guide/data_files/output__wEyL7bS9S6t_1.png" srcset="/img/loading.gif" alt></p><p>这里使用的是 <code>scipy</code> 中的旋转函数，需套用 <code>tf.py_function()</code> 才能在Tensorflow的eager_mode里面使用。</p><p>接下来使用 <code>Dataset.map</code> ：</p><pre><code class="lang-py">def tf_random_rotate_image(image, label):  im_shape = image.shape  [image,] = tf.py_function(random_rotate_image, [image], [tf.float32])  image.set_shape(im_shape)  return image, label</code></pre><p>在函数内部不但要注意输入和返回值是<code>(image, label)</code>，而且需要描述数据的<code>shape</code>和<code>type</code>，方便调试。</p><h2 id="时间序列数据窗口化"><a href="#时间序列数据窗口化" class="headerlink" title="时间序列数据窗口化"></a>时间序列数据窗口化</h2><p>时间序列数据的标签有所不同，一般以下一时刻的输入数据为标签，对未来进行一步一步的密集预测。比如：</p><pre><code class="lang-py">range_ds = tf.data.Dataset.range(100000)batches = range_ds.batch(10, drop_remainder=True)def dense_1_step(batch):  # Shift features and labels one step relative to each other.  return batch[:-1], batch[1:]predict_dense_1_step = batches.map(dense_1_step)for features, label in predict_dense_1_step.take(3):  print(features.numpy(), &quot; =&gt; &quot;, label.numpy())[0 1 2 3 4 5 6 7 8]  =&gt;  [1 2 3 4 5 6 7 8 9][10 11 12 13 14 15 16 17 18]  =&gt;  [11 12 13 14 15 16 17 18 19][20 21 22 23 24 25 26 27 28]  =&gt;  [21 22 23 24 25 26 27 28 29]</code></pre><p>如果要预测整个时间窗口而不是固定的偏移量，比如</p><pre><code class="lang-py">batches = range_ds.batch(15, drop_remainder=True)def label_next_5_steps(batch):  return (batch[:-5],   # Take the first 5 steps          batch[-5:])   # take the remainderpredict_5_steps = batches.map(label_next_5_steps)for features, label in predict_5_steps.take(3):  print(features.numpy(), &quot; =&gt; &quot;, label.numpy())[0 1 2 3 4 5 6 7 8 9]  =&gt;  [10 11 12 13 14][15 16 17 18 19 20 21 22 23 24]  =&gt;  [25 26 27 28 29][30 31 32 33 34 35 36 37 38 39]  =&gt;  [40 41 42 43 44]</code></pre><p>或者一个批次的标签和下个批次的输入有重叠：</p><pre><code class="lang-py">feature_length = 10label_length = 5features = range_ds.batch(feature_length, drop_remainder=True)labels = range_ds.batch(feature_length).skip(1).map(lambda labels: labels[:-5])predict_5_steps = tf.data.Dataset.zip((features, labels))for features, label in predict_5_steps.take(3):  print(features.numpy(), &quot; =&gt; &quot;, label.numpy())[0 1 2 3 4 5 6 7 8 9]  =&gt;  [10 11 12 13 14][10 11 12 13 14 15 16 17 18 19]  =&gt;  [20 21 22 23 24][20 21 22 23 24 25 26 27 28 29]  =&gt;  [30 31 32 33 34]</code></pre><p>有更方便的方法，那就是使用<code>Dataset.window</code>方法</p><pre><code class="lang-py">def make_window_dataset(ds, window_size, shift, stride):  windows = ds.window(window_size, shift=shift, stride=stride)  def sub_to_batch(sub):    return sub.batch(window_size, drop_remainder=True)  windows = windows.flat_map(sub_to_batch)  return windows</code></pre><p><code>Dataset.window(window_size, shift=shift, stride=stride)</code> 中的window_size代表窗口大小，即每个batch的元素个数；shift代表每次窗口移动的距离；stride代表选择元素的间隔</p><pre><code class="lang-py">ds = make_window_dataset(range_ds, window_size=10, shift = 5, stride=3)for example in ds.take(10):  print(example.numpy())[ 0  3  6  9 12 15 18 21 24 27][ 5  8 11 14 17 20 23 26 29 32][10 13 16 19 22 25 28 31 34 37][15 18 21 24 27 30 33 36 39 42][20 23 26 29 32 35 38 41 44 47][25 28 31 34 37 40 43 46 49 52][30 33 36 39 42 45 48 51 54 57][35 38 41 44 47 50 53 56 59 62][40 43 46 49 52 55 58 61 64 67][45 48 51 54 57 60 63 66 69 72]</code></pre><p>提取这些数据的标签方法：</p><pre><code class="lang-py">dense_labels_ds = ds.map(dense_1_step)for inputs,labels in dense_labels_ds.take(3):  print(inputs.numpy(), &quot;=&gt;&quot;, labels.numpy())[ 0  3  6  9 12 15 18 21 24] =&gt; [ 3  6  9 12 15 18 21 24 27][ 5  8 11 14 17 20 23 26 29] =&gt; [ 8 11 14 17 20 23 26 29 32][10 13 16 19 22 25 28 31 34] =&gt; [13 16 19 22 25 28 31 34 37]</code></pre><h2 id="重采样"><a href="#重采样" class="headerlink" title="重采样"></a>重采样</h2><p>有一些数据集，不同类别的数据分布不均匀。这个时候需要对那些不足的类别进行重采样。</p><p>给定信用卡欺诈（二分类）数据集，下面首先检查数据集中不同类别的占比</p><pre><code class="lang-py">def count(counts, batch):  features, labels = batch  class_1 = labels == 1  class_1 = tf.cast(class_1, tf.int32)  class_0 = labels == 0  class_0 = tf.cast(class_0, tf.int32)  counts[&#39;class_0&#39;] += tf.reduce_sum(class_0)  counts[&#39;class_1&#39;] += tf.reduce_sum(class_1)  return countscounts = creditcard_ds.take(10).reduce(    initial_state={&#39;class_0&#39;: 0, &#39;class_1&#39;: 0},    reduce_func = count)counts = np.array([counts[&#39;class_0&#39;].numpy(),                   counts[&#39;class_1&#39;].numpy()]).astype(np.float32)fractions = counts/counts.sum()print(fractions)[0.9953 0.0047]</code></pre><p>偏差很大，这样训练的二分类器只需全预测为正类，即可达到99.53%的正确率。</p><p>重采样数据集的一种方法是使用<code>sample_from_datasets</code> 。当每个类都有单独的<code>data.Dataset</code>时，此方法更适用。</p><p>正类和反类分别构建 <code>Dataset</code></p><pre><code class="lang-py">negative_ds = (  creditcard_ds    .unbatch()    .filter(lambda features, label: label==0)    .repeat())positive_ds = (  creditcard_ds    .unbatch()    .filter(lambda features, label: label==1)    .repeat())</code></pre><p>要使用<code>tf.data.experimental.sample_from_datasets</code>传递数据集以及每个数据集的权重</p><pre><code class="lang-py">balanced_ds = tf.data.experimental.sample_from_datasets(    [negative_ds, positive_ds], [0.5, 0.5]).batch(10)for features, labels in balanced_ds.take(10):  print(labels.numpy())[0 1 1 0 0 1 0 1 0 0][0 1 0 1 1 0 0 1 1 0][1 1 1 0 0 1 1 0 1 1][0 0 0 0 1 1 0 0 1 0][0 1 1 0 0 0 1 0 0 0][1 1 0 0 0 0 0 0 0 0][0 1 0 1 1 0 0 1 1 1][0 0 1 0 1 0 1 0 1 1][1 0 1 1 0 1 0 0 1 0][0 0 0 1 1 1 1 0 1 1]</code></pre><p>现在数据集就平衡了。</p><p>上述<code>experimental.sample_from_datasets</code>方法的一个问题是，每个类需要一个单独的<code>tf.data.Dataset</code>。</p><p>可以将<code>data.experimental.rejection_resample</code>函数应用于数据集，它仅加载一次，通过将多余元素将从数据集中删除以实现平衡。</p><p><code>data.experimental.rejection_resample</code>采用<code>class_func</code>参数，用于标记每个数据集元素所属的类别。</p><p>由于<code>Dataset</code>已经是<code>(features, label)</code>标记好的状态，因此只需</p><pre><code class="lang-py">def class_func(features, label):  return label</code></pre><p>重采样器输入的数据不能为batch后的Dataset，必须经过unbatch。重采样器还需要目标分布，以及可选的初始分布估计。最后经过map中的函数，直接删除掉extra_label即可。</p><pre><code class="lang-py">resampler = tf.data.experimental.rejection_resample(    class_func, target_dist=[0.5, 0.5], initial_dist=fractions)resample_ds = creditcard_ds.unbatch().apply(resampler).batch(10)balanced_ds = resample_ds.map(lambda extra_label, features_and_label: features_and_label)</code></pre><h2 id="数据集迭代器的checkpoint"><a href="#数据集迭代器的checkpoint" class="headerlink" title="数据集迭代器的checkpoint"></a>数据集迭代器的checkpoint</h2><p>没想到吧？不只是模型能使用checkpoint，Dataset的处理过程也可以使用checkpoint。如果您有一个很大的数据集，并且不想在每次重新启动时都从头开始，则这可能很有用。但是请注意，迭代器检查点可能很大，因为诸如shuffle和prefetch需要迭代器中的缓冲元素。</p><p>下面是示例：</p><pre><code class="lang-py">range_ds = tf.data.Dataset.range(20)iterator = iter(range_ds)ckpt = tf.train.Checkpoint(step=tf.Variable(0), iterator=iterator)manager = tf.train.CheckpointManager(ckpt, &#39;/tmp/my_ckpt&#39;, max_to_keep=3)print([next(iterator).numpy() for _ in range(5)])[0, 1, 2, 3, 4]save_path = manager.save()print([next(iterator).numpy() for _ in range(5)])[5, 6, 7, 8, 9]ckpt.restore(manager.latest_checkpoint)print([next(iterator).numpy() for _ in range(5)])[5, 6, 7, 8, 9]</code></pre><h2 id="在Keras中使用tf-data"><a href="#在Keras中使用tf-data" class="headerlink" title="在Keras中使用tf.data"></a>在Keras中使用<code>tf.data</code></h2><p>数据集的处理：</p><pre><code class="lang-py">train, test = tf.keras.datasets.fashion_mnist.load_data()images, labels = trainimages = images/255.0labels = labels.astype(np.int32)fmnist_train_ds = tf.data.Dataset.from_tensor_slices((images, labels))fmnist_train_ds = fmnist_train_ds.shuffle(5000).batch(32)</code></pre><p>模型构建：</p><pre><code class="lang-py">model = tf.keras.Sequential([  tf.keras.layers.Flatten(),  tf.keras.layers.Dense(10)])model.compile(optimizer=&#39;adam&#39;,              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),               metrics=[&#39;accuracy&#39;])</code></pre><p>模型训练：</p><pre><code class="lang-py">model.fit(fmnist_train_ds, epochs=2)</code></pre><p>如果在fit过程中你要对Dataset进行repeat，只需指定每个epochs使用的数据个数，然后不给repeat指定参数，数据集就会变成无限个，一定会满足epochs的要求。</p><pre><code class="lang-py">model.fit(fmnist_train_ds.repeat(), epochs=2, steps_per_epoch=20)</code></pre><p>同理，evaluate时也是一样的</p><pre><code class="lang-py">loss, accuracy = model.evaluate(fmnist_train_ds.repeat(), steps=10)print(&quot;Loss :&quot;, loss)print(&quot;Accuracy :&quot;, accuracy)Loss : 0.3501795828342438Accuracy : 0.8968750238418579</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>如果在使用机器学习算法解决问题过程中，数据预处理是主要问题的话，采用合适的数据组织手段可以帮助解决问题。 <code>tf.data</code> 能够快速处理大量数据，并将各个来源的数据归一化成合适的 <code>Dataset</code> 格式。</p><p>你可以对构建好的 <code>tf.data.Dataset</code> 做预处理操作，比如随机打乱、分批次、规划时间窗口、重采样等等。</p><p>经过处理后的 <code>Dataset</code> 对象可以直接输入到keras进行训练。</p></batchdataset></batchdataset>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用 &lt;code&gt;tf.data&lt;/code&gt; API 可以轻松处理大量数据，支持多样化的数据格式，还可以方便执行复杂的转换。本文介绍了不同类别源数据转化为 &lt;code&gt;tf.data.Dataset&lt;/code&gt; 的方法，以及 &lt;code&gt;Dataset&lt;/code&gt; 常见的预处理方法。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="preprocessing" scheme="https://superlova.github.io/tags/preprocessing/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】使用TensorBoard分析模型性能</title>
    <link href="https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8TensorBoard%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD/"/>
    <id>https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8TensorBoard%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD/</id>
    <published>2020-08-14T08:39:06.000Z</published>
    <updated>2020-08-15T01:04:23.193Z</updated>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第六篇，讲述的是如何使用TensorBoard分析模型的性能，优化模型的资源消耗。<br><a id="more"></a></p><p>TensorBoard可以监控模型的各个组分运行过程中的时间消耗和资源消耗，并根据这些数据对模型下一步优化提出建议。</p><p>首先我们安装性能分析的插件</p><pre><code class="lang-bash">!pip install -U tensorboard_plugin_profile</code></pre><p>定义TensorBoard的回调函数（数据预处理和模型定义略去不表），注意这里新的参数<code>profile_batch</code>只监控第500到520之间的20个Batch，避免监控过多导致模型运行效率过低。</p><pre><code class="lang-python"># Create a TensorBoard callbacklogs = &quot;logs/&quot; + datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)tboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs,                        histogram_freq=1,                        profile_batch=&#39;500,520&#39;)model.fit(ds_train,    epochs=2,    validation_data=ds_test,    callbacks=[tboard_callback])</code></pre><p>打开TensorBoard：</p><pre><code class="lang-bash"># Load the TensorBoard notebook extension.%load_ext tensorboard# Launch TensorBoard and navigate to the Profile tab to view performance profile%tensorboard --logdir=logs</code></pre><p><img src="/2020/08/14/【学习笔记】使用TensorBoard分析模型性能/2020-08-14-16-47-10.png" srcset="/img/loading.gif" alt></p><p>有非常多有用的信息，比如每个batch消耗的时间都花在哪里了：</p><p><img src="/2020/08/14/【学习笔记】使用TensorBoard分析模型性能/2020-08-14-17-09-32.png" srcset="/img/loading.gif" alt></p><p>还有针对耗时的改进意见：</p><p><img src="/2020/08/14/【学习笔记】使用TensorBoard分析模型性能/2020-08-14-17-09-56.png" srcset="/img/loading.gif" alt></p><p>有耗时最长的10大操作：</p><p><img src="/2020/08/14/【学习笔记】使用TensorBoard分析模型性能/2020-08-14-17-10-27.png" srcset="/img/loading.gif" alt></p><p>有性能监控选项，查看CPU活动和GPU活动。根据一般经验，始终保持设备（GPU / TPU）处于活动状态是我们的优化目标。</p><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/profiler_trace_viewer_bad_ip.png" srcset="/img/loading.gif" alt></p><p>查看事件跟踪，可以看到<code>tf_data_iterator_get_next</code> 操作在CPU上运行时GPU不活动。该操作负责处理输入数据并将其发送到GPU进行训练。因此我们的优化方法可以是使用tf.data API优化输入管道，缓存训练数据集并预取数据，以确保始终有可供GPU处理的数据。</p><pre><code class="lang-python"> (ds_train, ds_test), ds_info = tfds.load(    &#39;mnist&#39;,    split=[&#39;train&#39;, &#39;test&#39;],    shuffle_files=True,    as_supervised=True,    with_info=True,)ds_train = ds_train.map(normalize_img)ds_train = ds_train.batch(128)ds_train = ds_train.cache()ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)ds_test = ds_test.map(normalize_img)ds_test = ds_test.batch(128)ds_test = ds_test.cache()ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)model.fit(ds_train,        epochs=2,        validation_data=ds_test,        callbacks = [tboard_callback])%tensorboard --logdir=logs</code></pre><p>跟踪查看器显示<code>tf_data_iterator_get_next</code>操作执行得更快。因此，GPU获得了稳定的数据流以进行训练，并通过模型训练获得了更好的利用率。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是TensorBoard笔记的第六篇，讲述的是如何使用TensorBoard分析模型的性能，优化模型的资源消耗。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="TensorBoard" scheme="https://superlova.github.io/tags/TensorBoard/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】通过TensorBoard可视化词嵌入空间</title>
    <link href="https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%8D%E5%B5%8C%E5%85%A5%E7%A9%BA%E9%97%B4/"/>
    <id>https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%8D%E5%B5%8C%E5%85%A5%E7%A9%BA%E9%97%B4/</id>
    <published>2020-08-14T08:37:06.000Z</published>
    <updated>2020-08-14T08:38:23.417Z</updated>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第五篇，讲述的是如何使用TensorBoard对词嵌入空间可视化。<br><a id="more"></a></p><p>TensorBoard可以将词嵌入空间二维化，方便我们理解词嵌入空间的含义。</p><pre><code class="lang-python">from tensorboard.plugins import projector</code></pre><p>引入IMDb数据集。tensorflow_datasets是tensorflow的官方数据集库。</p><pre><code class="lang-python">import tensorflow_datasets as tfds(train_data, test_data), info = tfds.load(    &quot;imdb_reviews/subwords8k&quot;,    split=(tfds.Split.TRAIN, tfds.Split.TEST),    with_info=True,    as_supervised=True,)encoder = info.features[&quot;text&quot;].encoder# shuffle and pad the data.train_batches = train_data.shuffle(1000).padded_batch(    10, padded_shapes=((None,), ()))test_batches = test_data.shuffle(1000).padded_batch(    10, padded_shapes=((None,), ()))train_batch, train_labels = next(iter(train_batches))</code></pre><p>训练词嵌入模型</p><pre><code class="lang-python"># Create an embedding layerembedding_dim = 16embedding = tf.keras.layers.Embedding(encoder.vocab_size, embedding_dim)# Train this embedding as part of a keras modelmodel = tf.keras.Sequential(    [        embedding, # The embedding layer should be the first layer in a model.        tf.keras.layers.GlobalAveragePooling1D(),        tf.keras.layers.Dense(16, activation=&quot;relu&quot;),        tf.keras.layers.Dense(1),    ])# Compile modelmodel.compile(    optimizer=&quot;adam&quot;,    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),    metrics=[&quot;accuracy&quot;],)# Train modelhistory = model.fit(    train_batches, epochs=1, validation_data=test_batches, validation_steps=20)</code></pre><p>将数据保存成方便TensorBoard读取的形式</p><pre><code class="lang-python"># Set up a logs directory, so Tensorboard knows where to look for fileslog_dir=&#39;/logs/imdb-example/&#39;if not os.path.exists(log_dir):    os.makedirs(log_dir)# Save Labels separately on a line-by-line manner.with open(os.path.join(log_dir, &#39;metadata.tsv&#39;), &quot;w&quot;) as f:  for subwords in encoder.subwords:    f.write(&quot;{}\n&quot;.format(subwords))  # Fill in the rest of the labels with &quot;unknown&quot;  for unknown in range(1, encoder.vocab_size - len(encoder.subwords)):    f.write(&quot;unknown #{}\n&quot;.format(unknown))# Save the weights we want to analyse as a variable. Note that the first# value represents any unknown word, which is not in the metadata, so# we will remove that value.weights = tf.Variable(model.layers[0].get_weights()[0][1:])# Create a checkpoint from embedding, the filename and key are# name of the tensor.checkpoint = tf.train.Checkpoint(embedding=weights)checkpoint.save(os.path.join(log_dir, &quot;embedding.ckpt&quot;))# Set up configconfig = projector.ProjectorConfig()embedding = config.embeddings.add()# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`embedding.tensor_name = &quot;embedding/.ATTRIBUTES/VARIABLE_VALUE&quot;embedding.metadata_path = &#39;metadata.tsv&#39;projector.visualize_embeddings(log_dir, config)</code></pre><pre><code class="lang-python">%tensorboard --logdir /logs/imdb-example/</code></pre><p>你可以看到2维、3维空间中的Embedding，搜索某个单词在词嵌入空间中的位置，甚至可以采取不同的可视化方法：<br><img src="/2020/08/14/【学习笔记】通过TensorBoard可视化词嵌入空间/2020-08-13-19-45-25.png" srcset="/img/loading.gif" alt></p><p>打开该工具的时候我的电脑很卡。。。希望大家注意。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是TensorBoard笔记的第五篇，讲述的是如何使用TensorBoard对词嵌入空间可视化。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="TensorBoard" scheme="https://superlova.github.io/tags/TensorBoard/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】通过TensorBoard调整超参数</title>
    <link href="https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87TensorBoard%E8%B0%83%E6%95%B4%E8%B6%85%E5%8F%82%E6%95%B0/"/>
    <id>https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87TensorBoard%E8%B0%83%E6%95%B4%E8%B6%85%E5%8F%82%E6%95%B0/</id>
    <published>2020-08-14T08:34:46.000Z</published>
    <updated>2020-08-14T08:36:27.555Z</updated>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第四篇，讲述的是如何借助TensorBoard调整模型的超参数。<br><a id="more"></a></p><p>TensorBoard中的HParams仪表板是比较新颖的工具包，提供了多种调节超参数的工具，并且该工具还在不断更新中。</p><pre><code class="lang-python">from tensorboard.plugins.hparams import api as hp</code></pre><p>用过sklearn进行机器学习模型调参的同学应该体验过交叉验证调参的方法。通过提供许多不同的超参数选项，<code>GridSearchCV</code>将训练多个模型，并取性能最优的模型超参数。</p><pre><code class="lang-python">from sklearn.model_selection import GridSearchCVfrom sklearn.pipeline import make_pipelineX_train, X_test, y_train, y_test = train_test_split(df_train[&#39;text&#39;][:10000], df_train[&#39;label&#39;][:10000], random_state=0)pipe_logis = make_pipeline(TfidfVectorizer(min_df=5, ngram_range=(1,3)), LogisticRegression())param_grid = {&#39;logisticregression__C&#39;: [0.001, 0.01, 0.1, 1, 10]}grid = GridSearchCV(pipe_logis, param_grid, cv=5)grid.fit(X_train, y_train)print(&quot;Best params:\n{}\n&quot;.format(grid.best_params_))print(&quot;Best cross-validation score: {:.2f}&quot;.format(grid.best_score_))print(&quot;Test-set score: {:.2f}&quot;.format(grid.score(X_test, y_test)))</code></pre><p>在上面的程序中，grid中包含一个需要调节的超参数，即逻辑回归的C值。候选C值有5个，因此grid在fit过程中会训练五个模型，每个模型执行5次交叉验证（因为fit中cv参数为5）。</p><p>HParams也是采用类似的方法找超参数。首先我们定义候选超参数的变化范围。我们选择三个参数进行网格搜索，分别是Dense层的Unit数目、dropout的比例和优化器，每个超参数都有两种选择，因此一共需要训练八个模型。</p><p>最终模型的评价标准以Accuracy为准。具体代码如下所示：</p><pre><code class="lang-python">HP_NUM_UNITS = hp.HParam(&#39;num_units&#39;, hp.Discrete([16, 32]))HP_DROPOUT = hp.HParam(&#39;dropout&#39;, hp.RealInterval(0.1, 0.2))HP_OPTIMIZER = hp.HParam(&#39;optimizer&#39;, hp.Discrete([&#39;adam&#39;, &#39;sgd&#39;]))METRIC_ACCURACY = &#39;accuracy&#39;</code></pre><p>设置write句柄，这已经是传统艺能了。</p><pre><code class="lang-python">with tf.summary.create_file_writer(&#39;logs/hparam_tuning&#39;).as_default():  hp.hparams_config(    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],    metrics=[hp.Metric(METRIC_ACCURACY, display_name=&#39;Accuracy&#39;)],  )</code></pre><p>接下来我们定义待训练模型。模型本身非常简单，而且只训练一个epoch，这是考虑到要消耗平时八倍的时间而采取的tradeoff。</p><pre><code class="lang-python">def train_test_model(hparams):  model = tf.keras.models.Sequential([    tf.keras.layers.Flatten(),    tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=tf.nn.relu),    tf.keras.layers.Dropout(hparams[HP_DROPOUT]),    tf.keras.layers.Dense(10, activation=tf.nn.softmax),  ])  model.compile(      optimizer=hparams[HP_OPTIMIZER],      loss=&#39;sparse_categorical_crossentropy&#39;,      metrics=[&#39;accuracy&#39;],  )  model.fit(x_train, y_train, epochs=1) # Run with 1 epoch to speed things up for demo purposes  _, accuracy = model.evaluate(x_test, y_test)  return accuracy</code></pre><p>进行训练并记录模型输出；</p><pre><code class="lang-python">def run(run_dir, hparams):  with tf.summary.create_file_writer(run_dir).as_default():    hp.hparams(hparams)  # record the values used in this trial    accuracy = train_test_model(hparams)    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)</code></pre><p>之后我们进行网格搜索（其实就是遍历每种可能。搜索方法完全是自己定义的，你也可以使用随机搜索方法）：</p><pre><code class="lang-python">session_num = 0for num_units in HP_NUM_UNITS.domain.values:  for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):    for optimizer in HP_OPTIMIZER.domain.values:      hparams = {          HP_NUM_UNITS: num_units,          HP_DROPOUT: dropout_rate,          HP_OPTIMIZER: optimizer,      }      run_name = &quot;run-%d&quot; % session_num      print(&#39;--- Starting trial: %s&#39; % run_name)      print({h.name: hparams[h] for h in hparams})      run(&#39;logs/hparam_tuning/&#39; + run_name, hparams)      session_num += 1</code></pre><p>最后使用TensorBoard进行可视化：</p><pre><code class="lang-bash">%tensorboard --logdir logs/hparam_tuning</code></pre><p><img src="https://www.tensorflow.org/tensorboard/images/hparams_table.png?raw=1" srcset="/img/loading.gif" alt></p><p>我们可以通过TensorBoard发现很多有趣的现象：比如在本模型中，adam优化器比sgd要好等等。</p><p><img src="/2020/08/14/【学习笔记】通过TensorBoard调整超参数/2020-08-13-19-19-41.png" srcset="/img/loading.gif" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是TensorBoard笔记的第四篇，讲述的是如何借助TensorBoard调整模型的超参数。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="TensorBoard" scheme="https://superlova.github.io/tags/TensorBoard/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】用TensorBoard生成模型图</title>
    <link href="https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8TensorBoard%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%9B%BE/"/>
    <id>https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8TensorBoard%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%9B%BE/</id>
    <published>2020-08-14T08:31:40.000Z</published>
    <updated>2020-08-14T08:34:09.831Z</updated>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第三篇，讲述的是如何令TensorBoard生成模型架构图。<br><a id="more"></a></p><p>TensorBoard不但可以展示存在的图片和张量，还可以生成图片，诸如模型图等。通过TensorBoard的GRAPHS选项卡，可以快速查看模型结构的预览图，并确保其符合设计预期。</p><p>比如我们定义模型如下：</p><pre><code class="lang-python">model = keras.models.Sequential([    keras.layers.Flatten(input_shape=(28, 28)),    keras.layers.Dense(32, activation=&#39;relu&#39;),    keras.layers.Dropout(0.2),    keras.layers.Dense(10, activation=&#39;softmax&#39;)])model.compile(    optimizer=&#39;adam&#39;,    loss=&#39;sparse_categorical_crossentropy&#39;,    metrics=[&#39;accuracy&#39;])</code></pre><p><img src="/2020/08/14/【学习笔记】用TensorBoard生成模型图/2020-08-13-13-31-51.png" srcset="/img/loading.gif" alt></p><p>通过TensorBoard中的GRAPHS选项卡，我们看到执行图。图是倒置的，数据从下到上流动，因此与代码相比是上下颠倒的。</p><p>可以更改Tag，选择Keras，选择左边的Conceptual Graph查看概念图，双击Sequential，得到概念图。概念图更像是代码。</p><p><img src="/2020/08/14/【学习笔记】用TensorBoard生成模型图/2020-08-13-13-35-03.png" srcset="/img/loading.gif" alt></p><p>有的时候我们希望得到计算图，研究数据经过了何种计算。比如下面这个函数：</p><pre><code class="lang-python"># Sample data for your function.x = tf.random.uniform((3, 3))y = tf.random.uniform((3, 3))# The function to be traced.@tf.functiondef my_func(x, y):  # A simple hand-rolled layer.  return tf.nn.relu(tf.matmul(x, y))# z = my_func(x, y)</code></pre><p>我们希望得到它的计算图。首先需要使用<code>@tf.function</code>修饰被监控的函数，然后使用<code>tf.summary.trace_on()</code>在<code>z = my_func(x, y)</code>函数运行之前开始记录。</p><p><img src="https://as2.bitinn.net/uploads/legacy/og/cistioqrt008t8q5nh9gtx9og.1200.jpg" srcset="/img/loading.gif" alt="Trace On"></p><pre><code class="lang-python">tf.summary.trace_on(graph=True, profiler=True)z = my_func(x, y)</code></pre><p>定义日志目录名称和文件写入句柄，这些都是刻在DNA里的操作：</p><pre><code class="lang-python"># Set up logging.stamp = datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)logdir = &#39;logs/func/%s&#39; % stampwriter = tf.summary.create_file_writer(logdir)</code></pre><p>最后执行记录：</p><pre><code class="lang-python"># Call only one tf.function when tracing.with writer.as_default():  tf.summary.trace_export(      name=&quot;my_func_trace&quot;,      step=0,      profiler_outdir=logdir)</code></pre><pre><code class="lang-bash">%tensorboard --logdir logs/func</code></pre><p><img src="/2020/08/14/【学习笔记】用TensorBoard生成模型图/2020-08-13-13-47-57.png" srcset="/img/loading.gif" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是TensorBoard笔记的第三篇，讲述的是如何令TensorBoard生成模型架构图。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="TensorBoard" scheme="https://superlova.github.io/tags/TensorBoard/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】用TensorBoard展示图片</title>
    <link href="https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8TensorBoard%E5%B1%95%E7%A4%BA%E5%9B%BE%E7%89%87/"/>
    <id>https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8TensorBoard%E5%B1%95%E7%A4%BA%E5%9B%BE%E7%89%87/</id>
    <published>2020-08-14T08:28:29.000Z</published>
    <updated>2020-08-14T08:31:11.010Z</updated>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第二篇，讲述的是如何令TensorBoard展示外界已有的图片和Tensor。<br><a id="more"></a></p><p>我们可以利用TensorBoard展示图片类数据，或者通过tf.summary将张量类数据转化成图片。下面是对Fashion-MNIST数据集中部分图片的可视化：</p><h3 id="可视化单个图片"><a href="#可视化单个图片" class="headerlink" title="可视化单个图片"></a>可视化单个图片</h3><pre><code class="lang-python">import tensorflow as tffrom tensorflow import kerasfashion_mnist = keras.datasets.fashion_mnist(train_images, train_labels), (test_images, test_labels) = \    fashion_mnist.load_data()</code></pre><p>数据集中每个图像的形状都是2阶张量形状（28、28），分别表示高度和宽度</p><p>但是， tf.summary.image()期望包含(batch_size, height, width, channels)的4级张量。因此，张量需要重塑。</p><pre><code class="lang-python">img = np.reshape(train_images[0], (-1, 28, 28, 1))</code></pre><p>使用<code>tf.summary.image</code>将其转化为tensor，并利用TensorBoard可视化：</p><pre><code class="lang-python"> # Clear out any prior log data.!rm -rf logs# Sets up a timestamped log directory.logdir = &quot;logs/train_data/&quot; + datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)# Creates a file writer for the log directory.file_writer = tf.summary.create_file_writer(logdir)# Using the file writer, log the reshaped image.with file_writer.as_default():  tf.summary.image(&quot;Training data&quot;, img, step=0)</code></pre><p>转化后的图片被<code>tf.summary.create_file_writer</code>输出到logdir里面了。使用TensorBoard看看：</p><pre><code class="lang-bash">%tensorboard --logdir logs/train_data</code></pre><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_single.png" srcset="/img/loading.gif" alt></p><p>加载的过程可能有点慢，注意留足够的内存以免标签页崩溃。</p><p>你也可以使用左边的滑动条调节亮度、对比度和大小。</p><h3 id="可视化多张图片"><a href="#可视化多张图片" class="headerlink" title="可视化多张图片"></a>可视化多张图片</h3><p>调整<code>tf.summary.image</code>里面的参数<code>max_outputs</code>：</p><pre><code class="lang-python"> with file_writer.as_default():    # Don&#39;t forget to reshape.    images = np.reshape(train_images[0:25], (-1, 28, 28, 1))    tf.summary.image(&quot;25 training data examples&quot;, images, max_outputs=25, step=0)</code></pre><pre><code class="lang-bash">%tensorboard --logdir logs/train_data</code></pre><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_multiple.png" srcset="/img/loading.gif" alt></p><h3 id="可视化其他格式的图片"><a href="#可视化其他格式的图片" class="headerlink" title="可视化其他格式的图片"></a>可视化其他格式的图片</h3><p>有些图片不是tensor或者numpy.array，而是由诸如opencv、matplotlib生成的png图像，我们需要将其转化为tensor。</p><p>由于matplotlib适合生成复杂的数据图，因此先利用其他库生成图片，随后利用<code>tf.summary.image</code>将其转化为一个tensor再可视化，是一个比较方便的选择。</p><p>matplotlib生成数据集可视化：</p><pre><code class="lang-python"> # Clear out prior logging data.!rm -rf logs/plotslogdir = &quot;logs/plots/&quot; + datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)file_writer = tf.summary.create_file_writer(logdir)def plot_to_image(figure):  &quot;&quot;&quot;Converts the matplotlib plot specified by &#39;figure&#39; to a PNG image and  returns it. The supplied figure is closed and inaccessible after this call.&quot;&quot;&quot;  # Save the plot to a PNG in memory.  buf = io.BytesIO()  plt.savefig(buf, format=&#39;png&#39;)  # Closing the figure prevents it from being displayed directly inside  # the notebook.  plt.close(figure)  buf.seek(0)  # Convert PNG buffer to TF image  image = tf.image.decode_png(buf.getvalue(), channels=4)  # Add the batch dimension  image = tf.expand_dims(image, 0)  return imagedef image_grid():  &quot;&quot;&quot;Return a 5x5 grid of the MNIST images as a matplotlib figure.&quot;&quot;&quot;  # Create a figure to contain the plot.  figure = plt.figure(figsize=(10,10))  for i in range(25):    # Start next subplot.    plt.subplot(5, 5, i + 1, title=class_names[train_labels[i]])    plt.xticks([])    plt.yticks([])    plt.grid(False)    plt.imshow(train_images[i], cmap=plt.cm.binary)  return figure</code></pre><p>尔后，利用<code>tf.summary.image</code>转化：</p><pre><code class="lang-python"># Prepare the plotfigure = image_grid()# Convert to image and logwith file_writer.as_default():  tf.summary.image(&quot;Training data&quot;, plot_to_image(figure), step=0)</code></pre><p>最后，利用TensorBoard可视化：</p><pre><code class="lang-bash">%tensorboard --logdir logs/plots</code></pre><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_arbitrary.png" srcset="/img/loading.gif" alt></p><h3 id="在图片分类器中使用TensorBoard"><a href="#在图片分类器中使用TensorBoard" class="headerlink" title="在图片分类器中使用TensorBoard"></a>在图片分类器中使用TensorBoard</h3><p>之前我们通过TensorBoard了解了Fashion-MNIST数据集的概要，但是TensorBoard的功能不止于此。</p><p>首先构建分类模型：</p><pre><code class="lang-python">model = keras.models.Sequential([    keras.layers.Flatten(input_shape=(28, 28)),    keras.layers.Dense(32, activation=&#39;relu&#39;),    keras.layers.Dense(10, activation=&#39;softmax&#39;)])model.compile(    optimizer=&#39;adam&#39;,     loss=&#39;sparse_categorical_crossentropy&#39;,    metrics=[&#39;accuracy&#39;])</code></pre><p>我们想使用<a href="https://en.wikipedia.org/wiki/Confusion_matrix" target="_blank" rel="noopener">混淆矩阵</a>详细了解分类器对测试数据的性能。因此接下来定义一个函数，专门计算混淆矩阵。具体来说，</p><ol><li>使用model.predict预测该epoch的所有测试用例的标签，得到<code>test_pred</code></li><li>调用<code>sklearn.metrics.confusion_matrix</code>直接计算混淆矩阵</li><li>使用<code>matplotlib</code>将混淆矩阵可视化</li><li>将<code>matplotlib</code>生成的图片转为tensor，最后变成log储存</li></ol><p>下面是前两步所需的操作：</p><pre><code class="lang-python">def log_confusion_matrix(epoch, logs):    # Use the model to predict the values from the validation dataset.    test_pred_raw = model.predict(test_images)    test_pred = np.argmax(test_pred_raw, axis=1)    # Calculate the confusion matrix.    cm = sklearn.metrics.confusion_matrix(test_labels, test_pred)    # Log the confusion matrix as an image summary.    figure = plot_confusion_matrix(cm, class_names=class_names)    cm_image = plot_to_image(figure)    # Log the confusion matrix as an image summary.    with file_writer_cm.as_default():    tf.summary.image(&quot;Confusion Matrix&quot;, cm_image, step=epoch)</code></pre><p>下面是第三步所需的可视化函数：</p><pre><code class="lang-python">def plot_confusion_matrix(cm, class_names):    &quot;&quot;&quot;    Returns a matplotlib figure containing the plotted confusion matrix.    Args:    cm (array, shape = [n, n]): a confusion matrix of integer classes    class_names (array, shape = [n]): String names of the integer classes    &quot;&quot;&quot;    figure = plt.figure(figsize=(8, 8))    plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=plt.cm.Blues)    plt.title(&quot;Confusion matrix&quot;)    plt.colorbar()    tick_marks = np.arange(len(class_names))    plt.xticks(tick_marks, class_names, rotation=45)    plt.yticks(tick_marks, class_names)    # Normalize the confusion matrix.    cm = np.around(cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis], decimals=2)    # Use white text if squares are dark; otherwise black.    threshold = cm.max() / 2.    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):    color = &quot;white&quot; if cm[i, j] &gt; threshold else &quot;black&quot;    plt.text(j, i, cm[i, j], horizontalalignment=&quot;center&quot;, color=color)    plt.tight_layout()    plt.ylabel(&#39;True label&#39;)    plt.xlabel(&#39;Predicted label&#39;)    return figure</code></pre><p>下面是第四步所需的tensor转化和储存函数以及其他回调函数：</p><pre><code class="lang-bash"># Clear out prior logging data.!rm -rf logs/image</code></pre><pre><code class="lang-python">logdir = &quot;logs/image/&quot; + datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)# Define the basic TensorBoard callback.tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)file_writer_cm = tf.summary.create_file_writer(logdir + &#39;/cm&#39;)</code></pre><p>让我们开始训练：</p><pre><code class="lang-bash"># Start TensorBoard.%tensorboard --logdir logs/image</code></pre><pre><code class="lang-python"># Train the classifier.model.fit(    train_images,    train_labels,    epochs=5,    verbose=0, # Suppress chatty output    callbacks=[tensorboard_callback, cm_callback],    validation_data=(test_images, test_labels),)</code></pre><p>请注意，此时我先调用的TensorBoard，然后开始的训练，并且我设置了verbose=0，意味着信息完全通过TensorBoard动态展示。训练过程中你就可以看到参数的变化。</p><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_accuracy.png" srcset="/img/loading.gif" alt></p><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_cm.png" srcset="/img/loading.gif" alt></p><p>你还可以看到，随着训练的进行，矩阵是如何发生变化的：沿着对角线的正方形会逐渐变暗，而矩阵的其余部分趋向于0和白色。这意味着分类器正在不断改进。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是TensorBoard笔记的第二篇，讲述的是如何令TensorBoard展示外界已有的图片和Tensor。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="TensorBoard" scheme="https://superlova.github.io/tags/TensorBoard/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】TensorBoard简介</title>
    <link href="https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91TensorBoard%E7%AE%80%E4%BB%8B/"/>
    <id>https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91TensorBoard%E7%AE%80%E4%BB%8B/</id>
    <published>2020-08-14T07:28:09.000Z</published>
    <updated>2020-08-14T08:25:08.319Z</updated>
    
    <content type="html"><![CDATA[<p>简单介绍了TensorBoard的用途和使用方法。<br><a id="more"></a></p><h2 id="TensorBoard简介"><a href="#TensorBoard简介" class="headerlink" title="TensorBoard简介"></a>TensorBoard简介</h2><p>TensorBoard是Google开发的模型内部参数跟踪和可视化的调试工具。在Tensorflow中，用TensorBoard可以监控模型的各种指标的变化（如acc、loss的动态变化），可以将模型结构可视化，可以可视化词嵌入空间，可以分析模型性能，可以分析数据集的公平性等等，是一个非常强大且非常简单的工具。</p><p>TensorBoard核心就是回调函数和可视化操作面板。通过编写回调函数获取模型信息，通过命令行启动TensorBoard图形化界面。</p><p>TensorBoard的回调函数API为：</p><pre><code class="lang-python">tf.keras.callbacks.TensorBoard(    log_dir=&#39;logs&#39;, histogram_freq=0, write_graph=True, write_images=False,    update_freq=&#39;epoch&#39;, profile_batch=2, embeddings_freq=0,    embeddings_metadata=None, **kwargs)</code></pre><div class="table-container"><table><thead><tr><th>参数</th><th>含义</th></tr></thead><tbody><tr><td>log_dir</td><td>模型的信息保存目录</td></tr><tr><td>histogram_freq</td><td>模型激活和参数信息记录的频率，每隔几个epochs记录一次</td></tr><tr><td>write_graph</td><td>是否保存模型图文件</td></tr><tr><td>write_images</td><td>是否保存模型参数可视化图</td></tr><tr><td>update_freq</td><td>模型loss和其他metrics的记录频率，每隔几个batch更新一次</td></tr><tr><td>profile_batch</td><td>指定性能分析时使用的批次</td></tr><tr><td>embeddings_freq</td><td>embedding 层更新的频率</td></tr></tbody></table></div><p>在Colab中使用TensorBoard需输入：</p><pre><code class="lang-bash">%load_ext tensorboard</code></pre><p>为了跟踪模型训练过程，需要在模型的<code>fit</code>过程中添加回调函数</p><pre><code class="lang-python">tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)</code></pre><p>其中<code>log_dir</code>为你想储存log的目录，在教程中，<code>log_dir=&quot;logs/fit/&quot; + datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)</code>。</p><pre><code class="lang-python">model.fit(x=x_train,           y=y_train,           epochs=5,           validation_data=(x_test, y_test),           callbacks=[tensorboard_callback])</code></pre><p>在<code>log_dir</code>中生成了一系列的日志文件：</p><p><img src="/2020/08/14/【学习笔记】TensorBoard简介/2020-08-13-12-39-02.png" srcset="/img/loading.gif" alt></p><p>这些日志文件可以通过TensorBoard解析：</p><pre><code class="lang-shell">%tensorboard --logdir logs/fit</code></pre><p>在命令行中， 运行不带“％”的相同命令。结果如下：</p><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/quickstart_model_fit.png" srcset="/img/loading.gif" alt></p><p>在TensorBoard面板上，有四个选项卡：</p><ul><li><strong>Scalars</strong> 显示损失和指标在每个时期如何变化</li><li><strong>Graphs</strong> 可帮助您可视化模型</li><li><strong>Distributions</strong> 和 <strong>Histograms</strong> 显示张量随时间的分布</li></ul><p>如果使用tensorflow原生API训练模型，也可以利用<code>tf.summary</code>记录log，然后利用TensorBoard可视化。具体流程如下：</p><ol><li>使用 <code>tf.summary.create_file_writer()</code> 创建文件编写器；</li><li>使用 <code>tf.summary.scalar()</code> 记录感兴趣的指标</li><li>将 <code>LearningRateScheduler</code> 回调传递给 <code>Model.fit()</code></li><li>使用命令行<code>tensorboard --logdir logs/fit</code>打开可视化界面</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;简单介绍了TensorBoard的用途和使用方法。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="TensorBoard" scheme="https://superlova.github.io/tags/TensorBoard/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】新闻文本分类之注意力机制与预训练模型</title>
    <link href="https://superlova.github.io/2020/08/04/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%8E%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    <id>https://superlova.github.io/2020/08/04/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%8E%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-08-04T12:13:28.000Z</published>
    <updated>2020-08-04T15:56:45.063Z</updated>
    
    <content type="html"><![CDATA[<p>见多了优秀的文章，再写博客的时候就会感叹自己的学识浅薄。<br><a id="more"></a></p><p>本章介绍了Bert的原理和使用，具体包括pretrain和finetune两部分。</p><h2 id="基于深度学习的文本分类"><a href="#基于深度学习的文本分类" class="headerlink" title="基于深度学习的文本分类"></a>基于深度学习的文本分类</h2><h4 id="Transformer原理"><a href="#Transformer原理" class="headerlink" title="Transformer原理"></a>Transformer原理</h4><p>当用神经网络来处理大量的输入信息时，可以借鉴人脑的注意力机制，只选择一些关键的信息输入进行处理，来提高神经网络的效率。注意力机制可以单独使用，但更多地用作神经网络中的一个组件。基于循环神经网络的序列到序列模型的一个缺点是无法并行计算，为了提高并行计算效率以及捕捉长距离的依赖关系，我们可以使用自注意力模型（Self-Attention Model）来建立一个全连接的网络结构。</p><p>Transformer模型是一个基于多头自注意力的序列到序列模型，包含编码器和解码器两部分。</p><p><img src="https://img-blog.csdnimg.cn/20200714211046668.png" srcset="/img/loading.gif" alt></p><h3 id="基于Bert的文本分类"><a href="#基于Bert的文本分类" class="headerlink" title="基于Bert的文本分类"></a>基于Bert的文本分类</h3><p>分成Pretrain和Fine-Tune两部分。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;见多了优秀的文章，再写博客的时候就会感叹自己的学识浅薄。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="Classification" scheme="https://superlova.github.io/tags/Classification/"/>
    
      <category term="Deep Learning" scheme="https://superlova.github.io/tags/Deep-Learning/"/>
    
      <category term="attention" scheme="https://superlova.github.io/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】新闻文本分类之深度学习Word2Vec</title>
    <link href="https://superlova.github.io/2020/07/31/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Word2Vec/"/>
    <id>https://superlova.github.io/2020/07/31/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Word2Vec/</id>
    <published>2020-07-31T14:24:16.000Z</published>
    <updated>2020-07-31T14:27:44.969Z</updated>
    
    <content type="html"><![CDATA[<p>谦虚使人进步，骄傲使人落后。<br><a id="more"></a></p><p>最近课题内容较多时间紧张，同时我认为学习资料非常翔实，遂转发以做笔记。之后会在文章最后总结自己的思路和经验。</p><h1 id="Task5-基于深度学习的文本分类2"><a href="#Task5-基于深度学习的文本分类2" class="headerlink" title="Task5 基于深度学习的文本分类2"></a>Task5 基于深度学习的文本分类2</h1><p>在上一章节，我们通过FastText快速实现了基于深度学习的文本分类模型，但是这个模型并不是最优的。在本章我们将继续深入。</p><h2 id="基于深度学习的文本分类"><a href="#基于深度学习的文本分类" class="headerlink" title="基于深度学习的文本分类"></a>基于深度学习的文本分类</h2><p>本章将继续学习基于深度学习的文本分类。</p><h3 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>学习Word2Vec的使用和基础原理</li><li>学习使用TextCNN、TextRNN进行文本表示</li><li>学习使用HAN网络结构完成文本分类</li></ul><h3 id="文本表示方法-Part3"><a href="#文本表示方法-Part3" class="headerlink" title="文本表示方法 Part3"></a>文本表示方法 Part3</h3><h4 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h4><p>本节通过word2vec学习词向量。word2vec模型背后的基本思想是对出现在上下文环境里的词进行预测。对于每一条输入文本，我们选取一个上下文窗口和一个中心词，并基于这个中心词去预测窗口里其他词出现的概率。因此，word2vec模型可以方便地从新增语料中学习到新增词的向量表达，是一种高效的在线学习算法（online learning）。</p><p>word2vec的主要思路：通过单词和上下文彼此预测，对应的两个算法分别为：</p><ul><li><p>Skip-grams (SG)：预测上下文</p></li><li><p>Continuous Bag of Words (CBOW)：预测目标单词</p></li></ul><p>另外提出两种更加高效的训练方法：</p><ul><li><p>Hierarchical softmax</p></li><li><p>Negative sampling</p></li></ul><p><strong>1. Skip-grams原理和网络结构</strong></p><p>Word2Vec模型中，主要有Skip-Gram和CBOW两种模型，从直观上理解，Skip-Gram是给定input word来预测上下文。而CBOW是给定上下文，来预测input word。</p><p><img src="https://img-blog.csdnimg.cn/20200714210354729.png" srcset="/img/loading.gif" alt="skip_grams"></p><p>Word2Vec模型实际上分为了两个部分，第一部分为建立模型，第二部分是通过模型获取嵌入词向量。</p><p>Word2Vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”。</p><p>Skip-grams过程</p><p>假如我们有一个句子“The dog barked at the mailman”。</p><ol><li><p>首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为input word；</p></li><li><p>有了input word以后，我们再定义一个叫做skip_window的参数，它代表着我们从当前input word的一侧（左边或右边）选取词的数量。如果我们设置skip_window=2，那么我们最终获得窗口中的词（包括input word在内）就是[‘The’, ‘dog’，’barked’, ‘at’]。skip_window=2代表着选取左input word左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小span=2x2=4。另一个参数叫num_skips，它代表着我们从整个窗口中选取多少个不同的词作为我们的output word，当skip_window=2，num_skips=2时，我们将会得到两组 (input word, output word) 形式的训练数据，即 (‘dog’, ‘barked’)，(‘dog’, ‘the’)。</p></li><li><p>神经网络基于这些训练数据将会输出一个概率分布，这个概率代表着我们的词典中的每个词作为input word的output word的可能性。这句话有点绕，我们来看个例子。第二步中我们在设置skip_window和num_skips=2的情况下获得了两组训练数据。假如我们先拿一组数据 (‘dog’, ‘barked’) 来训练神经网络，那么模型通过学习这个训练样本，会告诉我们词汇表中每个单词当’dog’作为input word时，其作为output word的可能性。</p></li></ol><p>也就是说模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现。例如：如果我们向神经网络模型中输入一个单词“Soviet“，那么最终模型的输出概率中，像“Union”， ”Russia“这种相关词的概率将远高于像”watermelon“，”kangaroo“非相关词的概率。因为”Union“，”Russia“在文本中更大可能在”Soviet“的窗口中出现。</p><p>我们将通过给神经网络输入文本中成对的单词来训练它完成上面所说的概率计算。下面的图中给出了一些我们训练样本的例子。我们选定句子“The quick brown fox jumps over lazy dog”，设定我们的窗口大小为2（window_size=2），也就是说我们仅选输入词前后各两个词和输入词进行组合。下图中，蓝色代表input word，方框内代表位于窗口内的单词。</p><p><img src="https://img-blog.csdnimg.cn/20200721190035764.png" srcset="/img/loading.gif" alt="1"></p><p><img src="https://img-blog.csdnimg.cn/20200714210519939.png" srcset="/img/loading.gif" alt="2"></p><p>我们的模型将会从每对单词出现的次数中习得统计结果。例如，我们的神经网络可能会得到更多类似（“Soviet“，”Union“）这样的训练样本对，而对于（”Soviet“，”Sasquatch“）这样的组合却看到的很少。因此，当我们的模型完成训练后，给定一个单词”Soviet“作为输入，输出的结果中”Union“或者”Russia“要比”Sasquatch“被赋予更高的概率。</p><p>PS：input word和output word都会被我们进行one-hot编码。仔细想一下，我们的输入被one-hot编码以后大多数维度上都是0（实际上仅有一个位置为1），所以这个向量相当稀疏，那么会造成什么结果呢。如果我们将一个1 x 10000的向量和10000 x 300的矩阵相乘，它会消耗相当大的计算资源，为了高效计算，它仅仅会选择矩阵中对应的向量中维度值为1的索引行：</p><p><img src="https://img-blog.csdnimg.cn/20200714205344406.png" srcset="/img/loading.gif" alt></p><p><strong>2. Skip-grams训练</strong></p><p>由上部分可知，Word2Vec模型是一个超级大的神经网络（权重矩阵规模非常大）。例如：我们拥有10000个单词的词汇表，我们如果想嵌入300维的词向量，那么我们的输入-隐层权重矩阵和隐层-输出层的权重矩阵都会有 10000 x 300 = 300万个权重，在如此庞大的神经网络中进行梯度下降是相当慢的。更糟糕的是，你需要大量的训练数据来调整这些权重并且避免过拟合。百万数量级的权重矩阵和亿万数量级的训练样本意味着训练这个模型将会是个灾难</p><p>解决方案：</p><ul><li><p>将常见的单词组合（word pairs）或者词组作为单个“words”来处理</p></li><li><p>对高频次单词进行抽样来减少训练样本的个数</p></li><li><p>对优化目标采用“negative sampling”方法，这样每个训练样本的训练只会更新一小部分的模型权重，从而降低计算负担</p></li></ul><p><em>2.1 Word pairs and “phases”</em></p><p>一些单词组合（或者词组）的含义和拆开以后具有完全不同的意义。比如“Boston Globe”是一种报刊的名字，而单独的“Boston”和“Globe”这样单个的单词却表达不出这样的含义。因此，在文章中只要出现“Boston Globe”，我们就应该把它作为一个单独的词来生成其词向量，而不是将其拆开。同样的例子还有“New York”，“United Stated”等。</p><p>在Google发布的模型中，它本身的训练样本中有来自Google News数据集中的1000亿的单词，但是除了单个单词以外，单词组合（或词组）又有3百万之多。</p><p><em>2.2 对高频词抽样</em></p><p>在上一部分中，对于原始文本为“The quick brown fox jumps over the laze dog”，如果使用大小为2的窗口，那么我们可以得到图中展示的那些训练样本。</p><p><img src="https://img-blog.csdnimg.cn/20200714210458879.png" srcset="/img/loading.gif" alt="1"></p><p>但是对于“the”这种常用高频单词，这样的处理方式会存在下面两个问题：</p><ol><li><p>当我们得到成对的单词训练样本时，(“fox”, “the”) 这样的训练样本并不会给我们提供关于“fox”更多的语义信息，因为“the”在每个单词的上下文中几乎都会出现</p></li><li><p>由于在文本中“the”这样的常用词出现概率很大，因此我们将会有大量的（”the“，…）这样的训练样本，而这些样本数量远远超过了我们学习“the”这个词向量所需的训练样本数</p></li></ol><p>Word2Vec通过“抽样”模式来解决这种高频词问题。它的基本思想如下：对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关。</p><p>ωi 是一个单词，Z(ωi) 是 ωi 这个单词在所有语料中出现的频次，例如：如果单词“peanut”在10亿规模大小的语料中出现了1000次，那么 Z(peanut) = 1000/1000000000 = 1e - 6。</p><p>P(ωi) 代表着保留某个单词的概率：</p><p><img src="https://img-blog.csdnimg.cn/20200714205456898.png" srcset="/img/loading.gif" alt></p><p><em>2.3 Negative sampling</em></p><p>训练一个神经网络意味着要输入训练样本并且不断调整神经元的权重，从而不断提高对目标的准确预测。每当神经网络经过一个训练样本的训练，它的权重就会进行一次调整。</p><p>所以，词典的大小决定了我们的Skip-Gram神经网络将会拥有大规模的权重矩阵，所有的这些权重需要通过数以亿计的训练样本来进行调整，这是非常消耗计算资源的，并且实际中训练起来会非常慢。</p><p>负采样（negative sampling）解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。</p><p>当我们用训练样本 ( input word: “fox”，output word: “quick”) 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的词典大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为“negative” word。</p><p>当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“）。</p><p>PS: 在论文中，作者指出指出对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。</p><p>我们使用“一元模型分布（unigram distribution）”来选择“negative words”。个单词被选作negative sample的概率跟它出现的频次有关，出现频次越高的单词越容易被选作negative words。</p><p>每个单词被选为“negative words”的概率计算公式：</p><p><img src="https://img-blog.csdnimg.cn/20200714205545327.png" srcset="/img/loading.gif" alt></p><p>其中 f(ωi)代表着单词出现的频次，而公式中开3/4的根号完全是基于经验的。</p><p>在代码负采样的代码实现中，unigram table有一个包含了一亿个元素的数组，这个数组是由词汇表中每个单词的索引号填充的，并且这个数组中有重复，也就是说有些单词会出现多次。那么每个单词的索引在这个数组中出现的次数该如何决定呢，有公式，也就是说计算出的负采样概率*1亿=单词在表中出现的次数。</p><p>有了这张表以后，每次去我们进行负采样时，只需要在0-1亿范围内生成一个随机数，然后选择表中索引号为这个随机数的那个单词作为我们的negative word即可。一个单词的负采样概率越大，那么它在这个表中出现的次数就越多，它被选中的概率就越大。</p><p><strong>3. Hierarchical Softmax</strong></p><p><em>3.1 霍夫曼树</em></p><p>输入：权值为(w1,w2,…wn)的n个节点</p><p>输出：对应的霍夫曼树</p><ol><li><p>将(w1,w2,…wn)看做是有n棵树的森林，每个树仅有一个节点</p></li><li><p>在森林中选择根节点权值最小的两棵树进行合并，得到一个新的树，这两颗树分布作为新树的左右子树。新树的根节点权重为左右子树的根节点权重之和</p></li><li><p>将之前的根节点权值最小的两棵树从森林删除，并把新树加入森林</p></li><li><p>重复步骤 2 和 3 直到森林里只有一棵树为止</p></li></ol><p>下面我们用一个具体的例子来说明霍夫曼树建立的过程，我们有(a，b，c，d，e，f)共6个节点，节点的权值分布是(16，4，8，6，20，3)。</p><p> 首先是最小的b和f合并，得到的新树根节点权重是7.此时森林里5棵树，根节点权重分别是16，8，6，20，7。此时根节点权重最小的6，7合并，得到新子树，依次类推，最终得到下面的霍夫曼树。</p><p><img src="https://img-blog.csdnimg.cn/20200714210647687.png" srcset="/img/loading.gif" alt="3"></p><p>那么霍夫曼树有什么好处呢？一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。如何编码呢？一般对于一个霍夫曼树的节点（根节点除外），可以约定左子树编码为0，右子树编码为1。如上图，则可以得到c的编码是00。</p><p>  在word2vec中，约定编码方式和上面的例子相反，即约定左子树编码为1，右子树编码为0，同时约定左子树的权重不小于右子树的权重。</p><p>更多原理可参考：<a href="https://blog.csdn.net/lzw66666/article/details/78934893" target="_blank" rel="noopener">霍夫曼树原理</a></p><p><em>3.2Hierarchical Softmax过程</em></p><p>为了避免要计算所有词的softmax概率，word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。</p><p>霍夫曼树的建立：</p><ul><li><p>根据标签（label）和频率建立霍夫曼树（label出现的频率越高，Huffman树的路径越短）</p></li><li><p>Huffman树中每一叶子结点代表一个label</p></li></ul><p><img src="https://img-blog.csdnimg.cn/20200714205623583.png" srcset="/img/loading.gif" alt="4"></p><p>如上图所示：</p><p><img src="https://img-blog.csdnimg.cn/20200714205711676.png" srcset="/img/loading.gif" alt></p><p><img src="https://img-blog.csdnimg.cn/20200714205759860.png" srcset="/img/loading.gif" alt></p><p>注意：此时的theta是一个待定系数，它是由推导最大似然之后求解得到迭代式子。</p><p><img src="https://img-blog.csdnimg.cn/20200714205841871.png" srcset="/img/loading.gif" alt></p><p><strong>使用gensim训练word2vec</strong></p><pre><code>from gensim.models.word2vec import Word2Vecmodel = Word2Vec(sentences, workers=num_workers, size=num_features)</code></pre><p><strong>参考：</strong></p><ol><li><p><a href="http://www.hankcs.com/nlp/word-vector-representations-word2vec.html" target="_blank" rel="noopener">CS224n笔记2 词的向量表示：word2vec</a></p></li><li><p><a href="http://www.52nlp.cn/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E4%BA%8C%E8%AE%B2%E8%AF%8D%E5%90%91%E9%87%8F" target="_blank" rel="noopener">斯坦福大学深度学习与自然语言处理第二讲：词向量 </a></p></li><li><p><a href="https://www.cnblogs.com/iloveai/p/cs224d-lecture3-note.html" target="_blank" rel="noopener">(Stanford CS224d) Deep Learning and NLP课程笔记（三）：GloVe与模型的评估</a></p></li><li><p><a href="http://www.cnblogs.com/pinard/p/7249903.html" target="_blank" rel="noopener">http://www.cnblogs.com/pinard/p/7249903.html</a></p></li><li><p><a href="https://blog.csdn.net/yinkun6514/article/details/79218736" target="_blank" rel="noopener">https://blog.csdn.net/yinkun6514/article/details/79218736</a></p></li><li><p><a href="https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html" target="_blank" rel="noopener">https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html</a></p></li></ol><h4 id="TextCNN"><a href="#TextCNN" class="headerlink" title="TextCNN"></a>TextCNN</h4><p>TextCNN利用CNN（卷积神经网络）进行文本特征抽取，不同大小的卷积核分别抽取n-gram特征，卷积计算出的特征图经过MaxPooling保留最大的特征值，然后将拼接成一个向量作为文本的表示。</p><p>这里我们基于TextCNN原始论文的设定，分别采用了100个大小为2,3,4的卷积核，最后得到的文本向量大小为100*3=300维。</p><p><img src="https://img-blog.csdnimg.cn/20200714205932720.jpeg" srcset="/img/loading.gif" alt></p><h4 id="TextRNN"><a href="#TextRNN" class="headerlink" title="TextRNN"></a>TextRNN</h4><p>TextRNN利用RNN（循环神经网络）进行文本特征抽取，由于文本本身是一种序列，而LSTM天然适合建模序列数据。TextRNN将句子中每个词的词向量依次输入到双向双层LSTM，分别将两个方向最后一个有效位置的隐藏层拼接成一个向量作为文本的表示。</p><p><img src="https://img-blog.csdnimg.cn/20200714210806492.png" srcset="/img/loading.gif" alt="5"></p><h3 id="基于TextCNN、TextRNN的文本表示"><a href="#基于TextCNN、TextRNN的文本表示" class="headerlink" title="基于TextCNN、TextRNN的文本表示"></a>基于TextCNN、TextRNN的文本表示</h3><h4 id="TextCNN-1"><a href="#TextCNN-1" class="headerlink" title="TextCNN"></a>TextCNN</h4><ul><li>模型搭建</li></ul><pre><code>self.filter_sizes = [2, 3, 4]  # n-gram windowself.out_channel = 100self.convs = nn.ModuleList([nn.Conv2d(1, self.out_channel, (filter_size, input_size), bias=True) for filter_size in self.filter_sizes])</code></pre><ul><li>前向传播</li></ul><pre><code>pooled_outputs = []for i in range(len(self.filter_sizes)):    filter_height = sent_len - self.filter_sizes[i] + 1    conv = self.convs[i](batch_embed)    hidden = F.relu(conv)  # sen_num x out_channel x filter_height x 1    mp = nn.MaxPool2d((filter_height, 1))  # (filter_height, filter_width)    # sen_num x out_channel x 1 x 1 -&gt; sen_num x out_channel    pooled = mp(hidden).reshape(sen_num, self.out_channel)    pooled_outputs.append(pooled)</code></pre><h4 id="TextRNN-1"><a href="#TextRNN-1" class="headerlink" title="TextRNN"></a>TextRNN</h4><ul><li>模型搭建</li></ul><pre><code>input_size = config.word_dimsself.word_lstm = LSTM(    input_size=input_size,    hidden_size=config.word_hidden_size,    num_layers=config.word_num_layers,    batch_first=True,    bidirectional=True,    dropout_in=config.dropout_input,    dropout_out=config.dropout_hidden,)</code></pre><ul><li>前向传播</li></ul><pre><code>hiddens, _ = self.word_lstm(batch_embed, batch_masks)  # sent_len x sen_num x hidden*2hiddens.transpose_(1, 0)  # sen_num x sent_len x hidden*2if self.training:    hiddens = drop_sequence_sharedmask(hiddens, self.dropout_mlp)</code></pre><h3 id="使用HAN用于文本分类"><a href="#使用HAN用于文本分类" class="headerlink" title="使用HAN用于文本分类"></a>使用HAN用于文本分类</h3><p><a href="https://link.zhihu.com/?target=http%3A//www.aclweb.org/anthology/N16-1174" target="_blank" rel="noopener">Hierarchical Attention Network for Document Classification</a>(HAN)基于层级注意力，在单词和句子级别分别编码并基于注意力获得文档的表示，然后经过Softmax进行分类。其中word encoder的作用是获得句子的表示，可以替换为上节提到的TextCNN和TextRNN，也可以替换为下节中的BERT。</p><p><img src="https://img-blog.csdnimg.cn/20200714210015326.png" srcset="/img/loading.gif" alt="Sequence Intent Classification Using Hierarchical Attention..."></p><h3 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h3><p>本章介绍了Word2Vec的使用，以及TextCNN、TextRNN的原理和训练，最后介绍了用于长文档分类的HAN。</p><h3 id="本章作业"><a href="#本章作业" class="headerlink" title="本章作业"></a>本章作业</h3><ul><li>尝试通过Word2Vec训练词向量</li><li>尝试使用TextCNN、TextRNN完成文本表示</li><li>尝试使用HAN进行文本分类</li></ul><p><strong>参考：</strong></p><ol><li><a href="https://mp.weixin.qq.com/s/I-yeHQopTFdNk67Ir_iWiA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/I-yeHQopTFdNk67Ir_iWiA</a></li><li><a href="https://github.com/hecongqing/2018-daguan-competition" target="_blank" rel="noopener">https://github.com/hecongqing/2018-daguan-competition</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;谦虚使人进步，骄傲使人落后。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="Classification" scheme="https://superlova.github.io/tags/Classification/"/>
    
      <category term="Deep Learning" scheme="https://superlova.github.io/tags/Deep-Learning/"/>
    
      <category term="word2vec" scheme="https://superlova.github.io/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】新闻文本分类之深度学习FastText</title>
    <link href="https://superlova.github.io/2020/07/27/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0FastText/"/>
    <id>https://superlova.github.io/2020/07/27/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0FastText/</id>
    <published>2020-07-27T13:56:03.000Z</published>
    <updated>2020-07-27T15:11:39.605Z</updated>
    
    <content type="html"><![CDATA[<p>当你写东西或讲话的时候，始终要想到使每个普通工人都懂得，都相信你的号召，都决心跟着你走。要想到你究竟为什么人写东西，向什么人讲话。——《反对党八股》<br><a id="more"></a></p><p>在上一章节，我们使用传统机器学习算法来解决了文本分类问题，从本章开始我们将尝试使用深度学习方法。与传统机器学习不同，深度学习既提供特征提取功能，也可以完成分类的功能。</p><p>本次学习我们主要介绍FastText。</p><p>fastText是一个快速文本分类算法，与基于神经网络的分类算法相比有两大优点：<br>1、fastText在保持高精度的情况下加快了训练速度和测试速度<br>2、fastText不需要预训练好的词向量，fastText会自己训练词向量<br>3、fastText两个重要的优化：层级 Softmax、N-gram</p><pre><code class="lang-python">import fasttextmodel = fasttext.train_supervised(&#39;train.csv&#39;, lr=1.0, wordNgrams=2, verbose=2, minCount=1, epoch=25, loss=&quot;hs&quot;)val_pred = [model.predict(x)[0][0].split(&#39;__&#39;)[-1] for x in df_train.iloc[-5000:][&#39;text&#39;]]print(f1_score(df_train[&#39;label&#39;].values[-5000:].astype(str), val_pred, average=&#39;macro&#39;))0.8256254253081777</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当你写东西或讲话的时候，始终要想到使每个普通工人都懂得，都相信你的号召，都决心跟着你走。要想到你究竟为什么人写东西，向什么人讲话。——《反对党八股》&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="Classification" scheme="https://superlova.github.io/tags/Classification/"/>
    
      <category term="Deep Learning" scheme="https://superlova.github.io/tags/Deep-Learning/"/>
    
      <category term="FastText" scheme="https://superlova.github.io/tags/FastText/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】新闻文本分类之机器学习文本分类</title>
    <link href="https://superlova.github.io/2020/07/23/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    <id>https://superlova.github.io/2020/07/23/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</id>
    <published>2020-07-22T17:43:15.000Z</published>
    <updated>2020-07-27T14:18:20.439Z</updated>
    
    <content type="html"><![CDATA[<p>今我睹子之难穷也，吾非至于子之门则殆矣。<br><a id="more"></a></p><h1 id="文本表示方法实践"><a href="#文本表示方法实践" class="headerlink" title="文本表示方法实践"></a>文本表示方法实践</h1><p>自然语言总需要转换成数值等表示才能被线性模型等处理。下面利用task1&amp;2提到的编码方式进行实践。</p><h2 id="Label"><a href="#Label" class="headerlink" title="Label"></a>Label</h2><p>这种编码方式是把自然语言首先分词成基本语素单元，然后把不同的单元赋予唯一的整数编码。比如本次比赛提供的数据集就是该编码方式。每条数据都是整数序列，最大整数为7549，最小整数为0。</p><p>这样做的好处是相对节约内存，且实现简单；坏处是破坏了自然语言中部分语义，无法进一步进行诸如删除停用词、词根提取等操作。另外，编码的大小可能会让算法误以为词和词之间有大小关系。</p><p>因为原来的数据集就是此编码方法，故不再赘述。</p><h2 id="Bag-of-Words-CountVectorizer"><a href="#Bag-of-Words-CountVectorizer" class="headerlink" title="Bag of Words: CountVectorizer"></a>Bag of Words: CountVectorizer</h2><p>用于机器学习的文本表示有一种最简单的方法，也是最有效且最常用的方法，就是使用词袋（bag-of-words）表示。使用这种表示方式时，我们舍弃了输入文本中的大部分结构，如章节、段落、句子和格式，<strong>只计算语料库中每个单词在每个文本中的出现频次</strong>。舍弃结构并仅计算单词出现次数，这会让脑海中出现将文本表示为“袋”的画面。</p><pre><code class="lang-python">from sklearn.feature_extraction.text import CountVectorizerbagvect = CountVectorizer(max_df=.15)bagvect.fit(corpus)feature_names = bagvect.get_feature_names()print(&quot;Number of features: {}&quot;.format(len(feature_names)))print(&quot;First 20 features:\n{}&quot;.format(feature_names[:20]))Number of features: 4740First 20 features:[&#39;10&#39;, &#39;100&#39;, &#39;1000&#39;, &#39;1001&#39;, &#39;1002&#39;, &#39;1004&#39;, &#39;1005&#39;, &#39;1006&#39;, &#39;1007&#39;, &#39;1008&#39;, &#39;1009&#39;, &#39;101&#39;, &#39;1010&#39;, &#39;1012&#39;, &#39;1013&#39;, &#39;1014&#39;, &#39;102&#39;, &#39;1020&#39;, &#39;1022&#39;, &#39;1023&#39;]bag_of_words = bagvect.transform(corpus)print(&quot;bag_of_words: {}&quot;.format(repr(bag_of_words)))print(bag_of_words[0].toarray())bag_of_words: &lt;10000x4740 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39;    with 813365 stored elements in Compressed Sparse Row format&gt;[[0 0 0 ... 0 0 0]]</code></pre><p>词袋表示保存在一个 SciPy 稀疏矩阵中，这种数据格式只保存非零元素</p><p>矩阵的形状为 10000x4740，每行对应于两个数据点之一，每个特征对应于词表中的一个单词。当然，我们在这里选了10000个样本，如果把所有数据集都给转化成词袋向量，那么矩阵形状将会是 200000×6859。</p><h2 id="Hash编码实践"><a href="#Hash编码实践" class="headerlink" title="Hash编码实践"></a>Hash编码实践</h2><p><a href="https://chenk.tech/posts/eb79fc5f.html" target="_blank" rel="noopener">https://chenk.tech/posts/eb79fc5f.html</a></p><pre><code class="lang-python">from sklearn.feature_extraction.text import HashingVectorizerhvec = HashingVectorizer(n_features=10000)hvec.fit(corpus)h_words = hvec.transform(corpus)print(&quot;h_words: {}&quot;.format(repr(h_words)))h_words: &lt;10000x10000 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;    with 2739957 stored elements in Compressed Sparse Row format&gt;</code></pre><p>我选取了10000个样本，将其映射到10000个特征的Hash向量中。</p><pre><code class="lang-python">print(h_words[0]) (0, 0)    -0.009886463280261834  (0, 6)    -0.04943231640130917  (0, 9)    0.03954585312104734  (0, 26)    -0.04943231640130917  (0, 42)    0.01977292656052367  (0, 70)    0.01977292656052367  (0, 74)    -0.12852402264340385  (0, 94)    0.009886463280261834  (0, 99)    0.01977292656052367  (0, 109)    -0.009886463280261834  :    :  (0, 9642)    -0.009886463280261834  (0, 9650)    0.01977292656052367  (0, 9653)    0.009886463280261834  (0, 9659)    -0.04943231640130917  (0, 9715)    -0.009886463280261834  (0, 9721)    0.009886463280261834  (0, 9729)    -0.009886463280261834  (0, 9741)    0.009886463280261834  (0, 9765)    -0.01977292656052367  (0, 9781)    0.01977292656052367  (0, 9821)    -0.009886463280261834  (0, 9862)    0.009886463280261834  (0, 9887)    -0.009886463280261834  (0, 9932)    0.009886463280261834</code></pre><p>输出的含义，前面的元组代表了该特征在词袋中的位置，后面的数值代表了对应的Hash值。</p><h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><p>TF-IDF（Term Frequency-inverse Document Frequency）是一种针对关键词的统计分析方法，用于评估一个词对一个文件集或者一个语料库的重要程度。</p><pre><code class="lang-python">from sklearn.feature_extraction.text import TfidfVectorizertvec = TfidfVectorizer()tvec.fit(corpus)feature_names = tvec.get_feature_names()print(&quot;Number of features: {}&quot;.format(len(feature_names)))print(&quot;First 20 features:\n{}&quot;.format(feature_names[:20]))Number of features: 5333First 20 features:[&#39;10&#39;, &#39;100&#39;, &#39;1000&#39;, &#39;1001&#39;, &#39;1002&#39;, &#39;1004&#39;, &#39;1005&#39;, &#39;1006&#39;, &#39;1007&#39;, &#39;1008&#39;, &#39;1009&#39;, &#39;101&#39;, &#39;1010&#39;, &#39;1012&#39;, &#39;1013&#39;, &#39;1014&#39;, &#39;1018&#39;, &#39;102&#39;, &#39;1020&#39;, &#39;1022&#39;]t_words = tvec.transform(corpus)print(&quot;t_words: {}&quot;.format(repr(t_words)))print(t_words[0].toarray())t_words: &lt;10000x5333 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;    with 2797304 stored elements in Compressed Sparse Row format&gt;[[0. 0. 0. ... 0. 0. 0.]]# 找到数据集中每个特征的最大值max_value = t_words.max(axis=0).toarray().ravel()sorted_by_tfidf = max_value.argsort()# 获取特征名称feature_names = np.array(tvec.get_feature_names())print(&quot;Features with lowest tfidf:\n{}&quot;.format(feature_names[sorted_by_tfidf[:20]]))print(&quot;Features with highest tfidf: \n{}&quot;.format(feature_names[sorted_by_tfidf[-20:]]))sorted_by_idf = np.argsort(tvec.idf_)print(&quot;Features with lowest idf:\n{}&quot;.format(feature_names[sorted_by_idf[:100]]))Features with lowest tfidf:[&#39;6844&#39; &#39;6806&#39; &#39;7201&#39; &#39;5609&#39; &#39;5585&#39; &#39;5485&#39; &#39;3453&#39; &#39;7390&#39; &#39;2322&#39; &#39;2083&#39; &#39;1222&#39; &#39;2360&#39; &#39;319&#39; &#39;2520&#39; &#39;6268&#39; &#39;3105&#39; &#39;6049&#39; &#39;4888&#39; &#39;2390&#39; &#39;2849&#39;]Features with highest tfidf: [&#39;3198&#39; &#39;2346&#39; &#39;5480&#39; &#39;4375&#39; &#39;6296&#39; &#39;1710&#39; &#39;682&#39; &#39;354&#39; &#39;4381&#39; &#39;482&#39; &#39;5990&#39; &#39;2798&#39; &#39;5907&#39; &#39;3992&#39; &#39;418&#39; &#39;513&#39; &#39;4759&#39; &#39;6250&#39; &#39;6220&#39; &#39;1633&#39;]Features with lowest idf:[&#39;3750&#39; &#39;900&#39; &#39;648&#39; &#39;6122&#39; &#39;7399&#39; &#39;2465&#39; &#39;4811&#39; &#39;4464&#39; &#39;1699&#39; &#39;299&#39; &#39;2400&#39; &#39;3659&#39; &#39;3370&#39; &#39;2109&#39; &#39;4939&#39; &#39;669&#39; &#39;5598&#39; &#39;5445&#39; &#39;4853&#39; &#39;5948&#39; &#39;2376&#39; &#39;7495&#39; &#39;4893&#39; &#39;5410&#39; &#39;340&#39; &#39;619&#39; &#39;4659&#39; &#39;1460&#39; &#39;6065&#39; &#39;1903&#39; &#39;5560&#39; &#39;6017&#39; &#39;2252&#39; &#39;4516&#39; &#39;1519&#39; &#39;2073&#39; &#39;5998&#39; &#39;5491&#39; &#39;2662&#39; &#39;5977&#39; &#39;6093&#39; &#39;1324&#39; &#39;5780&#39; &#39;3915&#39; &#39;3800&#39; &#39;5393&#39; &#39;2210&#39; &#39;5915&#39; &#39;3223&#39; &#39;4490&#39; &#39;2490&#39; &#39;1375&#39; &#39;803&#39; &#39;1635&#39; &#39;7539&#39; &#39;4411&#39; &#39;4128&#39; &#39;7543&#39; &#39;5602&#39; &#39;1866&#39; &#39;5176&#39; &#39;2799&#39; &#39;4646&#39; &#39;3700&#39; &#39;5858&#39; &#39;307&#39; &#39;913&#39; &#39;25&#39; &#39;6045&#39; &#39;1702&#39; &#39;4822&#39; &#39;3099&#39; &#39;5330&#39; &#39;1920&#39; &#39;1567&#39; &#39;2614&#39; &#39;4190&#39; &#39;1080&#39; &#39;5510&#39; &#39;4149&#39; &#39;3166&#39; &#39;3530&#39; &#39;192&#39; &#39;5659&#39; &#39;3618&#39; &#39;4525&#39; &#39;3686&#39; &#39;6038&#39; &#39;1767&#39; &#39;5589&#39; &#39;5736&#39; &#39;6831&#39; &#39;7377&#39; &#39;4969&#39; &#39;1394&#39; &#39;6104&#39; &#39;7010&#39; &#39;6407&#39; &#39;5430&#39; &#39;23&#39;]</code></pre><h2 id="多个单词的词袋：N-gram《》"><a href="#多个单词的词袋：N-gram《》" class="headerlink" title="多个单词的词袋：N-gram《》"></a>多个单词的词袋：N-gram《》</h2><p>使用词袋表示的主要缺点之一是完全舍弃了单词顺序。因此，“it’s bad, not good at all”（电影很差，一点也不好）和“it’s good, not bad at all”（电影很好，还不错）这两个字符串的词袋表示完全相同，尽管它们的含义相反。将“not”（不）放在单词前面，这只是上下文很重要的一个例子（可能是一个极端的例子）。幸运的是，使用词袋表示时有一种获取上下文的方法，就是不仅考虑单一词例的计数，而且还考虑相邻的两个或三个词例的计数。两个词例被称为二元分词（bigram），三个词例被称为三元分词（trigram），更一般的词例序列被称为 n 元分词（n-gram）。我们可以通过改变 CountVectorizer 或 TfidfVectorizer 的 ngram_range 参数来改变作为特征的词例范围。ngram_range 参数是一个元组，包含要考虑的词例序列的最小长度和最大长度。</p><p>在大多数情况下，添加二元分词会有所帮助。添加更长的序列（一直到五元分词）也可能有所帮助，但这会导致特征数量的大大增加，也可能会导致过拟合，因为其中包含许多非常具体的特征。原则上来说，二元分词的数量是一元分词数量的平方，三元分词的数量是一元分词数量的三次方，从而导致非常大的特征空间。在实践中，更高的 n 元分词在数据中的出现次数实际上更少，原因在于（英语）语言的结构，不过这个数字仍然很大。</p><pre><code class="lang-python">from sklearn.feature_extraction.text import TfidfVectorizertvec = TfidfVectorizer(ngram_range=(1,3), min_df=5)tvec.fit(corpus)# 找到数据集中每个特征的最大值max_value = t_words.max(axis=0).toarray().ravel()sorted_by_tfidf = max_value.argsort()# 获取特征名称feature_names = np.array(tvec.get_feature_names())print(&quot;Features with lowest tfidf:\n{}&quot;.format(feature_names[sorted_by_tfidf[:20]]))print(&quot;Features with highest tfidf: \n{}&quot;.format(feature_names[sorted_by_tfidf[-20:]]))sorted_by_idf = np.argsort(tvec.idf_)print(&quot;Features with lowest idf:\n{}&quot;.format(feature_names[sorted_by_idf[:100]]))Features with lowest tfidf:[&#39;1008 5612&#39; &#39;100 5560&#39; &#39;1018 4089 5491&#39; &#39;101 648 900&#39; &#39;1006 3750 826&#39; &#39;1018 1066 3231&#39; &#39;101 5560 3568&#39; &#39;100 5589&#39; &#39;1018 2119 281&#39; &#39;101 5560 3659&#39; &#39;1018 1066 3166&#39; &#39;100 5598 1465&#39; &#39;1000 5011&#39; &#39;101 2662 4939&#39; &#39;100 5602&#39; &#39;101 873 648&#39; &#39;1006 2265 648&#39; &#39;1008 5640&#39; &#39;1008 5689&#39; &#39;101 856 531&#39;]Features with highest tfidf: [&#39;101 2087&#39; &#39;1018 1066 281&#39; &#39;101 648 3440&#39; &#39;1006 5640 3641&#39; &#39;101 760 4233&#39; &#39;1006 6017&#39; &#39;1018 1066 6983&#39; &#39;1014 3750 3659&#39; &#39;100 5430 2147&#39; &#39;100 5510 2471&#39; &#39;1018 1141&#39; &#39;1006 1866 5977&#39; &#39;1018 2119 3560&#39; &#39;1018 2662 3068&#39; &#39;101 1844 4486&#39; &#39;101 2304 3659&#39; &#39;1006 3750 5330&#39; &#39;101 5589&#39; &#39;1008 900 3618&#39; &#39;100 6122 2489&#39;]Features with lowest idf:[&#39;3750&#39; &#39;900&#39; &#39;648&#39; &#39;2465&#39; &#39;6122&#39; &#39;7399&#39; &#39;4811&#39; &#39;4464&#39; &#39;1699&#39; &#39;3659&#39; &#39;2400&#39; &#39;299&#39; &#39;3370&#39; &#39;2109&#39; &#39;4939&#39; &#39;5598&#39; &#39;669&#39; &#39;5445&#39; &#39;4853&#39; &#39;2376&#39; &#39;5948&#39; &#39;7495&#39; &#39;4893&#39; &#39;5410&#39; &#39;340&#39; &#39;619&#39; &#39;4659&#39; &#39;1460&#39; &#39;6065&#39; &#39;4516&#39; &#39;1903&#39; &#39;5560&#39; &#39;6017&#39; &#39;2252&#39; &#39;2073&#39; &#39;1519&#39; &#39;5491&#39; &#39;5998&#39; &#39;2662&#39; &#39;5977&#39; &#39;1324&#39; &#39;5780&#39; &#39;6093&#39; &#39;3915&#39; &#39;5393&#39; &#39;2210&#39; &#39;3800&#39; &#39;3223&#39; &#39;5915&#39; &#39;4490&#39; &#39;2490&#39; &#39;803&#39; &#39;1635&#39; &#39;4128&#39; &#39;1375&#39; &#39;7539&#39; &#39;4411&#39; &#39;7543&#39; &#39;5602&#39; &#39;2799&#39; &#39;1866&#39; &#39;5176&#39; &#39;5858&#39; &#39;4646&#39; &#39;3700&#39; &#39;307&#39; &#39;6045&#39; &#39;1702&#39; &#39;25&#39; &#39;913&#39; &#39;5330&#39; &#39;4822&#39; &#39;2614&#39; &#39;3099&#39; &#39;1920&#39; &#39;1567&#39; &#39;4190&#39; &#39;4149&#39; &#39;5510&#39; &#39;1080&#39; &#39;3166&#39; &#39;3659 3370&#39; &#39;3530&#39; &#39;192&#39; &#39;3618&#39; &#39;4525&#39; &#39;5659&#39; &#39;3686&#39; &#39;6038&#39; &#39;1767&#39; &#39;5736&#39; &#39;7377&#39; &#39;5589&#39; &#39;6831&#39; &#39;3370 3370&#39; &#39;1394&#39; &#39;4969&#39; &#39;5430&#39; &#39;7010&#39; &#39;6104&#39;]</code></pre><h1 id="无监督探索"><a href="#无监督探索" class="headerlink" title="无监督探索"></a>无监督探索</h1><h2 id="PCA可视化"><a href="#PCA可视化" class="headerlink" title="PCA可视化"></a>PCA可视化</h2><pre><code class="lang-python">from sklearn.decomposition import PCApca = PCA(n_components=2)pca_vec = pca.fit_transform(t_words.toarray())pca_vec.shape, pca.explained_variance_ratio_((10000, 2), array([0.03817303, 0.02684457]))</code></pre><p>我们成功将tfidf转化后的词向量压缩成2维向量，这样就能够在二维平面可视化了。</p><p>后面的<code>explained_variance_ratio_</code>代表着经过PCA算法压缩后，保留的信息量。这个数值还是偏低，因此这种PCA压缩方法仅适用于实验。</p><pre><code class="lang-python">plt.figure(figsize=(12,10))plt.scatter(pca_vec[:,0], pca_vec[:,1], c=labels)</code></pre><p><img src="/2020/07/23/【竞赛打卡】新闻文本分类之机器学习文本分类/1.png" srcset="/img/loading.gif" alt></p><h2 id="主题建模"><a href="#主题建模" class="headerlink" title="主题建模"></a>主题建模</h2><p>常用于文本数据的一种特殊技术是主题建模（topic modeling），这是描述将每个文档分配给一个或多个主题的任务（通常是无监督的）的概括性术语。这方面一个很好的例子是新闻数据，它们可以被分为“政治”“体育”“金融”等主题。如果为每个文档分配一个主题，那么这是一个文档聚类任务。我们学到的每个成分对应于一个主题，文档表示中的成分系数告诉我们这个文档与该主题的相关性强弱。通常来说，人们在谈论主题建模时，他们指的是一种叫作隐含狄利克雷分布（Latent Dirichlet Allocation，LDA）的特定分解方法。</p><p>我们将 LDA 应用于新闻数据集，来看一下它在实践中的效果。对于无监督的文本文档模型，通常最好删除非常常见的单词，否则它们可能会支配分析过程。我们将删除至少在15% 的文档中出现过的单词，并在删除前 15% 之后，将词袋模型限定为最常见的 10 000 个单词：</p><pre><code class="lang-python">vect = CountVectorizer(max_features=10000, max_df=.15)X = vect.fit_transform(corpus)from sklearn.decomposition import LatentDirichletAllocationlda = LatentDirichletAllocation(n_components=14, learning_method=&quot;batch&quot;, max_iter=25, random_state=0)# 我们在一个步骤中构建模型并变换数据# 计算变换需要花点时间，二者同时进行可以节省时间document_topics = lda.fit_transform(bag_of_words)# 对于每个主题（components_的一行），将特征排序（升序）# 用[:, ::-1]将行反转，使排序变为降序sorting = np.argsort(lda.components_, axis=1)[:, ::-1]# 从向量器中获取特征名称feature_names = np.array(bagvect.get_feature_names())plt.figure()plt.bar(x=range(14), height=document_topics[0])plt.xticks(list(range(14)))</code></pre><p><img src="/2020/07/23/【竞赛打卡】新闻文本分类之机器学习文本分类/2.png" srcset="/img/loading.gif" alt></p><p>由LDA确定的主题词如下：</p><pre><code class="lang-python">topic 0       topic 1       topic 2       topic 3       topic 4       --------      --------      --------      --------      --------      6654          1970          7349          3464          4412          4173          2716          7354          7436          7363          1219          4553          1684          5562          6689          6861          7042          5744          3289          4986          5006          5822          6569          5105          2506          7400          5099          1999          5810          3056          3508          3654          1351          3134          6220          6223          3021          56            3648          5117          6227          4967          4036          6308          6319          7257          3396          4223          1706          2695          topic 5       topic 6       topic 7       topic 8       topic 9       --------      --------      --------      --------      --------      4967          1334          1934          4114          7328          7528          5166          1146          3198          5547          3644          6143          532           517           4768          1899          2695          4802          812           3231          6678          1616          419           3090          5492          5744          7532          4089          4163          4080          6047          368           3725          5305          4120          1252          2918          2851          177           2331          5814          2968          6227          7251          3019          1170          3032          6639          2835          6613          topic 10      topic 11      topic 12      topic 13      --------      --------      --------      --------      3523          4902          5178          5122          3342          1258          6014          6920          6722          343           5920          5519          6352          4089          4603          7154          3186          5226          3648          4381          5179          810           4042          4760          4369          6284          1724          4412          3501          3477          4450          4595          2334          7127          657           3377          2722          7077          5803          7006</code></pre><h2 id="t-SNE可视化"><a href="#t-SNE可视化" class="headerlink" title="t-SNE可视化"></a>t-SNE可视化</h2><p>t-SNE是当前最流行的数据可视化方法。将TF-IDF转化后的向量可视化如下：</p><pre><code class="lang-python">from sklearn.manifold import TSNEwords_emb = TSNE(n_components=2).fit_transform(t_words)plt.figure(figsize=(12,10))plt.scatter(words_emb[:,0], words_emb[:,1], c=labels)</code></pre><p><img src="/2020/07/23/【竞赛打卡】新闻文本分类之机器学习文本分类/3.png" srcset="/img/loading.gif" alt></p><p>将HashingVectorizer转化后的向量经过t-SNE算法可视化结果分享如下：</p><pre><code class="lang-python">hash_words_emb = TSNE(n_components=2).fit_transform(h_words)plt.figure(figsize=(12,10))plt.scatter(hash_words_emb[:,0], hash_words_emb[:,1], c=labels)</code></pre><p><img src="/2020/07/23/【竞赛打卡】新闻文本分类之机器学习文本分类/4.png" srcset="/img/loading.gif" alt></p><h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><p>首先导入相关库</p><pre><code class="lang-python">from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizerfrom sklearn.pipeline import make_pipelinefrom sklearn.linear_model import LogisticRegression, RidgeClassifierfrom sklearn.model_selection import GridSearchCV, train_test_split, cross_val_scorefrom sklearn.multiclass import OneVsRestClassifierfrom sklearn.svm import SVCfrom sklearn.metrics import f1_score</code></pre><p><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model" target="_blank" rel="noopener">https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model</a></p><h2 id="RidgeClassifier-CountVectorizer"><a href="#RidgeClassifier-CountVectorizer" class="headerlink" title="RidgeClassifier+CountVectorizer"></a>RidgeClassifier+CountVectorizer</h2><p>首先我们使用教程中的范例</p><pre><code class="lang-python">X, y = df_train[&#39;text&#39;][:10000], df_train[&#39;label&#39;][:10000]vectorizer = CountVectorizer(max_features=3000)X = vectorizer.fit_transform(X)clf = RidgeClassifier()clf.fit(X[:5000], y[:5000]) # 使用前5000个样本进行训练val_pred = clf.predict(X[5000:]) # 使用后5000个样本进行预测print(f1_score(y[5000:], val_pred, average=&#39;macro&#39;))0.6322204326986258</code></pre><p>我们把10000个样本中，前5000个用于训练，后5000个用于测试，最终结果以F1指标展示，结果为0.63，不太令人满意。</p><h2 id="LogisticRegression-TFIDF"><a href="#LogisticRegression-TFIDF" class="headerlink" title="LogisticRegression+TFIDF"></a>LogisticRegression+TFIDF</h2><p>最常见的线性分类算法是 Logistic 回归。虽然 LogisticRegression 的名字中含有回归（regression），但它是一种分类算法，并不是回归算法，不应与 LinearRegression 混淆。<br>我们可以将 LogisticRegression 和 LinearSVC 模型应用到经过tfidf处理的新闻文本数据集上。</p><p>我们使用了sklearn中的划分数据集的方法<code>train_test_split</code>，将数据集划分成训练集和测试集两部分。但是这样一来，数据集中的测试集部分将不能被训练，未免有点可惜。</p><p>我们在训练时，采用了pipeline方式，Pipeline 类可以将多个处理步骤合并（glue）为单个 scikit-learn 估计器。Pipeline 类本身具有 fit、predict 和 score 方法，其行为与 scikit-learn 中的其 他模型相同。Pipeline 类最常见的用例是将预处理步骤（比如数据缩放）与一个监督模型 （比如分类器）链接在一起。</p><pre><code class="lang-python">%%timeX_train, X_test, y_train, y_test = train_test_split(df_train[&#39;text&#39;][:10000], df_train[&#39;label&#39;][:10000], random_state=0)pipe_logis = make_pipeline(TfidfVectorizer(min_df=5, ngram_range=(1,3)), LogisticRegression())param_grid = {&#39;logisticregression__C&#39;: [0.001, 0.01, 0.1, 1, 10]}grid = GridSearchCV(pipe_logis, param_grid, cv=5)grid.fit(X_train, y_train)print(&quot;Best params:\n{}\n&quot;.format(grid.best_params_))print(&quot;Best cross-validation score: {:.2f}&quot;.format(grid.best_score_))print(&quot;Test-set score: {:.2f}&quot;.format(grid.score(X_test, y_test)))Best params:{&#39;logisticregression__C&#39;: 10}Best cross-validation score: 0.91Test-set score: 0.92</code></pre><h2 id="SVC-tfidf"><a href="#SVC-tfidf" class="headerlink" title="SVC+tfidf"></a>SVC+tfidf</h2><p>这次我们使用非线性模型中大名鼎鼎的SVM模型，并采用交叉验证的方法划分数据集，不浪费任何一部分数据。</p><pre><code class="lang-python">X_train, y_train = df_train[&#39;text&#39;][:10000], df_train[&#39;label&#39;][:10000]pipe_svc = make_pipeline(TfidfVectorizer(min_df=5), SVC()) # decision_function_shape=&#39;ovr&#39;scores = cross_val_score(pipe_svc, X_train, y_train, cv=5, n_jobs=-1)print(scores.mean())0.8924</code></pre><h1 id="模型评价"><a href="#模型评价" class="headerlink" title="模型评价"></a>模型评价</h1><p>TR，FN，precision，recall等的进一步解释，请参考以下链接：<br><a href="https://www.zhihu.com/question/30643044" target="_blank" rel="noopener">https://www.zhihu.com/question/30643044</a></p><h1 id="进一步优化"><a href="#进一步优化" class="headerlink" title="进一步优化"></a>进一步优化</h1><p>使用机器学习模型+巧妙的特征工程，我们可以达到90%以上的精度，这在14分类问题中已经很惊人了。然而我们的工作并没有结束，还有许许多多的问题等着我们去探索。比如</p><ul><li>删除停用词、罕见词、其他常见词和不能反映特征的词</li><li>类别不平衡问题</li></ul><p>周志华《机器学习》中介绍到，分类学习方法都有一个共同的基本假设，即不同类别的训练样例数目相当。如果不同类别的训练样例数目稍有差别，对学习结果的影响通常也不大，但若样本类别数目差别很大，属于极端不均衡，则会对学习过程（模型训练）造成困扰。这些学习算法的设计背后隐含的优化目标是数据集上的分类准确度，而这会导致学习算法在不平衡数据上更偏向于含更多样本的多数类。多数不平衡学习（imbalance learning）算法就是为了解决这种“对多数类的偏好”而提出的。如果正负类样本类别不平衡比例超过4:1，那么其分类器会大大地因为数据不平衡性而无法满足分类要求</p><p>关于如何解决类别不平衡的问题，可以参考以下链接：<br><a href="https://zhuanlan.zhihu.com/p/84322912" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/84322912</a><br><a href="https://zhuanlan.zhihu.com/p/36381828" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/36381828</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今我睹子之难穷也，吾非至于子之门则殆矣。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="Classification" scheme="https://superlova.github.io/tags/Classification/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析</title>
    <link href="https://superlova.github.io/2020/07/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E5%88%86%E6%9E%90/"/>
    <id>https://superlova.github.io/2020/07/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E5%88%86%E6%9E%90/</id>
    <published>2020-07-22T04:03:05.000Z</published>
    <updated>2020-07-25T05:22:02.670Z</updated>
    
    <content type="html"><![CDATA[<p>不要对我有任何期待哦！<br><a id="more"></a></p><h1 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h1><p>Colab</p><pre><code class="lang-python">import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitimport seaborn as snsimport scipyfrom collections import Counter</code></pre><h1 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h1><pre><code class="lang-python">df_train = pd.read_csv(train_path, sep=&#39;\t&#39;)df_test = pd.read_csv(test_path, sep=&#39;\t&#39;)</code></pre><h1 id="数据探索"><a href="#数据探索" class="headerlink" title="数据探索"></a>数据探索</h1><h2 id="简单查看数据"><a href="#简单查看数据" class="headerlink" title="简单查看数据"></a>简单查看数据</h2><pre><code class="lang-python">df_train.head(), len(df_train)(   label                                               text 0      2  2967 6758 339 2021 1854 3731 4109 3792 4149 15... 1     11  4464 486 6352 5619 2465 4802 1452 3137 5778 54... 2      3  7346 4068 5074 3747 5681 6093 1777 2226 7354 6... 3      2  7159 948 4866 2109 5520 2490 211 3956 5520 549... 4      3  3646 3055 3055 2490 4659 6065 3370 5814 2465 5..., 200000)</code></pre><p>发现text域的数据是字符串。我们想要得到整数序列。可以用字符串分割<code>split()</code>。</p><pre><code class="lang-python">print(len(df_train[&#39;text&#39;][0]), type(df_train[&#39;text&#39;][0]))df_train.head()</code></pre><p><img src="/2020/07/22/【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析/1.png" srcset="/img/loading.gif" alt></p><h2 id="长度分布"><a href="#长度分布" class="headerlink" title="长度分布"></a>长度分布</h2><h3 id="describe"><a href="#describe" class="headerlink" title="describe"></a>describe</h3><p>这里与教程中的方法有所不同。vectorize是numpy中很方便的函数，作用和pandas中<code>apply</code>差不多。用法：</p><p><code>np.vectorize(function)(array)</code></p><p>输入待处理的array，以及逐元素处理函数function，返回经过处理后的ndarray。原来的array则不受影响。</p><p>当前我使用的函数<code>split_df</code>负责将一行数据按空格切分成整数列表，然后计算该列表的长度。</p><pre><code class="lang-python">def split_df(df_row):    return len(str(df_row).split())len_dist = np.vectorize(split_df)(df_train[&#39;text&#39;])len_test_dist = np.vectorize(split_df)(df_test[&#39;text&#39;])</code></pre><p>使用describe函数查看训练集和测试集中的数据长度分布</p><pre><code class="lang-python">print(pd.Series(len_dist).describe())print(pd.Series(len_test_dist).describe())count    200000.000000mean        907.207110std         996.029036min           2.00000025%         374.00000050%         676.00000075%        1131.000000max       57921.000000dtype: float64count    50000.000000mean       909.844960std       1032.313375min         14.00000025%        370.00000050%        676.00000075%       1133.000000max      41861.000000dtype: float64</code></pre><p>通过数据描述可以看到</p><p>训练集共200,000条新闻，每条新闻平均907个字符，最短的句子长度为2，最长的句子长度为57921，其中75%以下的数据长度在1131以下。</p><p>测试集共50,000条新闻，每条新闻平均909个字符，最短句子长度为14，最长句子41861,75%以下的数据长度在1133以下。</p><p>训练集和测试集就长度来说似乎是同一分布。</p><h3 id="直方图"><a href="#直方图" class="headerlink" title="直方图"></a>直方图</h3><p>绘制直方图查看训练集和测试集中的数据长度分布</p><pre><code class="lang-python">fig, ax = plt.subplots(1,1,figsize=(12,6))ax = plt.hist(x=len_dist, bins=100)ax = plt.hist(x=len_test_dist, bins=100)plt.xlim([0, max(max(len_dist), max(len_test_dist))])plt.xlabel(&quot;length of sample&quot;)plt.ylabel(&quot;number of sample&quot;)plt.legend([&#39;train_len&#39;,&#39;test_len&#39;])plt.show()</code></pre><p><img src="/2020/07/22/【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析/2.png" srcset="/img/loading.gif" alt></p><p>使用seaborn绘制更好的图</p><p>seaborn计算的纵坐标是频率，而不是出现次数。由于训练集和测试集的数据量不一样，因此用频率更加科学、更能看出是否符合同一分布。</p><pre><code class="lang-python">plt.figure(figsize=(15,5))ax = sns.distplot(len_dist, bins=100)ax = sns.distplot(len_test_dist, bins=100)plt.xlim([0, max(max(len_dist), max(len_test_dist))])plt.xlabel(&quot;length of sample&quot;)plt.ylabel(&quot;prob of sample&quot;)plt.legend([&#39;train_len&#39;,&#39;test_len&#39;])</code></pre><p><img src="/2020/07/22/【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析/3.png" srcset="/img/loading.gif" alt></p><p>通过直方图，我们能直观感受到训练集和测试集的长度分布都属于右偏分布。按理说分析到这份儿上就该停了。</p><h3 id="同分布验证"><a href="#同分布验证" class="headerlink" title="同分布验证"></a>同分布验证</h3><pre><code class="lang-python">import scipyscipy.stats.ks_2samp(len_dist, len_test_dist)Ks_2sampResult(statistic=0.004049999999999998, pvalue=0.5279614323123156)</code></pre><p>P值为0.52，比指定的显著水平（假设为5%）大，我们认为二者同分布。</p><h3 id="截断位置"><a href="#截断位置" class="headerlink" title="截断位置"></a>截断位置</h3><p>在输入模型进行训练之前，我们要把所有的数据长度统一化，数据肯定要截断。但是在什么位置截断合适呢？</p><p>考虑到数据长度分布是长尾分布，log一下看看是不是正态分布，如果是正态分布，使用3sigma法则作为截断的参考。如果不是，则就只能瞎猜了</p><p>测量拟合分布的均值和方差sigma原则</p><p>$1\sigma$原则：数值分布在$(\mu-\sigma,\mu+\sigma)$中的概率为0.6526；</p><p>$2\sigma$原则：数值分布在$(\mu-2\sigma,\mu+2\sigma)$中的概率为0.9544；</p><p>$3\sigma$原则：数值分布在$(\mu-3\sigma,\mu+3\sigma)$中的概率为0.9974；</p><p>由于“小概率事件”和假设检验的基本思想 “小概率事件”通常指发生的概率小于5%的事件，认为在一次试验中该事件是几乎不可能发生的。由此可见X落在$(\mu-3\sigma,\mu+3\sigma)$以外的概率小于千分之三，在实际问题中常认为相应的事件是不会发生的，基本上可以把区间$(\mu-3\sigma,\mu+3\sigma)$看作是随机变量X实际可能的取值区间，这称之为正态分布的“$3\sigma$”原则。</p><pre><code class="lang-python">log_len_dist = np.log(1+len_dist)log_len_test_dist = np.log(1+len_test_dist)plt.figure(figsize=(15,5))ax = sns.distplot(log_len_dist)ax = sns.distplot(log_len_test_dist)plt.xlabel(&quot;log length of sample&quot;)plt.ylabel(&quot;prob of log&quot;)plt.legend([&#39;train_len&#39;,&#39;test_len&#39;])</code></pre><p><img src="/2020/07/22/【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析/4.png" srcset="/img/loading.gif" alt></p><p>从log图上也能看出二者（很像）同分布。</p><p>下面我想验证一下我的猜想：该分布为正态分布，且训练集和测试集为同分布。</p><p>先验证训练集分布为正态分布：</p><pre><code class="lang-python">_, lognormal_ks_pvalue = scipy.stats.kstest(rvs=log_len_dist, cdf=&#39;norm&#39;)print(&#39;P value is &#39;, lognormal_ks_pvalue)P value is  0.0</code></pre><p>？0？？？拟合优度检验，p值为0，意思就是说这不是一个正态分布。<br>关于分布检验，参考<a href="https://blog.csdn.net/QimaoRyan/article/details/72861387" target="_blank" rel="noopener">这篇文章</a></p><p>之前我们把数据log了一下，但是这里有更科学的变换方式。log只是box-cox变换的特殊形式。我们使用box-cox变换再次做一下验证，是否为正态分布：</p><pre><code class="lang-python">trans_data, lam = scipy.stats.boxcox(len_dist+1)scipy.stats.normaltest(trans_data)NormaltestResult(statistic=1347.793358118494, pvalue=2.1398873511704724e-293)</code></pre><p>e后面跟了那么多负数，我佛了。这说明我们的假设不成立。</p><p>但总归是要猜一个截断值的。看log图上8.5的位置比较靠谱。np.exp(8.5)=4914约等于5000，因此我初步决定把截断长度定为5000。</p><h2 id="类别信息"><a href="#类别信息" class="headerlink" title="类别信息"></a>类别信息</h2><h3 id="简单查看类别信息表"><a href="#简单查看类别信息表" class="headerlink" title="简单查看类别信息表"></a>简单查看类别信息表</h3><p>先改造一下df_train，多加几个字段，分别是</p><ul><li>text-split，将text字段分词</li><li>len，每条新闻长度</li><li>first_char，新闻第一个字符</li><li>last_char，新闻最后一个字符</li><li>most_freq，新闻最常出现的字符</li></ul><pre><code class="lang-python">df_train[&#39;text_split&#39;] = df_train[&#39;text&#39;].apply(lambda x:x.split())df_train[&#39;len&#39;] = df_train[&#39;text&#39;].apply(lambda x:len(x.split()))df_train[&#39;first_char&#39;] = df_train[&#39;text_split&#39;].apply(lambda x:x[0])df_train[&#39;last_char&#39;] = df_train[&#39;text_split&#39;].apply(lambda x:x[-1])df_train[&#39;most_freq&#39;] = df_train[&#39;text_split&#39;].apply(lambda x:np.argmax(np.bincount(x)))df_train.head()</code></pre><p><img src="/2020/07/22/【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析/2020-07-25-12-22-01.png" srcset="/img/loading.gif" alt></p><p>构建一个类别信息表。</p><ul><li>count，该类别新闻个数</li><li>len_mean，该类别新闻平均长度</li><li>len_std，该类别新闻长度标准差</li><li>len_min，该类别新闻长度最小值</li><li>len_max，该类别新闻长度最大值</li><li>freq_fc，该类别新闻最常出现的第一个字符</li><li>freq_lc，该类别新闻最常出现的最后一个字符</li><li>freq_freq，该类别新闻最常出现的字符</li></ul><pre><code class="lang-python">df_train_info = pd.DataFrame(columns=[&#39;count&#39;,&#39;len_mean&#39;,&#39;len_std&#39;,&#39;len_min&#39;,&#39;len_max&#39;,&#39;freq_fc&#39;,&#39;freq_lc&#39;,&#39;freq_freq&#39;])for name, group in df_train.groupby(&#39;label&#39;):    count = len(group) # 该类别新闻数    len_mean = np.mean(group[&#39;len&#39;]) # 该类别长度平均值    len_std = np.std(group[&#39;len&#39;]) # 长度标准差    len_min = np.min(group[&#39;len&#39;]) # 最短的新闻长度    len_max = np.max(group[&#39;len&#39;]) # 最长的新闻长度    freq_fc = np.argmax(np.bincount(group[&#39;first_char&#39;])) # 最频繁出现的首词    freq_lc = np.argmax(np.bincount(group[&#39;last_char&#39;])) # 最频繁出现的末词    freq_freq = np.argmax(np.bincount(group[&#39;most_freq&#39;])) # 该类别最频繁出现的词    df_train_info.loc[name] = [count,len_mean,len_std,len_min,len_max,freq_fc,freq_lc,freq_freq]df_train_info</code></pre><p><img src="/2020/07/22/【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析/2020-07-25-12-30-24.png" srcset="/img/loading.gif" alt></p><h3 id="类别分布"><a href="#类别分布" class="headerlink" title="类别分布"></a>类别分布</h3><p>之前的讨论是从数据集总体验证同分布的，我们还需要验证训练集的类别足够均匀。</p><p>在数据集中标签的对应的关系如下</p><pre><code class="lang-python">label_2_index_dict = {&#39;科技&#39;: 0, &#39;股票&#39;: 1, &#39;体育&#39;: 2, &#39;娱乐&#39;: 3, &#39;时政&#39;: 4, &#39;社会&#39;: 5, &#39;教育&#39;: 6, &#39;财经&#39;: 7, &#39;家居&#39;: 8, &#39;游戏&#39;: 9, &#39;房产&#39;: 10, &#39;时尚&#39;: 11, &#39;彩票&#39;: 12, &#39;星座&#39;: 13}index_2_label_dict = {v:k for k,v in label_2_index_dict.items()}plt.figure()plt.bar(x=range(14), height=np.bincount(df_train[&#39;label&#39;]))plt.xlabel(&quot;label&quot;)plt.ylabel(&quot;number of sample&quot;)plt.xticks(range(14), list(index_2_label_dict.values()), fontproperties=zhfont, rotation=60)plt.show()</code></pre><p><img src="/2020/07/22/【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析/6.png" srcset="/img/loading.gif" alt></p><p>从统计结果可以看出</p><p>赛题的数据集类别分布存在较为不均匀的情况。在训练集中科技类新闻最多，其次是股票类新闻，最少的新闻是星座新闻。</p><p>科技类新闻最多，星座类新闻最少。这个国家的人大部分是唯物主义者哈，神秘学受众比较少（啊这，我在分析什么？）。</p><p>由于类别不均衡，会严重影响模型的精度。但是我们也是有办法应对的。</p><h3 id="类别长度"><a href="#类别长度" class="headerlink" title="类别长度"></a>类别长度</h3><pre><code class="lang-python">df_train[&#39;len&#39;] = df_train[&#39;text&#39;].apply(lambda x: len(x.split()))plt.figure()ax = sns.catplot(x=&#39;label&#39;, y=&#39;len&#39;, data=df_train, kind=&#39;strip&#39;)plt.xticks(range(14), list(index_2_label_dict.values()), fontproperties=zhfont, rotation=60)</code></pre><p><img src="/2020/07/22/【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析/7.png" srcset="/img/loading.gif" alt></p><p>在散点图中，股票类新闻的长度都飘到天上去了，可以看出股票分析类文章真的很容易写得又臭又长啊（发现：不同类别的文章长度不同，可以把长度作为一个Feature，以供机器学习模型训练）！</p><h2 id="字符分布"><a href="#字符分布" class="headerlink" title="字符分布"></a>字符分布</h2><p>训练集中总共包括6869个字，最大数字为7549，最小数字为0，其中编号3750的字出现的次数最多，编号3133的字出现的次数最少，仅出现一次。</p><pre><code class="lang-python"># 内存警告！！！没有8G内存不要运行该代码all_lines = &#39; &#39;.join(list(df_train[&#39;text&#39;]))word_count = Counter(all_lines.split(&quot; &quot;))word_count = sorted(word_count.items(), key=lambda d:d[1], reverse=True)print(len(word_count))# 6869print(word_count[0])# (&#39;3750&#39;, 7482224)print(word_count[-1])# (&#39;3133&#39;, 1)</code></pre><p>下面代码统计了不同字符在多少个句子中出现过，其中字符3750、字符900和字符648在20w新闻的覆盖率接近99%，很有可能是标点符号。</p><pre><code class="lang-python">%%timedf_train[&#39;text_unique&#39;] = df_train[&#39;text&#39;].apply(lambda x: &#39; &#39;.join(list(set(x.split(&#39; &#39;)))))all_lines = &#39; &#39;.join(list(df_train[&#39;text_unique&#39;]))word_count = Counter(all_lines.split(&quot; &quot;))word_count = sorted(word_count.items(), key=lambda d:int(d[1]), reverse=True)# 打印整个训练集中覆盖率前5的词for i in range(5):    print(&quot;{} occurs {} times, {}%&quot;.format(word_count[i][0], word_count[i][1], (word_count[i][1]/200000)*100))</code></pre><p>3750 occurs 197997 times, 98.9985%<br>900 occurs 197653 times, 98.8265%<br>648 occurs 191975 times, 95.9875%<br>2465 occurs 177310 times, 88.655%<br>6122 occurs 176543 times, 88.2715%</p><h2 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h2><p><strong>假设字符3750，字符900和字符648是句子的标点符号，请分析赛题每篇新闻平均由多少个句子构成？</strong></p><p>如果这是英文文章，那么3750应该是空格吧？如果3750是逗号怎么办？先要判断哪个是句号。</p><p>思路：该新闻的句子数为该个数。每条新闻最后的字符往往是句号，先看看每条新闻最后一个字符是什么：</p><pre><code class="lang-python">last_char = np.vectorize(lambda x:int(x.split()[-1]))(df_train[&#39;text&#39;])last_char_count = Counter(last_char)last_char_count = sorted(last_char_count.items(), key=lambda d:d[1], reverse=True)# 打印出现次数最多的前十个for i in range(10):    print(&quot;{}在新闻末尾出现了{}次&quot;.format(last_char_count[i][0], last_char_count[i][1]))900在新闻末尾出现了85040次2662在新闻末尾出现了39273次885在新闻末尾出现了14473次1635在新闻末尾出现了7379次2465在新闻末尾出现了7076次57在新闻末尾出现了3284次3231在新闻末尾出现了2758次1633在新闻末尾出现了2706次3568在新闻末尾出现了1504次2265在新闻末尾出现了1389次</code></pre><p>因此我们有理由认为900是句号。至于3750应该是逗号吧？猜的，理由是3750不太容易在新闻末尾出现。</p><p>但是除了句号之外，感叹号和问号照样能划分句子，我们试着将2662当作感叹号，将885当作问号。什么理由？猜的。</p><p>下面开始计算每篇新闻所含标点符号（900、2662、885）的个数，</p><pre><code class="lang-python">def sum_of_sep(row):    counter = Counter(row.split())    return counter.get(&#39;900&#39;, 0)+counter.get(&#39;2662&#39;, 0)+counter.get(&#39;885&#39;, 0)sum_sep = np.vectorize(sum_of_sep)(df_train[&#39;text&#39;])print(&quot;平均每条新闻的句子个数约为：&quot;, np.round(np.mean(sum_sep)))pd.Series(sum_sep).describe()平均每条新闻的句子个数约为： 19.0count    200000.000000mean         19.070155std          21.463798min           0.00000025%           7.00000050%          14.00000075%          24.000000max        1392.000000dtype: float64</code></pre><p>平均长度为19，其实这是把那些股票文章也算上了，拉高了平均值。75%的新闻长度都在24个句子以下。</p><p>给df_train_info新加一列sent_num，计算分词后的句子个数；sent_len为句子长度。</p><pre><code class="lang-python">list_num_sentence = []for name, group in df_train.groupby(&#39;label&#39;):    sum_sep_label = np.vectorize(sum_of_sep)(group[&#39;text&#39;])    num_sentence = np.mean(sum_sep_label)    list_num_sentence.append(num_sentence)df_train_info[&#39;sent_num&#39;] = list_num_sentencedf_train_info[&#39;sent_len&#39;] = df_train_info[&#39;len_mean&#39;] / df_train_info[&#39;sent_num&#39;]df_train_info</code></pre><p>不同类别的新闻，其句子长度和个数也是不同的。</p><p>之前我们分析，股票类文章往往很长，而社会（label=5）和教育（label=6）类文章的句子最多。家居（label=8）和时尚（label=11）类新闻的句子最少。游戏类（label=9）句子是最长的，社会（label=5）句子是最短的（发现：句子和个数长度也可以作为特征）。</p><p><img src="/2020/07/22/【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析/2020-07-25-13-00-28.png" srcset="/img/loading.gif" alt></p><p><strong>每类新闻中出现次数前10</strong></p><p>在每类新闻中出现频率最高的词汇，就是df_train_info表中的freq_freq列。可以看到，清一色的3750，这个字符我们在后期处理时可以拿掉。</p><pre><code class="lang-python">word_count_dict = {}for name, df in df_train.groupby(&#39;label&#39;):    # print(name, type(df))    all_text = &#39; &#39;.join(list(df[&#39;text&#39;].apply(lambda x: &#39; &#39;.join(list(x.split(&#39; &#39;))))))    word_count_single_class = Counter(all_text.split(&quot; &quot;))    word_count_single_class = sorted(word_count_single_class.items(), key=lambda d:int(d[1]), reverse = True)    word_count_dict[name] = word_count_single_classfor label in range(14):    print(index_2_label_dict[label], [x for x,_ in word_count_dict[label][:10]])科技 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;3370&#39;, &#39;4464&#39;, &#39;2465&#39;, &#39;6122&#39;, &#39;3659&#39;, &#39;7399&#39;, &#39;4939&#39;]股票 [&#39;3750&#39;, &#39;648&#39;, &#39;3370&#39;, &#39;900&#39;, &#39;4464&#39;, &#39;3659&#39;, &#39;5036&#39;, &#39;6250&#39;, &#39;1633&#39;, &#39;6065&#39;]体育 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;7399&#39;, &#39;6122&#39;, &#39;4939&#39;, &#39;4704&#39;, &#39;1667&#39;, &#39;5598&#39;, &#39;669&#39;]娱乐 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;6122&#39;, &#39;4939&#39;, &#39;4893&#39;, &#39;7399&#39;, &#39;669&#39;, &#39;803&#39;, &#39;1635&#39;]时政 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;4411&#39;, &#39;7399&#39;, &#39;4893&#39;, &#39;6122&#39;, &#39;4464&#39;, &#39;2400&#39;, &#39;4853&#39;]社会 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;6122&#39;, &#39;5598&#39;, &#39;4893&#39;, &#39;7399&#39;, &#39;4939&#39;, &#39;3370&#39;, &#39;669&#39;]教育 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;6248&#39;, &#39;2555&#39;, &#39;5620&#39;, &#39;2465&#39;, &#39;6122&#39;, &#39;5560&#39;, &#39;3370&#39;]财经 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;3370&#39;, &#39;5296&#39;, &#39;4464&#39;, &#39;6835&#39;, &#39;3659&#39;, &#39;6122&#39;, &#39;7399&#39;]家居 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;6122&#39;, &#39;4939&#39;, &#39;913&#39;, &#39;5560&#39;, &#39;7399&#39;, &#39;3961&#39;, &#39;4811&#39;]游戏 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;7328&#39;, &#39;6122&#39;, &#39;7399&#39;, &#39;5547&#39;, &#39;4939&#39;, &#39;3370&#39;, &#39;2465&#39;]房产 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;3370&#39;, &#39;2465&#39;, &#39;5560&#39;, &#39;3686&#39;, &#39;4464&#39;, &#39;3523&#39;, &#39;6122&#39;]时尚 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;4939&#39;, &#39;6122&#39;, &#39;5560&#39;, &#39;669&#39;, &#39;4811&#39;, &#39;7539&#39;, &#39;4893&#39;]彩票 [&#39;3750&#39;, &#39;4464&#39;, &#39;3370&#39;, &#39;648&#39;, &#39;2465&#39;, &#39;900&#39;, &#39;3659&#39;, &#39;6065&#39;, &#39;1667&#39;, &#39;2614&#39;]星座 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;4939&#39;, &#39;669&#39;, &#39;6122&#39;, &#39;4893&#39;, &#39;3864&#39;, &#39;4811&#39;, &#39;1465&#39;]</code></pre><h1 id="分析结果"><a href="#分析结果" class="headerlink" title="分析结果"></a>分析结果</h1><p>数据分析肯定要有结论，没有结论的数据分析是不完整的。</p><ol><li><p>训练集共200,000条新闻，每条新闻平均907个字符，最短的句子长度为2，最长的句子长度为57921，其中75%以下的数据长度在1131以下。测试集共50,000条新闻，每条新闻平均909个字符，最短句子长度为14，最长句子41861,75%以下的数据长度在1133以下。</p></li><li><p>训练集和测试集就长度来说似乎是同一分布，但是不属于正态分布。</p></li><li><p>把截断长度定为5000？</p></li><li><p>赛题的数据集类别分布存在较为不均匀的情况。在训练集中科技类新闻最多，其次是股票类新闻，最少的新闻是星座新闻。需要用采样方法解决。文章最长的是股票类新闻。不同类别的文章长度不同，可以把长度和句子个数作为一个Feature，以供机器学习模型训练。</p></li><li><p>训练集中总共包括6869个字，最大数字为7549，最小数字为0，其中编号3750的字出现的次数最多，编号3133的字出现的次数最少，仅出现一次，其中字符3750、字符900和字符648在20w新闻的覆盖率接近99%，很有可能是标点符号。</p></li><li><p>900很有可能是句号，2662和885则很有可能为感叹号和问号，3750出现频率很高但是基本不在新闻最后出现，因此初步判断为逗号。按照这种划分，训练集中每条新闻平均句子个数约为19。</p></li><li><p>在训练集中，不同类别新闻出现词汇有特色。但是需要把共有的常用词停用。自然想到利用TF-IDF编码方式。</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不要对我有任何期待哦！&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Feature Engineering" scheme="https://superlova.github.io/tags/Feature-Engineering/"/>
    
      <category term="Data Science" scheme="https://superlova.github.io/tags/Data-Science/"/>
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
  </entry>
  
</feed>
