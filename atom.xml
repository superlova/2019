<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Superlova</title>
  
  <subtitle>Welcome...</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://superlova.github.io/"/>
  <updated>2020-06-26T17:04:36.951Z</updated>
  <id>https://superlova.github.io/</id>
  
  <author>
    <name>Superlova</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【论文阅读笔记】Fuzzing: A Survey</title>
    <link href="https://superlova.github.io/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing-A-Survey/"/>
    <id>https://superlova.github.io/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing-A-Survey/</id>
    <published>2020-06-20T03:59:40.000Z</published>
    <updated>2020-06-26T17:04:36.951Z</updated>
    
    <content type="html"><![CDATA[<p>2018年CyberSecurity收录的一篇关于软件测试中模糊测试的综述。作者来自清华大学。文章名越短越霸气。<br><a id="more"></a></p><blockquote><p>警告！本文在写作过程中大量使用了谷歌翻译。</p></blockquote><h1 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h1><p>模糊测试几乎不需要了解目标，并且可以轻松扩展到大型应用程序，因此已成为最受欢迎的漏洞发现解决方案</p><p>模糊的随机性和盲目性导致发现错误的效率低下</p><p>反馈驱动的模糊模式（feedback-driven fuzzing mode）和遗传算法（genetic algorithms）的结合提供了更灵活和可自定义的模糊框架，并使模糊过程更加智能和高效。</p><h1 id="二、背景"><a href="#二、背景" class="headerlink" title="二、背景"></a>二、背景</h1><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/2020-06-26-22-37-23.png" srcset="/img/loading.gif" alt></p><h2 id="1-静态分析"><a href="#1-静态分析" class="headerlink" title="1.静态分析"></a>1.静态分析</h2><p>静态分析（Static Analysis）是对在没有实际执行程序的情况下执行的程序的分析。</p><p>通常对源代码执行静态分析，有时还对目标代码执行静态分析。通过分析词法，语法，语义特征以及数据流分析，模型检查，静态分析，可以检测到隐藏的错误。</p><p>静态分析的优点是检测速度快。</p><p>由于缺乏易于使用的漏洞检测模型，因此静态分析工具容易产生大量误报。因此，确定静态分析的结果仍然是一项艰巨的工作。</p><h2 id="2-动态分析"><a href="#2-动态分析" class="headerlink" title="2.动态分析"></a>2.动态分析</h2><p>动态分析（Dynamic Analysis）与静态分析相比，分析人员需要在实际系统或仿真器中执行目标程序。</p><p>通过监视运行状态并分析运行时知识，动态分析工具可以精确地检测程序错误。</p><p>动态分析的优点是精度高，缺点是速度慢，效率低，对测试人员的技术水平要求高，可扩展性差，并且难以进行大规模测试。</p><h2 id="3-符号执行"><a href="#3-符号执行" class="headerlink" title="3.符号执行"></a>3.符号执行</h2><p>符号执行（Symbolic Execution）是另一种发现漏洞的技术。</p><p>通过符号化程序输入，符号执行为每个执行路径维护了一组约束（constraint）。</p><p>执行之后，约束求解器（constraint solvers）将用于求解约束并确定导致执行的输入。</p><p>从技术上讲，符号执行可以覆盖程序中的任何执行路径，并且在小型程序的测试中已显示出良好的效果，但也存在许多限制。</p><p>首先，路径爆炸问题。随着程序规模的增长，执行状态会爆炸，这超出了约束求解器的求解能力。</p><p>第二，环境的相互作用。在符号执行中，当目标程序执行与符号执行环境之外的组件交互时，例如系统调用，处理信号等，可能会出现一致性问题。</p><p>因此符号执行仍然很难扩展到大型应用程序。</p><h2 id="4-模糊测试"><a href="#4-模糊测试" class="headerlink" title="4.模糊测试"></a>4.模糊测试</h2><p>模糊测试（Fuzzing）是目前最流行的漏洞发现技术。</p><p>从概念上讲，模糊测试从为目标应用程序生成大量正常和异常输入开始，并尝试通过将生成的输入提供给目标应用程序并监视执行状态来检测异常。</p><p>与其他技术相比，模糊测试易于部署并且具有良好的可扩展性和适用性，并且可以在有或没有源代码的情况下执行。</p><p>此外，由于模糊测试是在实际执行中执行的，因此它具有很高的准确性。</p><p>而且，模糊测试几乎不需要了解目标应用程序，并且可以轻松扩展到大型应用程序。</p><p>尽管模糊测试存在许多缺点，例如效率低和代码覆盖率低，但是，缺点却胜过缺点，模糊处理已成为当前最有效，最高效的最新漏洞发现技术。</p><h1 id="三、模糊测试介绍"><a href="#三、模糊测试介绍" class="headerlink" title="三、模糊测试介绍"></a>三、模糊测试介绍</h1><h2 id="1-模糊测试的工作过程"><a href="#1-模糊测试的工作过程" class="headerlink" title="1.模糊测试的工作过程"></a>1.模糊测试的工作过程</h2><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/2020-06-26-22-38-53.png" srcset="/img/loading.gif" alt></p><p>模糊测试包括四个主要阶段，即测试用例生成阶段，测试用例运行阶段，程序执行状态监视和异常分析。</p><p>模糊测试从生成一堆程序输入（即测试用例）开始。生成的测试用例的质量直接影响测试效果。</p><p>一方面，输入应尽可能满足测试程序对输入格式的要求。</p><p>另一方面，应充分破坏输入，以便对这些输入进行处理很可能会使程序失败。</p><p>根据目标程序，输入可以是具有不同文件格式的文件，网络通信数据，具有指定特征的可执行二进制文件等。</p><p>如何生成足够多的测试用例是Fuzzer面临的主要挑战。</p><p>在上一阶段生成测试用例后，将它们馈送到目标程序。</p><p>模糊测试器自动开始和完成目标程序的过程，并驱动目标程序的测试用例处理过程。</p><p>在执行之前，分析人员可以配置目标程序的启动和完成方式，并预定义参数和环境变量。</p><p>通常，模糊处理过程在预定义的超时、程序执行挂起或崩溃时停止。</p><p>模糊器在目标程序执行期间监视执行状态，以防异常和崩溃。</p><p>常用的异常监视方法包括监视特定的系统信号，崩溃和其他违规（violations）。</p><p>当捕获到违规时，模糊器将存储相应的测试用例，以供以后重播和分析。</p><p>在分析阶段，分析人员尝试确定捕获的违规的位置和根本原因。</p><p>自动崩溃分析是另一个重要的研究领域。</p><h2 id="2-模糊测试器的种类"><a href="#2-模糊测试器的种类" class="headerlink" title="2.模糊测试器的种类"></a>2.模糊测试器的种类</h2><h3 id="A-基于生成和基于突变"><a href="#A-基于生成和基于突变" class="headerlink" title="A.基于生成和基于突变"></a>A.基于生成和基于突变</h3><p>模糊器可以分为基于生成（generation based）和基于突变（mutation based）两类。</p><p>对于基于生成的模糊器，需要有关程序输入的知识。</p><p>对于文件格式模糊测试（file format fuzzing），通常会提供一个预定义文件格式的配置文件。测试用例根据配置文件生成。</p><p>有了给定的文件格式知识（file format knowledge），基于生成的模糊器生成的测试用例就可以更轻松地通过程序的验证，并且更有可能测试目标程序的更深层代码。</p><p>但是，如果没有友好的文档，分析文件格式将是一项艰巨的工作。</p><p>因此，基于变异的模糊器更容易启动并且更适用，并且被最新的模糊器广泛使用。</p><p>对于基于突变的模糊器，需要一组有效的初始输入（a set of valid initial inputs）。</p><p>测试用例是通过对初始输入和测试过程中产生的测试用例进行变异而生成的。</p><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/2020-06-26-22-43-30.png" srcset="/img/loading.gif" alt></p><h3 id="B-黑盒白盒灰盒"><a href="#B-黑盒白盒灰盒" class="headerlink" title="B.黑盒白盒灰盒"></a>B.黑盒白盒灰盒</h3><p>关于对程序源代码的依赖性（the dependence on program source<br>code）和程序分析的程度（the degree of program analysis），模糊器可以分为白盒，灰盒和黑盒（white box, gray box and black box）。</p><p>白盒模糊测试器可以访问程序的源代码，因此可以通过对源代码进行分析以及测试用例如何影响程序运行状态来收集更多信息。</p><p>黑盒模糊器会在不了解目标程序内部的情况下进行模糊测试。</p><p>灰箱模糊器也不使用源代码，但可以通过程序分析获得目标程序的内部信息。</p><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/2020-06-26-22-44-18.png" srcset="/img/loading.gif" alt></p><h3 id="C-定向模糊和基于覆盖的模糊"><a href="#C-定向模糊和基于覆盖的模糊" class="headerlink" title="C.定向模糊和基于覆盖的模糊"></a>C.定向模糊和基于覆盖的模糊</h3><p>根据探索程序的策略（the strategies of exploring the programs），模糊器可以分为定向模糊（directed fuzzing）和基于覆盖的模糊（coverage-based fuzzing）。</p><p>定向模糊器旨在生成覆盖目标代码和程序目标路径的测试用例（cover target code and target paths of programs），而基于覆盖率的模糊器旨在生成覆盖尽可能多的程序代码的测试用例（cover as much code of programs as possible）。</p><p>定向模糊器期望对程序进行更快的测试，而基于覆盖率的模糊器期望进行更彻底的测试并检测到尽可能多的错误。</p><p>对于定向模糊器和基于覆盖的模糊器，如何提取执行路径的信息是一个关键问题。</p><h3 id="D-哑模糊和智能模糊"><a href="#D-哑模糊和智能模糊" class="headerlink" title="D.哑模糊和智能模糊"></a>D.哑模糊和智能模糊</h3><p>根据对程序执行状态的监视和测试用例的生成之间是否存在反馈（whether there is a feedback between the monitoring of program execution state and testcase generation），模糊器可以分为哑类（dumb fuzz）和智能类（smart fuzz）。</p><p>智能模糊器会根据收集的信息（测试用例如何影响程序行为）来调整测试用例的生成。</p><p>对于基于变异的模糊测试器，反馈信息可用于确定应该对测试用例的哪一部分进行变异以及对它们进行变异的方式。</p><p>哑模糊测试器（Dumb fuzzers）具有更好的测试速度（speed），而智能模糊测试器可以生成更好的测试用例并获得更高的效率（efficiency）。</p><h2 id="3-模糊测试面临的挑战"><a href="#3-模糊测试面临的挑战" class="headerlink" title="3.模糊测试面临的挑战"></a>3.模糊测试面临的挑战</h2><h3 id="A-如何改变种子输入的挑战"><a href="#A-如何改变种子输入的挑战" class="headerlink" title="A.如何改变种子输入的挑战"></a>A.如何改变种子输入的挑战</h3><p>基于变异的模糊测试工具在进行变异时需要回答两个问题：（1）变异的位置，以及（2）变异的方式。</p><p>只有几个关键位置的突变会影响执行的控制流程。因此，如何在测试用例中定位这些关键位置非常重要。</p><p>此外，模糊器改变关键位置的方式是另一个关键问题，即如何确定可以将测试定向到程序有趣路径的值。</p><p>简而言之，测试用例的盲目突变会严重浪费测试资源，更好的变异策略可以显着提高模糊测试的效率。</p><h3 id="B-低代码覆盖率的挑战"><a href="#B-低代码覆盖率的挑战" class="headerlink" title="B.低代码覆盖率的挑战"></a>B.低代码覆盖率的挑战</h3><p>较高的代码覆盖率表示对程序执行状态的覆盖率更高，并且测试更加彻底。发现错误的可能性更高。</p><p>但是，大多数测试用例仅涵盖相同的少数路径。</p><p>因此，仅通过大量生成测试用例并投入测试资源来实现高覆盖率是不明智的选择。</p><p>基于覆盖率的模糊器试图借助程序分析技术（例如程序检测，program instrumentation）来解决问题。</p><h3 id="C-通过验证的挑战"><a href="#C-通过验证的挑战" class="headerlink" title="C.通过验证的挑战"></a>C.通过验证的挑战</h3><p>程序通常在解析和处理之前验证（Validation）输入。</p><p>验证可以保护程序，节省计算资源，并保护程序免受无效输入和恶意构造输入造成的损坏。</p><p>黑盒和灰盒模糊器生成的测试用例很难通过盲目生成策略（blind generation strategy）的验证，从而导致效率很低。</p><h1 id="四、覆盖引导的模糊测试"><a href="#四、覆盖引导的模糊测试" class="headerlink" title="四、覆盖引导的模糊测试"></a>四、覆盖引导的模糊测试</h1><p>为了实现深入而彻底的程序模糊测试，模糊测试人员应尝试<strong>遍历尽可能多的程序运行状态</strong>。</p><p>但是，由于程序行为的不确定性，因此<strong>没有用于程序状态的简单度量</strong>。</p><p>测量代码覆盖率成为一种近似的替代解决方案。代码覆盖率的增加代表了新的程序状态。此外，代码覆盖率很容易测量。</p><p>但是，代码覆盖率是一种近似的度量，因为在实践中，恒定的代码覆盖率并不表示恒定的程序状态数。使用此指标可能会导致某些信息丢失。</p><h2 id="1-代码覆盖率计数"><a href="#1-代码覆盖率计数" class="headerlink" title="1.代码覆盖率计数"></a>1.代码覆盖率计数</h2><p>在程序分析中，程序由基本块（basic blocks）组成。基本块是具有单个入口和出口点的代码段，基本块中的指令将顺序执行，并且只会执行一次。</p><p>在代码覆盖率测量中，最新技术将基本块作为最佳粒度（granularity）。原因包括：（1）基本块是程序执行的最小连贯单位；（2）测量功能或指令会导致信息丢失或冗余；（3）基本块可以由第一条指令的地址标识；以及基本块信息可通过代码检测轻松提取。</p><p>当前，有两种基于基本块的基本测量选择：简单地计算已执行的基本块和计算基本块的跃迁（transitions）。</p><p>在后一种方法中，程序被解释为图，顶点用于表示基本块，边用于表示基本块之间的跃迁。</p><p>后一种方法记录边缘，而前一种方法记录顶点。</p><p>但是实验表明仅对已执行的基本块进行计数将导致严重的信息丢失。</p><p>如图所示，如果首先执行程序路径（BB1，BB2，BB3，BB4），然后执行时遇到路径（BB1，BB2，BB4），则新边（BB2，BB4）信息为丢失。</p><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/2020-06-27-00-11-48.png" srcset="/img/loading.gif" alt></p><h2 id="2-基于覆盖的模糊测试的工作过程"><a href="#2-基于覆盖的模糊测试的工作过程" class="headerlink" title="2.基于覆盖的模糊测试的工作过程"></a>2.基于覆盖的模糊测试的工作过程</h2><p>算法1显示了基于覆盖的模糊器的一般工作过程。</p><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/alg1.png" srcset="/img/loading.gif" alt></p><p>测试从初始给定的种子输入开始。</p><p>在主模糊循环中，模糊器反复选择一个有趣的种子来进行后续的突变和测试用例的生成。然后在模糊器的监视下驱动目标程序以执行生成的测试用例。</p><p>收集触发崩溃的测试用例，并将其他有趣的用例添加到种子库中。</p><p>对于基于覆盖的模糊测试，达到新控制流<strong>边缘的测试用例</strong>被认为很有趣。</p><p>主模糊循环在预先配置的超时或中止信号时停止。</p><p>在模糊测试过程中，模糊器通过各种方法跟踪执行情况。</p><p>基本上，模糊器出于两个目的跟踪执行，即<strong>代码覆盖率</strong>和<strong>安全性违规</strong>（security violations）。</p><p>代码覆盖率信息用于进行彻底的程序状态探索，而安全违规跟踪则用于更好地发现错误。</p><p>下图显示了AFL（American Fuzzy Lop，一个非常有代表性的基于覆盖的模糊器）的工作过程。</p><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/2020-06-27-00-18-50.png" srcset="/img/loading.gif" alt></p><p>在执行覆盖范围收集之前，模糊器将对目标应用程序进行检测。</p><p>在主模糊循环中，提供初始种子输入后，（1）模糊器根据种子选择策略从种子库中选择喜欢的种子，比如AFL则选择最快和最小的种子。（2）根据变异策略对种子文件进行变异，并生成一堆测试用例。</p><p>AFL当前采用一些随机修改和测试用例拼接方法，包括长度和步长变化的顺序位翻转，小整数的顺序加法和减法以及已知有趣的整数（如0、1，INT_MAX等）的顺序插入。（3）测试用例已执行，并且执行情况正在跟踪中。</p><p>收集覆盖率信息以确定有趣的测试用例，即达到新控制流边缘的测试用例。</p><p>有趣的测试用例将添加到种子池中，以进行下一轮运行。</p><h2 id="3-关键问题"><a href="#3-关键问题" class="headerlink" title="3.关键问题"></a>3.关键问题</h2><p>前面的介绍表明，要运行一个高效的、基于覆盖的模糊，需要解决很多问题。最近一些研究如下表。</p><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/2020-06-27-00-22-32.png" srcset="/img/loading.gif" alt></p><h3 id="A-如何获得初始输入？"><a href="#A-如何获得初始输入？" class="headerlink" title="A.如何获得初始输入？"></a>A.如何获得初始输入？</h3><p>大多数最先进的基于覆盖的模糊器都采用了基于突变的测试用例生成策略，这在很大程度上取决于初始种子输入的质量。</p><p>良好的初始种子输入可以显著提高模糊的效率和效果。</p><p>具体来说，</p><p>(1)提供格式良好的种子输入可以节省构建一个种子输入所消耗的大量cpu时间；</p><p>(2)良好的初始输入可以满足复杂文件格式的要求，而复杂文件格式在突变阶段是很难猜测的；</p><p>(3)基于格式良好的种子输入的突变更容易产生测试用例，可以达到更深层次和难以达到的路径；</p><p>(4)良好的种子输入可以在多次测试中重复使用。</p><p>常用的收集种子输入的方法包括使用标准基准、从互联网上抓取和使用现有的POC（Proof of Concept）样本。</p><p>考虑到目标应用输入的多样性，从互联网上抓取是最直观的方法。</p><p>过量的种子输入会导致第一次干运行的时间浪费，从而带来另一个问题，即如何提炼初始语料。</p><p>AFL提供了一个工具，它可以提取一个最小的输入集，以达到相同的代码覆盖率。</p><h3 id="B-如何生成测试用例？"><a href="#B-如何生成测试用例？" class="headerlink" title="B.如何生成测试用例？"></a>B.如何生成测试用例？</h3><p>testcases的质量是影响模糊测试效率和效果的重要因素。</p><p>首先，好的testcases可以在较短的时间内探索更多的程序执行状态，覆盖更多的代码。</p><p>此外，好的测试用例可以针对潜在的脆弱位置，带来更快的程序bug发现。</p><p>因此，如何基于种子输入生成好的测试用例是一个重要的问题。</p><p>种子输入的突变涉及两个关键问题：在哪里突变和用什么值进行突变。</p><p>随着机器学习技术的发展和广泛应用，一些研究尝试使用机器学习技术来辅助生成testcases。</p><p>微软研究院的Godefroid等（2017）利用基于神经网络的统计机器学习技术来自动生成testcases。</p><p>具体来说，他们首先通过机器学习技术从一堆有效输入中学习输入格式，然后利用学习到的知识指导测试用例的生成。</p><p>他们介绍了微软Edge浏览器中PDF解析器的模糊过程。</p><p>虽然实验并没有给出一个令人鼓舞的结果，但仍是一个不错的尝试。</p><p>微软的Rajpal等人（2017）使用神经网络从过去的模糊探索中学习，并预测输入文件中哪些字节要突变。</p><p>Nichols等人（2017）使用生成对抗网络（GAN）模型来帮助用新颖的种子文件重新初始化系统。</p><p>实验表明，GAN比LSTM更快、更有效，并且有助于发现更多的代码路径。</p><h3 id="C-如何从种子池中选择种子？"><a href="#C-如何从种子池中选择种子？" class="headerlink" title="C.如何从种子池中选择种子？"></a>C.如何从种子池中选择种子？</h3><p>模糊器在主模糊循环中新一轮测试开始时，反复从种子池中选择种子进行突变。</p><p>以往的工作已经证明，良好的种子选择策略可以显著提高模糊效率，并帮助更快地找到更多的Bugs。</p><p>通过良好的种子选择策略，模糊器可以</p><ul><li>（1）优先选择更有帮助的种子，包括覆盖更多的代码，更容易触发漏洞；</li><li>（2）减少重复执行路径的浪费，节省计算资源；</li><li>（3）优化选择覆盖更深、更易受攻击的代码的种子，帮助更快地识别隐藏的漏洞。</li></ul><p>Böhme等人（2017）提出了AFLFast，一个基于覆盖的灰盒模糊器。在模糊过程中，AFLFast会测量执行路径的频率，优先处理被模糊次数较少的种子，并为行使低频路径的种子分配更多的能量。</p><p>Rawat等(2017)综合了静态和动态分析来识别难以到达的深层路径，并对到达深层路径的种子进行优先级排序。</p><p>AFLGo将一些易受攻击的代码定义为目标位置，并优化选择离目标位置较近的测试case。</p><p>已知漏洞的特征也可用于种子选择策略。然而，收集耗费资源的信息带来了沉重的开销，降低了模糊的效率。</p><h3 id="D-如何高效地测试应用程序？"><a href="#D-如何高效地测试应用程序？" class="headerlink" title="D.如何高效地测试应用程序？"></a>D.如何高效地测试应用程序？</h3><p>目标应用程序由主模糊循环中的模糊器反复启动和完成。</p><p>对于用户区应用程序的模糊处理，程序的创建和完成将消耗大量的cpu时间。频繁创建和完成该程序将严重降低模糊测试的效率。</p><p>AFL使用forkserver方法，该方法创建一个已加载程序的完全相同的克隆，并在每次运行时重复使用该克隆。</p><h1 id="五、模糊集成技术"><a href="#五、模糊集成技术" class="headerlink" title="五、模糊集成技术"></a>五、模糊集成技术</h1><p>使用随机突变方法的模糊模糊测试策略会导致大量无效测试用例，并且模糊测试效率较低。</p><p>当前，最先进的模糊器通常采用智能模糊策略。</p><p>智能模糊器通过程序分析技术来收集程序控制流和数据流信息，并因此利用收集到的信息来改进测试用例的生成。</p><p>由智能模糊器生成的测试用例具有更好的针对性，更有可能满足程序对数据结构和逻辑判断的要求。</p><p>下图描绘了智能模糊的示意图。</p><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/2020-06-27-00-36-55.png" srcset="/img/loading.gif" alt></p><p>表5中总结了模糊测试中集成的主要技术。</p><p><img src="/2020/06/20/【论文阅读笔记】Fuzzing-A-Survey/2020-06-27-00-37-51.png" srcset="/img/loading.gif" alt></p><h2 id="1-测试用例生成"><a href="#1-测试用例生成" class="headerlink" title="1.测试用例生成"></a>1.测试用例生成</h2><p>如前所述，模糊测试中的测试用例是基于生成的方法或基于变异的方法生成的。</p><p>如何生成满足复杂数据结构要求并更有可能触发难以到达的路径的测试用例是一个关键挑战。</p><p>在基于生成的模糊测试中，生成器根据输入数据格式的知识生成测试用例。</p><p>Work（Godefroid et al.2017）使用机器学习技术（特别是递归神经网络）来学习输入文件的语法，并因此使用所学的语法来生成满足格式要求的测试用例。</p><p>更多的最先进的模糊器采用基于变异的模糊策略。通过在突变过程中修改部分种子输入来生成测试用例。</p><p>在盲目的突变模糊化过程中，变异者使用随机值或几个特殊值随机修改种子字节，这被证明是效率很低的。因此，如何确定要修改的位置以及修改中使用的值是另一个关键挑战。</p><h2 id="2-程序执行"><a href="#2-程序执行" class="headerlink" title="2.程序执行"></a>2.程序执行</h2><p>执行阶段涉及的两个关键问题是<strong>如何指导模糊测试过程</strong>以及<strong>如何探索新路径</strong>。</p><p>仪表（Instrumentation）技术用于记录路径执行情况并计算基于覆盖率的模糊测试中的覆盖率信息。</p><p>测试执行过程中的另一个问题是探索新路径。符号执行技术在路径探索中具有天然的优势。</p><p>通过求解约束集，符号执行技术可以计算出满足特定条件要求的值。</p><p>反馈驱动的模糊测试提供了一种有效的引导测试方法，传统和新技术都可以发挥传感器的作用，以在测试执行过程中获取各种信息，并准确地指导模糊测试。</p><h1 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a>六、总结</h1><ul><li>反馈驱动的模糊模式（feedback-driven fuzzing mode）和遗传算法（genetic algorithms）的结合</li><li>测试用例生成阶段，测试用例运行阶段，程序执行状态监视和异常分析</li><li>根据探索程序的策略（the strategies of exploring the programs），模糊器可以分为定向模糊（directed fuzzing）和基于覆盖的模糊（coverage-based fuzzing）。</li><li>定向模糊器期望对程序进行更快的测试，而基于覆盖率的模糊器期望进行更彻底的测试并检测到尽可能多的错误。</li><li>对于定向模糊器和基于覆盖的模糊器，如何提取执行路径的信息是一个关键问题。</li><li>根据对程序执行状态的监视和测试用例的生成之间是否存在反馈（whether there is a feedback between the monitoring of program execution state and testcase generation），模糊器可以分为哑类（dumb fuzz）和智能类（smart fuzz）。</li><li>智能模糊器会根据收集的信息（测试用例如何影响程序行为）来调整测试用例的生成。</li><li>对于基于变异的模糊测试器，反馈信息可用于确定应该对测试用例的哪一部分进行变异以及对它们进行变异的方式。</li><li>基于变异的模糊测试工具在进行变异时需要回答两个问题：（1）变异的位置，以及（2）变异的方式。</li><li>覆盖率的定义。定义覆盖率时要防止信息丢失。代码覆盖率的增加代表了新的程序状态。此外，代码覆盖率很容易测量。边缘的测试用例被认为很有趣</li><li>通过验证的挑战。验证可以保护程序，节省计算资源，并保护程序免受无效输入和恶意构造输入造成的损坏。</li><li>一些研究尝试使用机器学习技术来辅助生成testcases。GAN比LSTM更快、更有效，并且有助于发现更多的代码路径。</li><li>如何从种子池中选择种子？</li><li>如何高效地测试应用程序？</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2018年CyberSecurity收录的一篇关于软件测试中模糊测试的综述。作者来自清华大学。文章名越短越霸气。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://superlova.github.io/categories/paper/"/>
    
    
      <category term="Software Testing" scheme="https://superlova.github.io/tags/Software-Testing/"/>
    
      <category term="Fuzzing" scheme="https://superlova.github.io/tags/Fuzzing/"/>
    
      <category term="Survey" scheme="https://superlova.github.io/tags/Survey/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】LSTM网络结构简介与对应的keras实现</title>
    <link href="https://superlova.github.io/2020/06/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91LSTM%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AF%B9%E5%BA%94%E7%9A%84keras%E5%AE%9E%E7%8E%B0/"/>
    <id>https://superlova.github.io/2020/06/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91LSTM%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AF%B9%E5%BA%94%E7%9A%84keras%E5%AE%9E%E7%8E%B0/</id>
    <published>2020-06-20T03:50:37.000Z</published>
    <updated>2020-06-26T12:39:35.089Z</updated>
    
    <content type="html"><![CDATA[<p>从理论和代码两个层面介绍了LSTM网络。<br><a id="more"></a></p><h2 id="一、理论来一波"><a href="#一、理论来一波" class="headerlink" title="一、理论来一波"></a>一、理论来一波</h2><p>循环神经网络（Recurrent Neural Network，RNN）是一类有短期记忆能力的神经网络。在循环神经网络中，神经元不但可以接受其他神经元的信息，也可以接受自身的信息，形成具有环路的网络结构。</p><p><img src="/2020/06/20/【学习笔记】LSTM网络结构简介与对应的keras实现/2020-06-20-11-51-21.png" srcset="/img/loading.gif" alt></p><p>长短期记忆网络（Long Short-Term Memory Network，LSTM）[Gers et al.,2000; Hochreiter et al., 1997] 是循环神经网络的一个变体，可以有效地解决简单循环神经网络的梯度爆炸或消失问题。</p><p><img src="/2020/06/20/【学习笔记】LSTM网络结构简介与对应的keras实现/2020-06-20-11-51-13.png" srcset="/img/loading.gif" alt></p><p>LSTM 网络引入一个新的内部状态（internal state） $\boldsymbol{c}_{t}\in \mathbb{R}^{\boldsymbol{D}}$ 专门进行线性的循环信息传递，同时（非线性地）输出信息给隐藏层的外部状态 $\boldsymbol{h}_{t}\in \mathbb{R}^{\boldsymbol{D}}$ 。这两个状态通过下式计算：</p><script type="math/tex; mode=display">\begin{array}{l}\boldsymbol{c}_{t}=\boldsymbol{f}_{t} \odot \boldsymbol{c}_{t-1}+\boldsymbol{i}_{t} \odot \tilde{\boldsymbol{c}}_{t} \\\boldsymbol{h}_{t}=\boldsymbol{o}_{t} \odot \tanh \left(\boldsymbol{c}_{t}\right)\end{array}</script><p>其中，$\odot$为向量逐元素乘积（代表左右两边向量维度相同）；$\boldsymbol{c}_{t-1}$为上一时刻的记忆单元；$\tilde{\boldsymbol{c}}\in \mathbb{R}^{\boldsymbol{D}}$是通过非线性函数得到的候选状态：</p><script type="math/tex; mode=display">\tilde{\boldsymbol{c}}_{t}=\tanh \left(\boldsymbol{W}_{c} \boldsymbol{x}_{t}+\boldsymbol{U}_{c} \boldsymbol{h}_{t-1}+\boldsymbol{b}_{c}\right)</script><p>在每个时刻 $t$，LSTM 网络的内部状态 $\boldsymbol{c}_{t}$ 记录了到当前时刻为止的历史信息。</p><p>LSTM内部多了三个gate，分别是forget、input、output。$\boldsymbol{f}_{t}\in [0,1]^{\boldsymbol{D}}$、$\boldsymbol{i}_{t}\in [0,1]^{\boldsymbol{D}}$、$\boldsymbol{o}_{t}\in [0,1]^{\boldsymbol{D}}$。这三个门与输入、隐状态和输出的维度应该相同，都是维度为输入序列维度n的向量（其实应该为n+1）$=D$。</p><p>与此同时，三个门的值依赖于$t$时刻的输入$x_t$、$t-1$时刻的隐变量$h_{t-1}$以及不同的权重矩阵($W_i$/$W_f$/$W_o$/$U_i$/$U_f$/$U_o$)。</p><p>门控机制（Gating Mechanism）是用来控制信息传递的路径的手段。</p><ul><li>遗忘门 $\boldsymbol{f}_{t}$ 控制上一个时刻的内部状态$\boldsymbol{c}_{t-1}$ 需要遗忘多少信息。</li><li>输入门 $\boldsymbol{i}_{t}$ 控制当前时刻的候选状态 ̃$\tilde{\boldsymbol{c}}_{t}$ 有多少信息需要保存。</li><li>输出门 $\boldsymbol{o}_{t}$ 控制当前时刻的内部状态 $\boldsymbol{c}_{t}$ 有多少信息需要输出给外部状态 $\boldsymbol{h}_{t}$。</li></ul><p>举个例子，当$\boldsymbol{f}_{t}=\mathbf{0}, \boldsymbol{i}_{t}=\mathbf{1}$时，记忆单元将历史信息清空，并将候选状态向量$\tilde{\boldsymbol{c}}_{t}$写入。但此时记忆单元 $\boldsymbol{c}_{t}$ 依然和上一时刻的历史信息相关。当$\boldsymbol{f}_{t}=\mathbf{1}, \boldsymbol{i}_{t}=\mathbf{0}$时，记忆单元将复制上一时刻的内容，不写入新的信息。</p><p>LSTM 网络中的“门”是一种“软”门，取值在 (0, 1) 之间，表示以一定的比例允许信息通过．三个门的计算方式为：</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{i}_{t} &=\sigma\left(\boldsymbol{W}_{i} \boldsymbol{x}_{t}+\boldsymbol{U}_{i} \boldsymbol{h}_{t-1}+\boldsymbol{b}_{i}\right) \\\boldsymbol{f}_{t} &=\sigma\left(\boldsymbol{W}_{f} \boldsymbol{x}_{t}+\boldsymbol{U}_{f} \boldsymbol{h}_{t-1}+\boldsymbol{b}_{f}\right) \\\boldsymbol{o}_{t} &=\sigma\left(\boldsymbol{W}_{o} \boldsymbol{x}_{t}+\boldsymbol{U}_{o} \boldsymbol{h}_{t-1}+\boldsymbol{b}_{o}\right)\end{aligned}</script><p>其中$\sigma(\cdot)$ 为 Logistic 函数，其输出区间为 (0, 1)；$\boldsymbol{x}_{t}$为当前时刻的输入。</p><h2 id="二、还是得看代码"><a href="#二、还是得看代码" class="headerlink" title="二、还是得看代码"></a>二、还是得看代码</h2><p>下面是我定义的一个专用于IMDb影评情感分析的二分类模型，包装在一个函数中。输入训练集、测试集及其标签，设定好参数就可以运行、训练。可以选择是否保存模型到本地。最后函数返回训练好的模型。</p><p>这个二分类模型中，输入是长度为80的整数列表（maxlen=80），代表着80个不同的单词构成的一句话。</p><p>如果有影评不够80个词，就在影评前面加足够的0，直到这条影评达到80个词为止。如果影评单词量大于80个，便截取前面的80个词。</p><p>每个整数都代表一个单词表中的单词。当然单词表的大小是固定的（num_words=10000个单词），如果出现不在单词表中的单词，固定将其编码成2，表示UNKNOWN（这条设置不在下面的代码中，属于数据预处理）。</p><p>第一层是Embedding层，负责将一句话中的每个单词映射成固定维度的词向量；</p><p>注意，每个单词（在这里是每个整数）都会变成固定维度（embedding_dim=128）的向量，因此每条影评从Embedding层输出后，都会变成80*128的矩阵。</p><p>第二层是LSTM层。如果你看了理论部分的叙述，就知道LSTM层中无论是隐状态$\boldsymbol{c}$、$\boldsymbol{h}$还是三个门$\boldsymbol{f}$、$\boldsymbol{i}$、$\boldsymbol{o}$，他们的维度都是$\boldsymbol{D}$。这个$\boldsymbol{D}$的大小就需要我们用参数<code>lstm_dim=32</code>来定义。这个参数越大，代表LSTM层的参数越多、泛化能力越强，也更难训练、更容易过拟合。</p><p>第三层是单个神经元的sigmoid层，在这里就直接转换成概率并分类了。</p><pre><code class="lang-python">def train_lstm(x_train, y_train, x_test, y_test,                num_words=10000,                maxlen=80,                embedding_dim=128,                lstm_dim=32,                batch_size=32,                epochs=10):    # 接收一个含有 100 个整数的序列，每个整数在 1 到 20000 之间    inputs = Input(shape=(maxlen,), dtype=&#39;int32&#39;, name=&#39;main_input&#39;)    # Embedding 层将输入序列编码为一个稠密向量的序列，    # 每个向量维度为 512。    x = Embedding(input_dim=num_words,                   input_length=maxlen,                   output_dim=embedding_dim,                   name=&#39;embedding&#39;)(inputs)    # LSTM 层把向量序列转换成单个向量，    # 它包含整个序列的上下文信息    lstm_output = LSTM(lstm_dim, name=&#39;lstm&#39;)(x)    # 插入辅助损失，    #使得即使在模型主损失很高的情况下，LSTM 层和 Embedding 层都能被平稳地训练    outputs = Dense(1, activation=&#39;sigmoid&#39;, name=&#39;output&#39;)(lstm_output)    model = Model(inputs=inputs, outputs=outputs)    model.compile(optimizer=&#39;adam&#39;,            loss=&#39;binary_crossentropy&#39;,            metrics=[&#39;accuracy&#39;])    model.fit(x_train, y_train,        batch_size=batch_size,        epochs=epochs,        validation_data=(x_test, y_test,))    score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)    print(&#39;Test score:&#39;, score)    print(&#39;Test accuracy:&#39;, acc)    # model.save(&quot;lstm_imdb.h5&quot;)    return model</code></pre><h2 id="三、LSTM返回所有时间步的hidden-state向量"><a href="#三、LSTM返回所有时间步的hidden-state向量" class="headerlink" title="三、LSTM返回所有时间步的hidden state向量"></a>三、LSTM返回所有时间步的hidden state向量</h2><p>数据经过LSTM层，输出的是最后一个时间步得到的output向量（即$\boldsymbol{h}_{finally}$），维度为$\boldsymbol{D}$。</p><p>其实LSTM能够在每个时间步都输出output（即$\boldsymbol{h}_{t}$），只不过我们把这些没到时间的半成品output选择性忽略了。</p><p>如果你想要堆叠LSTM层，也就是LSTM层下面还有LSTM，或者你<strong>需要所有时间步的</strong>$\boldsymbol{h}_{t}$，那么你可以在训练的时候把<code>return_sequences=True</code>写进LSTM参数之中。</p><p><img src="/2020/06/20/【学习笔记】LSTM网络结构简介与对应的keras实现/堆叠rnn.png" srcset="/img/loading.gif" alt></p><p>下面让我们来比较一下<code>return_sequences</code>参数开启之后输出值的变化。</p><h3 id="return-sequences-False"><a href="#return-sequences-False" class="headerlink" title="return_sequences=False"></a>return_sequences=False</h3><p>首先固定随机数种子。</p><pre><code class="lang-python">np.random.seed(0)tf.random.set_seed(0)</code></pre><p>然后构建输入Input向量和LSTM层，此时LSTM层使用默认参数<code>return_sequences=False</code>。</p><pre><code class="lang-python">input1 = Input(shape=(3,1)) # 输入是三维向量lstm1 = LSTM(1)(input1) # 内部hidden和cell的维度为1model = Model(inputs=input1, outputs=lstm1)</code></pre><p>构造一批输入，包括6个句子，每个句子三个单词，然后输入LSTM，查看LSTM层的输出。</p><pre><code class="lang-python">data = np.array([[0.1, 0.2, 0.3],                    [0.3, 0.2, 0.1],                    [0.2, 0.6, 0.3],                    [0.8, 0.2, 0.3],                    [0.3, 0.5, 0.1],                    [0.2, 0.6, 0.2]])print(model.predict(data))</code></pre><p>此时输出为：</p><pre><code class="lang-python">[[0.00844267] [0.00617958] [0.01279002] [0.01231858] [0.009055  ] [0.01108878]]Process finished with exit code 0</code></pre><h3 id="return-sequences-True"><a href="#return-sequences-True" class="headerlink" title="return_sequences=True"></a>return_sequences=True</h3><p>然后打开<code>return_sequences</code>的开关</p><pre><code class="lang-python">lstm1 = LSTM(1, return_sequences=True)(input1)</code></pre><p>此时的输出为：</p><pre><code>[[[0.00190693]  [0.00490441]  [0.00844267]] #  [[0.0055262 ]  [0.00704476]  [0.00617958]] # [[0.00374958]  [0.01259477]  [0.01279002]] # [[0.01337298]  [0.01142679]  [0.01231858]] # [[0.0055262 ]  [0.01206062]  [0.009055  ]] # [[0.00374958]  [0.01259477]  [0.01108878]]] #Process finished with exit code 0</code></pre><p>此为输出所有时间步的hidden state。鉴于一共6个测试输入，每个输入有3个feature，所以时间步也就三步。LSTM的输出结果从6个hidden state变成了6*3个hidden state。</p><h3 id="return-state-True"><a href="#return-state-True" class="headerlink" title="return_state=True"></a>return_state=True</h3><p>我们再来看另一个参数，这个参数能够控制LSTM输出cell state。</p><pre><code class="lang-python">lstm1 = LSTM(1, return_state=True)(input1)</code></pre><pre><code>[array([[0.00844267],       [0.00617958],       [0.01279002],       [0.01231858],       [0.009055  ],       [0.01108878]], dtype=float32), array([[0.00844267],       [0.00617958],       [0.01279002],       [0.01231858],       [0.009055  ],       [0.01108878]], dtype=float32), array([[0.01655067],       [0.01227413],       [0.02506882],       [0.02414548],       [0.01798305],       [0.02187706]], dtype=float32)]Process finished with exit code 0</code></pre><p>开启<code>return_state=True</code>之后，LSTM返回3个array，第一个array和第二个array一样，都是hidden state，和默认返回的一样。第三个array就是最后一个时间步的cell state。</p><h3 id="return-state-True-return-sequences-True"><a href="#return-state-True-return-sequences-True" class="headerlink" title="return_state=True, return_sequences=True"></a>return_state=True, return_sequences=True</h3><p>如果两个开关都打开，则结果变成</p><pre><code class="lang-python">lstm1 = LSTM(1, return_state=True, return_sequences=True)(input1)</code></pre><pre><code>[array([[[0.00190693],        [0.00490441],        [0.00844267]],       [[0.0055262 ],        [0.00704476],        [0.00617958]],       [[0.00374958],        [0.01259477],        [0.01279002]],       [[0.01337298],        [0.01142679],        [0.01231858]],       [[0.0055262 ],        [0.01206062],        [0.009055  ]],       [[0.00374958],        [0.01259477],        [0.01108878]]], dtype=float32), array([[0.00844267],       [0.00617958],       [0.01279002],       [0.01231858],       [0.009055  ],       [0.01108878]], dtype=float32), array([[0.01655067],       [0.01227413],       [0.02506882],       [0.02414548],       [0.01798305],       [0.02187706]], dtype=float32)]Process finished with exit code 0</code></pre><p>还是返回三个array，第一个是所有时间步的hidden state，这是开启<code>return_sequences=True</code>的效果；第二个则是原本LSTM的输出hidden state；第三个是开启<code>return_state=True</code>的效果，返回最后一个时间步的cell state</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;从理论和代码两个层面介绍了LSTM网络。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="RNN" scheme="https://superlova.github.io/tags/RNN/"/>
    
      <category term="LSTM" scheme="https://superlova.github.io/tags/LSTM/"/>
    
      <category term="Keras" scheme="https://superlova.github.io/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读笔记】Deep Text Classification Can be Fooled</title>
    <link href="https://superlova.github.io/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/"/>
    <id>https://superlova.github.io/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/</id>
    <published>2020-06-20T01:20:42.000Z</published>
    <updated>2020-06-23T12:41:07.656Z</updated>
    
    <content type="html"><![CDATA[<p>国内人民大学的一篇论文，被IJCAI-2018接收。主要研究文本领域的对抗样本生成，被测模型是文本分类领域的模型。<br><a id="more"></a></p><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>当前的对抗样本生成领域集中在图像的扰动和生成，文本领域少有涉及。</p><p><strong>文本对抗样本应当满足</strong>：</p><ul><li>使模型分类错误</li><li>与原样本相比，扰动难以察觉</li><li>实用性，即文本含义不发生改变</li></ul><p><strong>本文采用的生成思路</strong>：</p><ul><li>三种扰动策略：插入、修改和删除</li><li>自然语言水印</li><li>白盒+黑盒</li></ul><blockquote><p>自然语言水印（Natural Language Watermarking）是通过修改文本元素（如线条，文字或字符）的外观，或更改文本格式或字体（例如，通过-文字中的单词和字母间距）来嵌入信息的技术。</p></blockquote><h2 id="2-目标模型和数据集"><a href="#2-目标模型和数据集" class="headerlink" title="2. 目标模型和数据集"></a>2. 目标模型和数据集</h2><p><strong>字符级模型</strong>：</p><ul><li>以字母为单位，编码长度为字母表大小（26+空白符个数）</li><li>结构：6conv + 3fc</li><li>数据集：DBPedia，包含14个类别，56000训练、70000测试</li></ul><p><strong>单词级模型</strong>：</p><ul><li>以单词为单位，编码长度为单词表大小（数万到数十万）</li><li>结构：embedding + conv + maxpooling + fc(带dropout) + softmax</li><li>数据集：影评MR、产品评价CR、产品意见MPQA，都是二分类</li></ul><h2 id="3-白盒攻击"><a href="#3-白盒攻击" class="headerlink" title="3. 白盒攻击"></a>3. 白盒攻击</h2><p>所谓白盒攻击，就是在已知被测模型内部信息的情况下开展的攻击。由于已知信息较多，所以攻击起来也比较容易。</p><p>白盒攻击生成对抗样本的思想在图像领域用的比较多。比如利用网络参数和损失函数进行指导攻击过程的FGSM算法。</p><p>本文采用的白盒攻击手段，也是利用模型内部的参数和网络训练时的损失函数作为引导，但是不会直接生成对抗样本，而是<strong>首先识别对分类有重大贡献的文本项</strong>。</p><h3 id="HTP：Hot-Training-Phrases"><a href="#HTP：Hot-Training-Phrases" class="headerlink" title="HTP：Hot Training Phrases"></a>HTP：Hot Training Phrases</h3><p>以字符级模型为例，识别过程如下：</p><ol><li>输入训练样本x，计算cost gradients ∇x C(M, x, label)；</li><li>每个样本x选取令gradient最大的前50个字符定义为<strong>hot character</strong>；</li><li>包含3个及以上hot characters的单词定义为<strong>hot word</strong>；</li><li>相邻的hot words定义为<strong>hot phrase</strong>；</li><li>不同类别有不同的hot phrase，代表着该类对分类贡献最大的词组；</li><li>每个类别中最常出现的hot phrase定义为<strong>Hot Training Phrases (HTPs)</strong>。</li></ol><p>下图是DBPedia的Building类别文本数据集中的HTP词汇排名前10。<br><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/2020-06-23-20-11-30.png" srcset="/img/loading.gif" alt></p><h3 id="HSP：Hot-Sample-Phrases"><a href="#HSP：Hot-Sample-Phrases" class="headerlink" title="HSP：Hot Sample Phrases"></a>HSP：Hot Sample Phrases</h3><p>给定样本x，识别x中的hot phrase作为操作位置，该位置的单词或词组就定义为<strong>Hot Sample Phrases (HSPs)</strong>。</p><h3 id="Attacking-Character-level-DNN"><a href="#Attacking-Character-level-DNN" class="headerlink" title="Attacking Character-level DNN"></a>Attacking Character-level DNN</h3><h4 id="Insertion"><a href="#Insertion" class="headerlink" title="Insertion"></a><strong>Insertion</strong></h4><p>通过在样本的HSP位置插入以下内容实现变异操作：</p><ul><li>HTP；</li><li>可有可无的事实（文本水印算法生成）；</li><li>不伤害文本主语义的伪造事实（文本水印算法生成）。</li></ul><p>下面是三个通过插入红色文本造成标签改变的例子。<br><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/2020-06-23-20-11-43.png" srcset="/img/loading.gif" alt="Figure2"><br><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/2020-06-23-20-11-56.png" srcset="/img/loading.gif" alt="Figure3"><br><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Figure4.png" srcset="/img/loading.gif" alt="Figure4"></p><h4 id="Modification"><a href="#Modification" class="headerlink" title="Modification"></a><strong>Modification</strong></h4><p>对HSP稍加操作，比如typo-based watermarking technique（基于错别字的水印技术）：</p><ul><li>(1)替换以常见的错误拼写（需要有错别字语料库）</li><li>(2)替换以外观相似的字符</li></ul><p>下图是替换操作后生成对抗样本的一个例子。<br><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Figure5.png" srcset="/img/loading.gif" alt></p><p>下图是将film替换成flim后模型内部损失函数梯度的改变。<br><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Figure6.png" srcset="/img/loading.gif" alt></p><h4 id="Removal"><a href="#Removal" class="headerlink" title="Removal"></a><strong>Removal</strong></h4><p>删除HSP可降低模型对样本的confidence。只能删除HSPs中起辅助作用的词，要不然会改变本来的含义。</p><p>下面是通过删除来导致置信程度下降的例子。<br><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Figure7.png" srcset="/img/loading.gif" alt></p><h4 id="Combination"><a href="#Combination" class="headerlink" title="Combination"></a><strong>Combination</strong></h4><p>组合上述三种手法。</p><p><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Figure8.png" srcset="/img/loading.gif" alt></p><h3 id="Attacking-Word-level-DNN"><a href="#Attacking-Word-level-DNN" class="headerlink" title="Attacking Word-level DNN"></a>Attacking Word-level DNN</h3><p>单词级模型也是同理，不仅如此，甚至省了hot-character这一步。下面是几个例子。</p><p><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Figure9.png" srcset="/img/loading.gif" alt></p><p><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Figure10.png" srcset="/img/loading.gif" alt></p><h2 id="4-黑盒攻击"><a href="#4-黑盒攻击" class="headerlink" title="4. 黑盒攻击"></a>4. 黑盒攻击</h2><p>黑盒攻击显然不能通过比较Cost Gradient的方式确定HTP和HSP了。但是我们可以采用其他方法确定HTP和HSP。</p><p>具体地，我们通过生成一些测试样本来探测目标模型，判断哪些是Hot Phrases。</p><p>生成方法：</p><ul><li>用若干空格逐个代替单词（空格个数与单词长度相同）</li><li>将测试样本的分类结果与种子进行比较</li><li>偏差越大，相应单词对正确分类的重要性就越大</li><li>带来最大偏差的单词被标识为种子样本的HSP</li></ul><p><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Figure11.png" srcset="/img/loading.gif" alt></p><p>下面是黑盒攻击确定的HTP之后进行攻击的例子：</p><p><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Figure12.png" srcset="/img/loading.gif" alt></p><h2 id="5-Evaluation"><a href="#5-Evaluation" class="headerlink" title="5. Evaluation"></a>5. Evaluation</h2><h3 id="Q1-Can-our-method-perform-effective-source-target-misclassification-attack"><a href="#Q1-Can-our-method-perform-effective-source-target-misclassification-attack" class="headerlink" title="Q1: Can our method perform effective source/target misclassification attack?"></a>Q1: Can our method perform effective source/target misclassification attack?</h3><p>这个问题是问本方法能不能对模型实行定向的攻击，即“指哪打哪”，无论哪个类别的样本都能通过适当修改，突变成指定类别。</p><p><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Fig13.png" srcset="/img/loading.gif" alt></p><p><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Fig14.png" srcset="/img/loading.gif" alt></p><p>通过上表可知，source栏是源类别。target栏是目标类别，No栏是样本编号。本来都是类别source中定义的样本，经过右边三栏的突变方法，最终都以较高置信度被模型分类成了target栏中的类别。可以证明此方法确实能实现定向突变、定向攻击。</p><h3 id="Q2-Can-the-adversarial-samples-avoid-being-distinguished-by-human-observers-and-still-keep-the-utility"><a href="#Q2-Can-the-adversarial-samples-avoid-being-distinguished-by-human-observers-and-still-keep-the-utility" class="headerlink" title="Q2: Can the adversarial samples avoid being distinguished by human observers and still keep the utility?"></a>Q2: Can the adversarial samples avoid being distinguished by human observers and still keep the utility?</h3><p>这个问题是问对抗样本是不是能够避免被人类识别。毕竟文本突变还是很容易被人类识别的。</p><p>本论文是这么设计实验的：</p><ul><li>找23名学生，每个人都提供了20个文本样本，其中一半带有扰动，对每个样本进行手动分类</li><li>如果他们认为样品是人为修改的，则要求他们查明修改的位置</li><li>原样本准确率：94.2%</li><li>扰动样本准确率：94.8%</li><li>总共生成594个变化，有240个被受试者标记为已修改的位置，其中12个正确。准确率为12/240 = 5.0％，召回率为12/594 = 2.0％</li></ul><p>可以看到虽然样本数比较小，但是结果还是很显著的，在那23个同学都比较靠谱的前提下，该算法还是能够保证生成的文本与原文本差距不大的。</p><h3 id="Q3-Is-our-method-efficient-enough"><a href="#Q3-Is-our-method-efficient-enough" class="headerlink" title="Q3: Is our method efficient enough?"></a>Q3: Is our method efficient enough?</h3><p>算法效率其实不是很重要，毕竟在实践中，攻击者往往愿意花费更多时间来制作理想的对抗性样本。</p><p>白盒攻击（计算梯度、确定HTP），总共116小时，平均每类8.29小时；<br>黑盒攻击（生成样本、确定HTP），总共107小时，平均每类7.63小时。</p><h3 id="Q4-White-box-and-black-box-which-is-more-powerful"><a href="#Q4-White-box-and-black-box-which-is-more-powerful" class="headerlink" title="Q4: White-box and black-box, which is more powerful?"></a>Q4: White-box and black-box, which is more powerful?</h3><p>两种方式都有效并且彼此互补。</p><p>下图分别是黑盒和白盒生成的HTP比较，可以看到都是比较类似的。</p><p><img src="/2020/06/20/【论文阅读笔记】Deep-Text-Classification-Can-be-Fooled/Fig15.png" srcset="/img/loading.gif" alt></p><h2 id="6-读后感"><a href="#6-读后感" class="headerlink" title="6. 读后感"></a>6. 读后感</h2><p>其实本篇文章的思想并不复杂，核心是确定一段文本的HTP和HSP。所谓HTP可以认为是，模型一看到这种词就相信这句话是该类别的了。那如果把类别1的句子x中的HSP给替换成类别2的HTP，的确可能让模型以为句子x是类别2的句子了。</p><p>所以延伸出来一个方向，那就是确定HSP和HTP的方法上。对于白盒攻击，还是查看内部的信息，然后计算梯度，这是一种比较传统的方法。对于黑盒攻击，则是遍历所有可能删除的单词，从结果上来看，比较这些删除单词的重要程度。</p><p>所以说有没有其他方法能够衡量单词的重要程度？这是一个值得研究的方向。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;国内人民大学的一篇论文，被IJCAI-2018接收。主要研究文本领域的对抗样本生成，被测模型是文本分类领域的模型。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://superlova.github.io/categories/paper/"/>
    
    
      <category term="NLP" scheme="https://superlova.github.io/tags/NLP/"/>
    
      <category term="testing" scheme="https://superlova.github.io/tags/testing/"/>
    
      <category term="DNN" scheme="https://superlova.github.io/tags/DNN/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】机器学习中处理文本数据的常用方法</title>
    <link href="https://superlova.github.io/2020/06/09/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%A4%84%E7%90%86%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>https://superlova.github.io/2020/06/09/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%A4%84%E7%90%86%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95/</id>
    <published>2020-06-09T14:52:34.000Z</published>
    <updated>2020-06-10T00:44:12.566Z</updated>
    
    <content type="html"><![CDATA[<p>总结词袋模型、Tf-idf等文本特征提取方法<br><a id="more"></a></p><h2 id="一、词袋模型"><a href="#一、词袋模型" class="headerlink" title="一、词袋模型"></a>一、词袋模型</h2><p>文本数据通常被表示为由字符组成的字符串。我们需要先处理数据，然后才能对其应用机器学习算法。</p><p>在文本分析的语境中，数据集通常被称为语料库（corpus），每个由单个文本表示的数据点被称为文档（document）。</p><p>最简单的处理方法，是<strong>只计算语料库中每个单词在每个文本中的出现频次</strong>。这种文本处理模型称之为<strong>词袋模型</strong>。</p><p>不考虑词语出现的顺序，每个出现过的词汇单独作为一列特征，这些不重复的特征词汇集合为词表。</p><p>每一个文本都可以在很长的词表上统计出一个很多列的特征向量。如果每个文本都出现的词汇，一般被标记为<strong>停用词</strong>不计入特征向量。</p><p>为了搞清楚词袋模型，也就是<code>CountVectorizer</code>到底做了什么，我们执行以下代码：</p><pre><code class="lang-python">bards_words =[&quot;The fool doth think he is wise,&quot;,    &quot;but the wise man knows himself to be a fool&quot;]</code></pre><p>我们导入 CountVectorizer 并将其实例化，然后对 bards_words 进行拟合，如下所示：</p><pre><code class="lang-python">from sklearn.feature_extraction.text import CountVectorizervect = CountVectorizer()vect.fit(bards_words)</code></pre><p>拟合 CountVectorizer 包括训练数据的分词与词表的构建，我们可以通过 vocabulary_ 属性来访问词表：</p><pre><code class="lang-python">print(&quot;Vocabulary size: {}&quot;.format(len(vect.vocabulary_)))print(&quot;Vocabulary content:\n {}&quot;.format(vect.vocabulary_))#------------------#Vocabulary size: 13Vocabulary content:{&#39;the&#39;: 9, &#39;himself&#39;: 5, &#39;wise&#39;: 12, &#39;he&#39;: 4, &#39;doth&#39;: 2, &#39;to&#39;: 11, &#39;knows&#39;: 7,&#39;man&#39;: 8, &#39;fool&#39;: 3, &#39;is&#39;: 6, &#39;be&#39;: 0,  &#39;think&#39;: 10, &#39;but&#39;: 1}</code></pre><p>词表共包含 13 个词，从 “be” 到 “wise”。<br>我们可以调用 transform 方法来创建训练数据的词袋表示：</p><pre><code class="lang-python">bag_of_words = vect.transform(bards_words)print(&quot;bag_of_words: {}&quot;.format(repr(bag_of_words)))#--------------------#bag_of_words: &lt;2x13 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39;with 16 stored elements in Compressed Sparse Row format&gt;</code></pre><p>词袋表示保存在一个 SciPy 稀疏矩阵中，这种数据格式只保存非零元素。这个矩阵的形状为 2×13，每行对应于两个数据点之一，每个特征对应于词表中的一个单词。要想查看稀疏矩阵的实际内容，可以使用 toarray 方法将其转换为“密集的”NumPy 数组（保存所有 0 元素）：</p><pre><code class="lang-python">print(&quot;Dense representation of bag_of_words:\n{}&quot;.format(    bag_of_words.toarray()))#---------------------#Dense representation of bag_of_words:[[0 0 1 1 1 0 1 0 0 1 1 0 1][1 1 0 1 0 1 0 1 1 1 0 1 1]]</code></pre><p>删除没有信息量的单词，除了使用<code>min_df</code>参数设定词例至少需要在多少个文档中出现过之外，还可以通过添加停用词的方法。</p><h2 id="二、用tf-idf缩放数据"><a href="#二、用tf-idf缩放数据" class="headerlink" title="二、用tf-idf缩放数据"></a>二、用tf-idf缩放数据</h2><p>词频 - 逆向文档频率（term frequency–inverse document frequency，tf-idf）方法，对在某个特定文档中经常出现的术语给予很高的权重，但对在语料库的许多文档中都经常出现的术语给予的权重却不高。</p><p>scikit-learn 在两个类中实现了 tf-idf 方法：TfidfTransformer 和 TfidfVectorizer，前者接受 CountVectorizer 生成的稀疏矩阵并将其变换，后者接受文本数据并完成词袋特征提取与 tf-idf 变换。</p><p>单词w在文档d中的tf-idf分数为：</p><p><img src="/2020/06/09/【学习笔记】机器学习中处理文本数据的常用方法/2020-06-09-23-20-54.png" srcset="/img/loading.gif" alt></p><p>式中，tf为词频，Term Frequency, 表示一个词在一个文档中的出现频率。该频率最后要除以该文档的长度，用以归一化。</p><p>式中，$N$为总文档数，$N_w$为带有单词$w$的文档数。由于分子比分母大，所以该 $\log$ 值必不可能小于零。</p><pre><code class="lang-python">from sklearn.feature_extraction.text import TfidfVectorizercorpus=[&quot;I come to China to travel&quot;,&quot;This is a car polupar in China&quot;,&quot;I love tea and Apple &quot;,&quot;The work is to write some papers in science&quot;]tfidf = TfidfVectorizer()vector = tfidf.fit_transform(corpus)print(vector)#---------------#(0, 16)    0.4424621378947393(0, 3)    0.348842231691988(0, 15)    0.697684463383976(0, 4)    0.4424621378947393(1, 5)    0.3574550433419527(1, 9)    0.45338639737285463(1, 2)    0.45338639737285463(1, 6)    0.3574550433419527(1, 14)    0.45338639737285463(1, 3)    0.3574550433419527(2, 1)    0.5(2, 0)    0.5(2, 12)    0.5(2, 7)    0.5(3, 10)    0.3565798233381452(3, 8)    0.3565798233381452(3, 11)    0.3565798233381452(3, 18)    0.3565798233381452(3, 17)    0.3565798233381452(3, 13)    0.3565798233381452(3, 5)    0.2811316284405006(3, 6)    0.2811316284405006(3, 15)    0.2811316284405006</code></pre><p>返回值什么意思呢？(0, 16)代表第0个文档，第一个单词在单词表（词袋）中的位置是第16个，该单词的tf-idf值为0.44246213；第二个单词在词袋中第3个位置……</p><p>显然这是个经过压缩的系数矩阵，每一行的元组表明该元素在稀疏矩阵中的位置，其值为右边的tf-idf值，代表一个单词。可以通过<code>.toarray()</code>方法令其恢复到系数矩阵状态。</p><pre><code class="lang-python">print(vector.toarray().shape)print(len(vector.toarray()))print(type(vector.toarray()))print(vector.toarray())#-----------------------------#(4, 19)4&lt;class &#39;numpy.ndarray&#39;&gt;[[0. 0. 0. 0.34884223 0.44246214 0.  0. 0. 0. 0. 0. 0.  0. 0. 0. 0.69768446 0.44246214 0.  0. ] [0. 0. 0.4533864  0.35745504 0. 0.35745504  0.35745504 0. 0. 0.4533864  0. 0.  0. 0. 0.4533864  0. 0. 0.  0. ] [0.5 0.5 0. 0. 0. 0.  0. 0.5 0. 0. 0. 0.  0.5 0. 0. 0. 0. 0.  0. ] [0. 0. 0. 0. 0. 0.28113163  0.28113163 0. 0.35657982 0. 0.35657982 0.35657982  0. 0.35657982 0. 0.28113163 0. 0.35657982  0.35657982]]</code></pre><h2 id="三、多元词袋"><a href="#三、多元词袋" class="headerlink" title="三、多元词袋"></a>三、多元词袋</h2><p>词袋模型将一段文档拆分成单词后，忽略了单词的上下文可能对文档的含义造成影响。</p><p>因此可以通过</p><h2 id="四、英语的词干提取与词形还原"><a href="#四、英语的词干提取与词形还原" class="headerlink" title="四、英语的词干提取与词形还原"></a>四、英语的词干提取与词形还原</h2><h2 id="五、中文的分词"><a href="#五、中文的分词" class="headerlink" title="五、中文的分词"></a>五、中文的分词</h2><h2 id="六、主题建模与文档聚类"><a href="#六、主题建模与文档聚类" class="headerlink" title="六、主题建模与文档聚类"></a>六、主题建模与文档聚类</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;总结词袋模型、Tf-idf等文本特征提取方法&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
      <category term="IMDb" scheme="https://superlova.github.io/tags/IMDb/"/>
    
  </entry>
  
  <entry>
    <title>【经验分享】IMDb数据集的预处理</title>
    <link href="https://superlova.github.io/2020/06/09/%E3%80%90%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E3%80%91IMDb%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86/"/>
    <id>https://superlova.github.io/2020/06/09/%E3%80%90%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E3%80%91IMDb%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86/</id>
    <published>2020-06-09T14:48:03.000Z</published>
    <updated>2020-06-10T07:01:36.220Z</updated>
    
    <content type="html"><![CDATA[<p>IMDb从官网下载与从keras直接调用的处理方法是不同的。<br><a id="more"></a></p><h2 id="一、IMDb数据集的处理方法"><a href="#一、IMDb数据集的处理方法" class="headerlink" title="一、IMDb数据集的处理方法"></a>一、IMDb数据集的处理方法</h2><h3 id="1-官网下载法"><a href="#1-官网下载法" class="headerlink" title="1. 官网下载法"></a>1. 官网下载法</h3><pre><code class="lang-python">import pandas as pdimport numpy as npfrom sklearn.datasets import load_files</code></pre><pre><code class="lang-shell">!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz!tar -zxvf aclImdb_v1.tar.gz!ls</code></pre><p>对于 v1.0 版数据，其训练集大小是 75 000，而不是 25 000，因为其中还包含 50 000 个用于无监督学习的无标签文档。</p><p>在进行后续操作之前，建议先将这 50 000 个无标签文档从训练集中剔除。</p><pre><code class="lang-bash">!mkdir aclImdb/train_unlabel!mv aclImdb/train/unsupBow.feat aclImdb/train_unlabel!mv aclImdb/train/urls_unsup.txt aclImdb/train_unlabel!mv aclImdb/train/unsup aclImdb/train_unlabel</code></pre><pre><code class="lang-python">reviews_train = load_files(&quot;aclImdb/train/&quot;)text_train, y_train = reviews_train.data, reviews_train.targetreviews_test = load_files(&quot;aclImdb/test/&quot;)text_test, y_test = reviews_test.data, reviews_test.target# 删掉HTML换行符text_train = [doc.replace(b&quot;&lt;br /&gt;&quot;, b&quot; &quot;) for doc in text_train]text_test = [doc.replace(b&quot;&lt;br /&gt;&quot;, b&quot; &quot;) for doc in text_test]print(&quot;type of text_train: {}&quot;.format(type(text_train))) # 查看训练集类型：listprint(&quot;length of text_train: {}&quot;.format(len(text_train))) # 查看训练集大小print(&quot;text_train[1]:\n{}&quot;.format(text_train[1])) # 查看第二段文本print(&quot;Samples per class (training): {}&quot;.format(np.bincount(y_train))) # 查看数据集是否均等#------------------------------------------------#type of text_train: &lt;class &#39;list&#39;&gt;length of text_train: 25000text_train[1]:b&#39;Words can\&#39;t describe how bad this movie is. I can\&#39;t explain it by writing only. You have too see it for yourself to get at grip of how horrible a movie really can be. Not that I recommend you to do that. There are so many clich\xc3\xa9s, mistakes (and all other negative things you can imagine) here that will just make you cry. To start with the technical first, there are a LOT of mistakes regarding the airplane. I won\&#39;t list them here, but just mention the coloring of the plane. They didn\&#39;t even manage to show an airliner in the colors of a fictional airline, but instead used a 747 painted in the original Boeing livery. Very bad. The plot is stupid and has been done many times before, only much, much better. There are so many ridiculous moments here that i lost count of it really early. Also, I was on the bad guys\&#39; side all the time in the movie, because the good guys were so stupid. &quot;Executive Decision&quot; should without a doubt be you\&#39;re choice over this one, even the &quot;Turbulence&quot;-movies are better. In fact, every other movie in the world is better than this one.&#39;Samples per class (training): [12500 12500]</code></pre><p>采用词袋模型整理数据</p><pre><code class="lang-python">vect = CountVectorizer().fit(text_train)X_train = vect.transform(text_train)print(&quot;X_train:\n{}&quot;.format(repr(X_train)))#---------------------------#X_train:&lt;25000x74849 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39;with 3431196 stored elements in Compressed Sparse Row format&gt;</code></pre><p>X_train 是训练数据的词袋表示，其形状为 25 000×74 849，这表示词表中包含 74 849 个元素。数据被保存为 SciPy 稀疏矩阵。</p><p>访问词表的另一种方法是使用向量器（vectorizer）的 get_feature_name 方法，它将返回一个列表，每个元素对应于一个特征：</p><p>feature_names = vect.get_feature_names()<br>print(“Number of features: {}”.format(len(feature_names)))<br>print(“First 20 features:\n{}”.format(feature_names[:20]))<br>print(“Features 20010 to 20030:\n{}”.format(feature_names[20010:20030]))<br>print(“Every 2000th feature:\n{}”.format(feature_names[::2000]))</p><p>Number of features: 74849<br>First 20 features:<br>[‘00’, ‘000’, ‘0000000000001’, ‘00001’, ‘00015’, ‘000s’, ‘001’, ‘003830’,<br>‘006’, ‘007’, ‘0079’, ‘0080’, ‘0083’, ‘0093638’, ‘00am’, ‘00pm’, ‘00s’,’01’, ‘01pm’, ‘02’]<br>Features 20010 to 20030:<br>[‘dratted’, ‘draub’, ‘draught’, ‘draughts’, ‘draughtswoman’, ‘draw’, ‘drawback’,<br>‘drawbacks’, ‘drawer’, ‘drawers’, ‘drawing’, ‘drawings’, ‘drawl’,<br>‘drawled’, ‘drawling’, ‘drawn’, ‘draws’, ‘draza’, ‘dre’, ‘drea’]<br>Every 2000th feature:<br>[‘00’, ‘aesir’, ‘aquarian’, ‘barking’, ‘blustering’, ‘beête’, ‘chicanery’,<br>‘condensing’, ‘cunning’, ‘detox’, ‘draper’, ‘enshrined’, ‘favorit’, ‘freezer’,<br>‘goldman’, ‘hasan’, ‘huitieme’, ‘intelligible’, ‘kantrowitz’, ‘lawful’,<br>‘maars’, ‘megalunged’, ‘mostey’, ‘norrland’, ‘padilla’, ‘pincher’,<br>‘promisingly’, ‘receptionist’, ‘rivals’, ‘schnaas’, ‘shunning’, ‘sparse’,<br>‘subset’, ‘temptations’, ‘treatises’, ‘unproven’, ‘walkman’, ‘xylophonist’]</p><p>词表的前 10 个元素都是数字。所有这些数字都出现在评论中的某处，因此被提取为单词。</p><h3 id="2-使用keras自带的IMDb数据集"><a href="#2-使用keras自带的IMDb数据集" class="headerlink" title="2. 使用keras自带的IMDb数据集"></a>2. 使用keras自带的IMDb数据集</h3><pre><code class="lang-python">from tensorflow.keras.datasets import imdb(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000) # 仅保留训练数据中前10000个最经常出现的单词，低频单词被舍弃print(&#39;len of X_train: {}&#39;.format(len(X_train)))print(&#39;shape of X_train: {}&#39;.format(X_train.shape))print(&#39;first of X_train: {}&#39;.format(X_train[0]))print(&#39;training sample per class: {}&#39;.format(np.bincount(y_train)))#-------------------#len of X_train: 25000shape of X_train: (25000,)first of X_train: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]training sample per class: [12500 12500]</code></pre><p>可以看到keras已经把IMDb数据集给提前整理过了。此处每条数据都是一个向量，每个数值代表一个单词。数值的大小代表了该单词在单词表中的位置。显然，每条数据向量的长度不一定相同。</p><p>为了方便处理，我们可以规定每条文档的长度为maxlen</p><pre><code class="lang-python">from tensorflow.keras.preprocessing import sequenceprint(&#39;Pad sequences (samples x time)&#39;)x_train = sequence.pad_sequences(x_train, maxlen=maxlen)x_test = sequence.pad_sequences(x_test, maxlen=maxlen)print(&#39;x_train shape:&#39;, x_train.shape)print(&#39;x_test shape:&#39;, x_test.shape)#-------------------#Pad sequences (samples x time)x_train shape: (25000, 80)x_test shape: (25000, 80)</code></pre><p>训练集中一共25000条文档，其中12500个正类，12500个负类。每个文档都是由80个数字组成的向量。测试集亦然。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;IMDb从官网下载与从keras直接调用的处理方法是不同的。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="record" scheme="https://superlova.github.io/categories/record/"/>
    
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="IMDb" scheme="https://superlova.github.io/tags/IMDb/"/>
    
      <category term="preprocessing" scheme="https://superlova.github.io/tags/preprocessing/"/>
    
  </entry>
  
  <entry>
    <title>【经验分享】停电了怎么办？Python获取Windows电源连接信息</title>
    <link href="https://superlova.github.io/2020/06/08/%E3%80%90%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E3%80%91%E5%81%9C%E7%94%B5%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9FPython%E8%8E%B7%E5%8F%96Windows%E7%94%B5%E6%BA%90%E8%BF%9E%E6%8E%A5%E4%BF%A1%E6%81%AF/"/>
    <id>https://superlova.github.io/2020/06/08/%E3%80%90%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E3%80%91%E5%81%9C%E7%94%B5%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9FPython%E8%8E%B7%E5%8F%96Windows%E7%94%B5%E6%BA%90%E8%BF%9E%E6%8E%A5%E4%BF%A1%E6%81%AF/</id>
    <published>2020-06-08T01:42:45.000Z</published>
    <updated>2020-06-08T02:15:04.116Z</updated>
    
    <content type="html"><![CDATA[<p>一旦停电，就令笔记本电脑发出响声、发送信息……。看似简单的功能，该如何利用Python实现呢？<br><a id="more"></a></p><p>采用笔记本电脑办公的好处是不必害怕突然停电。然而笔记本电脑不可能使用电池工作太久，此时必须尽快通知管理人员，恢复供电。</p><p>看似简单的功能，只需在Windows中注册一个HANDLE，负责接收电源适配器更改这一事件即可。但是本人没有Windows编程和系统编程的经验，只对Python熟悉。如何实现这一功能？</p><p>废话不多说，下面是代码。</p><pre><code class="lang-python">import win32conimport win32apiimport win32guiimport timefrom ctypes import POINTER, windll, Structure, cast, CFUNCTYPE, c_int, c_uint, c_void_p, c_boolfrom comtypes import GUIDfrom ctypes.wintypes import HANDLE, DWORDPBT_POWERSETTINGCHANGE = 0x8013GUID_CONSOLE_DISPLAY_STATE = &#39;{6FE69556-704A-47A0-8F24-C28D936FDA47}&#39;GUID_ACDC_POWER_SOURCE = &#39;{5D3E9A59-E9D5-4B00-A6BD-FF34FF516548}&#39;GUID_BATTERY_PERCENTAGE_REMAINING = &#39;{A7AD8041-B45A-4CAE-87A3-EECBB468A9E1}&#39;GUID_MONITOR_POWER_ON = &#39;{02731015-4510-4526-99E6-E5A17EBD1AEA}&#39;GUID_SYSTEM_AWAYMODE = &#39;{98A7F580-01F7-48AA-9C0F-44352C29E5C0}&#39;class POWERBROADCAST_SETTING(Structure):    _fields_ = [(&quot;PowerSetting&quot;, GUID),                (&quot;DataLength&quot;, DWORD),                (&quot;Data&quot;, DWORD)]def wndproc(hwnd, msg, wparam, lparam):    if msg == win32con.WM_POWERBROADCAST:        if wparam == win32con.PBT_APMPOWERSTATUSCHANGE:            print(&#39;Power status has changed&#39;)        if wparam == win32con.PBT_APMRESUMEAUTOMATIC:            print(&#39;System resume&#39;)        if wparam == win32con.PBT_APMRESUMESUSPEND:            print(&#39;System resume by user input&#39;)        if wparam == win32con.PBT_APMSUSPEND:            print(&#39;System suspend&#39;)        if wparam == PBT_POWERSETTINGCHANGE:            print(&#39;Power setting changed...&#39;)            settings = cast(lparam, POINTER(POWERBROADCAST_SETTING)).contents            power_setting = str(settings.PowerSetting)            data_length = settings.DataLength            data = settings.Data            if power_setting == GUID_CONSOLE_DISPLAY_STATE:                if data == 0: print(&#39;Display off&#39;)                if data == 1: print(&#39;Display on&#39;)                if data == 2: print(&#39;Display dimmed&#39;)            elif power_setting == GUID_ACDC_POWER_SOURCE:                if data == 0: print(&#39;AC power&#39;)                if data == 1:                    print(&#39;Battery power&#39;)                    #################################################                    playsound(&#39;alert.mp3&#39;) # 此处自定义你的操作                    #################################################                if data == 2: print(&#39;Short term power&#39;)            elif power_setting == GUID_BATTERY_PERCENTAGE_REMAINING:                print(&#39;battery remaining: %s&#39; % data)            elif power_setting == GUID_MONITOR_POWER_ON:                if data == 0: print(&#39;Monitor off&#39;)                if data == 1: print(&#39;Monitor on&#39;)            elif power_setting == GUID_SYSTEM_AWAYMODE:                if data == 0: print(&#39;Exiting away mode&#39;)                if data == 1: print(&#39;Entering away mode&#39;)            else:                print(&#39;unknown GUID&#39;)        return True    return Falseif __name__ == &quot;__main__&quot;:    print(&quot;*** STARTING ***&quot;)    hinst = win32api.GetModuleHandle(None)    wndclass = win32gui.WNDCLASS()    wndclass.hInstance = hinst    wndclass.lpszClassName = &quot;testWindowClass&quot;    CMPFUNC = CFUNCTYPE(c_bool, c_int, c_uint, c_uint, c_void_p)    wndproc_pointer = CMPFUNC(wndproc)    wndclass.lpfnWndProc = {win32con.WM_POWERBROADCAST : wndproc_pointer}    try:        myWindowClass = win32gui.RegisterClass(wndclass)        hwnd = win32gui.CreateWindowEx(win32con.WS_EX_LEFT,                                     myWindowClass,                                     &quot;testMsgWindow&quot;,                                     0,                                     0,                                     0,                                     win32con.CW_USEDEFAULT,                                     win32con.CW_USEDEFAULT,                                     0,                                     0,                                     hinst,                                     None)    except Exception as e:        print(&quot;Exception: %s&quot; % str(e))    if hwnd is None:        print(&quot;hwnd is none!&quot;)    else:        print(&quot;hwnd: %s&quot; % hwnd)    guids_info = {                    &#39;GUID_MONITOR_POWER_ON&#39; : GUID_MONITOR_POWER_ON,                    &#39;GUID_SYSTEM_AWAYMODE&#39; : GUID_SYSTEM_AWAYMODE,                    &#39;GUID_CONSOLE_DISPLAY_STATE&#39; : GUID_CONSOLE_DISPLAY_STATE,                    &#39;GUID_ACDC_POWER_SOURCE&#39; : GUID_ACDC_POWER_SOURCE,                    &#39;GUID_BATTERY_PERCENTAGE_REMAINING&#39; : GUID_BATTERY_PERCENTAGE_REMAINING                 }    for name, guid_info in guids_info.items():        result = windll.user32.RegisterPowerSettingNotification(HANDLE(hwnd), GUID(guid_info), DWORD(0))        print(&#39;registering&#39;, name)        print(&#39;result:&#39;, hex(result))        print(&#39;lastError:&#39;, win32api.GetLastError())        print()    print(&#39;\nEntering loop&#39;)    while True:        win32gui.PumpWaitingMessages()        time.sleep(1)</code></pre><p>COM: The Component Object Model 组件对象模型，是微软的一套软件组件的二进制接口标准。COM使得跨编程语言的进程间通信、动态对象创建成为可能。</p><p>COM实质上是一种语言无关的对象实现方式，这使其可以在创建环境不同的场合、甚至跨计算机的分布环境下被复用。COM允许复用这些对象，而不必知道对象内部是如何实现，因为组件实现者必须提供良好定义的接口从而屏蔽实现细节。通过引用计数，组件对象自己负责动态创建与销毁，从而屏蔽了不同编程语言之间的内存分配语义差异。</p><p>对于某些应用程序来说，COM已经部分被.NET框架取代。.NET Framework是新一代的Microsoft Windows应用程序开发平台。</p><p>COM是基于组件对象方式概念来设计的，在基础中，至少要让每个组件都可以支持二个功能：</p><p>查询组件中有哪些接口<br>让组件做自我生命管理，此概念的实践即为引用计数（Reference Counting）</p><p>GUID 是一个 128 位整数（16 字节），COM将其用于计算机和网络的唯一标识符。全局唯一标识符（英语：Globally Unique Identifier，缩写：GUID；发音为/ˈɡuːɪd/或/ˈɡwɪd/）是一种由算法生成的唯一标识，通常表示成32个16进制数字（0－9，A－F）组成的字符串，如：{21EC2020-3AEA-1069-A2DD-08002B30309D}，它实质上是一个128位长的二进制整数。</p><p>Windows操作系统使用GUID来标识COM对象中的类和界面。一个脚本可以不需知道DLL的位置和名字直接通过GUID来激活其中的类或对象。</p><p>参考：<a href="https://stackoverflow.com/questions/48720924/python-3-detect-monitor-power-state-in-windows" target="_blank" rel="noopener">https://stackoverflow.com/questions/48720924/python-3-detect-monitor-power-state-in-windows</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一旦停电，就令笔记本电脑发出响声、发送信息……。看似简单的功能，该如何利用Python实现呢？&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="record" scheme="https://superlova.github.io/categories/record/"/>
    
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="Windows" scheme="https://superlova.github.io/tags/Windows/"/>
    
      <category term="PowerOff" scheme="https://superlova.github.io/tags/PowerOff/"/>
    
  </entry>
  
  <entry>
    <title>【经验分享】TensorFlow模型训练和保存</title>
    <link href="https://superlova.github.io/2020/06/03/%E3%80%90%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E3%80%91TensorFlow%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%92%8C%E4%BF%9D%E5%AD%98/"/>
    <id>https://superlova.github.io/2020/06/03/%E3%80%90%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E3%80%91TensorFlow%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%92%8C%E4%BF%9D%E5%AD%98/</id>
    <published>2020-06-03T14:35:56.000Z</published>
    <updated>2020-06-03T14:40:27.579Z</updated>
    
    <content type="html"><![CDATA[<p>使用LSTM训练最简单的IMDB影评分类任务，总结文本分类任务常见流程。<br><a id="more"></a></p><h2 id="1-模型训练和保存"><a href="#1-模型训练和保存" class="headerlink" title="1. 模型训练和保存"></a>1. 模型训练和保存</h2><h3 id="1-1-训练结束时保存"><a href="#1-1-训练结束时保存" class="headerlink" title="1.1 训练结束时保存"></a>1.1 训练结束时保存</h3><p>训练模型，使用fit函数。fit函数的参数如下。</p><pre><code class="lang-python">fit(    x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None,    validation_split=0.0, validation_data=None, shuffle=True, class_weight=None,    sample_weight=None, initial_epoch=0, steps_per_epoch=None,    validation_steps=None, validation_batch_size=None, validation_freq=1,    max_queue_size=10, workers=1, use_multiprocessing=False)</code></pre><p>x：训练数据<br>y：训练标签<br>batch_size：批次大小，默认为32<br>validation_data：在每个epoch结束之时计算loss等其他模型性能指标，不用做训练。<br>epoch：训练轮次<br>verbose：输出的详细程度，为1则输出进度条，表明每个epoch训练完成度；为0则什么也不输出，为2则很啰嗦地输出所有信息</p><p>最后保存模型用<code>model.save(&#39;xxx.h5&#39;)</code>，这里模型格式为HDF5，因此结尾为h5。</p><pre><code class="lang-python">model.fit(X_train, y_train, validation_data=(X_test, y_test), epoch=10, batch_size=64) scores = model.evaluate(X_test, y_test, verbose=0)print(&quot;Accuracy: %.2f%%&quot; % (scores[1]*100))model.save(&#39;models/sentiment-lstm.h5&#39;)</code></pre><h3 id="1-2-在训练期间保存模型（以-checkpoints-形式保存）"><a href="#1-2-在训练期间保存模型（以-checkpoints-形式保存）" class="headerlink" title="1.2 在训练期间保存模型（以 checkpoints 形式保存）"></a>1.2 在训练期间保存模型（以 checkpoints 形式保存）</h3><p>您可以使用训练好的模型而无需从头开始重新训练，或在您打断的地方开始训练，以防止训练过程没有保存。<code>tf.keras.callbacks.ModelCheckpoint</code> 允许在训练的过程中和结束时回调保存的模型。</p><pre><code class="lang-python">checkpoint_path = &quot;training_1/cp.ckpt&quot;checkpoint_dir = os.path.dirname(checkpoint_path)# 创建一个保存模型权重的回调函数cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,                                                 save_weights_only=True,                                                 verbose=1)# 使用新的回调函数训练模型model.fit(train_images,           train_labels,            epochs=10,          validation_data=(test_images,test_labels),          callbacks=[cp_callback])  # 通过回调训练# 这可能会生成与保存优化程序状态相关的警告。# 这些警告（以及整个笔记本中的类似警告）是防止过时使用，可以忽略。</code></pre><p>这将创建一个 TensorFlow checkpoint 文件集合，这些文件在每个 epoch 结束时更新</p><pre><code>cp.ckpt.data-00001-of-00002cp.ckpt.data-00000-of-00002  cp.ckpt.index</code></pre><p>默认的 tensorflow 格式仅保存最近的5个 checkpoint 。</p><h3 id="1-3-手动保存权重"><a href="#1-3-手动保存权重" class="headerlink" title="1.3 手动保存权重"></a>1.3 手动保存权重</h3><p>不必等待epoch结束，通过执行<code>save_weights</code>就可以生成ckpt文件。</p><pre><code class="lang-python"># 保存权重model.save_weights(&#39;./checkpoints/my_checkpoint&#39;)</code></pre><h2 id="2-模型加载"><a href="#2-模型加载" class="headerlink" title="2. 模型加载"></a>2. 模型加载</h2><h3 id="2-1-从h5文件中恢复"><a href="#2-1-从h5文件中恢复" class="headerlink" title="2.1 从h5文件中恢复"></a>2.1 从h5文件中恢复</h3><pre><code class="lang-python"># 重新创建完全相同的模型model=load_model(&#39;models/sentiment-lstm.h5&#39;)# 加载后重新编译模型，否则您将失去优化器的状态model.compile(loss=&#39;binary_crossentropy&#39;,optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) model.summary()</code></pre><p>加载模型的时候，损失函数等参数需要重新设置。</p><h3 id="2-2-从ckpt文件中断点续训"><a href="#2-2-从ckpt文件中断点续训" class="headerlink" title="2.2 从ckpt文件中断点续训"></a>2.2 从ckpt文件中断点续训</h3><p>仅恢复模型的权重时，必须具有与原始模型具有相同网络结构的模型。</p><pre><code class="lang-python"># 这个模型与ckpt保存的一样架构，只不过没经过fit训练model = create_model()# 加载权重model.load_weights(checkpoint_path)</code></pre><p>我们可以对回调函数增加一些新的设置，之前的回调函数每个epoch都覆盖掉之前的ckpt，现在我们想每过5个epoch保存一个新的断点：</p><pre><code class="lang-python"># 在文件名中包含 epoch (使用 `str.format`)checkpoint_path = &quot;training_2/cp-{epoch:04d}.ckpt&quot;checkpoint_dir = os.path.dirname(checkpoint_path)# 创建一个回调，每 5 个 epochs 保存模型的权重cp_callback = tf.keras.callbacks.ModelCheckpoint(    filepath=checkpoint_path,     verbose=1,     save_weights_only=True,    period=5)</code></pre><p>利用新的回调训练，并随后选择最新的断点文件：</p><pre><code class="lang-python"># 使用新的回调训练模型model.fit(train_images,               train_labels,              epochs=50,               callbacks=[cp_callback],              validation_data=(test_images,test_labels),              verbose=0)# 选择新的断点latest = tf.train.latest_checkpoint(checkpoint_dir)&gt;&gt;&gt; &#39;training_2/cp-0050.ckpt&#39;# 加载以前保存的权重model.load_weights(latest)</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用LSTM训练最简单的IMDB影评分类任务，总结文本分类任务常见流程。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="record" scheme="https://superlova.github.io/categories/record/"/>
    
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="TensorFlow" scheme="https://superlova.github.io/tags/TensorFlow/"/>
    
      <category term="SaveModel" scheme="https://superlova.github.io/tags/SaveModel/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】使用LSTM训练imdb情感分类模型</title>
    <link href="https://superlova.github.io/2020/06/03/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8LSTM%E8%AE%AD%E7%BB%83imdb%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/"/>
    <id>https://superlova.github.io/2020/06/03/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8LSTM%E8%AE%AD%E7%BB%83imdb%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-06-03T10:03:29.000Z</published>
    <updated>2020-06-03T14:41:37.751Z</updated>
    
    <content type="html"><![CDATA[<p>使用LSTM训练最简单的IMDB影评分类任务，总结文本分类任务常见流程。<br><a id="more"></a></p><h2 id="1-查看数据集"><a href="#1-查看数据集" class="headerlink" title="1. 查看数据集"></a>1. 查看数据集</h2><h3 id="1-1-官网上的数据集压缩包"><a href="#1-1-官网上的数据集压缩包" class="headerlink" title="1.1 官网上的数据集压缩包"></a>1.1 官网上的数据集压缩包</h3><p>从IMDB官网上下载的数据集，是一个压缩包<code>aclImdb_v1.tar.gz</code>。解压后的目录如下：<br><img src="/2020/06/03/【学习笔记】使用LSTM训练imdb情感分类模型/2020-06-03-18-12-58.png" srcset="/img/loading.gif" alt></p><ul><li><code>test</code></li><li><code>train</code></li><li><code>imdb.vocab</code></li><li><code>imdbEr.txt</code></li><li><code>README</code></li></ul><p>其内部不仅有完整的影评文件，还包含该影评的链接等信息。</p><h3 id="1-2-keras自带的数据集"><a href="#1-2-keras自带的数据集" class="headerlink" title="1.2 keras自带的数据集"></a>1.2 keras自带的数据集</h3><p>keras里的IMDB影评数据集，内部结构分为两个部分：影评部分和情感标签部分，也就是数据集的X和y部分。</p><p>X部分的每条影评都被编码为一个整数列表。另外，每个单词的在单词表中的编码越小，代表在影评中出现频率越高。这使得我们能在取数据时指定只使用某一出现频率内范围的单词（其他单词由于出现频率太低，可以直接标记为未知）。</p><p>“0”在数据集中代表“未知”单词。</p><p>我们采用内置的<code>load_data</code>函数来取出数据。</p><pre><code class="lang-python">tf.keras.datasets.imdb.load_data(    path=&#39;imdb.npz&#39;, num_words=None, skip_top=0, maxlen=None, seed=113,    start_char=1, oov_char=2, index_from=3, **kwargs)</code></pre><p>num_words: 即设定取出现频率在前num_words的单词。如果不填，所有单词表中的单词都会标记。<br>skip_top: 设定前skip_top频率出现的单词不予标记。这可能是由于高频出现的单词信息量太低（如the、a等）。<br>maxlen: 设定最大影评长度，超过该长度的影评都会被截断。<br>x_train, x_test: 返回影评列表，长度为影评个数（25000个训练，25000个测试），每个影评是整数数组。<br>y_train, y_test: 返回整数数组，长度为影评个数，代表影评的情感倾向（0或1）。</p><pre><code class="lang-python">from tensorflow.keras.datasets import imdbmax_features = 50000 # 取前50000个最常见的单词，组建词典(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)</code></pre><p>需要注意的是这个<code>max_features</code>与数据集的数目没有关系，不要搞混了。</p><h2 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2. 数据预处理"></a>2. 数据预处理</h2><pre><code class="lang-python">from tensorflow.keras.preprocessing import sequenceprint(&#39;Pad sequences (samples x time)&#39;)x_train = sequence.pad_sequences(x_train, maxlen=maxlen)x_test = sequence.pad_sequences(x_test, maxlen=maxlen)X_trainarray([[    0,     0,     0, ...,    19,   178,    32],       [    0,     0,     0, ...,    16,   145,    95],       [    0,     0,     0, ...,     7,   129,   113],       ...,       [    0,     0,     0, ...,     4,  3586, 22459],       [    0,     0,     0, ...,    12,     9,    23],       [    0,     0,     0, ...,   204,   131,     9]], dtype=int32)</code></pre><p>经过这个函数处理，每条影评被规整成了长度为500的整形元素列表，长度不够500个单词的影评，在最前面加0；长度不够的则在最后截断。</p><h2 id="3-模型构建"><a href="#3-模型构建" class="headerlink" title="3. 模型构建"></a>3. 模型构建</h2><p><strong>Embedding层</strong></p><p>在最开始我们加入了Embedding层，max_features是字典长度，也可以说是one-hot向量长度。<br>input_length=500为每个序列为500个单词构成。<br>input_shape=(max_features,)表明one-hot的维度，这两个都可以不填，直接通过fit的时候推断出来</p><p><strong>LSTM层</strong></p><p>LSTM层的参数是output_dim，这个参数可以自定义，因为它不受之前影响，只表明输出的维度。</p><p>同时也是是门结构（forget门、update门、output门）的维度。之所以理解成维度，是因为LSTM中隐藏单元个数这个概念不好理解。其实该参数名称为<code>units</code>，官方说法就是“隐藏单元个数”。</p><p>LSTM层的输入是形如（samples，timesteps，input_dim）的3D张量；输出是形如（samples，timesteps，output_dim）的3D张量，或者返回形如（samples，output_dim）的2D张量。二者区别在于，若LSTM层中参数<code>return_sequences=True</code>，就返回带时间步的张量。</p><p>若我们有很多LSTM层，我们可以把很多LSTM层串在一起，为了方便LSTM层与层之间的信息传递，可以设置<code>return_sequences=True</code>。但是最后一个LSTM层return_sequences通常为false，此时输出的就是每个样本的结果张量。</p><p>假如我们输入有25000个句子，每个句子都由500个单词组成，而每个单词用64维的词向量表示。那么样本数目samples=25000，时间步timesteps=500（可以简单地理解timesteps就是输入序列的长度input_length），前一层Embedding词向量输出维度input_dim=128。</p><p>也就是说通过LSTM，把词的维度由128转变成了100。</p><p>在LSTM层中还可以设置Dropout，这一点在之后会详细说明。</p><p><strong>全连接层</strong></p><p>汇总至一个神经元的全连接层，即sigmoid层，判断0或1即可。</p><pre><code class="lang-python">from tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Dense, Embedding, LSTMmodel = Sequential() model.add(Embedding(max_features, 128, input_length=500, input_shape=(max_features,))) model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))model.add(Dense(1, activation=&#39;sigmoid&#39;)) model.compile(loss=&#39;binary_crossentropy&#39;,optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) print(model.summary())</code></pre><h2 id="4-模型训练和保存"><a href="#4-模型训练和保存" class="headerlink" title="4. 模型训练和保存"></a>4. 模型训练和保存</h2><pre><code class="lang-python">model.fit(X_train, y_train, validation_data=(X_test, y_test), epoch=10, batch_size=64) scores = model.evaluate(X_test, y_test, verbose=0)print(&quot;Accuracy: %.2f%%&quot; % (scores[1]*100))model.save(&#39;models/sentiment-lstm.h5&#39;)</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用LSTM训练最简单的IMDB影评分类任务，总结文本分类任务常见流程。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="LSTM" scheme="https://superlova.github.io/tags/LSTM/"/>
    
      <category term="IMDB" scheme="https://superlova.github.io/tags/IMDB/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读笔记】Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey</title>
    <link href="https://superlova.github.io/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Adversarial-Attacks-on-Deep-Learning-Models-in-Natural-Language-Processing-A-Survey/"/>
    <id>https://superlova.github.io/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Adversarial-Attacks-on-Deep-Learning-Models-in-Natural-Language-Processing-A-Survey/</id>
    <published>2020-06-02T06:16:03.000Z</published>
    <updated>2020-06-02T06:50:40.788Z</updated>
    
    <content type="html"><![CDATA[<p>文本领域的对抗样本生成技术综述。<br><a id="more"></a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;文本领域的对抗样本生成技术综述。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://superlova.github.io/categories/paper/"/>
    
    
      <category term="NLP" scheme="https://superlova.github.io/tags/NLP/"/>
    
      <category term="Robustness" scheme="https://superlova.github.io/tags/Robustness/"/>
    
      <category term="Deep Neural Networks" scheme="https://superlova.github.io/tags/Deep-Neural-Networks/"/>
    
      <category term="survey" scheme="https://superlova.github.io/tags/survey/"/>
    
      <category term="Adversarial Attacks" scheme="https://superlova.github.io/tags/Adversarial-Attacks/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读笔记】 Fuzz Testing based Data Augmentation to Improve Robustness of Deep Neural Networks</title>
    <link href="https://superlova.github.io/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/"/>
    <id>https://superlova.github.io/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/</id>
    <published>2020-06-02T06:02:29.000Z</published>
    <updated>2020-06-26T17:30:26.814Z</updated>
    
    <content type="html"><![CDATA[<p>通过模糊测试的方法进行数据增强，数据增强新思路。新加坡国立大学作品，被ICSE’2020接收。<br><a id="more"></a></p><h1 id="内容概要"><a href="#内容概要" class="headerlink" title="内容概要"></a>内容概要</h1><ul><li>增强DNN的训练数据，从而增强其鲁棒性</li><li>将DNN数据扩充问题视为优化问题</li><li>使用遗传搜索来生成最合适的输入数据变体，用于训练DNN</li><li>学习识别跳过增强来加速训练</li><li>improve the robust accuracy of the DNN</li><li>reduce the average DNN training time by 25%, while still improving robust accuracy</li></ul><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>过拟合产生的问题：</p><ol><li>泛化性降低，测试集效果差；</li><li>鲁棒性不足，不能抵御样本微小扰动</li></ol><p>鲁棒性有两种：</p><ol><li>对抗鲁棒性，应对的是人为构造的扰动；</li><li>自然扰动鲁棒性，应对自然条件的变化。</li></ol><p>本文关注的是后一种鲁棒，采用数据增强手段模拟不同自然条件下的样本变化</p><h2 id="数据增强：传统软件"><a href="#数据增强：传统软件" class="headerlink" title="数据增强：传统软件"></a>数据增强：传统软件</h2><p>improves the generalization of generated programs by augmenting existing test suites</p><p>测试用例生成技术：随机测试，基于搜索的进化测试，符号执行，灰箱模糊测试等</p><p>AFL：涵盖更多的程序路径使我们能够找到代表性的测试并涵盖更多的程序功能</p><h2 id="数据增强：提升模型鲁棒性——研究现状"><a href="#数据增强：提升模型鲁棒性——研究现状" class="headerlink" title="数据增强：提升模型鲁棒性——研究现状"></a>数据增强：提升模型鲁棒性——研究现状</h2><p>基于梯度下降的鲁棒优化：</p><ul><li>尝试根据损失函数生成最差的变体，并将其添加到训练集中。</li><li>在自然环境变化（对于视觉应用）中突出显示的空间变换的输入空间是高度非凸的</li></ul><p><img src="/2020/06/02/【论文阅读笔记】Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-10-37.png" srcset="/img/loading.gif" alt></p><p>几乎所有用于提高鲁棒性的技术都会通过分析训练后的模型并随后生成对抗性示例在新数据上重新训练模型</p><h2 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h2><ul><li>将数据扩充形式化为优化问题，利用模糊测试（遗传算法）求解</li><li>提出选择性扩充策略，仅选择部分数据点进行扩充</li></ul><h1 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h1><h2 id="模糊测试"><a href="#模糊测试" class="headerlink" title="模糊测试"></a>模糊测试</h2><p><img src="/2020/06/02/【论文阅读笔记】Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-11-29.png" srcset="/img/loading.gif" alt></p><h2 id="Robustness-of-DNN"><a href="#Robustness-of-DNN" class="headerlink" title="Robustness of DNN"></a>Robustness of DNN</h2><p>DNN很容易遭受微小扰动的影响，产生完全不同的决策。DNN抵御输入扰动的能力被认为是鲁棒性。</p><p>由于这项工作聚焦于自然生成的而不是人工合成的扰动，因此像GAN等技术不被考虑在内。</p><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><p>数据增强是一种扩充数据集、避免样本少导致过拟合的有效方法。增强效果受增强方法影响很大。一般认为数据增强后训练的模型鲁棒性能得到一定程度的提升。</p><h1 id="SENSEI工具"><a href="#SENSEI工具" class="headerlink" title="SENSEI工具"></a>SENSEI工具</h1><h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><p>生成增强样本的过程，是多目标优化过程。</p><p>一方面求扰动δ，使得损失函数L最大；另一方面求模型参数θ，使损失函数L最小；求得此时的δ和θ。本质是一个二元优化找鞍点的问题。</p><p><img src="/2020/06/02/【论文阅读笔记】Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-20-58.png" srcset="/img/loading.gif" alt></p><p><img src="/2020/06/02/【论文阅读笔记】Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-21-07.png" srcset="/img/loading.gif" alt></p><h2 id="优化思路"><a href="#优化思路" class="headerlink" title="优化思路"></a>优化思路</h2><p>基于模糊测试的方法（如引导搜索）能更有效地找到要训练的数据点的最佳Mutator，从而提高鲁棒性</p><p>并非训练数据集中的所有数据点都很难学习。一些数据点代表训练集中的理想示例，而另一些则令人困惑；</p><p>对所有这些点进行相同的处理可能会浪费宝贵的训练时间。因此仅在具有挑战性的数据点上花费扩充工作。</p><h2 id="整体算法"><a href="#整体算法" class="headerlink" title="整体算法"></a>整体算法</h2><p><img src="/2020/06/02/【论文阅读笔记】Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-21-37.png" srcset="/img/loading.gif" alt></p><p><img src="/2020/06/02/【论文阅读笔记】Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-21-43.png" srcset="/img/loading.gif" alt></p><p>Optimal Augmentation Module<br>选择最难的样本（L13~16）<br>Selective Augmentation Module<br>跳过突变不大的样本（L10~12,17）<br>数据增强过程是在训练时即时发生的</p><h2 id="遗传算法部分"><a href="#遗传算法部分" class="headerlink" title="遗传算法部分"></a>遗传算法部分</h2><p>将染色体表示为一组操作，该操作将应用于给定输入以获得真实的变化</p><p>将x旋转1度，然后将其平移一个像素，模拟相机在现实生活中的角度和移动，来得出图像（x）的真实变化（x’）</p><p>种群是代表当前解决方案子集的一组染色体</p><p>种群由两个遗传算子组成：突变和杂交</p><p><img src="/2020/06/02/【论文阅读笔记】Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-22-47.png" srcset="/img/loading.gif" alt></p><p>通过合并两个随机选择的现有染色体来完成交叉以创建新染色体</p><ul><li>c1 = {旋转：1，平移：2，剪切：-0.15}和</li><li>c2 = {旋转：-1，平移：-3，剪切：0.1}</li><li>C = {旋转：1, 平移：-3，剪切：0.1}</li></ul><p>杂交算子在1和染色体长度（l）之间生成随机数r，取c1的1到r部分，与c2的r+1到l部分拼合成新的染色体</p><p>突变算子通过随机改变染色体中的单个操作（改变参数）来执行突变。</p><p>始终将生成的转换向量（染色体）应用于原始图像（而不是应用于已转换的数据），以防止生成的数据不真实</p><p>生成新种群后，将对其进行评估，并且仅将最佳集合作为当前种群传递给下一代（原算法L17）</p><p>适应度函数的设计在GA中起着重要作用，以测量给定解决方案的质量<br>根据DNN的经验损失定义适应度函数</p><script type="math/tex; mode=display">f_{\text {loss}}\left(x^{\prime}\right)=L\left(\theta, x^{\prime}, y\right)</script><p>在DNN的增强训练中应使用遭受DNN损失更大的变体，以使DNN更加健壮</p><h2 id="选择性增强"><a href="#选择性增强" class="headerlink" title="选择性增强"></a>选择性增强</h2><p>Sensei-SA会跳过已由M鲁棒分类的数据点<br>基于分类的鲁棒性：模型正确地分类了x和所有未改变类别的Mutator（x′）<br>基于损失的鲁棒性：x的预测损失或未改变类别（x’）的任何预测损失不大于损失阈值</p><p>如果种子是鲁棒的，则Sensei-SA不会对其进行数据增强；除非在随后的训练中将种子错误地分类，或预测损失小于阈值</p><h2 id="图像扰动"><a href="#图像扰动" class="headerlink" title="图像扰动"></a>图像扰动</h2><p>仿射变换操作、像素操作<br>旋转（x，d）：在[-30，30]范围内将x旋转d度。<br>平移（x，d）：在图像大小的[-10％，10％]范围内水平或垂直将x按d像素平移。<br>剪切（x，d）：水平剪切x，剪切因子d在[-0.1，0.1]范围内。<br>缩放（x，d）：以[0.9,1.1]的缩放系数d放大/缩小x<br>亮度（x，d）：在[-32，32]范围内为x的每个像素统一加减一个值<br>对比度（x，d）：将x的每个像素的RGB值按[0.8，1.2]范围内的因子d缩放。</p><h2 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h2><p>鲁棒性精度（robust accuracy）是指测试集中DNN的预测不随任何小的现实扰动而改变的图像比例</p><script type="math/tex; mode=display">\text {robust accuracy}=\frac{\text {nRobustInstances}}{\text {nInstances}}</script><h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><h2 id="1-Sensei是否可以有效解决鞍点问题？"><a href="#1-Sensei是否可以有效解决鞍点问题？" class="headerlink" title="1.Sensei是否可以有效解决鞍点问题？"></a>1.Sensei是否可以有效解决鞍点问题？</h2><p>关键是检查Sensei是否确实有效比最先进的技术更有效地找到损耗最大的变体<br>W-10在每一步为每个图像随机生成十个扰动，并用模型表现最差的图像替换原始图像<br>结果表明，Sensei更有效地解决了内部最大化问题<br><img src="/2020/06/02/【论文阅读笔记】Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-26-16.png" srcset="/img/loading.gif" alt></p><h2 id="2-Sensei是否比基于对抗样本再训练方法表现更好"><a href="#2-Sensei是否比基于对抗样本再训练方法表现更好" class="headerlink" title="2.Sensei是否比基于对抗样本再训练方法表现更好"></a>2.Sensei是否比基于对抗样本再训练方法表现更好</h2><p>对抗样本再训练方法：<br>i）使用原始训练数据训练模型；<br>ii）通过我们的转换（即使DNN蒙混的变体）生成对抗性示例；<br>iii）选择最佳对抗性示例，添加训练数据，然后重新训练5个模型</p><p><img src="/2020/06/02/【论文阅读笔记】Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-26-58.png" srcset="/img/loading.gif" alt></p><h2 id="3-Sensei能否提高鲁棒性同时保持精度"><a href="#3-Sensei能否提高鲁棒性同时保持精度" class="headerlink" title="3.Sensei能否提高鲁棒性同时保持精度"></a>3.Sensei能否提高鲁棒性同时保持精度</h2><p>mixup 是一种数据增强方法。mixup和Sensei都总体上改善了泛化性能。实际上，在标准泛化方面，mixup比Sensei更好；但是，在改善现实世界中自然发生的Mutator的鲁棒性方面，mixup效果不佳。<br><img src="/2020/06/02/【论文阅读笔记】Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-27-21.png" srcset="/img/loading.gif" alt></p><h2 id="4-选择性数据增强（-Sensei-SA-）的效果和效率"><a href="#4-选择性数据增强（-Sensei-SA-）的效果和效率" class="headerlink" title="4.选择性数据增强（ Sensei-SA ）的效果和效率"></a>4.选择性数据增强（ Sensei-SA ）的效果和效率</h2><p>与W-10相比，Sensei-SA减少了25％的训练时间，而鲁棒性则提高了3％。<br><img src="/2020/06/02/【论文阅读笔记】Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-27-38.png" srcset="/img/loading.gif" alt></p><h2 id="5-对超参数的敏感程度"><a href="#5-对超参数的敏感程度" class="headerlink" title="5.对超参数的敏感程度"></a>5.对超参数的敏感程度</h2><p>突变体集合规模最好为10~15</p><p>神经元覆盖率在鲁棒性评估方面表现出与损失函数相似的性能</p><p>基于神经元覆盖的适应度函数比基于损失的适应度函数将训练时间增加了50％。 原因是神经元覆盖率的计算比训练损失要昂贵<br><img src="/2020/06/02/【论文阅读笔记】Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-27-55.png" srcset="/img/loading.gif" alt><br><img src="/2020/06/02/【论文阅读笔记】Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-28-00.png" srcset="/img/loading.gif" alt></p><p>在判断样本鲁棒性上，基于损失的选择均优于基于分类的选择。</p><p>基于损失的选择足以跳过足够数量的数据点，从而平均减少25％的训练时间。<br><img src="/2020/06/02/【论文阅读笔记】Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-28-12.png" srcset="/img/loading.gif" alt></p><p>Loss threshold越大，训练时间越短，然而鲁棒准确性也下降</p><p>Cifar10数据集比其他数据集对loss threshold更为敏感<br><img src="/2020/06/02/【论文阅读笔记】Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-28-22.png" srcset="/img/loading.gif" alt></p><h1 id="Threads-to-Validity"><a href="#Threads-to-Validity" class="headerlink" title="Threads to Validity"></a>Threads to Validity</h1><p>our results may not generalize to other datasets, or models, or for other applications</p><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><h2 id="测试充足性指标"><a href="#测试充足性指标" class="headerlink" title="测试充足性指标"></a>测试充足性指标</h2><ul><li>DeepXplore，Neuron Coverage</li><li>DeepGauge，k截面神经元覆盖率和神经元边界覆盖率</li><li>惊奇度 surprise adequacy</li><li>MODE，执行状态差分分析以识别模型的错误特征，然后在此基础上执行训练输入选择</li></ul><h2 id="测试用例生成"><a href="#测试用例生成" class="headerlink" title="测试用例生成"></a>测试用例生成</h2><ul><li>对抗性测试，有选择地修改几个像素，将对抗性实例的生成建模为优化问题，并使用一阶优化算法解决优化问题。生成机器学习也可以用来生成对抗性输入</li><li>但是对于自然产生的变化（旋转和平移等），由于其变换非凸，不利于一阶优化</li><li>TensorFuzz，不适合我们的数据增强驱动的鲁棒性训练目标</li><li>DeepTest和DeepRoad，通过metamorphic testing来生成暴露DNN bug的测试用例</li></ul><h2 id="测试合并策略，Test-incorporation-strategy"><a href="#测试合并策略，Test-incorporation-strategy" class="headerlink" title="测试合并策略，Test incorporation strategy"></a>测试合并策略，Test incorporation strategy</h2><ul><li>绝大多数DNN测试用例生成技术首先使用经过训练的DNN生成测试（或对抗实例），然后使用它们重新训练DNN，以提高其准确性或健壮性。</li><li>AutoAugment ，使用强化学习在搜索空间中找到最佳的扩增策略，从而使神经网络达到最高精度</li><li>Mixup，是最近提出的最先进的数据增强技术，但是在良性变异中，鲁棒性不佳。</li><li>Engstrom，除了没用遗传算法之外都一样</li></ul><h2 id="稳健性模型，Robust-models"><a href="#稳健性模型，Robust-models" class="headerlink" title="稳健性模型，Robust models"></a>稳健性模型，Robust models</h2><ul><li>基于正则化的白盒方法，通过修改DNN损失函数，并在标准经验损失中加入一个不变量来正则化，提高深度神经网络模型的鲁棒性</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;通过模糊测试的方法进行数据增强，数据增强新思路。新加坡国立大学作品，被ICSE’2020接收。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://superlova.github.io/categories/paper/"/>
    
    
      <category term="Data Augmentation" scheme="https://superlova.github.io/tags/Data-Augmentation/"/>
    
      <category term="Robustness" scheme="https://superlova.github.io/tags/Robustness/"/>
    
      <category term="Deep Neural Networks" scheme="https://superlova.github.io/tags/Deep-Neural-Networks/"/>
    
      <category term="Fuzz Testing" scheme="https://superlova.github.io/tags/Fuzz-Testing/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读笔记】EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks</title>
    <link href="https://superlova.github.io/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91EDA-Easy-Data-Augmentation-Techniques-for-Boosting-Performance-on-Text-Classification-Tasks/"/>
    <id>https://superlova.github.io/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91EDA-Easy-Data-Augmentation-Techniques-for-Boosting-Performance-on-Text-Classification-Tasks/</id>
    <published>2020-06-02T06:01:54.000Z</published>
    <updated>2020-06-02T06:50:44.567Z</updated>
    
    <content type="html"><![CDATA[<p>这篇论文介绍了一个文本领域的数据增强工具，提出了一些数据增强方法。<br><a id="more"></a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇论文介绍了一个文本领域的数据增强工具，提出了一些数据增强方法。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://superlova.github.io/categories/paper/"/>
    
    
      <category term="Data Augmentation" scheme="https://superlova.github.io/tags/Data-Augmentation/"/>
    
      <category term="NLP" scheme="https://superlova.github.io/tags/NLP/"/>
    
      <category term="Deep Neural Networks" scheme="https://superlova.github.io/tags/Deep-Neural-Networks/"/>
    
      <category term="Text Classification" scheme="https://superlova.github.io/tags/Text-Classification/"/>
    
  </entry>
  
  <entry>
    <title>Datawhale——SVHN——Task05：模型集成</title>
    <link href="https://superlova.github.io/2020/06/02/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task05%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90/"/>
    <id>https://superlova.github.io/2020/06/02/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task05%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90/</id>
    <published>2020-06-02T02:18:31.000Z</published>
    <updated>2020-06-03T00:37:16.358Z</updated>
    
    <content type="html"><![CDATA[<p>三个臭皮匠，顶个诸葛亮。<br><a id="more"></a></p><h2 id="1-集成学习"><a href="#1-集成学习" class="headerlink" title="1. 集成学习"></a>1. 集成学习</h2><p>集成学习的一般结构，是首先产生一组“个体学习器”，再用<strong>某种策略</strong>将其结合起来。类似于“三个臭皮匠，顶个诸葛亮”的思想。</p><p><img src="/2020/06/02/Datawhale——SVHN——Task05：模型集成/2020-06-02-22-32-43.png" srcset="/img/loading.gif" alt></p><p>如果自己的<strong>个体学习器</strong>性能不是很令人满意，使用集成学习将能够提升一定的性能，集成学习器一般都能够获得比个体学习器要好的效果。</p><p>集成学习效果要提升，要尽可能满足两个条件：</p><ol><li>个体学习器性能不太差；</li><li>个体学习器之间不能太相似，结构、所用数据差异越大越好。</li></ol><p>可以证明，如果个体学习器之间的决策误差不存在关联，决策相互独立，那么随着个体学习器数量的增多，集成学习器的错误率将指数下降。</p><p>根据个体学习器的生成方式，目前的集成学习方法分成两大类：</p><ol><li>个体学习器之间存在强依赖关系、必须串行生成的序列化方法，代表为Boosting；</li><li>学习器之间不存在强依赖关系、可以并行的方法，代表为Bagging和随机森林。</li></ol><p>集成学习只能在一定程度上提高精度，并需要耗费较大的训练时间。具体的集成学习方法需要与验证集划分方法结合。</p><h3 id="1-1-Boosting"><a href="#1-1-Boosting" class="headerlink" title="1.1 Boosting"></a>1.1 Boosting</h3><p>Boosting算法是一类能将弱学习器提升为强学习器的算法。基本思想是：先利用初始训练集训练一个基本学习器，再基于基本学习器的表现，对训练样本做出调整，改变样本分布，使得先前被分类错误的训练样本在随后受到更多关注。如此重复训练，直到基学习器的数目达到指定的数值。最终将这几个基学习器进行加权结合。</p><h3 id="1-2-Bagging"><a href="#1-2-Bagging" class="headerlink" title="1.2 Bagging"></a>1.2 Bagging</h3><p>欲得到泛化性能强的集成学习模型，个体学习器之间应当相互独立。但是完全独立是做不到的，即便模型架构完全不同、训练数据完全不一样，这些个体学习器也是为了解决同一个任务而训练的，训练数据之间肯定存在关系，从而导致模型的决策存在相关性。因此Bagging算法就是想要尽可能提升个体学习器之间的差异性。</p><p>一种可能的做法是对训练样本进行采样，产生若干个不同的子集，每个子集都训练一个个体学习器。但是这样学习得到的个体学习器都没能获得足够的训练样本，因此我们可以进行折中，采用互相存在交集的分割方法分割数据集，然后训练模型。</p><p>随机森林本质上是许多决策树的集合，其中每棵树都和其他树略有不同。随机森林背后的思想是，每棵树的预测可能都相对较好，但可能对部分数据过拟合。如果构造很多树，并且每棵树的预测都很好，但都以不同的方式过拟合，那么我们可以对这些树的结果取平均值来降低过拟合。既能减少过拟合又能保持树的预测能力，这可以在数学上严格证明。</p><h2 id="2-深度学习中的集成方法"><a href="#2-深度学习中的集成方法" class="headerlink" title="2. 深度学习中的集成方法"></a>2. 深度学习中的集成方法</h2><h3 id="2-1-Dropout"><a href="#2-1-Dropout" class="headerlink" title="2.1 Dropout"></a>2.1 Dropout</h3><p>每个训练批次中，在更新权重之前，随机让一部分的节点停止工作，增加模型训练时的精度提升难度。</p><p><img src="/2020/06/02/Datawhale——SVHN——Task05：模型集成/2020-06-02-23-33-21.png" srcset="/img/loading.gif" alt></p><p>需要注意的是，训练的时候加dropout，测试的时候以及实际使用时，是不需要dropout的。这就像平时训练的时候腿上绑上沙袋，战时就能够获得更卓越的效果。有效的缓解模型过拟合</p><p><img src="/2020/06/02/Datawhale——SVHN——Task05：模型集成/2020-06-02-23-36-15.png" srcset="/img/loading.gif" alt></p><p>直观来讲，Dropout法使得每个节点都无法单纯依赖其他节点而滥竽充数，因为随时随地自己的同伴就可能被dropout。这样训练时，每个节点都会学到更多的知识。从而提升整体学习器的性能。因此这也算是集成学习。</p><h3 id="2-2-测试集数据扩增"><a href="#2-2-测试集数据扩增" class="headerlink" title="2.2 测试集数据扩增"></a>2.2 测试集数据扩增</h3><p>测试集数据扩增（Test Time Augmentation，简称TTA）也是常用的集成学习技巧，数据扩增不仅可以在训练时候用，而且可以同样在预测时候进行数据扩增，对同一个样本预测三次，然后对三次结果进行平均。</p><h2 id="3-结果后处理"><a href="#3-结果后处理" class="headerlink" title="3. 结果后处理"></a>3. 结果后处理</h2><ul><li>统计图片中每个位置字符出现的频率，使用规则修正结果；</li><li>单独训练一个字符长度预测模型，用来预测图片中字符个数，并修正结果。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;三个臭皮匠，顶个诸葛亮。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="datawhale" scheme="https://superlova.github.io/tags/datawhale/"/>
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="Ensemble" scheme="https://superlova.github.io/tags/Ensemble/"/>
    
      <category term="Boosting" scheme="https://superlova.github.io/tags/Boosting/"/>
    
      <category term="Bagging" scheme="https://superlova.github.io/tags/Bagging/"/>
    
  </entry>
  
  <entry>
    <title>numpy拼合数组方法大全</title>
    <link href="https://superlova.github.io/2020/06/01/numpy%E6%8B%BC%E5%90%88%E6%95%B0%E7%BB%84/"/>
    <id>https://superlova.github.io/2020/06/01/numpy%E6%8B%BC%E5%90%88%E6%95%B0%E7%BB%84/</id>
    <published>2020-06-01T08:39:58.000Z</published>
    <updated>2020-06-02T04:23:43.383Z</updated>
    
    <content type="html"><![CDATA[<p>numpy的一大特色就是其内部的矩阵向量运算。矩阵之间的拼接方法，你掌握多少？<br><a id="more"></a></p><h2 id="1-append拼接"><a href="#1-append拼接" class="headerlink" title="1. append拼接"></a>1. append拼接</h2><p>我们都知道对于Python原生列表list来说，append是最方便的添加元素的方法，一句list.append(elem)就能在列表最后添加一个元素。</p><p>在numpy中，append也是一个直观且好用的方法，np.append(A,B)能够直接拼合两个ndarray数组。</p><p>首先我们新建两个三维数组，一个全为零，一个全为一。</p><pre><code class="lang-python">C = np.zeros((2,2,2))D = np.ones((2,2,2))print(&quot;C: &quot;, C, C.shape)print(&quot;D: &quot;, D, D.shape)C:  [[[0. 0.]  [0. 0.]] [[0. 0.]  [0. 0.]]] shape=(2, 2, 2)D:  [[[1. 1.]  [1. 1.]] [[1. 1.]  [1. 1.]]] shape=(2, 2, 2)</code></pre><p>然后我们采用不同的方法将其拼合在一起。</p><p>首先是append(C,D)这种直观的方法，可以看到C和D都被展开成了一维。</p><pre><code class="lang-python">np.append(C,D)array([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.])</code></pre><p>在很多时候我们希望数组拼接时能够保持原有的维度，按照行拼接/列拼接/其他维度拼接。此时只需要改动append的参数axis即可。</p><pre><code class="lang-python">np.append(C,D,axis=0)array([[[0., 0.],        [0., 0.]],       [[0., 0.],        [0., 0.]],       [[1., 1.],        [1., 1.]],       [[1., 1.],        [1., 1.]]])np.append(C,D,axis=1)array([[[0., 0.],        [0., 0.],        [1., 1.],        [1., 1.]],       [[0., 0.],        [0., 0.],        [1., 1.],        [1., 1.]]])np.append(C,D,axis=2)array([[[0., 0., 1., 1.],        [0., 0., 1., 1.]],       [[0., 0., 1., 1.],        [0., 0., 1., 1.]]])</code></pre><p>对于三维数组，axis=0为层，axis=1为行，axis=2为列。这不难理解，因为确定单位数组中元素位置的坐标就是(层，行，列)</p><h2 id="2-concatenate拼接"><a href="#2-concatenate拼接" class="headerlink" title="2. concatenate拼接"></a>2. concatenate拼接</h2><p>concatenate从字面意义上就让人明白这个函数专门负责数组拼接。不仅仅是两个，还可以负责多个数组一起拼接。理论上来说concatenate的速度和内存消耗都比append要小，但我并没有实际做实验验证。</p><pre><code class="lang-python">np.concatenate((C,D)) # default axis=0array([[[0., 0.],        [0., 0.]],       [[0., 0.],        [0., 0.]],       [[1., 1.],        [1., 1.]],       [[1., 1.],        [1., 1.]]])np.concatenate((C,D), axis=1) # =np.append(C,D,axis=1)array([[[0., 0.],        [0., 0.],        [1., 1.],        [1., 1.]],       [[0., 0.],        [0., 0.],        [1., 1.],        [1., 1.]]])</code></pre><h2 id="3-stack系列"><a href="#3-stack系列" class="headerlink" title="3. stack系列"></a>3. stack系列</h2><p>stack系列函数包括np.stack/hstack/vstack/dstack/column_stack/row_stack，顾名思义，hstack是按照横向拼接，vstack竖着拼接，dstack则是层叠数组。其实我最烦这种抽象描述了，因为二维数组和三维数组/高维数组的抽象描述根本不一致，还是axis好。不明白axis的同学可以看<a href="https://superlova.github.io/2020/05/19/numpy%E4%B8%ADaxis%E7%9A%84%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3/">这篇文章</a>。</p><pre><code class="lang-python">np.hstack((C,D)) # = np.append(C,D,axis=1) = np.column_stack()array([[[0., 0.],        [0., 0.],        [1., 1.],        [1., 1.]],       [[0., 0.],        [0., 0.],        [1., 1.],        [1., 1.]]])np.vstack((C,D)) # =np.append(C,D,axis=0) = np.row_stack()array([[[0., 0.],        [0., 0.]],       [[0., 0.],        [0., 0.]],       [[1., 1.],        [1., 1.]],       [[1., 1.],        [1., 1.]]])np.dstack((C,D)) # =np.append(C,D,aixs=2)array([[[0., 0., 1., 1.],        [0., 0., 1., 1.]],       [[0., 0., 1., 1.],        [0., 0., 1., 1.]]])</code></pre><h2 id="4-np-r"><a href="#4-np-r" class="headerlink" title="4. np.r_[]"></a>4. np.r_[]</h2><p>神奇的numpy总能给出神奇的解法。np.r_是构建数组/拼合数组的最简便写法，但不一定是好理解的。这种写法和之前写的append没什么不同，但是更加简洁。你也可以使用np.r_做出更加复杂的功能。</p><p>一言以蔽之，np.r_[]表达式能够快速使得多个在中括号里面的array/array切片，按照axis=0拼接起来。</p><p>np.r_[]存在两种使用情况：</p><ol><li>如果中括号内部是由若干逗号(comma,)分隔的array，就将他们按照axis=0拼接起来。</li><li>如果中括号内部包括矩阵切片(slices)或者标量(scalars)，就将他们全部变成一维数组首尾相接。</li></ol><p><strong>注意：</strong></p><ul><li>中括号<code>[3:6:1]</code>内部代表的切片，其含义相当于<code>np.arange(3,6,1)</code>，即在<code>[3,6)</code>范围内，从3开始走一步取一个元素，也就是<code>[3,4,5]</code>。</li><li>中括号<code>[0:5:3j]</code>在最后加了字母<code>j</code>，相当于<code>np.linspace(0,5,3,endpoint=True)</code>，在<code>[0,5]</code>范围内，均匀地取三个元素。</li></ul><pre><code class="lang-python">np.r_[C,D] # =np.append(C,D,axis=0)array([[[0., 0.],        [0., 0.]],       [[0., 0.],        [0., 0.]],       [[1., 1.],        [1., 1.]],       [[1., 1.],        [1., 1.]]])np.r_[0:10:3, 0:5:4j]array([0.        , 3.        , 6.        , 9.        , 0.        ,       1.66666667, 3.33333333, 5.        ])</code></pre><p>在中括号内，如果最开始是一个<strong>特定的字符串</strong>，np.r_会试图根据字符串的含义，改变其输出格式。</p><ul><li><code>np.r_[&#39;r&#39;, index_expression]</code>和<code>np.r_[&#39;c&#39;, index_expression]</code>将输出从array类型转变成matrix类型。<code>np.r_[&#39;c&#39;, index_expression]</code>会把一维index_expression组装成N*1的列向量。</li></ul><pre><code class="lang-python">np.r_[&quot;r&quot;, 0:10:3, 0:5:4j]matrix([[0.        , 3.        , 6.        , 9.        , 0.        ,         1.66666667, 3.33333333, 5.        ]])np.r_[&quot;c&quot;, 0:10:3, 0:5:4j]matrix([[0.        ],        [3.        ],        [6.        ],        [9.        ],        [0.        ],        [1.66666667],        [3.33333333],        [5.        ]])</code></pre><ul><li><code>np.r_[&quot;n&quot;, index_expression]</code>前面字符串是整数，则拼接时将会按照axis=n进行拼接。</li></ul><pre><code class="lang-python">np.r_[&quot;-1&quot;,C,D]array([[[0., 0., 1., 1.],        [0., 0., 1., 1.]],       [[0., 0., 1., 1.],        [0., 0., 1., 1.]]])</code></pre><h2 id="5-np-c"><a href="#5-np-c" class="headerlink" title="5. np.c_"></a>5. np.c_</h2><p>在日常使用时，我们经常需要按照最后一个维度拼合两个数组，也就是np.r_[‘-1’,index_expression]。此时我们可以直接使用<code>np.c_[]</code></p><pre><code class="lang-python">np.c_[C,D] # =np.append(C,D,axis=2)array([[[0., 0., 1., 1.],        [0., 0., 1., 1.]],       [[0., 0., 1., 1.],        [0., 0., 1., 1.]]])np.c_[0:10:3, 0:5:4j]array([[0.        , 0.        ],       [3.        , 1.66666667],       [6.        , 3.33333333],       [9.        , 5.        ]])</code></pre><p>关于numpy中的<code>np.c_</code>和<code>np.r_</code>相关知识，可以参考<a href="https://numpy.org/devdocs/reference/generated/numpy.r_.html#numpy.r_" target="_blank" rel="noopener">官方文档</a>，里面有关于中括号前参数的详细解释。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;numpy的一大特色就是其内部的矩阵向量运算。矩阵之间的拼接方法，你掌握多少？&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="numpy" scheme="https://superlova.github.io/tags/numpy/"/>
    
      <category term="array" scheme="https://superlova.github.io/tags/array/"/>
    
      <category term="concatenate" scheme="https://superlova.github.io/tags/concatenate/"/>
    
  </entry>
  
  <entry>
    <title>移动硬盘文件或目录损坏且无法读取解决方法</title>
    <link href="https://superlova.github.io/2020/05/31/%E7%A7%BB%E5%8A%A8%E7%A1%AC%E7%9B%98%E6%96%87%E4%BB%B6%E6%88%96%E7%9B%AE%E5%BD%95%E6%8D%9F%E5%9D%8F%E4%B8%94%E6%97%A0%E6%B3%95%E8%AF%BB%E5%8F%96%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/"/>
    <id>https://superlova.github.io/2020/05/31/%E7%A7%BB%E5%8A%A8%E7%A1%AC%E7%9B%98%E6%96%87%E4%BB%B6%E6%88%96%E7%9B%AE%E5%BD%95%E6%8D%9F%E5%9D%8F%E4%B8%94%E6%97%A0%E6%B3%95%E8%AF%BB%E5%8F%96%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</id>
    <published>2020-05-31T07:33:24.000Z</published>
    <updated>2020-05-31T09:06:18.674Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h2><p>家里的移动硬盘寿命已有4年之久，里面存储了200多G的学习资料（字面意思）。今天我将其插在系统为win10的电脑上，却出现了以下情况：</p><ul><li>硬盘通电指示灯亮；</li><li>右下角托盘区域出现usb插入提示，并可以点击“安全删除硬件”；</li><li>在“我的电脑”界面，显示“本地磁盘 D:”，但是双击之后出现错误“文件或目录损坏且无法读取”。</li></ul><p>重启电脑、重新插拔、更换另一台win7系统的电脑，都是该状况。至此基本确定是移动硬盘本身的问题。</p><h2 id="硬盘参数："><a href="#硬盘参数：" class="headerlink" title="硬盘参数："></a>硬盘参数：</h2><ul><li>黑甲虫 640G 移动机械硬盘</li><li>磁盘格式为NTFS</li><li>使用4年有余</li><li>之前出现过数据丢失的状况，转移敏感数据之后，在该盘中只留有非敏感的学习资料，约280G</li></ul><p>查阅资料可知，我这种错误大概率是由于某次未断电插拔硬盘导致的文件目录错误。好消息是，这种错误可以通过一句简单的指令解决。</p><h2 id="解决方案："><a href="#解决方案：" class="headerlink" title="解决方案："></a>解决方案：</h2><ol><li>打开cmd</li><li>输入 chkdsk D: /f 请注意，我的移动硬盘盘符为D:</li></ol><p>参考：<a href="https://cloud.tencent.com/developer/article/1487000" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1487000</a></p><p>chkdsk 参数说明：</p><p>volume 指定驱动器(后面跟一个冒号)、装入点或卷名。<br>filename 仅用于 FAT/FAT32: 指定要检查是否有碎片的文件<br>/F 修复磁盘上的错误。<br>/V　 在 FAT/FAT32 上: 显示磁盘上每个文件的完整路径和名称。在 NTFS 上: 如果有清除消息，将其显示。<br>/R 查找不正确的扇区并恢复可读信息(隐含 /F)。<br>/L:size 仅用于 NTFS:? 将日志文件大小改成指定的 KB 数。如果没有指定大小，则显示当前的大小。<br>/X 如果必要，强制卷先卸下。卷的所有打开的句柄就会无效(隐含 /F)<br>/I 仅用于 NTFS: 对索引项进行强度较小的检查<br>/C 仅用于 NTFS: 跳过文件夹结构的循环检查。<br>/I 和 /C 命令行开关跳过卷的某些检查，减少运行 Chkdsk 所需的时间</p><h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>这种错误一般产生于外置移动硬盘上面，或者外置U盘等等。之所以产生这些问题，一般有以下几个原因：</p><ol><li>没有点击“安全删除硬件”直接拔USB接口导致系统没有完成读写操作。这会使得文件目录不完整，损坏文件目录系统。</li><li>劣质产品，或者劣质硬盘盒。硬盘盒内部的电源、电路供电不稳定，也会产生文件系统错误的状况。</li><li>停电了</li></ol><h2 id="恢复效果质量"><a href="#恢复效果质量" class="headerlink" title="恢复效果质量"></a>恢复效果质量</h2><p>如果是大移动硬盘并且是NTFS分区格式的，恢复质量十分理想，基本都能成功恢复文件和目录结构。</p><p>如果是FAT或FAT32格式，根据损坏程度不同，恢复质量效果比NTFS格式结构的分区稍差一些，所以日常使用建议使用NTFS格式分区，其数据安全性更高一些。</p><p>一般情况下，CHKDSK可以成功修复出错的分区。但仍有可能没有反应。此时建议不要拔出设备，重启电脑，再观察是否仍然错误。 如果故障依然存在，可以尝试用EasyRecovery、R-STUDIO等软件恢复分区数据。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题描述：&quot;&gt;&lt;a href=&quot;#问题描述：&quot; class=&quot;headerlink&quot; title=&quot;问题描述：&quot;&gt;&lt;/a&gt;问题描述：&lt;/h2&gt;&lt;p&gt;家里的移动硬盘寿命已有4年之久，里面存储了200多G的学习资料（字面意思）。今天我将其插在系统为win10的电脑上，
      
    
    </summary>
    
    
      <category term="record" scheme="https://superlova.github.io/categories/record/"/>
    
    
      <category term="hardware" scheme="https://superlova.github.io/tags/hardware/"/>
    
      <category term="移动硬盘" scheme="https://superlova.github.io/tags/%E7%A7%BB%E5%8A%A8%E7%A1%AC%E7%9B%98/"/>
    
  </entry>
  
  <entry>
    <title>numpy中delete的使用方法</title>
    <link href="https://superlova.github.io/2020/05/31/np-delete%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>https://superlova.github.io/2020/05/31/np-delete%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</id>
    <published>2020-05-31T05:10:08.000Z</published>
    <updated>2020-05-31T09:03:47.140Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍np.delete中的参数及使用方法<br><a id="more"></a></p><h2 id="Python中列表元素删除"><a href="#Python中列表元素删除" class="headerlink" title="Python中列表元素删除"></a>Python中列表元素删除</h2><p>在列表中删除元素，我们可以：</p><pre><code class="lang-python">list_a = [1,2,3,4,5]list_a.pop(-1)print(list_a) # [1,2,3,4]del list_a[0]print(list_a) # [2,3,4]del list[1:]print(list_a) # [2]</code></pre><h2 id="在numpy的ndarray中删除元素"><a href="#在numpy的ndarray中删除元素" class="headerlink" title="在numpy的ndarray中删除元素"></a>在numpy的ndarray中删除元素</h2><p>numpy中的数组ndarray是定长数组，对ndarray的处理不像对python中列表的处理那么方便。想要删除ndarray中的元素，我们往往只能退而求其次，返回一个没有对应元素的副本。在numpy中我们一般使用delete函数。此外，numpy的delete是可以删除数组的整行和整列的。</p><p>简单介绍一下<code>np.delete</code>：</p><pre><code class="lang-python">numpy.delete(arr, obj, axis=None)</code></pre><ul><li>arr：输入数组</li><li>obj：切片，整数，表示哪个子数组要被移除</li><li>axis：删除子数组的轴</li><li>返回：一个新的子数组</li></ul><p>下面是使用举例：</p><pre><code class="lang-python">A = np.arange(15).reshape((3,5))print(A)[[ 0  1  2  3  4] [ 5  6  7  8  9] [10 11 12 13 14]]B = np.delete(A, 1) # 先把A给ravel成一维数组，再删除第1个元素。C = np.delete(A, 1, axis=0) # axis=0代表按行操作D = np.delete(A, 1, axis=1) # axis=1代表按列操作print(A) # 并没有改变，delete不会操作原数组。[[ 0  1  2  3  4] [ 5  6  7  8  9] [10 11 12 13 14]]print(B) # 先把A给ravel成一维数组，再删除第1个元素。[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14]print(C) # axis=0代表按行操作[[ 0  1  2  3  4] [10 11 12 13 14]]print(D) # axis=1代表按列操作[[ 0  2  3  4] [ 5  7  8  9] [10 12 13 14]]</code></pre><p>不了解axis的读者可以看我写的<a href="https://superlova.github.io/2020/05/19/numpy%E4%B8%ADaxis%E7%9A%84%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3/">这篇文章</a>。</p><h2 id="在np-delete的index参数中应用切片操作"><a href="#在np-delete的index参数中应用切片操作" class="headerlink" title="在np.delete的index参数中应用切片操作"></a>在np.delete的index参数中应用切片操作</h2><p>index参数必须是个由整数元素组成的列表，内部存放着的整数代表着目标array的下标。</p><p>当我想实现删除从第5个到第100个之间的所有元素时，不能使用slice，这就比较尴尬了。</p><pre><code class="lang-python">In [5]: np.delete(x, [3:6])  File &quot;&lt;ipython-input-215-0a5bf5cc05ba&gt;&quot;, line 1    np.delete(x, [3:6])                   ^SyntaxError: invalid syntax</code></pre><p>我们没办法在函数参数部分让其接受slice。怎么解决呢？我们可以把参数从<code>[start:end]</code>换成<code>A[start:end]</code>吗？</p><pre><code class="lang-python">A = np.arange(10)*2print(A)[ 0  2  4  6  8 10 12 14 16 18]B = np.delete(A, A[1:4]) # 搞错了吧！预期结果：0 8 10 12 14 16 18print(B)[ 0  2  6 10 14 16 18]</code></pre><p>我们这段代码能够执行，但是不是我们想要的结果。什么原因呢？是因为np.delete的index参数接受的是下标数组，而A[1:4]=[2,4,6]，那么np.delete就忠实地执行了删除第2、4、6个元素的任务。但我们的本意只是想删除下标从1到4的元素而已。</p><pre><code class="lang-python">D = np.delete(A, [1,2,3])print(D) # [ 0  8 10 12 14 16 18]</code></pre><p>要想使用slice，可以采用下列方式：1. <code>slice</code>函数或者<code>range</code>函数；2. <code>np.s_</code></p><pre><code class="lang-python">C = np.delete(A, slice(1,4))print(C) # [ 0  8 10 12 14 16 18]E = np.delete(A, np.s_[1:4])print(E) # [ 0  8 10 12 14 16 18]</code></pre><p>其实<code>np.s_[1:4]</code>只不过是很方便产生slice(1,4)的一种方式而已。</p><h2 id="其他实用的方法"><a href="#其他实用的方法" class="headerlink" title="其他实用的方法"></a>其他实用的方法</h2><p>除此之外，我们还可以采用mask的方式选择原数组中的元素组成新数组</p><pre><code class="lang-python">mask = np.ones((len(A),), dtype=bool)mask[[1,2,3]] = Falseprint(A[mask]) # [ 0  8 10 12 14 16 18]</code></pre><p>或者干脆采用数组拼合的方式</p><pre><code class="lang-python">G = np.empty(len(A)-len(A[1:4]), dtype=int)G[0:1] = A[0:1]G[1:len(G)] = A[4:]print(G) # [ 0  8 10 12 14 16 18]</code></pre><p>后两种方法不像我们想象的那么没用，反而很常见，尤其是mask方法。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍np.delete中的参数及使用方法&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="numpy" scheme="https://superlova.github.io/tags/numpy/"/>
    
      <category term="slice" scheme="https://superlova.github.io/tags/slice/"/>
    
  </entry>
  
  <entry>
    <title>Datawhale——SVHN——Task04：模型训练与验证</title>
    <link href="https://superlova.github.io/2020/05/30/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task04%EF%BC%9A%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%AA%8C%E8%AF%81/"/>
    <id>https://superlova.github.io/2020/05/30/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task04%EF%BC%9A%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%AA%8C%E8%AF%81/</id>
    <published>2020-05-30T13:13:01.000Z</published>
    <updated>2020-05-30T14:17:39.933Z</updated>
    
    <content type="html"><![CDATA[<p>模型的训练和验证过程，基本到了最后咯。<br><a id="more"></a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;模型的训练和验证过程，基本到了最后咯。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="datawhale" scheme="https://superlova.github.io/tags/datawhale/"/>
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="Validation" scheme="https://superlova.github.io/tags/Validation/"/>
    
      <category term="Verification" scheme="https://superlova.github.io/tags/Verification/"/>
    
  </entry>
  
  <entry>
    <title>Datawhale——SVHN——Task03：字符识别</title>
    <link href="https://superlova.github.io/2020/05/27/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task03%EF%BC%9A%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB/"/>
    <id>https://superlova.github.io/2020/05/27/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task03%EF%BC%9A%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB/</id>
    <published>2020-05-27T05:30:18.000Z</published>
    <updated>2020-06-02T06:57:49.385Z</updated>
    
    <content type="html"><![CDATA[<p>介绍常见的字符识别模型<br><a id="more"></a></p><h2 id="用-tf-data-加载图片"><a href="#用-tf-data-加载图片" class="headerlink" title="用 tf.data 加载图片"></a>用 tf.data 加载图片</h2><h2 id="用CNN建立字符识别模型"><a href="#用CNN建立字符识别模型" class="headerlink" title="用CNN建立字符识别模型"></a>用CNN建立字符识别模型</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍常见的字符识别模型&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="datawhale" scheme="https://superlova.github.io/tags/datawhale/"/>
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="Digit Recognition" scheme="https://superlova.github.io/tags/Digit-Recognition/"/>
    
  </entry>
  
  <entry>
    <title>Datawhale——SVHN——Task02：数据扩增</title>
    <link href="https://superlova.github.io/2020/05/23/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task02%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E/"/>
    <id>https://superlova.github.io/2020/05/23/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task02%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E/</id>
    <published>2020-05-23T13:15:18.000Z</published>
    <updated>2020-06-02T06:57:05.053Z</updated>
    
    <content type="html"><![CDATA[<p>训练模型第一步、数据读取和扩增！<br><a id="more"></a></p><h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><p>图像领域的数据读取方法，使用Pillow或者OpenCV内置的函数即可。</p><h2 id="数据扩增"><a href="#数据扩增" class="headerlink" title="数据扩增"></a>数据扩增</h2><p>在读取图像时，还可以对原始图像添加扰动等，这就启发我们一件事：是不是对原数据增加一些扰动，就可以使其变成新的数据呢？</p><p>下面介绍利用该思想的数据扩增环节。</p><h3 id="1-数据扩增为什么会有用"><a href="#1-数据扩增为什么会有用" class="headerlink" title="1. 数据扩增为什么会有用"></a>1. 数据扩增为什么会有用</h3><p>数据扩增的最常见作用，是增加数据集，用以缓解样本量不足导致的模型过拟合现象，从而提升模型的泛化性能。</p><p>究其本质，还是扩展数据集的多样性。</p><p>试想一下，我们如果想要试图训练一个完美模型，必然要利用完美的架构+完美的训练集，这个完美的训练集必然要覆盖到样本空间的方方面面。</p><p>我们当然不可能真的搞到无限多的样本。所以为了尽可能趋近于这个目标，就要试图以有限的数据集覆盖无限的样本空间。</p><p>每个样本在样本空间中就是一个坐标点。通过添加扰动，就能生成许多个在该样本点附近的增强样本。</p><p>上一个利用样本空间中添加扰动、从而生成与原样本很相似的应用，叫做<strong>生成对抗样本</strong>。</p><p>数据增强和对抗样本生成之间的区别在于，数据增强要保证扰动之后样本不能和原样本有区别；然而对抗样本生成则保证<strong>必须</strong>与原样本有区别。</p><p>有一些学者也通过将对抗样本添加至模型重训练的方法，使模型的泛化性能得到了提高。这说明数据增强和对抗样本的生效原理是一样的，都是通过扩大样本覆盖的样本空间的程度，通俗来讲就是模型见多识广了，再碰到新的问题也不怕了。</p><p>可以参考这篇论文：<a href="https://arxiv.org/abs/2003.08773" target="_blank" rel="noopener">Do CNNs Encode Data Augmentations?</a></p><h3 id="2-常见的数据扩增"><a href="#2-常见的数据扩增" class="headerlink" title="2. 常见的数据扩增"></a>2. 常见的数据扩增</h3><h4 id="2-1-图像数据扩增"><a href="#2-1-图像数据扩增" class="headerlink" title="2.1 图像数据扩增"></a>2.1 图像数据扩增</h4><ul><li><p>色彩抖动（Color Jittering）<br>调整图片的亮度、饱和度、对比度，针对图像的颜色进行的数据增强。<br>对比度受限自适应直方图均衡化算法（Clahe），锐化（Sharpen），凸点（Emboss）</p></li><li><p>主成分噪声（PCA  Jittering）<br>首先按照RGB三个颜色通道计算均值和标准差，对网络的输入数据进行规范化；再在整个训练集上计算协方差矩阵，进行特征分解，得到特征向量和特征值，最后做PCA Jittering；最后对RGB空间做PCA，然后对主成分做一个(0, 0.1)的高斯扰动。</p></li><li><p>弹性变换（Elastic Transform）</p></li></ul><p>算法一开始是由Patrice等人在2003年的ICDAR上发表的《Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis》提出的，最开始应用在mnist手写体数字识别数据集中。当前也有很多人把该方法应用到手写体汉字的识别问题中。</p><p>首先对于图像中的每个像素点，产生对应的随机数对$(\Delta x, \Delta y)$，大小介于-1~1之间，分别表示该像素点的x方向和y方向的移动距离；<br>然后生成一个以0为均值，以σ为标准差的高斯核$k_{nn}$，并用前面的随机数与之做卷积，并将结果作用于原图像。</p><p><img src="https://www.kaggleusercontent.com/kf/1181488/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..JOqe3f7Joxv4qWUFbLMQew.CgYzaNwIIFF7jVY1bX0NSs1-ti8NMrPcp61few_93xUuYBRF6ug3wh2awESp_gDWx1BvlZAI45TF3uJ5vNqVLNPS4sIjHMM1oX721fmldeIQAnh84dOKzvTaIK-J0HFhapl_dyBAcV7yOQY_GQYR2MJi3SrPLxvekt-t1hEL0pSdWIl3wbeghVLpgUBg1VmFUFvkti7XL0GltuXOPuQMylsbdbz4GpKX-4gIWyVr301jLLOH-woNbaJuPN0DI4346ok8sJIoG2k7ZdBLq9xRunhHHe4UvmWx2Dj0OK9g8vjKZGA2pRQcxd6_-vWj1KLYyFEWzBWK67rMSFLsQ3zi287T1NK8VJ9C0fLm6FdrQSA4v3BrgbHMwWijkcvG0MdMIdRdZxWbhmxYQ_eKLGntll-2k_1UMFID03h4qjFU-1p2HWv8VktUcpHhvUtu-N26j0vOuWXI484Ttwm2kBkiPZxD0jAfIUVIpSP6pVOTd-E6drxGbr_bqcycOJP2BSzOb5fMqCuP7f_c0B3fnVgVU7-SCb5ngcW6M4ayyy9SNfpobLc2pYj47aTlMmz0iXRrofaEcXiVzhlKz_r8EUUXAkPw5F_X5heFp2S_0hJpKpQJPsssty6FG_PwzN7b5BoSa2mxcNpqiX4ADy-J1dwXzgQesePXPj3rVyQAjxcQmQfXNKVKCo6ASCMBkJsnSQb3.npeRt-2aWHuMiEMkWLKuxA/__results___files/__results___5_1.png" srcset="/img/loading.gif" alt></p><p>参考：<br><a href="https://www.kaggle.com/jiqiujia/elastic-transform-for-data-augmentation" target="_blank" rel="noopener">https://www.kaggle.com/jiqiujia/elastic-transform-for-data-augmentation</a><br><a href="https://blog.csdn.net/lhanchao/article/details/54234490" target="_blank" rel="noopener">https://blog.csdn.net/lhanchao/article/details/54234490</a></p><p>还有诸如透视变换（Perspective Transform）、分段仿射变换（Piecewise Affine transforms）、枕形畸变（Pincushion Distortion）等不同的图像变换操作。</p><p>根据Datawhale大佬分享，对于本次题目（SVHN街道彩色数字识别），最常见的、最有效的数据扩增方法是：</p><ul><li>随机改变大小（resize）</li><li>随机切割（randomcrop），即从原始图像中，随机的crop出一些图像。</li></ul><p>鉴于本次数据集中的图片大小不一，一般一开始我们都需要resize到指定大小。但也有的文章中提到了，先对图片resize会使得图片长宽比发生变化，造成失真。所以我们要具体问题具体分析。</p><p>我在博客中也有分享过一篇讲述图像数据增强的<a href>相关论文</a>，大家可以看一下。</p><h4 id="2-2-文本数据增强"><a href="#2-2-文本数据增强" class="headerlink" title="2.2 文本数据增强"></a>2.2 文本数据增强</h4><p>此部分参考我的<a href>文本数据增强</a>，而且由于本次赛题并不必进行文本数据增强，因此就不在这里赘述了。</p><h3 id="3-数据扩增实战——使用tensorflow"><a href="#3-数据扩增实战——使用tensorflow" class="headerlink" title="3.. 数据扩增实战——使用tensorflow"></a>3.. 数据扩增实战——使用tensorflow</h3><p>大家都用的Pytorch吗？不会只有我自己用tensorflow吧。我来给大家介绍一下tensorflow是怎么做数据扩增的。</p><p>参考：<a href="https://www.tensorflow.org/tutorials/images/data_augmentation" target="_blank" rel="noopener">TensorFlow Core</a></p><h4 id="1-准备"><a href="#1-准备" class="headerlink" title="1. 准备"></a>1. 准备</h4><pre><code class="lang-python"># 首先安装一个tensorflow_docs的库!pip install git+https://github.com/tensorflow/docsimport urllib # 负责下载网上的图片import tensorflow as tffrom tensorflow.keras.datasets import mnistfrom tensorflow.keras import layersAUTOTUNE = tf.data.experimental.AUTOTUNEimport tensorflow_docs as tfdocsimport tensorflow_docs.plotsimport tensorflow_datasets as tfdsimport PIL.Image # 大名鼎鼎PILimport matplotlib.pyplot as pltimport matplotlib as mplmpl.rcParams[&#39;figure.figsize&#39;] = (12, 5)import numpy as np</code></pre><p>下载一张示例图片：</p><pre><code class="lang-python">image_path = tf.keras.utils.get_file(&quot;cat.jpg&quot;, &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/320px-Felis_catus-cat_on_snow.jpg&quot;)PIL.Image.open(image_path)</code></pre><p><img src="/2020/05/23/Datawhale——SVHN——Task02：数据扩增/2020-05-23-22-29-50.png" srcset="/img/loading.gif" alt></p><p>将该图片解析成tensor</p><pre><code class="lang-python">image_string=tf.io.read_file(image_path)image=tf.image.decode_jpeg(image_string,channels=3)</code></pre><p>定义一个函数，用于可视化图像。</p><pre><code class="lang-python">def visualize(original, augmented):  fig = plt.figure()  plt.subplot(1,2,1)  plt.title(&#39;Original image&#39;)  plt.imshow(original)  plt.subplot(1,2,2)  plt.title(&#39;Augmented image&#39;)  plt.imshow(augmented)</code></pre><h4 id="2-执行数据扩增"><a href="#2-执行数据扩增" class="headerlink" title="2. 执行数据扩增"></a>2. 执行数据扩增</h4><p><strong>翻转图像</strong></p><pre><code class="lang-python">flipped = tf.image.flip_left_right(image)visualize(image, flipped)</code></pre><p><img src="/2020/05/23/Datawhale——SVHN——Task02：数据扩增/2020-05-23-22-32-30.png" srcset="/img/loading.gif" alt></p><p><strong>灰度处理</strong></p><pre><code class="lang-python">grayscaled = tf.image.rgb_to_grayscale(image)visualize(image, tf.squeeze(grayscaled))plt.colorbar()</code></pre><p><img src="/2020/05/23/Datawhale——SVHN——Task02：数据扩增/2020-05-23-22-33-51.png" srcset="/img/loading.gif" alt></p><p><strong>改变图像饱和度</strong></p><pre><code class="lang-python">saturated = tf.image.adjust_saturation(image, 3)visualize(image, saturated)</code></pre><p><img src="/2020/05/23/Datawhale——SVHN——Task02：数据扩增/2020-05-23-22-35-05.png" srcset="/img/loading.gif" alt></p><p><strong>改变图像亮度</strong></p><pre><code class="lang-python">bright = tf.image.adjust_brightness(image, 0.4)visualize(image, bright)</code></pre><p><img src="/2020/05/23/Datawhale——SVHN——Task02：数据扩增/2020-05-23-22-35-39.png" srcset="/img/loading.gif" alt></p><p><strong>旋转图像</strong></p><pre><code class="lang-python">rotated = tf.image.rot90(image)visualize(image, rotated)</code></pre><p><img src="/2020/05/23/Datawhale——SVHN——Task02：数据扩增/2020-05-23-22-36-11.png" srcset="/img/loading.gif" alt></p><p><strong>中心放大并裁剪图像</strong></p><pre><code class="lang-python">cropped = tf.image.central_crop(image, central_fraction=0.5)visualize(image,cropped)</code></pre><p><img src="/2020/05/23/Datawhale——SVHN——Task02：数据扩增/2020-05-23-22-36-51.png" srcset="/img/loading.gif" alt></p><p>等等此类操作，不一而足。大家感兴趣的可以查阅tensorflow的<code>tf.image</code>文档。</p><h4 id="3-使用扩增数据集训练"><a href="#3-使用扩增数据集训练" class="headerlink" title="3. 使用扩增数据集训练"></a>3. 使用扩增数据集训练</h4><p>我们构造一个模型，该模型架构为纯全连接网络，数据集为MNIST手写数字识别数据集。我们可以直接在tensorflow_datasets这个库中使用这个数据集。</p><pre><code class="lang-python">dataset, info =  tfds.load(&#39;mnist&#39;, as_supervised=True, with_info=True)train_dataset, test_dataset = dataset[&#39;train&#39;], dataset[&#39;test&#39;]num_train_examples= info.splits[&#39;train&#39;].num_examples</code></pre><p>编写函数执行对原来数据集的扩增操作。</p><pre><code class="lang-python">def convert(image, label):  image = tf.image.convert_image_dtype(image, tf.float32) # Cast and normalize the image to [0,1]  return image, labeldef augment(image,label):  image,label = convert(image, label)  image = tf.image.convert_image_dtype(image, tf.float32) # Cast and normalize the image to [0,1]  image = tf.image.resize_with_crop_or_pad(image, 34, 34) # Add 6 pixels of padding  image = tf.image.random_crop(image, size=[28, 28, 1]) # Random crop back to 28x28  image = tf.image.random_brightness(image, max_delta=0.5) # Random brightness  return image,label</code></pre><pre><code class="lang-python">BATCH_SIZE = 64# Only use a subset of the data so it&#39;s easier to overfit, for this tutorialNUM_EXAMPLES = 2048</code></pre><p>创建扩增后的数据集</p><pre><code class="lang-python">augmented_train_batches = (    train_dataset    # Only train on a subset, so you can quickly see the effect.    .take(NUM_EXAMPLES)    .cache()    .shuffle(num_train_examples//4)    # The augmentation is added here.    .map(augment, num_parallel_calls=AUTOTUNE)    .batch(BATCH_SIZE)    .prefetch(AUTOTUNE))</code></pre><p>为了对照，我们创建没有扩增的数据集。</p><pre><code class="lang-python">non_augmented_train_batches = (    train_dataset    # Only train on a subset, so you can quickly see the effect.    .take(NUM_EXAMPLES)    .cache()    .shuffle(num_train_examples//4)    # No augmentation.    .map(convert, num_parallel_calls=AUTOTUNE)    .batch(BATCH_SIZE)    .prefetch(AUTOTUNE))</code></pre><p>设置验证集。验证集与数据增不增强无关，反正我们不使用验证机训练，只用于最后的打分。</p><pre><code class="lang-python">validation_batches = (    test_dataset    .map(convert, num_parallel_calls=AUTOTUNE)    .batch(2*BATCH_SIZE))</code></pre><p>建立模型。注意这个模型纯粹是为了体现数据扩增的效果而专门构建的，因为卷积网络CNN即便是不用数据扩增也能很好地解决MNIST手写数字识别问题，这样比较起来效果就不明显了。两层4096个神经元的全连接网络，激活函数为RELU。最后是一个softmax层分类。</p><pre><code class="lang-python">def make_model():  model = tf.keras.Sequential([      layers.Flatten(input_shape=(28, 28, 1)),      layers.Dense(4096, activation=&#39;relu&#39;),      layers.Dense(4096, activation=&#39;relu&#39;),      layers.Dense(10)  ])  model.compile(optimizer = &#39;adam&#39;,                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),                metrics=[&#39;accuracy&#39;])  return model</code></pre><p>先使用<strong>没有经过数据扩增</strong>的数据训练模型，并记录其精度变化和loss变化：</p><pre><code class="lang-python">model_without_aug = make_model()no_aug_history = model_without_aug.fit(non_augmented_train_batches, epochs=50, validation_data=validation_batches)</code></pre><p>再使用<strong>经过扩增的数据</strong>训练模型，并记录。</p><pre><code class="lang-python">model_with_aug = make_model()aug_history = model_with_aug.fit(augmented_train_batches, epochs=50, validation_data=validation_batches)</code></pre><p>最后绘制图标，看一下表现。</p><p>首先是精度随着训练轮次的变化曲线：</p><pre><code class="lang-python">plotter = tfdocs.plots.HistoryPlotter()plotter.plot({&quot;Augmented&quot;: aug_history, &quot;Non-Augmented&quot;: no_aug_history}, metric = &quot;accuracy&quot;)plt.title(&quot;Accuracy&quot;)plt.ylim([0.75,1])</code></pre><p><img src="/2020/05/23/Datawhale——SVHN——Task02：数据扩增/2020-05-23-22-49-01.png" srcset="/img/loading.gif" alt></p><p>从图中可以看出，橙色线（没有数据增强的模型）在训练的时候很容易过拟合，但是在验证集上的精度不及蓝色线（数据增强的模型）。</p><p>再来看loss变化。</p><pre><code class="lang-python">plotter = tfdocs.plots.HistoryPlotter()plotter.plot({&quot;Augmented&quot;: aug_history, &quot;Non-Augmented&quot;: no_aug_history}, metric = &quot;loss&quot;)plt.title(&quot;Loss&quot;)plt.ylim([0,1])</code></pre><p><img src="/2020/05/23/Datawhale——SVHN——Task02：数据扩增/2020-05-23-22-51-30.png" srcset="/img/loading.gif" alt></p><p>这里看的就更明显了，橙色线在训练时的loss很快就下降到趋近0，这说明模型已经很难从未经增强的数据中学到东西了，产生了严重的过拟合。</p><p>而蓝色线直到最后也在逐步地学习之中，我们可以得出结论，数据增强的确有助于避免过拟合、增强模型的泛化性能。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;训练模型第一步、数据读取和扩增！&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="datawhale" scheme="https://superlova.github.io/tags/datawhale/"/>
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="Data Augmentation" scheme="https://superlova.github.io/tags/Data-Augmentation/"/>
    
  </entry>
  
  <entry>
    <title>如何评价推荐系统以及其他智能系统</title>
    <link href="https://superlova.github.io/2020/05/20/%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F/"/>
    <id>https://superlova.github.io/2020/05/20/%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F/</id>
    <published>2020-05-20T13:43:38.000Z</published>
    <updated>2020-05-21T00:51:02.937Z</updated>
    
    <content type="html"><![CDATA[<p>看到一篇介绍推荐系统的评价方法与指标的文章，刚好本课题组也有个同学在做推荐系统相关的课题，并且该文章的评价指标部分对我目前的课题有启发作用，因此转载。<br><a id="more"></a></p><h1 id="如何评价推荐系统以及其他智能系统"><a href="#如何评价推荐系统以及其他智能系统" class="headerlink" title="如何评价推荐系统以及其他智能系统"></a>如何评价推荐系统以及其他智能系统</h1><h2 id="1-评价推荐系统"><a href="#1-评价推荐系统" class="headerlink" title="1. 评价推荐系统"></a>1. <a href="https://www.jianshu.com/p/9d7c228eee59" target="_blank" rel="noopener">评价推荐系统</a></h2><p>评测一个推荐系统时，需要考虑用户、物品提供商、推荐系统提供网站的利益，一个好的推荐系统是能够令三方共赢的系统。比如弹窗广告就不是一个好的推荐系统。</p><h3 id="1-1-评测实验方法"><a href="#1-1-评测实验方法" class="headerlink" title="1.1 评测实验方法"></a>1.1 评测实验方法</h3><h4 id="1-1-1-离线实验（offline-experiment）"><a href="#1-1-1-离线实验（offline-experiment）" class="headerlink" title="1.1.1 离线实验（offline experiment）"></a>1.1.1 离线实验（offline experiment）</h4><p>离线实验的方法的步骤如下：</p><p>a）通过日志系统获得用户行为数据，并按照一定格式生成一个标准的数据集；<br>b）将数据集按照一定的规则分成训练集和测试集；<br>c）在训练集上训练用户兴趣模型，在测试集上进行预测；<br>d）通过事先定义的离线指标，评测算法在测试集上的预测结果。</p><p>从以上步骤看出，离线实验的都是在数据集上完成的。意味着，它不需要一个实际的系统作为支撑，只需要有一个从日志中提取的数据集即可。</p><p>离线实验的优点是：</p><ul><li>不需要有对实际系统的控制权；</li><li>不需要用户参与实践；</li><li>速度快，可以测试大量算法；</li></ul><p>缺点是：</p><ul><li>数据集的稀疏性限制了适用范围，例如一个数据集中没有包含某用户的历史行为，则无法评价对该用户的推荐结果；</li><li>评价结果的客观性，无法得到用户主观性的评价；</li><li>难以找到离线评价指标和在线真实反馈(如 点击率、转化率、点击深度、购买客单价、购买商 品类别等)之间的关联关系；</li></ul><h4 id="1-1-2-用户调查（user-study）"><a href="#1-1-2-用户调查（user-study）" class="headerlink" title="1.1.2 用户调查（user study）"></a>1.1.2 用户调查（user study）</h4><p>用户调查需要一些真实的用户，让他们在需要测试的推荐系统上完成一些任务。在他们完成任务时，需要观察和记录用户的行为，并让他们回答一些问题。</p><p>最后，我们通过分析他们的行为和答案，了解测试系统的性能。</p><p>用户调查的优点是：</p><ul><li>可以获得用户主观感受的指标，出错后容易弥补；</li></ul><p>缺点是：</p><ul><li>招募测试用户代价较大；</li><li>无法组织大规模的测试用户，统计意义不足；</li></ul><h4 id="1-1-3-在线实验（online-experiment）"><a href="#1-1-3-在线实验（online-experiment）" class="headerlink" title="1.1.3 在线实验（online experiment）"></a>1.1.3 在线实验（online experiment）</h4><p>在完成离线实验和用户调查之后，可以将系统上线做AB测试，将它和旧算法进行比较。</p><p>在线实验最常用的评测算法是【A/B测试】，它通过一定的规则将用户随机分成几组，对不同组的用户采用不同的算法，然后通过统计不同组的评测指标，比较不同算法的好坏。</p><p>它的核心思想是:</p><p>a) 多个方案并行测试;<br>b) 每个方案只有一个变量不同;<br>c) 以某种规则优胜劣汰。</p><p>其中第2点暗示了A/B 测试的应用范围：A/B测试必须是单变量。</p><p>对于推荐系统的评价中，唯一变量就是—推荐算法。</p><p>有个<a href="http://www.abtests.com" target="_blank" rel="noopener">很棒的网站</a>，里面有很多通过实际AB测试提高网站用户满意度的例子。</p><p>AB测试的优点是：</p><ul><li>可以公平获得不同算法实际在线时的性能指标，包括商业上关注的指标；</li></ul><p>缺点是：</p><ul><li>周期较长，必须进行长期的实验才能得到可靠的结果；</li></ul><h4 id="1-1-4-总结"><a href="#1-1-4-总结" class="headerlink" title="1.1.4 总结"></a>1.1.4 总结</h4><p>一般来说，一个新的推荐算法最终上线，需要完成上述的3个实验。</p><ul><li>首先，通过<strong>离线实验</strong>证明它在很多离线指标上优于现有的算法；</li><li>其次，通过<strong>用户调查</strong>确定用户满意度不低于现有的算法；</li><li>最后，通过<strong>在线AB测试</strong>确定它在我们关心的指标上优于现有的算法；</li></ul><h3 id="1-2-评测指标"><a href="#1-2-评测指标" class="headerlink" title="1.2 评测指标"></a>1.2 评测指标</h3><p>评测指标用于评测推荐系统的性能，有些可以定量计算，有些只能定性描述。</p><h4 id="1）用户满意度"><a href="#1）用户满意度" class="headerlink" title="1）用户满意度"></a>1）用户满意度</h4><p>用户满意度是评测推荐系统的重要指标，无法离线计算，只能通过用户调查或者在线实验获得。</p><p>调查问卷，需要考虑到用户各方面的感受，用户才能针对问题给出准确的回答。</p><p>在线系统中，用户满意度通过统计用户行为得到。比如用户如果购买了推荐的商品，就表示他们在一定程度上满意，可以用购买率度量用户满意度。</p><p>一般情况，我们可以用用户点击率、停留时间、转化率等指标度量用户的满意度。</p><h4 id="2）预测准确度"><a href="#2）预测准确度" class="headerlink" title="2）预测准确度"></a>2）预测准确度</h4><p>预测准确度，度量的是推荐系统预测用户行为的能力。 是推荐系统最重要的离线评测指标。</p><p>大部分的关于推荐系统评测指标的研究，都是针对预测准确度的。因为该指标可以通过离线实验计算，方便了学术界的研究人员。</p><p>由于离线的推荐算法有不同的研究方向，准确度指标也不同，根据研究方向，可分为：预测评分准确度和TopN推荐。</p><h5 id="a）预测评分准确度"><a href="#a）预测评分准确度" class="headerlink" title="a）预测评分准确度"></a>a）预测评分准确度</h5><p>预测评分的准确度，衡量的是算法预测的评分与用户的实际评分的贴近程度。</p><p>这针对于一些需要用户给物品评分的网站。</p><p>预测评分的准确度指标，一般通过以下指标计算：</p><p><strong>平均绝对误差（MAE）</strong></p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-51-32.png" srcset="/img/loading.gif" alt></p><p>MAE因其计算简单、通俗易懂得到了广泛的应用。但MAE指标也有一定的局限性，因为对MAE指标贡献比较大的往往是那种很难预测准确的低分商品。</p><p>所以即便推荐系统A的MAE值低于系统B，很可能只是由于系统A更擅长预测这部分低分商品的评分，即系统A比系统B能更好的区分用户非常讨厌和一般讨厌的商品，显然这样区分的意义不大。</p><p><strong>均方根误差（RMSE）</strong></p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-51-51.png" srcset="/img/loading.gif" alt></p><p>Netflix认为RMSE加大了对预测不准的用户物品评分的惩罚（平方项的惩罚），因而对系统的评测更加苛刻。</p><p>研究表明，如果评分系统是基于整数建立的（即用户给的评分都是整数），那么对预测结果取整数会降低MAE的误差。</p><h5 id="b）TopN推荐"><a href="#b）TopN推荐" class="headerlink" title="b）TopN推荐"></a>b）TopN推荐</h5><p>网站提供推荐服务时，一般是给用户一个个性化的推荐列表，这种推荐叫做TopN推荐。</p><p>TopN推荐的预测准确率，一般通过2个指标度量：</p><p><strong>准确率（precision）</strong></p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-52-10.png" srcset="/img/loading.gif" alt></p><p><strong>召回率（recall）</strong></p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-52-18.png" srcset="/img/loading.gif" alt></p><p>R(u)是根据用户在训练集上的行为给用户做出的推荐列表，T(u)是用户在测试集上的行为列表。</p><p>TopN推荐更符合实际的应用需求，比如预测用户是否会看一部电影，比预测用户看了电影之后会给它什么评分更重要。</p><p><strong>ROC曲线、AUC曲线、F值</strong></p><p>分类任务一般都有这三兄弟。</p><p>除此之外，还有Hit Rate (HR)等。</p><h4 id="3）覆盖率"><a href="#3）覆盖率" class="headerlink" title="3）覆盖率"></a>3）覆盖率</h4><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-52-29.png" srcset="/img/loading.gif" alt></p><p>覆盖率（coverage）是描述一个推荐系统对物品长尾的发掘能力。</p><p>最简单的定义是，推荐系统推荐出来的物品占总物品的比例。</p><p>假设系统的用户集合为U，推荐系统给每个用户推荐一个长度为N的物品列表R(u)，覆盖率公式为：</p><p>覆盖率是内容提供者关心的指标，覆盖率为100%的推荐系统可以将每个物品都推荐给至少一个用户。</p><p>除了推荐物品的占比，还可以通过研究物品在推荐列表中出现的次数分布，更好的描述推荐系统的挖掘长尾的能力。</p><p>如果分布比较平，说明推荐系统的覆盖率很高；如果分布陡峭，说明分布系统的覆盖率较低。</p><p>信息论和经济学中有两个著名指标，可以定义覆盖率：</p><p><strong>信息熵</strong></p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-52-40.png" srcset="/img/loading.gif" alt></p><p>p(i)是物品i的流行度除以所有物品流行度之和。</p><p><strong>基尼系数（Gini Index）</strong></p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-52-57.png" srcset="/img/loading.gif" alt></p><p>p(ij)是按照物品流行度p()从小到大排序的物品列表中第j个物品。</p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-53-09.png" srcset="/img/loading.gif" alt></p><p><strong>评测马太效应</strong></p><p>马太效应，是指强者越强，弱者越弱的效应。推荐系统的初衷是希望消除马太效应，使得各物品都能被展示给对它们感兴趣的人群。</p><p>但是，很多研究表明，现在的主流推荐算法（协同过滤）是具有马太效应的。评测推荐系统是否具有马太效应可以使用基尼系数。</p><p>如，G1是从初始用户行为中计算出的物品流行度的基尼系数，G2是从推荐列表中计算出的物品流行度的基尼系数，那么如果G1&gt;G2，就说明推荐算法具有马太效应。</p><h4 id="4）多样性"><a href="#4）多样性" class="headerlink" title="4）多样性"></a>4）多样性</h4><p>为了满足用户广泛的兴趣，推荐列表需要能够覆盖用户不同兴趣的领域，即需要具有多样性。</p><p>多样性描述了推荐列表中物品两两之间的不相似性。假设s(i,j)在[0,1]区间定义了物品i和j之间的相似度，那么用户u的推荐列表R(u)的多样性定义如下：</p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-53-44.png" srcset="/img/loading.gif" alt></p><p>推荐系统整体多样性可以定义为所有用户推荐列表多样性的平均值：</p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-53-51.png" srcset="/img/loading.gif" alt></p><h4 id="5）新颖性"><a href="#5）新颖性" class="headerlink" title="5）新颖性"></a>5）新颖性</h4><p>新颖性也是影响用户体验的重要指标之一。它指的是向用户推荐非热门非流行物品的能力。</p><p>评测新颖度最简单的方法，是利用推荐结果的平均流行度，因为越不热门的物品，越可能让用户觉得新颖。</p><p>此计算比较粗糙，需要配合用户调查准确统计新颖度。</p><h4 id="6）惊喜度"><a href="#6）惊喜度" class="headerlink" title="6）惊喜度"></a>6）惊喜度</h4><p>推荐结果和用户的历史兴趣不相似，但却让用户满意，这样就是惊喜度很高。</p><p>目前惊喜度还没有公认的指标定义方式，最近几年研究的人很多，深入研究可以参考一些论文。</p><h4 id="7）信任度"><a href="#7）信任度" class="headerlink" title="7）信任度"></a>7）信任度</h4><p>如果用户信任推荐系统，就会增加用户和推荐系统的交互。</p><p>提高信任度的方式有两种：</p><ul><li>增加系统透明度：提供推荐解释，让用户了解推荐系统的运行机制。</li><li>利用社交网络，通过好友信息给用户做推荐</li></ul><p>度量信任度的方式，只能通过问卷调查。</p><h4 id="8）实时性"><a href="#8）实时性" class="headerlink" title="8）实时性"></a>8）实时性</h4><p>推荐系统的实时性，包括两方面：</p><ul><li>实时更新推荐列表满足用户新的行为变化；</li><li>将新加入系统的物品推荐给用户；</li></ul><h4 id="9）健壮性"><a href="#9）健壮性" class="headerlink" title="9）健壮性"></a>9）健壮性</h4><p>任何能带来利益的算法系统都会被攻击，最典型的案例就是搜索引擎的作弊与反作弊斗争。</p><p>健壮性（robust，鲁棒性）衡量了推荐系统抗击作弊的能力。</p><p>2011年的推荐系统大会专门有一个推荐系统健壮性的教程，作者总结了很多作弊方法，最著名的是行为注入攻击（profile injection attack）。</p><p>就是注册很多账号，用这些账号同时购买A和自己的商品。此方法针对亚马逊的一种推荐方法，“购买商品A的用户也经常购买的其他商品”。</p><p>评测算法的健壮性，主要利用模拟攻击：</p><ul><li>a）给定一个数据集和算法，用算法给数据集中的用户生成推荐列表；</li><li>b）用常用的攻击方法向数据集中注入噪声数据；</li><li>c）利用算法在有噪声的数据集上再次生成推荐列表；</li><li>d）通过比较攻击前后推荐列表的相似度评测算法的健壮性。</li></ul><p>提高系统健壮性的方法：</p><ul><li>选择健壮性高的算法；</li><li>选择代价较高的用户行为，如购买行为比浏览行为代价高；</li><li>在使用数据前，进行攻击检测，从而对数据进行清理。</li></ul><h4 id="10）商业目标"><a href="#10）商业目标" class="headerlink" title="10）商业目标"></a>10）商业目标</h4><p>设计推荐系统时，需要考虑最终的商业目标。不同网站具有不同的商业目标，它与网站的盈利模式息息相关。</p><p>总结：</p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-54-09.png" srcset="/img/loading.gif" alt></p><p>作者认为，对于可以离线优化的指标，在给定覆盖率、多样性、新颖性等限制条件下，应尽量优化预测准确度。</p><h3 id="1-3-评测维度"><a href="#1-3-评测维度" class="headerlink" title="1.3 评测维度"></a>1.3 评测维度</h3><p>如果推荐系统的评测报告中，包含了不同维度下的系统评测指标，就能帮我们全面了解系统性能。一般评测维度分3种：</p><p>用户维度，主要包括用户的人口统计学信息、活跃度以及是不是新用户等；<br>物品维度，包括物品的属性信息、流行度、平均分以及是不是新加入的物品等；<br>时间维度，包括季节，是工作日还是周末，白天还是晚上等；</p><h2 id="2-评价智能系统"><a href="#2-评价智能系统" class="headerlink" title="2. 评价智能系统"></a>2. 评价智能系统</h2><p>【未完成】</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;看到一篇介绍推荐系统的评价方法与指标的文章，刚好本课题组也有个同学在做推荐系统相关的课题，并且该文章的评价指标部分对我目前的课题有启发作用，因此转载。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="testing" scheme="https://superlova.github.io/tags/testing/"/>
    
      <category term="Recommendation System" scheme="https://superlova.github.io/tags/Recommendation-System/"/>
    
      <category term="Metrics" scheme="https://superlova.github.io/tags/Metrics/"/>
    
  </entry>
  
  <entry>
    <title>Datawhale——SVHN——Task01：赛题理解</title>
    <link href="https://superlova.github.io/2020/05/20/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task01%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/"/>
    <id>https://superlova.github.io/2020/05/20/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task01%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/</id>
    <published>2020-05-20T08:38:22.000Z</published>
    <updated>2020-06-03T00:36:27.312Z</updated>
    
    <content type="html"><![CDATA[<p>本次新人赛是Datawhale与天池联合发起的0基础入门系列赛事第二场 —— 零基础入门CV之街景字符识别比赛。<br><a id="more"></a></p><h1 id="Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解"><a href="#Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解" class="headerlink" title="Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解"></a>Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解</h1><h2 id="1-大赛简介"><a href="#1-大赛简介" class="headerlink" title="1. 大赛简介"></a>1. 大赛简介</h2><p>本次新人赛是Datawhale与天池联合发起的0基础入门系列赛事第二场 —— 零基础入门CV之街景字符识别比赛。</p><h3 id="1-1-赛题数据介绍"><a href="#1-1-赛题数据介绍" class="headerlink" title="1.1 赛题数据介绍"></a>1.1 赛题数据介绍</h3><p>赛题来源自Google街景图像中的门牌号数据集（The Street View House Numbers Dataset, SVHN），该数据来自真实场景的门牌号。</p><p>训练集数据包括3W张照片，验证集数据包括1W张照片，每张照片包括颜色图像和对应的编码类别和具体位置</p><h3 id="1-2-参赛规则"><a href="#1-2-参赛规则" class="headerlink" title="1.2 参赛规则"></a>1.2 参赛规则</h3><ul><li>比赛允许使用CIFAR-10和ImageNet数据集的预训练模型，不允许使用其他任何预训练模型和任何外部数据；</li><li>报名成功后，选手下载数据，在本地调试算法，提交结果；</li><li>提交后将进行实时评测；每天排行榜更新时间为12:00和20:00，按照评测指标得分从高到低排序；排行榜将选择历史最优成绩进行展示。</li></ul><h3 id="1-3-数据集简介"><a href="#1-3-数据集简介" class="headerlink" title="1.3 数据集简介"></a>1.3 数据集简介</h3><p>所有的数据（训练集、验证集和测试集）的标注使用JSON格式，并使用文件名进行索引。</p><div class="table-container"><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td>top</td><td>左上角坐标X</td></tr><tr><td>height</td><td>字符高度</td></tr><tr><td>left</td><td>左上角最表Y</td></tr><tr><td>width</td><td>字符宽度</td></tr><tr><td>label</td><td>字符编码</td></tr></tbody></table></div><p>字符的坐标具体如下所示：<br><img src="/2020/05/20/Datawhale——SVHN——Task01：赛题理解/字符坐标.png" srcset="/img/loading.gif" alt="坐标">  </p><p>在比赛数据（训练集和验证集）中，同一张图片中可能包括一个或者多个字符，因此在比赛数据的JSON标注中，会有两个字符的边框信息：<br>|原始图片|图片JSON标注|<br>|——|——-|<br><img src="/2020/05/20/Datawhale——SVHN——Task01：赛题理解/原始图片.png" srcset="/img/loading.gif" alt="19">    | <img src="/2020/05/20/Datawhale——SVHN——Task01：赛题理解/原始图片标注.png" srcset="/img/loading.gif" alt="标注">  |</p><p>在<a href="http://ufldl.stanford.edu/housenumbers/" target="_blank" rel="noopener">SVHN官网</a>上下载数据集的话，有两种格式可供选择：</p><h4 id="第一种格式"><a href="#第一种格式" class="headerlink" title="第一种格式"></a>第一种格式</h4><p>第一种格式的数据集，除了原始图像之外还包括边界框信息。每个tar.gz文件都包含png格式的原始图像，以及一个digitStruct.mat文件。边界框信息存储在digitStruct.mat文件中，而不是直接绘制在数据集中的图像上。以<code>.mat</code>结尾的文件是matlab专用文件，可以使用python的scipy库打开，内部装的是类似json的字典。</p><p>digitStruct中的每个元素都有以下字段：</p><ul><li>name是一个字符串，其中包含相应图像的文件名。</li><li>bbox是一个结构数组，包含图像中每个数字边界框的位置，大小和标签。</li></ul><p>例如：digitStruct（300）.bbox（2）.height则是第300张图片中第二个数字边界框的高度。</p><h4 id="第二种格式"><a href="#第二种格式" class="headerlink" title="第二种格式"></a>第二种格式</h4><p>为了降低难度，将上述第一种格式的图片都被剪切到只剩下有效字符，并被缩放至32*32像素的标准大小，以方便识别。图片的边界框经过适当的选择，避免出现扭曲等状况。尽管如此，这种处理还是可能导致引入一些错误信息。</p><p>train_32x32.mat和test_32x32.mat，<code>.mat</code>文件中包含两个变量,X是一个4D的矩阵，维度是(32,32,3,n),n是数据个数,y是label变量。看一下前十张图：</p><pre><code class="lang-python">import scipy.io as sioimport matplotlib.pyplot as pltprint (&#39;Loading Matlab data.&#39;)mat = sio.loadmat(&#39;train_32x32.mat&#39;)data = mat[&#39;X&#39;]label = mat[&#39;y&#39;]for i in range(10):    plt.subplot(2,5,i+1)    plt.title(label[i][0])    plt.imshow(data[...,i])    plt.axis(&#39;off&#39;)plt.show()</code></pre><p><img src="/2020/05/20/Datawhale——SVHN——Task01：赛题理解/2020-05-30-22-17-20.png" srcset="/img/loading.gif" alt></p><h3 id="1-4-成绩评定方式"><a href="#1-4-成绩评定方式" class="headerlink" title="1.4 成绩评定方式"></a>1.4 成绩评定方式</h3><p>评价标准为准确率。<br>选手提交结果与实际图片的编码进行对比，以编码整体识别准确率为评价指标，结果越大越好，具体计算公式如下：</p><p> Score=编码识别正确的数量/测试集图片数量   </p><h3 id="1-5-结果提交格式"><a href="#1-5-结果提交格式" class="headerlink" title="1.5 结果提交格式"></a>1.5 结果提交格式</h3><p>提交前请确保预测结果的格式与sample_submit.csv中的格式一致，以及提交文件后缀名为csv。<br>形式如下：<br>file_name, file_code<br>0010000.jpg,451<br>0010001.jpg,232<br>0010002.jpg,45<br>0010003.jpg,67<br>0010004.jpg,191<br>0010005.jpg,892 </p><h2 id="2-数据读取"><a href="#2-数据读取" class="headerlink" title="2. 数据读取"></a>2. 数据读取</h2><p>JSON中标签的读取方式：  </p><pre><code class="lang-python">import jsontrain_json = json.load(open(&#39;../input/train.json&#39;))# 数据标注处理def parse_json(d):    arr = np.array([        d[&#39;top&#39;], d[&#39;height&#39;], d[&#39;left&#39;],  d[&#39;width&#39;], d[&#39;label&#39;]    ])    arr = arr.astype(int)    return arrimg = cv2.imread(&#39;../input/train/000000.png&#39;)arr = parse_json(train_json[&#39;000000.png&#39;])plt.figure(figsize=(10, 10))plt.subplot(1, arr.shape[1]+1, 1)plt.imshow(img)plt.xticks([]); plt.yticks([])for idx in range(arr.shape[1]):    plt.subplot(1, arr.shape[1]+1, idx+2)    plt.imshow(img[arr[0, idx]:arr[0, idx]+arr[1, idx],arr[2, idx]:arr[2, idx]+arr[3, idx]])    plt.title(arr[4, idx])    plt.xticks([]); plt.yticks([])</code></pre><p><img src="/2020/05/20/Datawhale——SVHN——Task01：赛题理解/19.png" srcset="/img/loading.gif" alt="19"></p><h2 id="3-解题思路"><a href="#3-解题思路" class="headerlink" title="3. 解题思路"></a>3. 解题思路</h2><p>赛题思路分析：赛题本质是分类问题，需要对图片的字符进行识别。但赛题给定的数据图片中不同图片中包含的字符数量不等，如下图所示。有的图片的字符个数为2，有的图片字符个数为3，有的图片字符个数为4。 </p><div class="table-container"><table><thead><tr><th>字符属性</th><th>图片</th></tr></thead><tbody><tr><td>字符：42   字符个数：2</td><td><img src="/2020/05/20/Datawhale——SVHN——Task01：赛题理解/42.png" srcset="/img/loading.gif" alt="标注"></td></tr><tr><td>字符：241   字符个数：3</td><td><img src="/2020/05/20/Datawhale——SVHN——Task01：赛题理解/2411.png" srcset="/img/loading.gif" alt="标注"></td></tr><tr><td>字符：7358   字符个数：4</td><td><img src="/2020/05/20/Datawhale——SVHN——Task01：赛题理解/7358.png" srcset="/img/loading.gif" alt="标注"></td></tr></tbody></table></div><p>因此本次赛题的难点是需要对不定长的字符进行识别，与传统的图像分类任务有所不同。为了降低参赛难度，我们提供了一些解题思路供大家参考：</p><ul><li>简单入门思路：定长字符识别    </li></ul><p>可以将赛题抽象为一个定长字符识别问题，在赛题数据集中大部分图像中字符个数为2-4个，最多的字符    个数为6个。<br>因此可以对于所有的图像都抽象为6个字符的识别问题，字符23填充为23XXXX，字符231填充为231XXX。<br><img src="/2020/05/20/Datawhale——SVHN——Task01：赛题理解/23xxxxxx.png" srcset="/img/loading.gif" alt="标注">   </p><p>经过填充之后，原始的赛题可以简化了6个字符的分类问题。在每个字符的分类中会进行11个类别的分类，假如分类为填充字符，则表明该字符为空。    </p><ul><li>专业字符识别思路：不定长字符识别 </li></ul><p><img src="/2020/05/20/Datawhale——SVHN——Task01：赛题理解/不定长字符识别.png" srcset="/img/loading.gif" alt="标注"> </p><p>在字符识别研究中，有特定的方法来解决此种不定长的字符识别问题，比较典型的有CRNN字符识别模型。<br>在本次赛题中给定的图像数据都比较规整，可以视为一个单词或者一个句子。   </p><ul><li>专业分类思路：检测再识别</li></ul><p>在赛题数据中已经给出了训练集、验证集中所有图片中字符的位置，因此可以首先将字符的位置进行识别，利用物体检测的思路完成。   </p><p><img src="/2020/05/20/Datawhale——SVHN——Task01：赛题理解/检测.png" srcset="/img/loading.gif" alt="IMG"> </p><p>此种思路需要参赛选手构建字符检测模型，对测试集中的字符进行识别。选手可以参考物体检测模型SSD或者YOLO来完成。    </p><h2 id="4-Baseline思路：将不定长字符转换为定长字符的识别问题，并使用CNN完成训练和验证"><a href="#4-Baseline思路：将不定长字符转换为定长字符的识别问题，并使用CNN完成训练和验证" class="headerlink" title="4. Baseline思路：将不定长字符转换为定长字符的识别问题，并使用CNN完成训练和验证"></a>4. Baseline思路：将不定长字符转换为定长字符的识别问题，并使用CNN完成训练和验证</h2><h3 id="4-1-运行环境及安装示例"><a href="#4-1-运行环境及安装示例" class="headerlink" title="4.1 运行环境及安装示例"></a>4.1 运行环境及安装示例</h3><ul><li>运行环境要求：Python2/3，Pytorch1.x，内存4G，有无GPU都可以。         </li></ul><p>下面给出python3.7+ torch1.3.1gpu版本的环境安装示例：      </p><ul><li><p>首先在Anaconda中创建一个专门用于本次天池练习赛的虚拟环境。          </p><blockquote><p>$conda create -n py37_torch131 python=3.7      </p></blockquote></li><li><p>激活环境，并安装pytorch1.3.1                                     </p><blockquote><p>$source activate py37_torch131<br>$conda install pytorch=1.3.1 torchvision cudatoolkit=10.0                     </p></blockquote></li><li><p>通过下面的命令一键安装所需其它依赖库     </p><blockquote><p>$pip install jupyter tqdm opencv-python matplotlib pandas                                  </p></blockquote></li><li><p>启动notebook，即可开始baseline代码的学习                  </p><blockquote><p>$jupyter-notebook   </p></blockquote></li><li><p>假设所有的赛题输入文件放在../input/目录下，首先导入常用的包：</p></li></ul><pre><code class="lang-python">import os, sys, glob, shutil, jsonos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &#39;0&#39;import cv2from PIL import Imageimport numpy as npfrom tqdm import tqdm, tqdm_notebookimport torchtorch.manual_seed(0)torch.backends.cudnn.deterministic = Falsetorch.backends.cudnn.benchmark = Trueimport torchvision.models as modelsimport torchvision.transforms as transformsimport torchvision.datasets as datasetsimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torch.autograd import Variablefrom torch.utils.data.dataset import Dataset</code></pre><h3 id="4-2-步骤"><a href="#4-2-步骤" class="headerlink" title="4.2 步骤"></a>4.2 步骤</h3><ul><li>赛题数据读取（封装为Pytorch的Dataset和DataLoder）</li><li>构建CNN模型（使用Pytorch搭建）</li><li>模型训练与验证</li><li>模型结果预测</li></ul><h4 id="步骤1：定义好读取图像的Dataset"><a href="#步骤1：定义好读取图像的Dataset" class="headerlink" title="步骤1：定义好读取图像的Dataset"></a>步骤1：定义好读取图像的Dataset</h4><pre><code class="lang-python">class SVHNDataset(Dataset):    def __init__(self, img_path, img_label, transform=None):        self.img_path = img_path        self.img_label = img_label         if transform is not None:            self.transform = transform        else:            self.transform = None    def __getitem__(self, index):        img = Image.open(self.img_path[index]).convert(&#39;RGB&#39;)        if self.transform is not None:            img = self.transform(img)        # 设置最长的字符长度为5个        lbl = np.array(self.img_label[index], dtype=np.int)        lbl = list(lbl)  + (5 - len(lbl)) * [10]        return img, torch.from_numpy(np.array(lbl[:5]))    def __len__(self):        return len(self.img_path)</code></pre><h4 id="步骤2：定义好训练数据和验证数据的Dataset"><a href="#步骤2：定义好训练数据和验证数据的Dataset" class="headerlink" title="步骤2：定义好训练数据和验证数据的Dataset"></a>步骤2：定义好训练数据和验证数据的Dataset</h4><pre><code class="lang-python">train_path = glob.glob(&#39;../input/train/*.png&#39;)train_path.sort()train_json = json.load(open(&#39;../input/train.json&#39;))train_label = [train_json[x][&#39;label&#39;] for x in train_json]print(len(train_path), len(train_label))train_loader = torch.utils.data.DataLoader(    SVHNDataset(train_path, train_label,                transforms.Compose([                    transforms.Resize((64, 128)),                    transforms.RandomCrop((60, 120)),                    transforms.ColorJitter(0.3, 0.3, 0.2),                    transforms.RandomRotation(5),                    transforms.ToTensor(),                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])    ])),     batch_size=40,     shuffle=True,     num_workers=10,)val_path = glob.glob(&#39;../input/val/*.png&#39;)val_path.sort()val_json = json.load(open(&#39;../input/val.json&#39;))val_label = [val_json[x][&#39;label&#39;] for x in val_json]print(len(val_path), len(val_label))val_loader = torch.utils.data.DataLoader(    SVHNDataset(val_path, val_label,                transforms.Compose([                    transforms.Resize((60, 120)),                    # transforms.ColorJitter(0.3, 0.3, 0.2),                    # transforms.RandomRotation(5),                    transforms.ToTensor(),                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])    ])),     batch_size=40,     shuffle=False,     num_workers=10,)</code></pre><h4 id="步骤3：定义好字符分类模型，使用renset18的模型作为特征提取模块"><a href="#步骤3：定义好字符分类模型，使用renset18的模型作为特征提取模块" class="headerlink" title="步骤3：定义好字符分类模型，使用renset18的模型作为特征提取模块"></a>步骤3：定义好字符分类模型，使用renset18的模型作为特征提取模块</h4><pre><code class="lang-python">class SVHN_Model1(nn.Module):    def __init__(self):        super(SVHN_Model1, self).__init__()        model_conv = models.resnet18(pretrained=True)        model_conv.avgpool = nn.AdaptiveAvgPool2d(1)        model_conv = nn.Sequential(*list(model_conv.children())[:-1])        self.cnn = model_conv        self.fc1 = nn.Linear(512, 11)        self.fc2 = nn.Linear(512, 11)        self.fc3 = nn.Linear(512, 11)        self.fc4 = nn.Linear(512, 11)        self.fc5 = nn.Linear(512, 11)    def forward(self, img):                feat = self.cnn(img)        # print(feat.shape)        feat = feat.view(feat.shape[0], -1)        c1 = self.fc1(feat)        c2 = self.fc2(feat)        c3 = self.fc3(feat)        c4 = self.fc4(feat)        c5 = self.fc5(feat)        return c1, c2, c3, c4, c5</code></pre><h4 id="步骤4：定义好训练、验证和预测模块"><a href="#步骤4：定义好训练、验证和预测模块" class="headerlink" title="步骤4：定义好训练、验证和预测模块"></a>步骤4：定义好训练、验证和预测模块</h4><pre><code class="lang-python">def train(train_loader, model, criterion, optimizer):    # 切换模型为训练模式    model.train()    train_loss = []    for i, (input, target) in enumerate(train_loader):        if use_cuda:            input = input.cuda()            target = target.cuda()        c0, c1, c2, c3, c4 = model(input)        loss = criterion(c0, target[:, 0]) + \                criterion(c1, target[:, 1]) + \                criterion(c2, target[:, 2]) + \                criterion(c3, target[:, 3]) + \                criterion(c4, target[:, 4])        # loss /= 6        optimizer.zero_grad()        loss.backward()        optimizer.step()        if i % 100 == 0:            print(loss.item())        train_loss.append(loss.item())    return np.mean(train_loss)def validate(val_loader, model, criterion):    # 切换模型为预测模型    model.eval()    val_loss = []    # 不记录模型梯度信息    with torch.no_grad():        for i, (input, target) in enumerate(val_loader):            if use_cuda:                input = input.cuda()                target = target.cuda()            c0, c1, c2, c3, c4 = model(input)            loss = criterion(c0, target[:, 0]) + \                    criterion(c1, target[:, 1]) + \                    criterion(c2, target[:, 2]) + \                    criterion(c3, target[:, 3]) + \                    criterion(c4, target[:, 4])            # loss /= 6            val_loss.append(loss.item())    return np.mean(val_loss)def predict(test_loader, model, tta=10):    model.eval()    test_pred_tta = None    # TTA 次数    for _ in range(tta):        test_pred = []        with torch.no_grad():            for i, (input, target) in enumerate(test_loader):                if use_cuda:                    input = input.cuda()                c0, c1, c2, c3, c4 = model(input)                output = np.concatenate([                    c0.data.numpy(),                     c1.data.numpy(),                    c2.data.numpy(),                     c3.data.numpy(),                    c4.data.numpy()], axis=1)                test_pred.append(output)        test_pred = np.vstack(test_pred)        if test_pred_tta is None:            test_pred_tta = test_pred        else:            test_pred_tta += test_pred    return test_pred_tta</code></pre><h4 id="步骤5：迭代训练和验证模型"><a href="#步骤5：迭代训练和验证模型" class="headerlink" title="步骤5：迭代训练和验证模型"></a>步骤5：迭代训练和验证模型</h4><pre><code class="lang-python">model = SVHN_Model1()criterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), 0.001)best_loss = 1000.0use_cuda = Falseif use_cuda:    model = model.cuda()for epoch in range(2):    train_loss = train(train_loader, model, criterion, optimizer, epoch)    val_loss = validate(val_loader, model, criterion)    val_label = [&#39;&#39;.join(map(str, x)) for x in val_loader.dataset.img_label]    val_predict_label = predict(val_loader, model, 1)    val_predict_label = np.vstack([        val_predict_label[:, :11].argmax(1),        val_predict_label[:, 11:22].argmax(1),        val_predict_label[:, 22:33].argmax(1),        val_predict_label[:, 33:44].argmax(1),        val_predict_label[:, 44:55].argmax(1),    ]).T    val_label_pred = []    for x in val_predict_label:        val_label_pred.append(&#39;&#39;.join(map(str, x[x!=10])))    val_char_acc = np.mean(np.array(val_label_pred) == np.array(val_label))    print(&#39;Epoch: {0}, Train loss: {1} \t Val loss: {2}&#39;.format(epoch, train_loss, val_loss))    print(val_char_acc)    # 记录下验证集精度    if val_loss &lt; best_loss:        best_loss = val_loss        torch.save(model.state_dict(), &#39;./model.pt&#39;)</code></pre><p>训练两个2 Epoch后，输出的训练日志为：</p><p>Epoch: 0, Train loss: 3.1      Val loss: 3.4 验证集精度：0.3439<br>Epoch: 1, Train loss: 2.1      Val loss: 2.9 验证集精度：0.4346     </p><h4 id="步骤6：对测试集样本进行预测，生成提交文件"><a href="#步骤6：对测试集样本进行预测，生成提交文件" class="headerlink" title="步骤6：对测试集样本进行预测，生成提交文件"></a>步骤6：对测试集样本进行预测，生成提交文件</h4><pre><code class="lang-python">test_path = glob.glob(&#39;../input/test_a/*.png&#39;)test_path.sort()test_label = [[1]] * len(test_path)print(len(val_path), len(val_label))test_loader = torch.utils.data.DataLoader(    SVHNDataset(test_path, test_label,                transforms.Compose([                    transforms.Resize((64, 128)),                    transforms.RandomCrop((60, 120)),                    # transforms.ColorJitter(0.3, 0.3, 0.2),                    # transforms.RandomRotation(5),                    transforms.ToTensor(),                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])    ])),     batch_size=40,     shuffle=False,     num_workers=10,)test_predict_label = predict(test_loader, model, 1)test_label = [&#39;&#39;.join(map(str, x)) for x in test_loader.dataset.img_label]test_predict_label = np.vstack([    test_predict_label[:, :11].argmax(1),    test_predict_label[:, 11:22].argmax(1),    test_predict_label[:, 22:33].argmax(1),    test_predict_label[:, 33:44].argmax(1),    test_predict_label[:, 44:55].argmax(1),]).Ttest_label_pred = []for x in test_predict_label:    test_label_pred.append(&#39;&#39;.join(map(str, x[x!=10])))import pandas as pddf_submit = pd.read_csv(&#39;../input/test_A_sample_submit.csv&#39;)df_submit[&#39;file_code&#39;] = test_label_preddf_submit.to_csv(&#39;renset18.csv&#39;, index=None)</code></pre><p><strong>在训练完成2个Epoch后，模型在测试集上的成绩应该在0.33左右。</strong>    </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次新人赛是Datawhale与天池联合发起的0基础入门系列赛事第二场 —— 零基础入门CV之街景字符识别比赛。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="datawhale" scheme="https://superlova.github.io/tags/datawhale/"/>
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
  </entry>
  
</feed>
