<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Superlova</title>
  
  <subtitle>Welcome...</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://superlova.github.io/"/>
  <updated>2020-08-27T12:33:04.774Z</updated>
  <id>https://superlova.github.io/</id>
  
  <author>
    <name>Superlova</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【竞赛打卡】leetcode打卡：查找算法2</title>
    <link href="https://superlova.github.io/2020/08/27/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91leetcode%E6%89%93%E5%8D%A1%EF%BC%9A%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%952/"/>
    <id>https://superlova.github.io/2020/08/27/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91leetcode%E6%89%93%E5%8D%A1%EF%BC%9A%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%952/</id>
    <published>2020-08-27T11:32:22.000Z</published>
    <updated>2020-08-27T12:33:04.774Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-两数之和"><a href="#1-两数之和" class="headerlink" title="1 两数之和"></a>1 两数之和</h2><pre><code class="lang-cpp">class Solution {public:    vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) {        if (nums.empty()) return {};        unordered_map&lt;int, int&gt; hash;        vector&lt;int&gt; res;        // construct dict        for (int i = 0; i &lt; nums.size(); ++i) {            hash[nums[i]] = i;        }        for (int i = 0; i &lt; nums.size(); ++i) {            int temp = target - nums[i];            auto iter = hash.find(temp);            if (iter != hash.end() &amp;&amp; iter-&gt;second != i) {                res.push_back(i);                res.push_back(iter-&gt;second);                break;            }        }        return res;    }};</code></pre><p>思想：用hash把待查数组保存起来，这样再次查找的时间就是O(1)了。</p><h2 id="15-三数之和"><a href="#15-三数之和" class="headerlink" title="15 三数之和"></a>15 三数之和</h2><pre><code class="lang-py">class Solution:    def threeSum(self, nums: List[int]) -&gt; List[List[int]]:        res = []        size = len(nums)        if size &lt; 3: return res        nums.sort()        for i in range(size-2):            if i &gt; 0 and nums[i] == nums[i-1]: continue            j = i + 1            k = size - 1            while j &lt; k:                ans = nums[i] + nums[j] + nums[k]                if (ans &gt; 0): k = k - 1                elif (ans &lt; 0): j = j + 1                else:                    res.append([nums[i], nums[j], nums[k]])                    while j &lt; size and nums[j] == nums[j-1]: j += 1                    k -= 1                    while k &gt;= 0 and nums[k] == nums[k+1]: k -= 1        return res</code></pre><p>思想：用三个下标<code>i,j,k</code>遍历所有可能。首先排序，然后不断缩小i、j和k的区间。</p><h2 id="16-最接近的三数之和"><a href="#16-最接近的三数之和" class="headerlink" title="16 最接近的三数之和"></a>16 最接近的三数之和</h2><pre><code class="lang-cpp">class Solution {public:    int threeSumClosest(vector&lt;int&gt;&amp; nums, int target) {        const int size = nums.size();        if (size &lt;= 3) return std::accumulate(nums.begin(), nums.end(), 0); // 0是累加的初值        std::sort(nums.begin(), nums.end());        int result = nums[0] + nums[1] + nums[2]; // 初值        for (int i = 0; i &lt; size - 2; ++i) {            int j = i + 1;            int k = size - 1;            while (j &lt; k) {                int temp = nums[i] + nums[j] + nums[k];                if (std::abs(target - temp) &lt; std::abs(target - result)) {                    result = temp;                }                if (result == target) { // 直接找到了                    return result;                }                if (temp &gt; target) {                    --k; // temp太大，需要缩小右边界                } else {                    ++j; // temp太小，需要缩小左边界                }            }        }        return result;    }};</code></pre><p>思路：还是利用三个下标<code>i,j,k</code>遍历全部数组。中途不断保存和target最近的temp值。</p><h2 id="18-四数之和"><a href="#18-四数之和" class="headerlink" title="18 四数之和"></a>18 四数之和</h2><pre><code class="lang-cpp">class Solution {public:    vector&lt;vector&lt;int&gt;&gt; fourSum(vector&lt;int&gt;&amp; nums, int target) {        sort(nums.begin(),nums.end());        vector&lt;vector&lt;int&gt; &gt; res;        if(nums.size()&lt;4) return res;        int a,b,c,d,_size=nums.size();        for(a=0;a&lt;_size-3;a++){            if(a&gt;0&amp;&amp;nums[a]==nums[a-1]) continue;      //确保nums[a] 改变了            for(b=a+1;b&lt;_size-2;b++){                if(b&gt;a+1&amp;&amp;nums[b]==nums[b-1])continue;   //确保nums[b] 改变了                c=b+1,d=_size-1;                while(c&lt;d){                    if(nums[a]+nums[b]+nums[c]+nums[d]&lt;target)                        c++;                    else if(nums[a]+nums[b]+nums[c]+nums[d]&gt;target)                        d--;                    else{                        res.push_back({nums[a],nums[b],nums[c],nums[d]});                        while(c&lt;d&amp;&amp;nums[c+1]==nums[c])      //确保nums[c] 改变了                            c++;                        while(c&lt;d&amp;&amp;nums[d-1]==nums[d])      //确保nums[d] 改变了                            d--;                        c++;                        d--;                    }                }            }        }        return res;    }};</code></pre><p>思路：四指针。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-两数之和&quot;&gt;&lt;a href=&quot;#1-两数之和&quot; class=&quot;headerlink&quot; title=&quot;1 两数之和&quot;&gt;&lt;/a&gt;1 两数之和&lt;/h2&gt;&lt;pre&gt;&lt;code class=&quot;lang-cpp&quot;&gt;class Solution {
public:
    
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】数据挖掘实践——总结与反思</title>
    <link href="https://superlova.github.io/2020/08/25/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%80%BB%E7%BB%93%E4%B8%8E%E5%8F%8D%E6%80%9D/"/>
    <id>https://superlova.github.io/2020/08/25/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%80%BB%E7%BB%93%E4%B8%8E%E5%8F%8D%E6%80%9D/</id>
    <published>2020-08-25T15:28:03.000Z</published>
    <updated>2020-08-25T15:33:14.930Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>这次组队学习确实是自己不太擅长的领域。借着上次NLP组队学习的冲劲，本以为想在不熟悉的领域也能至少学点东西，但确实是小看了这次学习的难度。</p><p>回头来看，组队学习进入到特征工程的时候，我已经完全一脸懵逼了。我很想跟上大家的节奏，但是确实一个是没有时间，另一个是差太多了，已经进入了恐慌区，脑海里也是拒绝的心态。我知道这次组队学习已经彻底失败了。</p><p>唉！内心充满了挫败感。这次组队学习唯一的收获就是，获得了这次比赛的Baseline。自此之后我就要投入到秋招了，我曾参加过CV赛事、NLP赛事，再加上这个结构化赛事，我的比赛经历算是圆满了，接下来的日子就是研究Baseline、好好准备简历了，也该收收心了。</p><p>祝自己好运。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;这次组队学习确实是自己不太擅长的领域。借着上次NLP组队学习的冲劲，本以为想在不熟悉的领域也能至少学点东西，但确实是小看了这次学习的难度。&lt;/p&gt;
&lt;p&gt;回头来看，组队学习进入到特征工程的时候，我已经完全一脸懵逼了。我很想跟上大家的节奏，
      
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="Data Mining" scheme="https://superlova.github.io/tags/Data-Mining/"/>
    
      <category term="Time Series Analysis" scheme="https://superlova.github.io/tags/Time-Series-Analysis/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】数据挖掘实践——建模预测</title>
    <link href="https://superlova.github.io/2020/08/24/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E5%BB%BA%E6%A8%A1%E9%A2%84%E6%B5%8B/"/>
    <id>https://superlova.github.io/2020/08/24/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E5%BB%BA%E6%A8%A1%E9%A2%84%E6%B5%8B/</id>
    <published>2020-08-24T15:05:13.000Z</published>
    <updated>2020-08-24T15:41:46.921Z</updated>
    
    <content type="html"><![CDATA[<p>我也是加把劲骑士！<br><a id="more"></a></p><pre><code class="lang-py">import matplotlib.pyplot as pltimport pandas as pdimport numpy as npfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dense, Add, Dropout, Flatten, TimeDistributedfrom tensorflow.keras.models import Model, Sequentialfrom tensorflow.keras import activations, optimizers, regularizersfrom tensorflow.keras.callbacks import EarlyStoppingimport tensorflow.keras.backend as kbfrom tensorflow import kerasfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.metrics import mean_squared_error, mean_absolute_errorfrom time import *user_balance = pd.read_csv(&#39;Purchase Redemption Data/user_balance_table.csv&#39;)df_tmp = user_balance.groupby([&#39;report_date&#39;])[&#39;total_purchase_amt&#39;, &#39;total_redeem_amt&#39;].sum()df_tmp.index = pd.to_datetime(df_tmp.index, format=&#39;%Y%m%d&#39;)holidays = (&#39;20130813&#39;, &#39;20130902&#39;, &#39;20131001&#39;, &#39;20131111&#39;, &#39;20130919&#39;, &#39;20131225&#39;, &#39;20140101&#39;, &#39;20140130&#39;, &#39;20140131&#39;,           &#39;20140214&#39;, &#39;20140405&#39;, &#39;20140501&#39;, &#39;20140602&#39;, &#39;20140802&#39;, &#39;20140901&#39;, &#39;20140908&#39;)def create_features(timeindex):    n = len(timeindex)    features = np.zeros((n, 4))    features[:, 0] = timeindex.day.values/31    features[:, 1] = timeindex.month.values/12    features[:, 2] = timeindex.weekday.values/6    for i in range(n):        if timeindex[i].strftime(&#39;%Y%m%d&#39;) in holidays:            features[i, 3] = 1    return featuresfeatures = create_features(df_tmp.index)september = pd.to_datetime([&#39;201409%02d&#39; % i for i in range(1, 31)])features_sep = create_features(september)scaler_pur = MinMaxScaler()scaler_red = MinMaxScaler()data_pur = scaler_pur.fit_transform(df_tmp.values[:, 0:1])data_red = scaler_red.fit_transform(df_tmp.values[:, 1:2])def create_dataset(data, back, forward=30):    n_samples = len(data) - back - forward + 1    X, Y = np.zeros((n_samples, back, data.shape[-1])), np.zeros((n_samples, forward, data.shape[-1]))    for i in range(n_samples):        X[i, ...] = data[i:i+back, :]        Y[i, ...] = data[i+back:i+back+forward, :]    return X, Ydef build_cnn(X_trn, lr, n_outputs, dropout_rate):    inputs = Input(X_trn.shape[1:])    z = Conv1D(64, 14, padding=&#39;valid&#39;, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)(inputs)#     z = MaxPooling1D(2)(z)    z = Conv1D(128, 7, padding=&#39;valid&#39;, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)(z)    z = MaxPooling1D(2)(z)    z = Conv1D(256, 3, padding=&#39;valid&#39;, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)(z)    z = Conv1D(256, 3, padding=&#39;valid&#39;, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)(z)    z = MaxPooling1D(2)(z)    z = Flatten()(z)    z = Dropout(dropout_rate)(z)    z = Dense(128, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)(z)    z = Dropout(dropout_rate)(z)    z = Dense(84, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)(z)    outputs = Dense(n_outputs)(z)    model = Model(inputs=inputs, outputs=outputs)    adam = optimizers.Adam(lr=lr)    model.compile(loss=&#39;mse&#39;, optimizer=adam, metrics=[&#39;mae&#39;])    model.summary()    return modelback = 60forward = 30X_pur_data, Y_pur_data = create_dataset(data_pur, back, forward)X_red_data, Y_red_data = create_dataset(data_red, back, forward)X_features, Y_features = create_dataset(features, back, forward)Y_features = np.concatenate((Y_features, np.zeros((Y_features.shape[0], back-forward, Y_features.shape[-1]))), axis=1)# X_pur, X_red = np.concatenate((X_pur_data, X_features, Y_features), axis=-1), np.concatenate((X_red_data, X_features, Y_features), axis=-1)# X_pur_trn, X_pur_val, X_red_trn, X_red_val = X_pur[:-forward, ...], X_pur[-1:, ...], X_red[:-forward, ...], X_red[-1:, ...]# Y_pur_trn, Y_pur_val, Y_red_trn, Y_red_val = Y_pur_data[:-forward, ...], Y_pur_data[-1:, ...], Y_red_data[:-forward, ...], Y_red_data[-1:, ...]Y_fea_sep = np.concatenate((features_sep, np.zeros((back-forward, features_sep.shape[-1]))), axis=0)# X_pur_tst = np.concatenate((data_pur[-back:, :], features[-back:, :], Y_fea_sep), axis=-1)[None, ...]# X_red_tst = np.concatenate((data_red[-back:, :], features[-back:, :], Y_fea_sep), axis=-1)[None, ...]X = np.concatenate((X_pur_data, X_red_data, X_features, Y_features), axis=-1)Y = np.concatenate((Y_pur_data, Y_red_data), axis=1)X_trn, X_val, Y_trn, Y_val = X[:-forward, ...], X[-1:, ...], Y[:-forward, ...], Y[-1:, ...]X_tst = np.concatenate((data_pur[-back:, :], data_red[-back:, :], features[-back:, :], Y_fea_sep), axis=-1)[None, ...]cnn_pur = build_cnn(X_trn, lr=0.0008, n_outputs=2*forward, dropout_rate=0.5)history = cnn_pur.fit(X_trn, Y_trn, batch_size=32, epochs=1000, verbose=2,                       validation_data=(X_val, Y_val),                     callbacks=[EarlyStopping(monitor=&#39;val_mae&#39;, patience=200, restore_best_weights=True)])plt.figure(figsize=(8, 5))plt.plot(history.history[&#39;mae&#39;], label=&#39;train mae&#39;)plt.plot(history.history[&#39;val_mae&#39;], label=&#39;validation mae&#39;)plt.ylim([0, 0.2])plt.legend()plt.show()def plot_prediction(y_pred, y_true):    plt.figure(figsize=(16,4))    plt.plot(np.squeeze(y_pred), label=&#39;prediction&#39;)    plt.plot(np.squeeze(y_true), label=&#39;true&#39;)    plt.legend()    plt.show()    print(&#39;MAE: %.3f&#39; % mean_absolute_error(np.squeeze(y_pred), np.squeeze(y_true)))pred = cnn.predict(X_val)plot_prediction(pred, Y_val)history = cnn.fit(X, Y, batch_size=32, epochs=500, verbose=2,                     callbacks=[EarlyStopping(monitor=&#39;mae&#39;, patience=30, restore_best_weights=True)])plt.figure(figsize=(8, 5))plt.plot(history.history[&#39;mae&#39;], label=&#39;train mae&#39;)plt.legend()plt.show()print(cnn.evaluate(X, Y, verbose=2))pred_tst = cnn.predict(X_tst)pur_sep = scaler_pur.inverse_transform(pred_tst[:, :forward].transpose())red_sep = scaler_red.inverse_transform(pred_tst[:, forward:].transpose())test_user = pd.DataFrame({&#39;report_date&#39;: [20140900 + i for i in range(1, 31)]})test_user[&#39;pur&#39;] = pur_sep.astype(&#39;int&#39;)test_user[&#39;red&#39;] = red_sep.astype(&#39;int&#39;)test_user.to_csv(&#39;submission.csv&#39;, encoding=&#39;utf-8&#39;, index=None, header=None)from google.colab import filesfiles.download(&quot;submission.csv&quot;)</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我也是加把劲骑士！&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="Data Mining" scheme="https://superlova.github.io/tags/Data-Mining/"/>
    
      <category term="Time Series Analysis" scheme="https://superlova.github.io/tags/Time-Series-Analysis/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】数据挖掘实践——特征工程</title>
    <link href="https://superlova.github.io/2020/08/24/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    <id>https://superlova.github.io/2020/08/24/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</id>
    <published>2020-08-24T15:04:54.000Z</published>
    <updated>2020-08-24T15:40:34.810Z</updated>
    
    <content type="html"><![CDATA[<p>我也是加把劲骑士！<br><a id="more"></a></p><p>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。由此可见，特征工程在机器学习中占有相当重要的地位。在实际应用当中，可以说特征工程是机器学习成功的关键。</p><p>那特征工程是什么？</p><p>特征工程是利用数据领域的相关知识来创建能够使机器学习算法达到最佳性能的特征的过程。</p><p>特征工程又包含了 Data PreProcessing（数据预处理）、Feature Extraction（特征提取）、Feature Selection（特征选择）和 Feature construction（特征构造）等子问题，本章内容主要讨论特征构造的方法。</p><p>创造新的特征是一件十分困难的事情，需要丰富的专业知识和大量的时间。机器学习应用的本质基本上就是特征工程。<br>——Andrew Ng</p><p>对于时间型数据来说，即可以把它转换成连续值，也可以转换成离散值。</p><p>1.连续值时间特征<br>持续时间（单页浏览时长）；间隔时间；上次购买/点击离现在的时长；产品上线到现在经过的时长；2.离散值时间特征<br>1）时间特征拆解</p><p>年；月；日；时；分；数；一天中的第几分钟；星期几；一年中的第几天；一年中的第几个周；一天中哪个时间段：凌晨、早晨、上午、中午、下午、傍晚、晚上、深夜；一年中的哪个季度；</p><p>2）时间特征判断<br>是否闰年；是否月初；是否月末；是否季节初；是否季节末；是否年初；是否年尾；是否周末；是否公共假期；是否营业时间；两个时间间隔之间是否包含节假日/特殊日期；</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我也是加把劲骑士！&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="Data Mining" scheme="https://superlova.github.io/tags/Data-Mining/"/>
    
      <category term="Time Series Analysis" scheme="https://superlova.github.io/tags/Time-Series-Analysis/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】leetcode打卡：查找算法</title>
    <link href="https://superlova.github.io/2020/08/24/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91leetcode%E6%89%93%E5%8D%A1%EF%BC%9A%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/"/>
    <id>https://superlova.github.io/2020/08/24/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91leetcode%E6%89%93%E5%8D%A1%EF%BC%9A%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/</id>
    <published>2020-08-24T00:23:08.000Z</published>
    <updated>2020-08-24T14:59:07.827Z</updated>
    
    <content type="html"><![CDATA[<p>见多了优秀的文章，再写博客的时候就会感叹自己的学识浅薄。<br><a id="more"></a></p><h2 id="leetcode-35-搜索插入位置"><a href="#leetcode-35-搜索插入位置" class="headerlink" title="leetcode 35 搜索插入位置"></a>leetcode 35 搜索插入位置</h2><p>给定一个无重复元素的排序数组和一个目标值，在数组中找到目标值，并返回其索引。如果目标值不存在于数组中，返回它将会被按顺序插入的位置。</p><p>思路：简单的二分搜索。注意边界条件。注意初始化条件是<code>L = 0, R = nums.size()</code>。</p><p>代码：</p><pre><code class="lang-cpp">class Solution {public:    int searchInsert(vector&lt;int&gt;&amp; nums, int target) {        int L = 0, R = nums.size();        if (nums.empty() || target &lt; nums[L]) return 0;        if (nums[R-1] &lt; target) return R;        while (L &lt;= R) {            int i = (L + R) / 2;            if (nums[i] &lt; target) {                L = i + 1;            } else if (target &lt; nums[i]) {                R = i - 1;            } else if (target == nums[i]){                return i;            } //else         }        return L;    }};</code></pre><h2 id="leetcode-202-快乐数"><a href="#leetcode-202-快乐数" class="headerlink" title="leetcode 202 快乐数"></a>leetcode 202 快乐数</h2><p>先分享一下直观的解法</p><p>检查一个数是快乐数，就不断执行<code>n=compute(n)</code>这一步，然后检查n是否为1就行了。但是一旦一个数不是快乐数，则必定是陷于某个数字循环中。比如2这个非快乐数，它的计算过程如下：</p><pre><code>241637588914542204 &lt;- 注意这里的4已经出现过</code></pre><p>我的思路很简单。只需将出现过的n都保存在一个字典中，如果新计算的n已经存在于字典中了，那就意味着陷入了计算循环，非快乐数。</p><pre><code class="lang-cpp">class Solution {public:    bool isHappy(int n) {        unordered_map&lt;int, int&gt; hash;        while (n != 1) {            n = compute(n);            auto iter = hash.find(n);            if (iter != hash.end()) return false;            ++hash[n];        }        return true;    }    int compute(int n) {        int res = 0, bit = 0;        while (n) {            bit = n % 10;            n = n / 10;            res += bit * bit;         }        return res;    }};</code></pre><p>这个问题还可以转化为检测链表是否存在环路的问题。就可以使用快慢指针法。<code>compute</code>函数不变，只需把主函数部分变成：</p><pre><code class="lang-cpp">class Solution {public:    bool isHappy(int n) {        int slow = n;        int fast = compute(n);        while (slow != fast &amp;&amp; fast != 1) {            slow = compute(slow);            fast = compute(compute(fast));        }        return fast == 1;    }    int compute(int n) {        int res = 0, bit = 0;        while (n) {            bit = n % 10;            n = n / 10;            res += bit * bit;         }        return res;    }};</code></pre><h2 id="leetcode-205-同构字符串"><a href="#leetcode-205-同构字符串" class="headerlink" title="leetcode 205 同构字符串"></a>leetcode 205 同构字符串</h2><p>将两个字符串翻译为数字，最后比较数字是否相同即可。</p><pre><code class="lang-cpp">class Solution {public:    bool isIsomorphic(string s, string t) {        s = translate(s);        t = translate(t);        return s == t;    }    string translate(string s) {        int count = 0;        unordered_map&lt;char, int&gt; hash;        string res = &quot;&quot;;        for (auto c : s) {            auto iter = hash.find(c);            if (iter != hash.end()) res += std::to_string(iter-&gt;second);            else hash[c] = count++;        }        return res;    }};</code></pre><h2 id="leetcode-242-有效的字母异位词"><a href="#leetcode-242-有效的字母异位词" class="headerlink" title="leetcode 242 有效的字母异位词"></a>leetcode 242 有效的字母异位词</h2><p>总体思路还是哈希表，保存两个字符串出现的字符类别和次数，如若相等则true。</p><p>可以进一步优化，即使用一个哈希表，遍历s的时候构建哈希，遍历t的时候删减对应哈希的元素，如果哈希表的数值低于0，就说明为false。</p><p>万一删减不到零呢？其实这种情况是不会出现的，因为我们在循环伊始，检查两字符串的长度必须相同。</p><pre><code class="lang-cpp">class Solution {public:    bool isAnagram(string s, string t) {        if (s.size() != t.size()) return false;        vector&lt;int&gt; table(26, 0);        for (auto c : s) {            ++table[c - &#39;a&#39;];        }        for (auto c : t) {            --table[c - &#39;a&#39;];            if (table[c - &#39;a&#39;] &lt; 0) return false;        }        return true;    }};</code></pre><h2 id="leetcode-290-单词规律"><a href="#leetcode-290-单词规律" class="headerlink" title="leetcode 290 单词规律"></a>leetcode 290 单词规律</h2><p>还是将其翻译成中间表示，然后比较中间表示是否同一。</p><pre><code class="lang-cpp">#include&lt;regex&gt;#include &lt;iterator&gt;class Solution {public:    bool wordPattern(string pattern, string str) {        vector&lt;string&gt; str_array;        std::regex r(&quot;\\s+&quot;);        std::sregex_token_iterator pos(str.cbegin(), str.cend(), r, -1); // -1代表你对正则表达式匹配的内容不感兴趣        std::sregex_token_iterator end;        for (; pos != end; ++pos) {            str_array.push_back(*pos);        }        if (pattern.size() != str_array.size()) return false;        unordered_map&lt;char, int&gt; hash_char;        unordered_map&lt;string, int&gt; hash_string;        for (int i = 0; i &lt; pattern.size(); ++i) {            auto iter_char = hash_char.find(pattern[i]);            auto iter_string = hash_string.find(str_array[i]);            if (iter_char != hash_char.end() &amp;&amp; iter_string != hash_string.end()) {                if (iter_char-&gt;second != iter_string-&gt;second) return false;            } else if (iter_char == hash_char.end() &amp;&amp; iter_string == hash_string.end()) {                hash_char[pattern[i]] = i;                hash_string[str_array[i]] = i;            } else return false;        }        return true;    }};</code></pre><h2 id="leetcode-349-两个数组的交集"><a href="#leetcode-349-两个数组的交集" class="headerlink" title="leetcode 349 两个数组的交集"></a>leetcode 349 两个数组的交集</h2><p>显然是用hash。</p><pre><code class="lang-cpp">class Solution {public:    vector&lt;int&gt; intersection(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) {        if (nums1.empty() || nums2.empty()) return {};        unordered_map&lt;int, int&gt; hash;        vector&lt;int&gt; res;        for (auto c : nums1) {            hash[c] = 1;        }        for (auto c : nums2) {            auto iter = hash.find(c);            if (iter != hash.end()) hash[c] = 0;        }        for (auto iter = hash.begin(); iter != hash.end(); ++iter) {            if (iter-&gt;second == 0) res.push_back(iter-&gt;first);        }        return res;    }};</code></pre><h2 id="leetcode-350-两个数组的交集-II"><a href="#leetcode-350-两个数组的交集-II" class="headerlink" title="leetcode 350 两个数组的交集 II"></a>leetcode 350 两个数组的交集 II</h2><p>带重复元素了。由于hash本身就可以记录每个元素出现的次数，那么我们每当发现一个元素，执行的不是<code>hash[nums[i]] = 1</code>，而是<code>hash[nums[i]]++</code>。</p><pre><code class="lang-cpp">class Solution {public:    vector&lt;int&gt; intersect(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) {        if (nums1.empty() || nums2.empty()) return {};        unordered_map&lt;int, int&gt; hash;        vector&lt;int&gt; res;        for (int i = 0; i &lt; nums1.size(); ++i) {            ++hash[nums1[i]];        }        for (int i = 0; i &lt; nums2.size(); ++i) {            auto iter = hash.find(nums2[i]);            if (iter != hash.end() &amp;&amp; iter-&gt;second != 0) {                res.push_back(nums2[i]);                --iter-&gt;second;            }        }        return res;    }};</code></pre><h2 id="leetcode-451-根据字符出现频率排序"><a href="#leetcode-451-根据字符出现频率排序" class="headerlink" title="leetcode 451 根据字符出现频率排序"></a>leetcode 451 根据字符出现频率排序</h2><pre><code class="lang-cpp">class Solution {public:    string frequencySort(string s) {        if (s.empty()) return &quot;&quot;;        unordered_map&lt;char, int&gt; hash;        for (auto c : s) {            ++hash[c];        }        sort(s.begin(), s.end(), [&amp;hash](char lhs, char rhs) {            return hash[lhs] &gt; hash[rhs] || (hash[lhs] == hash[rhs] &amp;&amp; lhs &lt; rhs);        });        return s;    }};</code></pre><h2 id="leetcode-540-有序数组中的单一元素"><a href="#leetcode-540-有序数组中的单一元素" class="headerlink" title="leetcode 540 有序数组中的单一元素"></a>leetcode 540 有序数组中的单一元素</h2><pre><code class="lang-cpp">class Solution {public:    int singleNonDuplicate(vector&lt;int&gt;&amp; nums) {        if (nums.size() == 1) return nums[0];        return helper(nums, 0, nums.size()-1);    }    int helper(vector&lt;int&gt;&amp; nums, int start, int end) {        if (end == start) return nums[start];        int mid = start + (end - start) / 2;        if (nums[mid-1] == nums[mid]) { // 中点左边相同，须删除中点和左边元素            int left_len = mid - start - 1;            int right_len = end - mid;            if (left_len % 2 != 0) { // 如果删除后左边长度为奇数则递归左边                return helper(nums, start, mid-2);            } else { // 如果删除后右边长度为奇数则递归右边                return helper(nums, mid+1, end);            }        } else if (nums[mid] == nums[mid+1]) {  // 中点右边相同，须删除中点和右边元素            int left_len = mid - start;            int right_len = end - mid - 1;            if (left_len % 2 != 0) { // 如果删除后左边长度为奇数则递归左边                return helper(nums, start, mid-1);            } else { // 如果删除后右边长度为奇数则递归右边                return helper(nums, mid+2, end);            }        } else return nums[mid];    }};</code></pre><h2 id="leetcode-410-分割数组的最大值"><a href="#leetcode-410-分割数组的最大值" class="headerlink" title="leetcode 410 分割数组的最大值"></a>leetcode 410 分割数组的最大值</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;见多了优秀的文章，再写博客的时候就会感叹自己的学识浅薄。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="algorithm" scheme="https://superlova.github.io/tags/algorithm/"/>
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="leetcode" scheme="https://superlova.github.io/tags/leetcode/"/>
    
      <category term="searching" scheme="https://superlova.github.io/tags/searching/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】leetcode打卡：动态规划</title>
    <link href="https://superlova.github.io/2020/08/23/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91leetcode%E6%89%93%E5%8D%A1%EF%BC%9A%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    <id>https://superlova.github.io/2020/08/23/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91leetcode%E6%89%93%E5%8D%A1%EF%BC%9A%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</id>
    <published>2020-08-22T23:52:25.000Z</published>
    <updated>2020-08-23T07:56:40.733Z</updated>
    
    <content type="html"><![CDATA[<p>见多了优秀的文章，再写博客的时候就会感叹自己的学识浅薄。<br><a id="more"></a></p><h2 id="leetcode-198-打家劫舍"><a href="#leetcode-198-打家劫舍" class="headerlink" title="leetcode 198 打家劫舍"></a>leetcode 198 打家劫舍</h2><p>动态规划</p><p>设置<code>dp[i]</code>为前i个元素中打劫所得最高金额</p><p>构建状态转移方程：</p><pre><code class="lang-cpp">dp[i] = max(dp[i-1], dp[i-2]+nums[i]);</code></pre><p>边界条件：</p><pre><code class="lang-cpp">dp[0] = nums[0];dp[1] = max(nums[0], nums[1]);</code></pre><p>代码：</p><pre><code class="lang-cpp">class Solution {public:    int rob(vector&lt;int&gt;&amp; nums) {        if (nums.empty()) return 0;        if (nums.size() == 1) return nums[0];        vector&lt;int&gt; dp(nums.size(), 0);        dp[0] = nums[0];        dp[1] = max(nums[0], nums[1]);        for (int i = 2; i &lt; nums.size(); ++i) {            dp[i] = max(dp[i-1], dp[i-2]+nums[i]);        }        return dp[nums.size()-1];    }};</code></pre><h2 id="leetcode-674-最长连续递增子序列"><a href="#leetcode-674-最长连续递增子序列" class="headerlink" title="leetcode 674 最长连续递增子序列"></a>leetcode 674 最长连续递增子序列</h2><p>不必使用动态规划，直接一遍遍历，碰到<code>nums[i] &lt; nums[i+1]</code>就递增计数器，保留计数器最大值即可：</p><pre><code class="lang-cpp">class Solution {public:    int findLengthOfLCIS(vector&lt;int&gt;&amp; nums) {        if (nums.empty()) return 0;        int count = 1;        int maxCount = 1;        for (int i = 0; i &lt; nums.size() - 1; ++i) {            if (nums[i] &lt; nums[i+1]) {                ++count;                maxCount = max(count, maxCount);            } else {                count = 1;            }        }        return maxCount;    }};</code></pre><p>后来我悟了，这就是动态规划，只不过我利用<code>maxCount</code>来代替了dp数组。我真是个天才（误）！</p><h2 id="leetcode-5-最长回文子串"><a href="#leetcode-5-最长回文子串" class="headerlink" title="leetcode 5 最长回文子串"></a>leetcode 5 最长回文子串</h2><p>本来是想定义dp[i][j]，表达字符串从i到j的子串中的最长回文子串的。后来想想这样定义不合适，不如把dp定义为一个bool数组，用来标记从i到j子串是否为回文串即可。</p><p>状态转移关系：dp[i][j] = dp[i+1][j-1] &amp;&amp; (s[i] == s[j])</p><p>即s[i:j]为回文串的条件为s[i+1][j-1]为回文串，且s[i] == s[j]</p><p>边界条件：<br>dp[i][j] = true if i == j<br>dp[i][j] = false if i &gt; j<br>dp[i][i+1] = (s[i] == s[i+1])</p><p>最后遍历所有dp[i][j]=true的项，返回最长的子串即可</p><p>需要注意的一点：我们在遍历双层循环的时候，应该j在外，i在内。想想为什么？如果循环结构是这样的：</p><pre><code class="lang-cpp">for (int i = 0; i &lt; s.size(); ++i) {    for (int j = i+1; j &lt; s.size(); ++j) {        // TODO    }}</code></pre><p>那 i、j的变化为：<br>0,0<br>0,1<br>0,2<br>0,3-&gt;这里就不对了，因为dp[0][3]需要用到dp[1][2]的值。而i=1时的所有dp都还没求呢。</p><p>代码：</p><pre><code class="lang-cpp">class Solution {public:    string longestPalindrome(string s) {        if (s.empty()) return &quot;&quot;;        int size = s.size();        vector&lt;vector&lt;bool&gt;&gt; dp(size, vector&lt;bool&gt;(size, false));        string ans = &quot;&quot;;        for (int j = 0; j &lt; size; ++j) {            for (int i = 0; i &lt;= j; ++i) {                if (j == i) dp[i][j] = true;                else if (j == i+1) dp[i][j] = (s[i] == s[j]);                else dp[i][j] = (dp[i+1][j-1]) &amp;&amp; (s[i] == s[j]);                if (dp[i][j] &amp;&amp; ans.size() &lt; j-i+1) ans = s.substr(i, j-i+1);            }        }        return ans;    }};</code></pre><h2 id="leetcode-213-打家劫舍2"><a href="#leetcode-213-打家劫舍2" class="headerlink" title="leetcode 213 打家劫舍2"></a>leetcode 213 打家劫舍2</h2><p>打家劫舍升级版，贼不能同时打劫头尾。</p><p>也好办，拆分成两个动态规划，一个规定不能打劫nums[0]，另一个规定不能打劫nums[size-1]，最后返回更大的那个即可。</p><p>代码：</p><pre><code class="lang-cpp">class Solution {public:    int rob(vector&lt;int&gt;&amp; nums) {        if (nums.empty()) return 0;        int size = nums.size();        if (size == 1) return nums[0];        if (size == 2) return max(nums[0], nums[1]);        vector&lt;int&gt; dp_robfirst(size, 0);        vector&lt;int&gt; dp_roblast(size, 0);        dp_robfirst[0] = nums[0];        dp_robfirst[1] = max(nums[0], nums[1]);        for (int i = 2; i &lt; size-1; ++i) {            dp_robfirst[i] = max(dp_robfirst[i-1], dp_robfirst[i-2] + nums[i]);        }        dp_roblast[0] = 0;        dp_roblast[1] = nums[1];        for (int i = 2; i &lt; size; ++i) {            dp_roblast[i] = max(dp_roblast[i-1], dp_roblast[i-2] + nums[i]);        }        return max(dp_robfirst[size-2], dp_roblast[size-1]);    }};</code></pre><h2 id="leetcode-516-最长回文子序列"><a href="#leetcode-516-最长回文子序列" class="headerlink" title="leetcode 516 最长回文子序列"></a>leetcode 516 最长回文子序列</h2><p>这次的dp含义为从i到j子串中最长的回文序列长度。</p><p>转移方程：</p><p>dp[i][j] = dp[i+1][j-1] if s[i] == s[j]<br>dp[i][j] = max(dp[i+1][j], dp[i][j-1]) if s[i] != s[j]</p><p>注意，i从大遍历到小，j从小遍历到大。最后返回dp[0][size-1]</p><p>边界条件：dp[i][j] = 1 if i == j</p><p>代码：</p><pre><code class="lang-cpp">class Solution {public:    int longestPalindromeSubseq(string s) {        if (s.empty()) return 0;        int size = s.size();        vector&lt;vector&lt;int&gt;&gt; dp(size, vector&lt;int&gt;(size, 0));        for (int i = size-1; i &gt;= 0; --i) {            for (int j = i; j &lt; size; ++j) {                if (i == j) dp[i][j] = 1;                else if (s[i] == s[j]) dp[i][j] = dp[i+1][j-1] + 2;                else dp[i][j] = max(dp[i+1][j], dp[i][j-1]);            }        }        return dp[0][size-1];    }};</code></pre><h2 id="leetcode-72-编辑距离"><a href="#leetcode-72-编辑距离" class="headerlink" title="leetcode 72 编辑距离"></a>leetcode 72 编辑距离</h2><p>代码：</p><pre><code class="lang-cpp">class Solution {public:    int minDistance(string word1, string word2) {        int M = word1.size();        int N = word2.size();        // if (word1.empty() || word2.empty()) return abs(M-N);        vector&lt;vector&lt;int&gt;&gt; dp(M+1, vector&lt;int&gt;(N+1, 0));        //initial        for (int i = 0; i &lt;= M; ++i) {            dp[i][0] = i;        }        for (int i = 0; i &lt;= N; ++i) {            dp[0][i] = i;        }        //dp        for (int i = 1; i &lt;= M; ++i) {            for (int j = 1; j &lt;= N; ++j) {                if (word1[i-1] == word2[j-1]) {                    dp[i][j] = min(dp[i - 1][j - 1], 1 + dp[i - 1][j]);                    dp[i][j] = min(dp[i][j], 1 + dp[i][j - 1]);                } else {                    dp[i][j] = min(dp[i - 1][j - 1], dp[i - 1][j]);                    dp[i][j] = 1 + min(dp[i][j], dp[i][j - 1]);                }            }        }        return dp[M][N];    }};</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;见多了优秀的文章，再写博客的时候就会感叹自己的学识浅薄。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="algorithm" scheme="https://superlova.github.io/tags/algorithm/"/>
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="leetcode" scheme="https://superlova.github.io/tags/leetcode/"/>
    
      <category term="Dynamic Programming" scheme="https://superlova.github.io/tags/Dynamic-Programming/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】数据挖掘实践——时间序列模型</title>
    <link href="https://superlova.github.io/2020/08/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/"/>
    <id>https://superlova.github.io/2020/08/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-08-22T15:40:04.000Z</published>
    <updated>2020-08-22T15:43:49.737Z</updated>
    
    <content type="html"><![CDATA[<p>不要停下来啊！<br><a id="more"></a><br>AR模型是自回归模型，AutoRegression的简称。是一种较为朴素的时间序列数据处理方法，利用同一变量的前n期来预测本期的变量的数值，并且假定为线性关系。这种分析方法中，自变量不是其他的影响因素，而是变量本身的历史数据，利用xt-n来预测xt，因此被称为自回归。</p><p>MA模型：移动平均模型将序列{xt}表示为白噪声的线性加权。</p><p>在一个平稳的随机过程中，如果既有自回归的特性，又有移动平均过程的特性，则需要对两个模型进行混合使用，也即是较为普遍的ARMA模型，一般记为ARMA(p,q)。</p><p>ARIMA(p,d,q)是差分自回归移动平均模型，是运用最为广泛的一种时间序列分析模型。p,q的意义不变，其中d的含义是将时间序列化为平稳时间序列所做的差分次数。</p><p>在建立时间序列的模型后，我们要对时间序列数据进行多重检验，以确定该数据符合我们的统计学上的分析准则。主要的检验要观察数据的自相关性，阶数识别和单位根检验（ADF检验）。对于ARMA模型来说，最难的步骤是进行阶数的识别。ADF检验是时间序列中最为重要的检验之一，帮助我们准确判断时间序列数据是否平稳，是为后期的Johansen 检验、Granger 检验等的基础。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不要停下来啊！&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="Data Mining" scheme="https://superlova.github.io/tags/Data-Mining/"/>
    
      <category term="Time Series Analysis" scheme="https://superlova.github.io/tags/Time-Series-Analysis/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】数据挖掘实践——时间序列规则</title>
    <link href="https://superlova.github.io/2020/08/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E8%A7%84%E5%88%99/"/>
    <id>https://superlova.github.io/2020/08/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E8%A7%84%E5%88%99/</id>
    <published>2020-08-22T15:38:15.000Z</published>
    <updated>2020-08-22T15:43:47.259Z</updated>
    
    <content type="html"><![CDATA[<p>不要停下来啊！<br><a id="more"></a></p><p>时间序列基本规则法<br>提取时间序列的周期性特征进行预测，参考：时间序列规则法快速入门<br>观察序列，当序列存在周期性时，可以用线性回归-利用时间特征做线性回归做为baseline</p><p>时间序列是指将同一统计指标的数值按其发生的时间先后顺序排列而成的数列。</p><p>时间序列分析的主要目的是根据已有的历史数据对未来进行预测。与面板数据不同，面板数据侧重于同一时间点不同样本的数值，而时间序列侧重于同一统计指标在时间的不同点的数值。时间序列有两个重要指标，一个是资料所属的时间，另一个是时间上的统计指标数值。时间序列可以描述社会经济现象在不同时间的发展状态和过程，也可以根据历史数据进行合理的未来推测。</p><p>一般地，我们认为一个随机游走的变量会服从正态分布。</p><p>提取时间的周期性特点做为特征，此时训练集每条样本为”时间特征-&gt;目标值”，时间序列的依赖关系被剔除，不需要严格依赖滑窗截取训练样本。常见是将时间用0-1哑变量表达，有以下若干种特征：</p><ul><li>将星期转化为了0-1变量，从周一至周天，独热编码共7个变量</li><li>将节假日转化为0-1变量，视具体节假日数目，可简单分为两类，”有假日”-“无假日”，独热编码共2个变量；或赋予不同编码值，如区分国庆、春节、劳动节等使用1、2、3表示</li><li>将月初转化为0-1变量，简单分两类表示为”是月初”-“非月初”，共2个特征</li><li>类似的月中、月初可以转化为0-1变量</li><li>控制时间粒度，区分是weekday or weekend</li></ul><p>按列提取中位数是一种简单而有效的提取周期因子的方法。中位数十分鲁棒，不受极端值的影响。但中位数损失了很多信息。实践中，可以在此基础上进一步优化。比如可以提取一个均值和一个中位数，然后将均值和中位数融合。融合的比例按照测试集的表现来确定。也可以根据与预测周的时间距离来赋予不同的权重。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不要停下来啊！&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="Data Mining" scheme="https://superlova.github.io/tags/Data-Mining/"/>
    
      <category term="Time Series Analysis" scheme="https://superlova.github.io/tags/Time-Series-Analysis/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】数据挖掘实践——数据探索和分析</title>
    <link href="https://superlova.github.io/2020/08/20/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%86%E6%9E%90/"/>
    <id>https://superlova.github.io/2020/08/20/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%86%E6%9E%90/</id>
    <published>2020-08-20T15:40:07.000Z</published>
    <updated>2020-08-20T15:49:38.181Z</updated>
    
    <content type="html"><![CDATA[<p>我也是加把劲骑士！<br><a id="more"></a></p><pre><code class="lang-py">file_names = glob.glob(&#39;Purchase Redemption Data/*.csv&#39;)file_names[&#39;Purchase Redemption Data/mfd_bank_shibor.csv&#39;, &#39;Purchase Redemption Data/comp_predict_table.csv&#39;, &#39;Purchase Redemption Data/user_profile_table.csv&#39;, &#39;Purchase Redemption Data/mfd_day_share_interest.csv&#39;, &#39;Purchase Redemption Data/user_balance_table.csv&#39;]</code></pre><h2 id="1-用户信息表-user-profile-table"><a href="#1-用户信息表-user-profile-table" class="headerlink" title="1. 用户信息表 user_profile_table"></a>1. 用户信息表 user_profile_table</h2><p>我们总共随机抽取了约 3 万用户。</p><p>其中部分用户在 2014 年 9 月份第一次出现，这部分用户只在测试数据中（真的太鉴了这个）。</p><p>因此用户信息表是约 2.8 万 个用户的基本数据，在原始数据的基础上处理后，主要包含了用户的性别、城市和星座。</p><p><img src="/2020/08/20/【竞赛打卡】数据挖掘实践——数据探索和分析/2020-08-20-23-43-55.png" srcset="/img/loading.gif" alt></p><h2 id="2-用户申购赎回数据表-user-balance-table"><a href="#2-用户申购赎回数据表-user-balance-table" class="headerlink" title="2. 用户申购赎回数据表 user_balance_table"></a>2. 用户申购赎回数据表 user_balance_table</h2><p>里面有 20130701 至 20140831 申购和赎回信息、以及所有的子类目信息。</p><p>数据经过脱敏处理。脱敏之后的数据，基本保持了原数据趋势。</p><p>数据主要包括用户操作时间和操作记录，其中操作记录包括申购和赎回两个部分。</p><p>金额的单位是分，即 0.01 元人民币。 如果用户今日消费总量为0，即consume_amt=0，则四个字类目为空。</p><div class="table-container"><table><thead><tr><th>字段名称</th><th>含义</th></tr></thead><tbody><tr><td>report_date</td><td>日期</td></tr><tr><td>tBalance</td><td>今日余额</td></tr><tr><td>yBalance</td><td>昨日余额</td></tr><tr><td>total_purchase_amt</td><td>今日总购买量 = 直接购买 + 收益</td></tr><tr><td>direct_purchase_amt</td><td>今日直接购买量</td></tr><tr><td>purchase_bal_amt</td><td>今日支付宝余额购买量</td></tr><tr><td>purchase_bank_amt</td><td>今日银行卡购买量</td></tr><tr><td>total_redeem_amt</td><td>今日总赎回量 = 消费 + 转出</td></tr><tr><td>consume_amt</td><td>今日消费总量</td></tr><tr><td>transfer_amt</td><td>今日转出总量</td></tr><tr><td>tftobal_amt</td><td>今日转出到支付宝余额总量</td></tr><tr><td>tftocard_amt</td><td>今日转出到银行卡总量</td></tr><tr><td>share_amt</td><td>今日收益</td></tr><tr><td>category1</td><td>今日类目 1 消费总额</td></tr><tr><td>category2</td><td>今日类目 2 消费总额</td></tr><tr><td>category3</td><td>今日类目 3 消费总额</td></tr><tr><td>category4</td><td>今日类目 4 消费总额</td></tr></tbody></table></div><p><img src="/2020/08/20/【竞赛打卡】数据挖掘实践——数据探索和分析/2020-08-20-23-47-13.png" srcset="/img/loading.gif" alt></p><p>注 1 ：上述的数据都是经过脱敏处理的，收益为重新计算得到的，计算方法按照简化后的计算方式处理，具体计算方式在下节余额宝收益计算方式中描述。</p><p>注 2 ：脱敏后的数据保证了今日余额 = 昨日余额 + 今日申购 - 今日赎回，不会出现负值。</p><h2 id="3-收益率表-mfd-day-share-interest"><a href="#3-收益率表-mfd-day-share-interest" class="headerlink" title="3. 收益率表 mfd_day_share_interest"></a>3. 收益率表 mfd_day_share_interest</h2><p>收益表为余额宝在 14 个月内的收益率表 。</p><p><img src="/2020/08/20/【竞赛打卡】数据挖掘实践——数据探索和分析/2020-08-20-23-47-55.png" srcset="/img/loading.gif" alt></p><div class="table-container"><table><thead><tr><th>列名</th><th>含义</th></tr></thead><tbody><tr><td>mfd_date</td><td>日期</td></tr><tr><td>mfd_daily_yield</td><td>万份收益，即 1 万块钱的收益。</td></tr><tr><td>mfd_7daily_yield</td><td>七日年化收益率（ % ）</td></tr></tbody></table></div><h2 id="4-上海银行间同业拆放利率（Shibor）表-df-mfd-bank-shibor"><a href="#4-上海银行间同业拆放利率（Shibor）表-df-mfd-bank-shibor" class="headerlink" title="4. 上海银行间同业拆放利率（Shibor）表 df_mfd_bank_shibor"></a>4. 上海银行间同业拆放利率（Shibor）表 df_mfd_bank_shibor</h2><p>银行间拆借利率表是 14 个月期间银行之间的拆借利率（皆为年化利率）</p><p><img src="/2020/08/20/【竞赛打卡】数据挖掘实践——数据探索和分析/2020-08-20-23-48-31.png" srcset="/img/loading.gif" alt></p><div class="table-container"><table><thead><tr><th>列名</th><th>类型</th><th>含义</th><th>示例</th></tr></thead><tbody><tr><td>mfd_date</td><td>String</td><td>日期</td><td>20140102</td></tr><tr><td>Interest_O_N</td><td>Double</td><td>隔夜利率（%）</td><td>2.8</td></tr><tr><td>Interest_1_W</td><td>Double</td><td>1周利率（%）</td><td>4.25</td></tr><tr><td>Interest_2_W</td><td>Double</td><td>2周利率（%）</td><td>4.9</td></tr><tr><td>Interest_1_M</td><td>Double</td><td>1个月利率（%）</td><td>5.04</td></tr><tr><td>Interest_3_M</td><td>Double</td><td>3个月利率（%）</td><td>4.91</td></tr><tr><td>Interest_6_M</td><td>Double</td><td>6个月利率（%）</td><td>4.79</td></tr><tr><td>Interest_9_M</td><td>Double</td><td>9个月利率（%）</td><td>4.76</td></tr><tr><td>Interest_1_Y</td><td>Double</td><td>1年利率（%）</td><td>4.78</td></tr></tbody></table></div><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><h2 id="收益计算方式"><a href="#收益计算方式" class="headerlink" title="收益计算方式"></a>收益计算方式</h2><p>本赛题的余额宝收益方式，主要基于实际余额宝收益计算方法，但是进行了一定的简化，此处计算简化的地方如下：</p><p>首先，收益计算的时间不再是会计日，而是自然日，以 0 点为分隔，如果是 0 点之前转入或者转出的金额算作昨天的，如果是 0 点以后转入或者转出的金额则算作今天的。</p><p>然后，收益的显示时间，即实际将第一份收益打入用户账户的时间为如下表格，以周一转入周三显示为例，如果用户在周一存入 10000 元，即 1000000 分，那么这笔金额是周一确认，周二是开始产生收益，用户的余额还是 10000 元，在周三将周二产生的收益打入到用户的账户中，此时用户的账户中显示的是 10001.1 元，即 1000110 分。其他时间的计算按照表格中的时间来计算得到。</p><div class="table-container"><table><thead><tr><th>转入时间</th><th>首次显示收益时间</th></tr></thead><tbody><tr><td>周一</td><td>周三</td></tr><tr><td>周二</td><td>周四</td></tr><tr><td>周三</td><td>周五</td></tr><tr><td>周四</td><td>周六</td></tr><tr><td>周五</td><td>下周二</td></tr><tr><td>周六</td><td>下周三</td></tr><tr><td>周天</td><td>下周三</td></tr></tbody></table></div><h2 id="提交格式"><a href="#提交格式" class="headerlink" title="提交格式"></a>提交格式</h2><div class="table-container"><table><thead><tr><th>字段</th><th>类型</th><th>含义</th><th>示例</th></tr></thead><tbody><tr><td>report_date</td><td>bigint</td><td>日期</td><td>20140901</td></tr><tr><td>purchase</td><td>bigint</td><td>申购总额</td><td>40000000</td></tr><tr><td>redeem</td><td>bigint</td><td>赎回总额</td><td>30000000</td></tr></tbody></table></div><p><img src="/2020/08/20/【竞赛打卡】数据挖掘实践——数据探索和分析/2020-08-20-23-49-17.png" srcset="/img/loading.gif" alt></p><p>每一行数据是一天对申购、赎回总额的预测值， 2014 年 9 月每天一行数据，共 30 行数据。 Purchase 和 redeem 都是金额数据，精确到分，而不是精确到元。</p><p>评分数据格式要求与“选手结果数据样例文件”一致，结果表命名为：tc_comp_predict_table， 字段之间以逗号为分隔符</p><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><p>评估指标的设计主要期望选手对未来 30 天内每一天申购和赎回的总量数据预测的越准越好，同时考虑到可能存在的多种情况。</p><p>譬如有些选手在 30 天中 29 天预测都是非常精准的但是某一天预测的结果可能误差很大，而有些选手在 30 天中每天的预测都不是很精准误差较大，如果采用绝对误差则可能导致前者的成绩比后者差，而在实际业务中可能更倾向于前者。</p><p>所以最终选用积分式的计算方法：每天的误差选用相对误差来计算，然后根据用户预测申购和赎回的相对误差，通过得分函数映射得到一个每天预测结果的得分，将 30 天内的得分汇总，然后结合实际业务的倾向，对申购赎回总量预测的得分情况进行加权求和，得到最终评分。具体的操作如下：</p><p>1) 计算所有用户在测试集上每天的申购及赎回总额与实际情况总额的误差。</p><p><img src="https://gtms02.alicdn.com/tps/i2/TB1UMhaHVXXXXbWXpXXmP_sPXXX-372-168.png" srcset="/img/loading.gif" alt></p><p>2) 申购预测得分与 Purchasei 相关，赎回预测得分与 Redeemi 相关 , 误差与得分之间的计算公式不公布，但保证该计算公式为单调递减的，即误差越小，得分越高，误差与大，得分越低。当第 i 天的申购误差 Purchasei =0 ，这一天的得分为 10 分；当 Purchasei &gt; 0.3 ，其得分为 0 。</p><p>3) 最后公布总积分 = 申购预测得分 <em>45%+ 赎回预测得分 </em>55% 。 </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我也是加把劲骑士！&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="Data Mining" scheme="https://superlova.github.io/tags/Data-Mining/"/>
    
      <category term="Time Series Analysis" scheme="https://superlova.github.io/tags/Time-Series-Analysis/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】Fuzzing学习笔记3——灰盒Fuzzing</title>
    <link href="https://superlova.github.io/2020/08/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03%E2%80%94%E2%80%94%E7%81%B0%E7%9B%92Fuzzing/"/>
    <id>https://superlova.github.io/2020/08/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03%E2%80%94%E2%80%94%E7%81%B0%E7%9B%92Fuzzing/</id>
    <published>2020-08-19T17:01:27.000Z</published>
    <updated>2020-08-19T17:04:57.391Z</updated>
    
    <content type="html"><![CDATA[<p>灰盒变异的模糊测试。<br><a id="more"></a></p><p>普通fuzzing：从0开始构造测试用例<br>突变fuzzing：从seed开始构造测试用例<br>灰盒fuzzing：有引导的突变fuzzing</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>上回书说到，AFL是突变fuzzer，通过把种子字符串进行些微修改，得到变异体；此外AFL还会将一个种子的前半部分与另一个种子的后半部分连接，形成变异体。</p><p>AFL也是个灰盒fuzzer，这是由于AFL需要使用程序内部信息（即覆盖率）。AFL不是白盒，因为AFL没有对程序进行约束求解、程序分析之类的，只是简单获取了一个覆盖率。如果生成的样本能够提升覆盖率，那么就将这个样本添加进种子队列以供下次突变使用（这就意味着突变体有重复突变的可能）。</p><p>AFL计算覆盖率的方法，是通过在每个分支的跳转指令后执行一段标记代码。这样就可以做到，监控每个输入导致的激活分支，以及每个分支被激活的大概频率。注入代码这个环节通常在编译时完成。对于Python，可以在未经处理的情况下执行覆盖率信息收集。</p><h2 id="突变算法和种子"><a href="#突变算法和种子" class="headerlink" title="突变算法和种子"></a>突变算法和种子</h2><p>引入Mutator类。Mutator类是个过程类，包装了对输入inp的突变方法。</p><pre><code class="lang-py">class Mutator(object):    def __init__(self):        self.mutators = [            self.delete_random_character,            self.insert_random_character,            self.flip_random_character    def insert_random_character(self,s):        &quot;&quot;&quot;Returns s with a random character inserted&quot;&quot;&quot;        pos = random.randint(0, len(s))        random_character = chr(random.randrange(32, 127))        return s[:pos] + random_character + s[pos:]    def delete_random_character(self,s):        &quot;&quot;&quot;Returns s with a random character deleted&quot;&quot;&quot;        if s == &quot;&quot;:            return self.insert_random_character(s)        pos = random.randint(0, len(s) - 1)        return s[:pos] + s[pos + 1:]    def flip_random_character(self,s):        &quot;&quot;&quot;Returns s with a random bit flipped in a random position&quot;&quot;&quot;        if s == &quot;&quot;:            return self.insert_random_character(s)        pos = random.randint(0, len(s) - 1)        c = s[pos]        bit = 1 &lt;&lt; random.randint(0, 6)        new_c = chr(ord(c) ^ bit)        return s[:pos] + new_c + s[pos + 1:]    def mutate(self, inp):        &quot;&quot;&quot;Return s with a random mutation applied&quot;&quot;&quot;        mutator = random.choice(self.mutators)        return mutator(inp)</code></pre><p>使用Mutator是只需实例化Mutator，然后调用mutate()方法即可。</p><pre><code class="lang-py">Mutator().mutate(&quot;good&quot;)&#39;cood&#39;</code></pre><h2 id="精力分配（Power-Schedule）"><a href="#精力分配（Power-Schedule）" class="headerlink" title="精力分配（Power Schedule）"></a>精力分配（Power Schedule）</h2><p>模糊测试是一种执行很慢的测试方法。既然并不是每个测试用例种子都值得分配同样的精力，那么试图发现那些更令人感兴趣的种子就是理所当然的选择了。</p><p>我们把一个种子从种群中被选中的可能性称为种子的能量（energy）。我们希望优先突变和执行那些更有希望发现待测程序错误的种子，不希望在无进步的种子身上浪费精力。</p><p>决定种子能量分配的算法称为“功率表”（Power Schedule）。AFL的功率表会将更多的能量分配给那些长度较短、执行速度较快、覆盖率增加较多的种子。</p><p>由此，每个种子需要额外维护其能量。构建Seed类如下：</p><pre><code class="lang-py">class Seed(object):        def __init__(self, data):        &quot;&quot;&quot;Set seed data&quot;&quot;&quot;        self.data = data    def __str__(self):        &quot;&quot;&quot;Returns data as string representation of the seed&quot;&quot;&quot;        return self.data    __repr__ = __str__</code></pre><p>下面是功率表PowerSchedule类的定义：</p><pre><code class="lang-py">class PowerSchedule(object):        def assignEnergy(self, population):        &quot;&quot;&quot;Assigns each seed the same energy&quot;&quot;&quot;        for seed in population:            seed.energy = 1    def normalizedEnergy(self, population):        &quot;&quot;&quot;Normalize energy&quot;&quot;&quot;        energy = list(map(lambda seed: seed.energy, population))        sum_energy = sum(energy)  # Add up all values in energy        norm_energy = list(map(lambda nrg: nrg/sum_energy, energy))        return norm_energy    def choose(self, population):        &quot;&quot;&quot;Choose weighted by normalized energy.&quot;&quot;&quot;        import numpy as np        self.assignEnergy(population)        norm_energy = self.normalizedEnergy(population)        seed = np.random.choice(population, p=norm_energy)        return seed</code></pre><h2 id="灰盒fuzzing与黑盒fuzzing的比较"><a href="#灰盒fuzzing与黑盒fuzzing的比较" class="headerlink" title="灰盒fuzzing与黑盒fuzzing的比较"></a>灰盒fuzzing与黑盒fuzzing的比较</h2><p>首先定义不使用coverage的黑盒fuzzer，MutationFuzzer 类：</p><pre><code class="lang-py">class MutationFuzzer(Fuzzer):    def __init__(self, seeds, mutator, schedule):        self.seeds = seeds        self.mutator = mutator        self.schedule = schedule        self.inputs = []        self.reset()    def reset(self):        &quot;&quot;&quot;Reset the initial population and seed index&quot;&quot;&quot;        self.population = list(map(lambda x: Seed(x), self.seeds))        self.seed_index = 0    def create_candidate(self):        &quot;&quot;&quot;Returns an input generated by fuzzing a seed in the population&quot;&quot;&quot;        seed = self.schedule.choose(self.population)        # Stacking: Apply multiple mutations to generate the candidate        candidate = seed.data        trials = min(len(candidate), 1 &lt;&lt; random.randint(1,5))        for i in range(trials):            candidate = self.mutator.mutate(candidate)        return candidate    def fuzz(self):        &quot;&quot;&quot;Returns first each seed once and then generates new inputs&quot;&quot;&quot;        if self.seed_index &lt; len(self.seeds):            # Still seeding            self.inp = self.seeds[self.seed_index]            self.seed_index += 1        else:            # Mutating            self.inp = self.create_candidate()        self.inputs.append(self.inp)        return self.inp</code></pre><p>MutationFuzzer 是由一组初始种子、一个突变器和一个功率表构成的。在整个模糊化过程中，它维护着一个名为population的种子语料库。create_candidate对某个种子执行多次突变，fuzz先试图返回正常种子，随后返回突变种子。</p><p>population_coverage 是预先定义好的覆盖率计算库，返回（all_coverage，cumulative_coverage）。其中all_coverage是所有输入所覆盖的语句集，cumulative_coverage是随着执行输入数量的增加而覆盖的语句数量。</p><p>下面是GrayBox fuzzing的实现：</p><pre><code class="lang-py">class GreyboxFuzzer(MutationFuzzer):        def reset(self):        &quot;&quot;&quot;Reset the initial population, seed index, coverage information&quot;&quot;&quot;        super().reset()        self.coverages_seen = set()        self.population = [] # population is filled during greybox fuzzing    def run(self, runner):        &quot;&quot;&quot;Run function(inp) while tracking coverage.           If we reach new coverage,           add inp to population and its coverage to population_coverage        &quot;&quot;&quot;        result, outcome = super().run(runner)        new_coverage = frozenset(runner.coverage())        if new_coverage not in self.coverages_seen:            # We have new coverage            seed = Seed(self.inp)            seed.coverage = runner.coverage()            self.coverages_seen.add(new_coverage)            self.population.append(seed)        return (result, outcome)</code></pre><p>经过计算，分别得到覆盖率变化趋势blackbox_coverage和greybox_coverage，可视化如下：</p><p><img src="/2020/08/20/【学习笔记】Fuzzing学习笔记3——灰盒Fuzzing/2020-08-20-00-42-27.png" srcset="/img/loading.gif" alt></p><p>可以看到，灰盒fuzzing的覆盖率增长明显比黑盒要好。</p><h2 id="增强后的灰盒fuzzer"><a href="#增强后的灰盒fuzzer" class="headerlink" title="增强后的灰盒fuzzer"></a>增强后的灰盒fuzzer</h2><p>通过修改功率表PowerSchedule，使那些能激活不寻常的path的input具有更高的energy。不寻常的path指的是激活次数比较小。</p><p>有多种方法计算一个种子的能量。上述的要求形式化为具体定义即为</p><script type="math/tex; mode=display">e(s)=\frac{1}{f(p(s))^a}</script><p>其中$s$是种子<br>$p(s)$为$s$激活的path<br>$f(p)$返回path激活的次数<br>$a$是给定的超参数<br>$e(s)$是种子$s$被分配的能量</p><p>下面是按照此思想设置的PowerSchedule：</p><pre><code class="lang-py">class AFLFastSchedule(PowerSchedule):     def __init__(self, exponent):        self.exponent = exponent    def assignEnergy(self, population):        &quot;&quot;&quot;Assign exponential energy inversely proportional to path frequency&quot;&quot;&quot;        for seed in population:            seed.energy = 1 / (self.path_frequency[getPathID(seed.coverage)] ** self.exponent)</code></pre><p>改进的灰盒Fuzzer：</p><pre><code class="lang-py">class CountingGreyboxFuzzer(GreyboxFuzzer):    def reset(self):        &quot;&quot;&quot;Reset path frequency&quot;&quot;&quot;        super().reset()        self.schedule.path_frequency = {}    def run(self, runner):        &quot;&quot;&quot;Inform scheduler about path frequency&quot;&quot;&quot;        result, outcome = super().run(runner)        path_id = getPathID(runner.coverage())        if not path_id in self.schedule.path_frequency:            self.schedule.path_frequency[path_id] = 1        else:            self.schedule.path_frequency[path_id] += 1        return(result, outcome)</code></pre><p>覆盖率变化如图所示</p><p><img src="/2020/08/20/【学习笔记】Fuzzing学习笔记3——灰盒Fuzzing/2020-08-20-00-52-08.png" srcset="/img/loading.gif" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>三篇Fuzzing文章到此为止。</p><p>Fuzzing 的方法是通过大量生成input，来找出被测程序的错误的方法。</p><p>Fuzzing的关键点之一在于input生成方法，其二在于input的排序方法，其三在于软件内部信息的获取和应用。</p><p>如果input完全是自己构建的，那么这种方法称之为generational fuzzing</p><p>如果input是通过原始种子略微修改后得到的，那么这种fuzzing为Mutational fuzzing。</p><p>如果Mutator可以经过一定的程序信息的引导，那么这叫做GrayBox Fuzzing，比如覆盖率引导的模糊测试</p><p>如果Seed经过Power Schedule的精力分配，随后Mutator根据Seed的精力大小排序，那么这种方法称之为Boosted GrayBox Fuzzing</p><p>使用到的类：</p><p>Runner：待测程序的基类</p><p>Fuzzer：模糊测试器的基类</p><p>Seed：测试用例种子的基类</p><p>PowerSchedule：功率表的基类</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;灰盒变异的模糊测试。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Fuzzing" scheme="https://superlova.github.io/tags/Fuzzing/"/>
    
      <category term="Testing" scheme="https://superlova.github.io/tags/Testing/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】Fuzzing学习笔记2——基于变异的Fuzzing</title>
    <link href="https://superlova.github.io/2020/08/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E5%8F%98%E5%BC%82%E7%9A%84Fuzzing/"/>
    <id>https://superlova.github.io/2020/08/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E5%8F%98%E5%BC%82%E7%9A%84Fuzzing/</id>
    <published>2020-08-19T15:48:14.000Z</published>
    <updated>2020-08-19T15:50:49.438Z</updated>
    
    <content type="html"><![CDATA[<p>基于变异的模糊测试。<br><a id="more"></a></p><p><a href="https://www.fuzzingbook.org/html/MutationFuzzer.html" target="_blank" rel="noopener">https://www.fuzzingbook.org/html/MutationFuzzer.html</a></p><p>大多数随机生成的输入在语法上都是无效的，程序很快就会检测并拒绝这些输入，这样便达不到深入测试Runner内部的目的。因此我们必须试图生成有效的输入。</p><p>本节我们介绍Mutation Fuzzing，这种基于变异的方法对现有输入进行小的更改，这些更改可能仍使输入保持有效，但仍会表现出新的行为。 </p><p>要对字符串形式的输入进行变异（Mutate），具体来说，就是执行随机插入字符、删除字符、修改字符等操作。Mutational fuzzing的特点是基于一个有效的原始输入，与之前凭空捏造出来一个input的generational fuzzing不同。</p><p>随机删除</p><pre><code class="lang-py">def delete_random_character(s):    &quot;&quot;&quot;Returns s with a random character deleted&quot;&quot;&quot;    if s == &quot;&quot;:        return s    pos = random.randint(0, len(s) - 1)    # print(&quot;Deleting&quot;, repr(s[pos]), &quot;at&quot;, pos)    return s[:pos] + s[pos + 1:]seed_input = &quot;A quick brown fox&quot;for i in range(10):    x = delete_random_character(seed_input)    print(repr(x))&#39;A uick brown fox&#39;&#39;A quic brown fox&#39;&#39;A quick brown fo&#39;&#39;A quic brown fox&#39;&#39;A quick bown fox&#39;&#39;A quick bown fox&#39;&#39;A quick brown fx&#39;&#39;A quick brown ox&#39;&#39;A quick brow fox&#39;&#39;A quic brown fox&#39;</code></pre><p>随机插入</p><pre><code class="lang-py">def insert_random_character(s):    &quot;&quot;&quot;Returns s with a random character inserted&quot;&quot;&quot;    pos = random.randint(0, len(s))    random_character = chr(random.randrange(32, 127))    # print(&quot;Inserting&quot;, repr(random_character), &quot;at&quot;, pos)    return s[:pos] + random_character + s[pos:]for i in range(10):    print(repr(insert_random_character(seed_input)))&#39;A quick brvown fox&#39;&#39;A quwick brown fox&#39;&#39;A qBuick brown fox&#39;&#39;A quick broSwn fox&#39;&#39;A quick brown fvox&#39;&#39;A quick brown 3fox&#39;&#39;A quick brNown fox&#39;&#39;A quick brow4n fox&#39;&#39;A quick brown fox8&#39;&#39;A equick brown fox&#39;</code></pre><p>随机替换</p><pre><code class="lang-py">def flip_random_character(s):    &quot;&quot;&quot;Returns s with a random bit flipped in a random position&quot;&quot;&quot;    if s == &quot;&quot;:        return s    pos = random.randint(0, len(s) - 1)    c = s[pos]    bit = 1 &lt;&lt; random.randint(0, 6)    new_c = chr(ord(c) ^ bit)    # print(&quot;Flipping&quot;, bit, &quot;in&quot;, repr(c) + &quot;, giving&quot;, repr(new_c))    return s[:pos] + new_c + s[pos + 1:]for i in range(10):    print(repr(flip_random_character(seed_input)))&#39;A quick bRown fox&#39;&#39;A quici brown fox&#39;&#39;A&quot;quick brown fox&#39;&#39;A quick brown$fox&#39;&#39;A quick bpown fox&#39;&#39;A quick brown!fox&#39;&#39;A 1uick brown fox&#39;&#39;@ quick brown fox&#39;&#39;A quic+ brown fox&#39;&#39;A quick bsown fox&#39;</code></pre><p>只要我们有一些原始输入，这些输入是有效的，那么我们基于原始输入的变异也应该是有效的。</p><p>多重变异</p><p>假设我们这里有个方法mutate()，能对字符串执行变异操作。那么连续变异50次，输入会变成什么样子？</p><pre><code class="lang-py">seed_input = &quot;http://www.google.com/search?q=fuzzing&quot;mutations = 50inp = seed_inputfor i in range(mutations):    if i % 5 == 0:        print(i, &quot;mutations:&quot;, repr(inp))    inp = mutate(inp)0 mutations: &#39;http://www.google.com/search?q=fuzzing&#39;5 mutations: &#39;http:/L/www.googlej.com/seaRchq=fuz:ing&#39;10 mutations: &#39;http:/L/www.ggoWglej.com/seaRchqfu:in&#39;15 mutations: &#39;http:/L/wwggoWglej.com/seaR3hqf,u:in&#39;20 mutations: &#39;htt://wwggoVgle&quot;j.som/seaR3hqf,u:in&#39;25 mutations: &#39;htt://fwggoVgle&quot;j.som/eaRd3hqf,u^:in&#39;30 mutations: &#39;htv://&gt;fwggoVgle&quot;j.qom/ea0Rd3hqf,u^:i&#39;35 mutations: &#39;htv://&gt;fwggozVle&quot;Bj.qom/eapRd[3hqf,u^:i&#39;40 mutations: &#39;htv://&gt;fwgeo6zTle&quot;Bj.\&#39;qom/eapRd[3hqf,tu^:i&#39;45 mutations: &#39;htv://&gt;fwgeo]6zTle&quot;BjM.\&#39;qom/eaR[3hqf,tu^:i&#39;</code></pre><p>可以看到变异体已经几乎无法识别了。我们通过多次变异，获得了更加多样的输入。</p><p>MutationFuzzer的实现</p><pre><code class="lang-py">class MutationFuzzer(Fuzzer):    def __init__(self, seed, min_mutations=2, max_mutations=10):        self.seed = seed        self.min_mutations = min_mutations        self.max_mutations = max_mutations        self.reset()    def reset(self):        self.population = self.seed        self.seed_index = 0    def mutate(self, inp):        return mutate(inp)    def create_candidate(self):        candidate = random.choice(self.population)        trials = random.randint(self.min_mutations, self.max_mutations)        for i in range(trials):            candidate = self.mutate(candidate)        return candidate    def fuzz(self):        if self.seed_index &lt; len(self.seed):            # Still seeding            self.inp = self.seed[self.seed_index]            self.seed_index += 1        else:            # Mutating            self.inp = self.create_candidate()        return self.inp</code></pre><p><code>create_candidate()</code>随机选取种子<code>candidate</code>，然后将这个种子随机突变<code>trials</code>次，返回经过多次突变的<code>candidate</code>。</p><p><code>fuzz()</code>方法一开始返回的是未经突变的种子样本，当种子挑选完毕后，返回突变样本。这样可以确保每次调用fuzz()，得到的输出是不一样的。</p><p>Mutational Fuzzing成功的关键在于引导这些突变的方法—即保留那些特别有价值的样本。</p><p>覆盖率引导</p><p>我们可以利用被测程序来引导测试用例生成。以前我们只是收集程序执行成功或者失败的信息，现在我们可以收集多点信息，比如运行时代码覆盖率。</p><p>利用覆盖率引导变异的Fuzzing，最成功的实践是<a href="http://lcamtuf.coredump.cx/afl/" target="_blank" rel="noopener">American fuzzy loop</a>，即AFL。</p><p>AFL会生成“成功”的测试用例。AFL认为，所谓“成功”是指找到了一条新的程序执行路径。AFL不断地突变新路径的输入，如果产生了新的路径，输入会保留下来。</p><p>为了获得程序运行时的覆盖率信息，我们需要重新定义Runner。FunctionRunner类负责包装一个被测函数。</p><pre><code class="lang-py">class FunctionRunner(Runner):    def __init__(self, function):        &quot;&quot;&quot;Initialize.  `function` is a function to be executed&quot;&quot;&quot;        self.function = function    def run_function(self, inp):        return self.function(inp)    def run(self, inp):        try:            result = self.run_function(inp)            outcome = self.PASS        except Exception:            result = None            outcome = self.FAIL        return result, outcome</code></pre><p>而FunctionCoverageRunner在此基础上增加了覆盖率计算模块<code>Coverage</code>。</p><pre><code class="lang-py">class FunctionCoverageRunner(FunctionRunner):    def run_function(self, inp):        with Coverage() as cov:            try:                result = super().run_function(inp)            except Exception as exc:                self._coverage = cov.coverage()                raise exc        self._coverage = cov.coverage()        return result    def coverage(self):        return self._coverage</code></pre><p>下面改写Fuzzer类。</p><pre><code class="lang-py">class MutationCoverageFuzzer(MutationFuzzer):    def reset(self):        super().reset()        self.coverages_seen = set()        # Now empty; we fill this with seed in the first fuzz runs        self.population = []    def run(self, runner):        &quot;&quot;&quot;Run function(inp) while tracking coverage.           If we reach new coverage,           add inp to population and its coverage to population_coverage        &quot;&quot;&quot;        result, outcome = super().run(runner)        new_coverage = frozenset(runner.coverage())        if outcome == Runner.PASS and new_coverage not in self.coverages_seen:            # We have new coverage            self.population.append(self.inp)            self.coverages_seen.add(new_coverage)        return result</code></pre><p><code>MutationCoverageFuzzer</code>内部保存测试用例队列<code>population</code>和覆盖率队列<code>coverages_seen</code>。如果fuzz的input产生了新的coverage，则将该input添加到population中，并将该coverage添加到coverage_seen中。</p><p>由此，我们得到的population中的每个input都能够使得程序产生不同的coverage，这背后可能是程序的不同执行路径，也就增加了inputs的多样性。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;基于变异的模糊测试。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Fuzzing" scheme="https://superlova.github.io/tags/Fuzzing/"/>
    
      <category term="Testing" scheme="https://superlova.github.io/tags/Testing/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】Fuzzing学习笔记1——认识Fuzzing的基本单元</title>
    <link href="https://superlova.github.io/2020/08/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E8%AE%A4%E8%AF%86Fuzzing%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8D%95%E5%85%83/"/>
    <id>https://superlova.github.io/2020/08/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E8%AE%A4%E8%AF%86Fuzzing%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8D%95%E5%85%83/</id>
    <published>2020-08-19T12:10:10.000Z</published>
    <updated>2020-08-19T15:50:15.044Z</updated>
    
    <content type="html"><![CDATA[<p>Fuzzing又称模糊测试。<br><a id="more"></a></p><p><a href="https://www.fuzzingbook.org/html/Fuzzer.html" target="_blank" rel="noopener">https://www.fuzzingbook.org/html/Fuzzer.html</a></p><h2 id="Fuzzing测试是什么？"><a href="#Fuzzing测试是什么？" class="headerlink" title="Fuzzing测试是什么？"></a>Fuzzing测试是什么？</h2><blockquote><p>Create random inputs, and see if they break things.</p></blockquote><p>说得简单、纯粹点，Fuzzing是一种软件测试方法，通过不断生成不同的输入，使被测程序崩溃、出错，以此改进程序本身的软件测试方法。由于Fuzzing的核心在于生成软件测试用例，因此这种方法又被称为<strong>生成软件测试</strong>。</p><h2 id="构建第一个fuzzer"><a href="#构建第一个fuzzer" class="headerlink" title="构建第一个fuzzer"></a>构建第一个fuzzer</h2><pre><code class="lang-py">import randomdef fuzzer(max_length=100, char_start=32, char_range=32):    &quot;&quot;&quot;A string of up to `max_length` characters       in the range [`char_start`, `char_start` + `char_range`]&quot;&quot;&quot;    string_length = random.randrange(0, max_length + 1)    out = &quot;&quot;    for i in range(0, string_length):        out += chr(random.randrange(char_start, char_start + char_range))    return outfuzzer()4$)&gt;(,-&amp;!$25;&gt;6=27= 5)9?300(.466&amp;(&#39;$*,,1:8&#39; ,$/99&gt;&#39;*=(</code></pre><p>此fuzzer的作用是生成一堆随机字符。要想只生成26个字母，那么</p><pre><code class="lang-py">fuzzer(100, ord(&#39;a&#39;), 26)ueffzgwltwmspvmowihhtjmgsixofnvntnqmr</code></pre><h2 id="Fuzzing关键的两个部件："><a href="#Fuzzing关键的两个部件：" class="headerlink" title="Fuzzing关键的两个部件："></a>Fuzzing关键的两个部件：</h2><h3 id="Fuzzer类"><a href="#Fuzzer类" class="headerlink" title="Fuzzer类"></a>Fuzzer类</h3><p><code>Fuzzer</code>，是所有<code>fuzzer</code>的基类，<code>RandomFuzzer</code> 是其简单实现。<code>Fuzzer</code> 的<code>fuzz()</code>接口返回一个字符串，字符串内容是根据不同实现逻辑而构造出来的。</p><p>比如Fuzzer的实现RandomFuzzer，其<code>fuzz()</code>就是随机生成的字符串。</p><pre><code class="lang-py">&gt;&gt;&gt; random_fuzzer = RandomFuzzer(min_length=10, max_length=20, char_start=65, char_range=26)&gt;&gt;&gt; random_fuzzer.fuzz()&#39;XGZVDDPZOOW&#39;</code></pre><p>Fuzzer的run()接口负责运行一个Runner对象。</p><p>下面是Fuzzer的代码架构：</p><pre><code class="lang-py">class Fuzzer(object):    def __init__(self):        pass    def fuzz(self):        &quot;&quot;&quot;Return fuzz input&quot;&quot;&quot;        return &quot;&quot;    def run(self, runner=Runner()):        &quot;&quot;&quot;Run `runner` with fuzz input&quot;&quot;&quot;        return runner.run(self.fuzz())    def runs(self, runner=PrintRunner(), trials=10):        &quot;&quot;&quot;Run `runner` with fuzz input, `trials` times&quot;&quot;&quot;        # Note: the list comprehension below does not invoke self.run() for subclasses        # return [self.run(runner) for i in range(trials)]        outcomes = []        for i in range(trials):            outcomes.append(self.run(runner))        return outcomes</code></pre><p>此时Fuzzer基类的fuzz()接口还没有功能。派生类RandomFuzzer则实现了fuzz()：</p><pre><code class="lang-py">class RandomFuzzer(Fuzzer):    def __init__(self, min_length=10, max_length=100,                 char_start=32, char_range=32):        &quot;&quot;&quot;Produce strings of `min_length` to `max_length` characters           in the range [`char_start`, `char_start` + `char_range`]&quot;&quot;&quot;        self.min_length = min_length        self.max_length = max_length        self.char_start = char_start        self.char_range = char_range    def fuzz(self):        string_length = random.randrange(self.min_length, self.max_length + 1)        out = &quot;&quot;        for i in range(0, string_length):            out += chr(random.randrange(self.char_start,                                        self.char_start + self.char_range))        return out</code></pre><p>有了RandomFuzzer，我们可以生成一些随机的字符串了。</p><pre><code class="lang-py">random_fuzzer = RandomFuzzer(min_length=20, max_length=20)for i in range(10):    print(random_fuzzer.fuzz())&#39;&gt;23&gt;33)(&amp;&quot;09.377.*3*+:5 ? (?1$4&lt;&gt;!?3&gt;.&#39;4+3/(3 (0%!&gt;!(+9%,#$/51$2964&gt;;)2417&lt;9&quot;2&amp;907.. !7:&amp;--&quot;=$7&#39;,7*(5=5&#39;.!*+&amp;&gt;&quot;)6%9)=,/?:&amp;5) &quot;;.0!=6&gt;3+&gt;)=,6&amp;,?:!#2))- ?:)=63&#39;-,)9#839%)?&amp;(0&lt;6(&quot;*;)4?!(49+8=-&#39;&amp;499%?&lt; &#39;</code></pre><h3 id="Runner类"><a href="#Runner类" class="headerlink" title="Runner类"></a>Runner类</h3><p><code>Runner</code>，是所有待测程序的基类。一个Fuzzer 与一个Runner搭配。</p><p>Runner类含有run(input)接口，负责接收input并执行，返回 (result, outcome)，result是Runner在运行时的信息和细节，而outcom代表着这次运行的结果。</p><p>运行结果为枚举对象 outcome，含义为程序运行结果，有(PASS, FAIL, or UNRESOLVED)三种可能。</p><ul><li>Runner.PASS：测试通过，run()输出正确。</li><li>Runner.FAIL：测试失败，结果错误。</li><li>Runner.UNRESOLVED：没有输出，这一般代表runner无法应对输入而崩溃。</li></ul><p>Runner的大体架构如下：</p><pre><code class="lang-py">class Runner(object):    # Test outcomes    PASS = &quot;PASS&quot;    FAIL = &quot;FAIL&quot;    UNRESOLVED = &quot;UNRESOLVED&quot;    def __init__(self):        &quot;&quot;&quot;Initialize&quot;&quot;&quot;        pass    def run(self, inp):        &quot;&quot;&quot;Run the runner with the given input&quot;&quot;&quot;        return (inp, Runner.UNRESOLVED)</code></pre><p>想要实现其他Runner，只需继承Runner即可。</p><pre><code class="lang-py">class PrintRunner(Runner):    def run(self, inp):        &quot;&quot;&quot;Print the given input&quot;&quot;&quot;        print(inp)        return (inp, Runner.UNRESOLVED)</code></pre><pre><code>p = PrintRunner()(result, outcome) = p.run(&quot;Some input&quot;)Some input</code></pre><p>对于PrintRunner，我们无法验证其结果，因此通通返回UNRESOLVED。</p><p>下面是一个Runner的派生类ProgramRunner的代码，此架构代表了大多数程序。</p><pre><code class="lang-py">class ProgramRunner(Runner):    def __init__(self, program):        &quot;&quot;&quot;Initialize.  `program` is a program spec as passed to `subprocess.run()`&quot;&quot;&quot;        self.program = program    def run_process(self, inp=&quot;&quot;):        &quot;&quot;&quot;Run the program with `inp` as input.  Return result of `subprocess.run()`.&quot;&quot;&quot;        return subprocess.run(self.program,                              input=inp,                              stdout=subprocess.PIPE,                              stderr=subprocess.PIPE,                              universal_newlines=True)    def run(self, inp=&quot;&quot;):        &quot;&quot;&quot;Run the program with `inp` as input.  Return test outcome based on result of `subprocess.run()`.&quot;&quot;&quot;        result = self.run_process(inp)        if result.returncode == 0:            outcome = self.PASS        elif result.returncode &lt; 0:            outcome = self.FAIL        else:            outcome = self.UNRESOLVED        return (result, outcome)</code></pre><h2 id="Fuzzing-实例"><a href="#Fuzzing-实例" class="headerlink" title="Fuzzing 实例"></a>Fuzzing 实例</h2><pre><code class="lang-py">cat = ProgramRunner(program=&quot;cat&quot;)cat.run(&quot;hello&quot;)random_fuzzer = RandomFuzzer(min_length=20, max_length=20)random_fuzzer.runs(cat, 10)[(CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&#39;3976%%&amp;+%6=(1)3&amp;3:&lt;9&#39;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&#39;33$#42$ 11=*%$20=&lt;.-&#39;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&#39;&quot;?&lt;\&#39;#8 &lt;/:*%9.--\&#39;97!&#39;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&quot;/0-#(03/!#60&#39;+6&gt;&amp;&amp;72&quot;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&quot;=,+:,6&#39;5:950+&gt;&lt;3(*()&quot;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&quot; 379+0?&#39;%3137=2:4605&quot;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&quot;02&gt;!$&lt;/&#39;*81.#&lt;/22&gt;+:&quot;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&quot;=-&lt;&#39;3-#88*%&amp;*9&lt; +1&amp;&amp;&quot;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&#39;2;;0=3&amp;6=8&amp;30&amp;&lt;-;?*;&#39;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&#39;/#05=*3($&gt;::#7!0=12+&#39;, stderr=&#39;&#39;),  &#39;PASS&#39;)]</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Fuzzing又称模糊测试。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Fuzzing" scheme="https://superlova.github.io/tags/Fuzzing/"/>
    
      <category term="Testing" scheme="https://superlova.github.io/tags/Testing/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】leetcode打卡：分治算法</title>
    <link href="https://superlova.github.io/2020/08/19/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91leetcode%E6%89%93%E5%8D%A1%EF%BC%9A%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95/"/>
    <id>https://superlova.github.io/2020/08/19/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91leetcode%E6%89%93%E5%8D%A1%EF%BC%9A%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95/</id>
    <published>2020-08-19T10:21:29.000Z</published>
    <updated>2020-08-19T16:06:25.728Z</updated>
    
    <content type="html"><![CDATA[<p>见多了优秀的文章，再写博客的时候就会感叹自己的学识浅薄。<br><a id="more"></a></p><h2 id="leetcode-50-Pow-x-n"><a href="#leetcode-50-Pow-x-n" class="headerlink" title="leetcode 50 Pow(x,n)"></a>leetcode 50 Pow(x,n)</h2><p><strong>题目描述</strong></p><p>实现 pow(x, n) ，即计算 x 的 n 次幂函数。</p><p><strong>算法描述</strong>：</p><p>Pow算法有快速幂实现方法。</p><p>快速幂，二进制取幂（Binary Exponentiation，也称平方法），是一个在 $O(\log(n))$ 的时间内计算 $a^n$ 的小技巧，而暴力的计算需要 $O(n)$ 的时间。而这个技巧也常常用在非计算的场景，因为它可以应用在任何具有结合律的运算中。其中显然的是它可以应用于模意义下取幂、矩阵幂等运算。</p><p>计算a的n次方表示将n个a连乘在一起。然而当a和n太大的时候，这种方法就不太适用了。</p><p>不过我们知道，$a^{b+c}=a^b\cdot a^c$，$a^{2b}=(a^b)^2$。</p><p>快速幂的想法是，我们将取幂的任务按照指数的 <strong>二进制表示</strong> 来分割成更小的任务。</p><p>我们将 n 表示为 2 进制，举一个例子：</p><p>$3^{13}=3^{(1101)_2}=3^8\cdot 3^4\cdot 3^1$</p><p>因此只需把n转化成二进制，然后分解成对应的权值即可简化计算。</p><p>为什么这样能简化计算？因为n的二进制形式长度最长只有$O(\log(n))$。原问题被我们转化成了形式相同的子问题的乘积。</p><p><strong>实现</strong>：</p><pre><code class="lang-cpp">class Solution {public:    double myPow(double x, int n) {        if (n &lt; 0) {            return 1 / myPow(x, -n);        }        double base = x;        double res = 1.0;        for (; n != 0; n &gt;&gt;= 1) {            if (n &amp; 0x1) res *= base;            base *= base;        }        return res;    }};</code></pre><p>上面的代码在循环的过程中将二进制位为 1 时对应的幂累乘到答案中。</p><blockquote><p><a href="https://oi-wiki.org/" target="_blank" rel="noopener">https://oi-wiki.org/</a></p></blockquote><h2 id="leetcode-53-最大子序和"><a href="#leetcode-53-最大子序和" class="headerlink" title="leetcode 53 最大子序和"></a>leetcode 53 最大子序和</h2><p><strong>题目描述</strong></p><p>给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。</p><p><strong>算法描述</strong>：</p><p>给定一个数组，下标从start到end，即[start,end]。要求这其中的最大连续子数组之和。</p><p>分解成如下子问题：查找[start,mid]中的最大连续子数组之和，查找[mid,end]中的最大连续子数组之和，最后比较二者哪个更大。</p><p>但是最大连续子数组可能是跨越mid的数组，所以递归的时候要额外计算mid及其周围元素之和的最大值，用此值与前面两个区间的值比较。</p><p><strong>实现</strong></p><pre><code class="lang-cpp">class Solution {public:    int maxSubArray(vector&lt;int&gt;&amp; nums) {        return find(nums, 0, nums.size()-1);    }    int find(vector&lt;int&gt;&amp; nums, int start, int end) {        if (start == end) return nums[start];        if (start &gt; end) return INT_MIN;        int mid = start + (end - start) / 2;        int left_max = 0, right_max = 0, ml = 0, mr = 0;        left_max = find(nums, start, mid-1);        right_max = find(nums, mid+1, end);        for (int i = mid-1, sum = 0; i &gt;= start; --i) {            sum += nums[i];            if (sum &gt; ml) ml = sum;        }        for (int i = mid+1, sum = 0; i &lt;= end; ++i) {            sum += nums[i];            if (sum &gt; mr) mr = sum;        }        return max(max(left_max, right_max), ml + mr + nums[mid]);    }};</code></pre><p>在代码中，<code>left_max</code>为[start,mid)区间内的最大连续子数组和，<code>right_max</code>为(mid,end]区间内的最大连续子数组和。</p><p>而跨越中心mid的计算方法，则是通过两个for循环，从mid开始一个往前遍历得到最大值<code>ml</code>，一个往后遍历得到<code>mr</code>，最后得到<code>ml + mr + nums[mid]</code>即可。</p><p>结果为三者的最大值。</p><blockquote><p><a href="https://www.bilibili.com/video/BV19t411k7jR" target="_blank" rel="noopener">https://www.bilibili.com/video/BV19t411k7jR</a></p></blockquote><h2 id="leetcode-169-多数元素"><a href="#leetcode-169-多数元素" class="headerlink" title="leetcode 169 多数元素"></a>leetcode 169 多数元素</h2><p><strong>题目描述</strong></p><p>给定一个大小为 n 的数组，找到其中的多数元素。多数元素是指在数组中出现次数大于 ⌊ n/2 ⌋ 的元素。</p><p>你可以假设数组是非空的，并且给定的数组总是存在多数元素。</p><p><strong>算法描述</strong>：</p><p>如果数 a 是数组 nums 的众数，如果我们将 nums 分成两部分，那么 a 必定是至少一部分的众数。</p><p>这样一来，我们就可以使用分治法解决这个问题：将数组分成左右两部分，分别求出左半部分的众数 a1 以及右半部分的众数 a2，随后在 a1 和 a2 中选出正确的众数。</p><p><strong>实现</strong></p><p>遍历法：</p><pre><code class="lang-cpp">class Solution {public:    int majorityElement(vector&lt;int&gt;&amp; nums) {        int res = -1;        int count = 0;        for (auto c : nums) {            if (!count) {                res = c;            }            if (res == c) {                ++count;            } else {                --count;            }        }        return res;    }};</code></pre><p>分治法：</p><pre><code class="lang-cpp">class Solution {public:    int majorityElement(vector&lt;int&gt;&amp; nums) {        return majorityElement_inrange(nums, 0, nums.size()-1);    }private:    int majorityElement_inrange(vector&lt;int&gt;&amp; nums, int lo, int hi) {        if (lo == hi) return nums[lo];        if (lo &gt; hi) return -1;        int mid = lo + (hi - lo) / 2;        int left_maj = majorityElement_inrange(nums, lo, mid);        int right_maj = majorityElement_inrange(nums, mid+1, hi);        return (count_in_range(nums, lo, hi, left_maj) &gt; count_in_range(nums, lo, hi, right_maj)) ? left_maj : right_maj;    }    int count_in_range(vector&lt;int&gt;&amp; nums, int lo, int hi, int val) {        int count = 0;        for (int i = lo; i &lt;= hi; ++i) {            if (nums[i] == val) count++;        }        return count;    }};</code></pre><blockquote><p><a href="https://leetcode-cn.com/problems/majority-element/solution/duo-shu-yuan-su-by-leetcode-solution/" target="_blank" rel="noopener">https://leetcode-cn.com/problems/majority-element/solution/duo-shu-yuan-su-by-leetcode-solution/</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;见多了优秀的文章，再写博客的时候就会感叹自己的学识浅薄。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="algorithm" scheme="https://superlova.github.io/tags/algorithm/"/>
    
      <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
      <category term="leetcode" scheme="https://superlova.github.io/tags/leetcode/"/>
    
      <category term="divide-and-conquer" scheme="https://superlova.github.io/tags/divide-and-conquer/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】使用tf.data对预处理过程优化</title>
    <link href="https://superlova.github.io/2020/08/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8tf-data%E5%AF%B9%E9%A2%84%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E4%BC%98%E5%8C%96/"/>
    <id>https://superlova.github.io/2020/08/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8tf-data%E5%AF%B9%E9%A2%84%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E4%BC%98%E5%8C%96/</id>
    <published>2020-08-15T15:18:43.000Z</published>
    <updated>2020-08-15T16:12:20.962Z</updated>
    
    <content type="html"><![CDATA[<p>本文是关于 <code>tf.data</code> 介绍的第二篇，主要介绍一些数据预处理方面的优化方法，诸如并行化预处理映射函数、使用缓存等。<br><a id="more"></a></p><p>构建一个机器学习模型时，由于数据预处理过程不能使用GPU进行加速，因此格外耗时。背后的原因可能是CPU、网络或者缓存等复杂的因素。因此要研究如何提升数据预处理的效率，首先需要控制实验的变量。想实现这一点，构造一个虚假的数据集比较可行。</p><p>通过构建一个虚假的数据集，从<code>tf.data.Dataset</code>继承的类，称为<code>ArtificialDataset</code>。该数据集模拟三件事：</p><ol><li>生成<code>num_samples</code>样本（默认为3）</li><li>在第一个模拟打开文件的项目之前睡眠一段时间</li><li>在产生每个项目以模拟从文件读取数据之前先休眠一段时间</li></ol><pre><code class="lang-py">class ArtificialDataset(tf.data.Dataset):    def _generator(num_samples):        # Opening the file        time.sleep(0.03)        for sample_idx in range(num_samples):            # Reading data (line, record) from the file            time.sleep(0.015)            yield (sample_idx,)    def __new__(cls, num_samples=3):        return tf.data.Dataset.from_generator(            cls._generator,            output_types=tf.dtypes.int64,            output_shapes=(1,),            args=(num_samples,)        )</code></pre><p>构建 <code>benchmark</code> ，通过模拟训练的方式，计算该数据预处理模式的耗时：</p><pre><code class="lang-py">def benchmark(dataset, num_epochs=2):    start_time = time.perf_counter()    for epoch_num in range(num_epochs):        for sample in dataset:            # Performing a training step            time.sleep(0.01)    tf.print(&quot;Execution time:&quot;, time.perf_counter() - start_time)</code></pre><p>我们先来不加任何优化地运行一次benchmark：</p><pre><code class="lang-py">benchmark(ArtificialDataset())Execution time: 0.33306735700000445</code></pre><p>此时模型的执行时间图如图所示：<br><img src="https://www.tensorflow.org/guide/images/data_performance/naive.svg" srcset="/img/loading.gif" alt="Naive"></p><p>时间消耗是这样的：先是打开文件，然后从文件中获取数据项，然后使用数据进行训练。这种执行方式，当数据进行预处理，模型就空闲；当模型开始训练，管道又空闲下来了。预处理和训练这两部分明显可以重叠。</p><p><code>tf.data</code> API提供了<code>tf.data.Dataset.prefetch</code>转换。它可以用于将数据生成时间与数据消耗时间分开。转换使用后台线程和内部缓冲区预取元素。要预取的元素数量应等于（或可能大于）单个训练步骤消耗的批次数量。将预取的元素数量设置为<code>tf.data.experimental.AUTOTUNE</code> ，这将提示<code>tf.data</code>运行时在运行时动态调整值。</p><pre><code class="lang-py">benchmark(    ArtificialDataset()    .prefetch(tf.data.experimental.AUTOTUNE))Execution time: 0.20504431599999862</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/prefetched.svg" srcset="/img/loading.gif" alt="Prefetched"></p><p>时间有了明显优化，因为数据的生产和消费有了些许重叠。</p><p>在实际工作中，输入数据可以远程存储在其他计算机上。在本地和远程存储之间存在以下差异：</p><ol><li>到达第一个字节的时间：从远程存储读取文件的第一个字节所花费的时间要比从本地存储中读取文件的时间长几个数量级。</li><li>读取吞吐量：虽然远程存储通常提供较大的聚合带宽，但是读取单个文件可能只能使用此带宽的一小部分。</li></ol><p>此外，一旦将原始字节加载到内存中，可能还需要对数据进行反序列化和/或解密，这需要进行额外的计算。不管数据是本地存储还是远程存储，都存在这种开销，但是<strong>如果数据没有有效地预取，则在远程情况下会更糟</strong>。</p><p>可以使用<code>tf.data.Dataset.interleave</code>转换来<strong>并行化数据加载步骤</strong>， <code>cycle_length</code> 表明可以一起处理的数据集数量， <code>num_parallel_calls</code> 则是并行度。</p><pre><code class="lang-py">benchmark(    tf.data.Dataset.range(2)    .interleave(        ArtificialDataset,        num_parallel_calls=tf.data.experimental.AUTOTUNE    )Execution time: 0.18243273299958673</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/parallel_interleave.svg" srcset="/img/loading.gif" alt="Parallel interleave"></p><p>该图可以显示<code>interleave</code>变换的行为，从两个可用的数据集中获取样本。这次，两个数据集的读取并行进行，从而减少了全局数据处理时间</p><h2 id="并行预处理操作"><a href="#并行预处理操作" class="headerlink" title="并行预处理操作"></a>并行预处理操作</h2><p>在准备数据时，可能需要对输入元素进行预处理。可以使用<code>tf.data.Dataset.map(f)</code>转换，其含义为将某个转换<code>f</code>作用于数据集<code>Dataset</code>中的每个元素。这里有个很重要的前提条件，由于输入元素彼此独立，因此预处理可以跨多个CPU内核并行化。因此<code>map</code>转换也提供<code>num_parallel_calls</code>参数来指定并行度。关于并行度的选择上，<code>map</code>转换支持<code>tf.data.experimental.AUTOTUNE</code>，而不必人工定义。</p><p>首先定义伪操作：</p><pre><code class="lang-py">def mapped_function(s):    # Do some hard pre-processing    tf.py_function(lambda: time.sleep(0.03), [], ())    return s</code></pre><p>我们来测试伪操作，此时没有任何并行优化：</p><pre><code class="lang-py">benchmark(    ArtificialDataset()    .map(mapped_function))Execution time: 0.4592052289999913</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/sequential_map.svg" srcset="/img/loading.gif" alt="Sequential mapping"></p><p>现在，使用相同的预处理功能，但将其并行应用于多个样本。</p><pre><code class="lang-py">benchmark(    ArtificialDataset()    .map(        mapped_function,        num_parallel_calls=tf.data.experimental.AUTOTUNE    ))Execution time: 0.3045882669994171</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/parallel_map.svg" srcset="/img/loading.gif" alt="Parallel mapping"></p><p>可以在图上看到预处理步骤重叠，从而减少了单次迭代的总时间。</p><p><code>tf.data.Dataset.cache</code>转换可以在内存中或本地存储上缓存数据集。这样可以避免在每个epoch执行某些重复性操作（例如打开文件和读取数据）。</p><pre><code class="lang-py">benchmark(    ArtificialDataset()    .map(  # Apply time consuming operations before cache        mapped_function    ).cache(    ),    5)Execution time: 0.3795637040002475</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/cached_dataset.svg" srcset="/img/loading.gif" alt="Cached dataset"></p><p>第一个epoch执行一次cache之前的转换（例如文件打开和数据读取）。下一个epoch将重用cache转换所缓存的数据。</p><p>这里涉及到一个<code>map</code>和<code>cache</code>操作谁先谁后的问题。有一个原则，如果<code>map</code>操作很复杂、昂贵，那么先<code>map</code>再<code>cache</code>，下次不用<code>map</code>了。如果<code>cache</code>过大而无法放入缓冲区，则先<code>cache</code>后<code>map</code>，或者试图采用一些数据预处理方法以减少资源使用。</p><h2 id="向量化数据预处理操作"><a href="#向量化数据预处理操作" class="headerlink" title="向量化数据预处理操作"></a>向量化数据预处理操作</h2><p>所谓向量化，即使得<code>mapping</code>操作能够一次处理一<code>batch</code>数据。这样做肯定可以加速，因为避免了繁杂的数据读取时间。对用户定义的函数进行向量化处理，并且对数据集应用<code>batch</code>转换再进入<code>mapping</code>。在某种情况下，这个做法非常有用。</p><p>首先定义一个数据集操作<code>increment</code>，负责把每个元素的值+1。另外之前的例子里面使用了毫秒级别的<code>sleep</code>操作，这会掩盖我们优化的结果。这次我们把它拿掉。</p><p>下面是未经向量化优化的<code>increment</code>操作耗时：</p><pre><code class="lang-py">fast_dataset = tf.data.Dataset.range(10000)def fast_benchmark(dataset, num_epochs=2):    start_time = time.perf_counter()    for _ in tf.data.Dataset.range(num_epochs):        for _ in dataset:            pass    tf.print(&quot;Execution time:&quot;, time.perf_counter() - start_time)def increment(x):    return x+1fast_benchmark(    fast_dataset    # Apply function one item at a time    .map(increment)    # Batch    .batch(256))Execution time: 0.7625284370005829</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/scalar_map.svg" srcset="/img/loading.gif" alt="Scalar map"></p><p>与之对比，经过向量化后，耗时明显减少：</p><pre><code class="lang-py">fast_benchmark(    fast_dataset    .batch(256)    # Apply function on a batch of items    # The tf.Tensor.__add__ method already handle batches    .map(increment))Execution time: 0.04735958700075571</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/vectorized_map.svg" srcset="/img/loading.gif" alt="Vectorized map"></p><h2 id="减少内存占用"><a href="#减少内存占用" class="headerlink" title="减少内存占用"></a>减少内存占用</h2><p>许多转换（包括interleave ， prefetch和shuffle ）各自维护内部缓冲区。如果传递给map转换的用户定义函数更改了元素的大小，则映射转换的顺序以及缓冲元素的转换会影响内存使用。</p><p>通常，我们建议选择导致内存占用减少的顺序，除非需要不同的顺序才能提高性能。</p><p>对于缓存，我们建议除非转换后的数据难以保存到缓冲区，否则一律先<code>map</code>再<code>cache</code>。如果你有两个<code>map</code>，其中一个比较耗时<code>time_consuming_mapping</code>，另一个比较耗内存<code>memory_consuming_mapping</code>，那么其实你可以将其拆分成两部分；</p><pre><code class="lang-py">dataset.map(time_consuming_mapping).cache().map(memory_consuming_mapping)</code></pre><p>这样，耗时部分仅在第一个epoch执行，并且避免了使用过多的缓存空间。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>使用<code>tf.data</code>，并采用合理的优化手段，就能让你的数据预处理过程节约很多时间。这些手段有：</p><ul><li>使用<code>prefetch</code>转换可以使生产者和消费者的工作重叠。</li><li>使用<code>interleave</code>变换并行化数据读取变换。</li><li>通过设置<code>num_parallel_calls</code>参数来并行化<code>map</code>转换 。</li><li>在第一个epoch使用<code>cache</code>转换将数据缓存在内存中</li><li>向量化传递给<code>map</code>转换的用户定义函数</li><li>应用<code>interleave</code> ， <code>prefetch</code>和<code>shuffle</code>转换时， 逐渐减少内存使用 。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是关于 &lt;code&gt;tf.data&lt;/code&gt; 介绍的第二篇，主要介绍一些数据预处理方面的优化方法，诸如并行化预处理映射函数、使用缓存等。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="preprocessing" scheme="https://superlova.github.io/tags/preprocessing/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】tf.data基本使用</title>
    <link href="https://superlova.github.io/2020/08/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91tf-data%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"/>
    <id>https://superlova.github.io/2020/08/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91tf-data%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</id>
    <published>2020-08-15T08:01:09.000Z</published>
    <updated>2020-08-15T15:20:36.629Z</updated>
    
    <content type="html"><![CDATA[<p>使用 <code>tf.data</code> API 可以轻松处理大量数据，支持多样化的数据格式，还可以方便执行复杂的转换。本文介绍了不同类别源数据转化为 <code>tf.data.Dataset</code> 的方法，以及 <code>Dataset</code> 常见的预处理方法。<br><a id="more"></a></p><h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>最近几年我们在机器学习的加速计算领域取得了一些突破。虽然我们进行指标运算和矩阵运算所需的时间大大减少了，但是提供数据加速的CPU却没能跟上相应的步伐，这就成为了预处理中的瓶颈。我们本以为可以通过构建更复杂的模型来减少对硬件的需求，但是CPU的效率还是取决于他们拥有多少RAM。</p><p><img src="/2020/08/15/【学习笔记】tf-data基本使用/2020-08-15-22-34-14.png" srcset="/img/loading.gif" alt></p><p>对于一些数据集很大的问题，解决问题的方法除了提升CPU的性能之外，还有对数据集进行分批次处理，但是分批预处理需要不断进行训练和合并操作，这对预处理增加了难度。<code>tf.data</code> 可以帮助解决数据集过大造成的预处理瓶颈问题。</p><p><code>tf.data</code> 是tensorflow的数据预处理框架。它速度快、灵活且易于使用。</p><p>假设你的数据以TFRecord格式存在磁盘，需要将这些数据读取、处理并训练模型，可以先通过<code>TFRecordDataset</code> 开始处理这些数据：</p><pre><code class="lang-py">dataset = tf.data.TFRecordDataset(&#39;.../*.tfrecord&#39;)</code></pre><p>然后执行一些数据预处理操作，这个过程可能很消耗资源：</p><pre><code class="lang-py">dataset = dataset.map(expensive_preprocess)</code></pre><p>随后你需要打乱数据，以降低模型训练过程中过拟合的可能性：</p><pre><code class="lang-py">dataset = dataset.shuffle(buffer_size=1024)</code></pre><p>然后我们需要分批次，以令模型加速计算</p><pre><code class="lang-py">dataset = dataset.batch(batch_size=32)</code></pre><p>最终要完成pipeline搭建，这样可以保证模型在运行一个batch的数据时，另一批数据进行预处理以提升效率。</p><pre><code class="lang-py">dataset = dataset.prefetch()</code></pre><p>将数据输入到模型，我们可以开始训练了。</p><pre><code class="lang-py">model = tf.keras.Model(...)model.fit(dataset)</code></pre><p>上面就是数据从读取到处理到训练的全部流程，称之为管道（pipeline）。</p><p>处理大量原始数据，要经过多次函数变换，这些函数变换都是可重用的。使用<code>tf.data</code>将这些变换整理成管道，一方面可以简化复杂输入的预处理过程，另一方面，由于<code>Dataset</code> 对象可迭代，可以执行分批处理。使用<code>tf.data.Dataset</code>可以方便地整合操作、构造数据集。</p><p>有两种方法构造可供训练使用的<code>Dataset</code>数据集：</p><ol><li>从文件、内存中直接构建<code>Dataset</code></li><li>从其他<code>Dataset</code>中转化</li></ol><p>如果打算从内存中读取数据构建 <code>Dataset</code> ，有 <code>tf.data.Dataset.from_tensors()</code> 和 <code>tf.data.Dataset.from_tensor_slices()</code> 可供选择；如果打算从 <code>TFRecord</code> 格式的文件中读取数据，可以调用 <code>tf.data.TFRecordDataset()</code> 。</p><p>当 <code>Dataset</code> 对象构建好了之后，通过使用 <code>Dataset.map()</code> 为其中每个元素施加变换、使用 <code>Dataset.batch()</code> 为整批元素添加变换等等对数据进行预处理。</p><h2 id="从内存或文件中构造Dataset"><a href="#从内存或文件中构造Dataset" class="headerlink" title="从内存或文件中构造Dataset"></a>从内存或文件中构造<code>Dataset</code></h2><h3 id="从内存中的array构造Dataset"><a href="#从内存中的array构造Dataset" class="headerlink" title="从内存中的array构造Dataset"></a>从内存中的array构造<code>Dataset</code></h3><p>如果你的所有数据都在内存中，那么最简单构造 <code>Dataset</code> 的方式就是，先将其利用 <code>tf.Tensor</code> 转成tensor，后使用<code>Dataset.from_tensor_slices()</code>。</p><pre><code class="lang-python">train, test = tf.keras.datasets.fashion_mnist.load_data()images, labels = trainimages = images/255dataset = tf.data.Dataset.from_tensor_slices((images, labels))dataset</code></pre><h3 id="从生成器构造Dataset"><a href="#从生成器构造Dataset" class="headerlink" title="从生成器构造Dataset"></a>从生成器构造<code>Dataset</code></h3><p>你也可以利用 <code>Dataset.from_generator</code> 从Python的生成器来构造 <code>Dataset</code> ，比如从 <code>preprocessing.image.ImageDataGenerator</code> 构造 <code>Dataset</code>。但这种方法受制于Python的GIL，因此效率不会太高。</p><p>首先下载花朵图片数据集，一共3670张花朵图片，分成五个类别。</p><pre><code class="lang-python">flowers = tf.keras.utils.get_file(    &#39;flower_photos&#39;,    &#39;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&#39;,    untar=True)</code></pre><p>利用 <code>preprocessing.image.ImageDataGenerator</code> 定义数据增强操作，然后将其套用到花朵数据集上。</p><pre><code class="lang-python">img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)images, labels = next(img_gen.flow_from_directory(flowers))</code></pre><p>最后构造 <code>Dataset</code> 。</p><pre><code class="lang-python">ds = tf.data.Dataset.from_generator(    img_gen.flow_from_directory, args=[flowers],     output_types=(tf.float32, tf.float32),     output_shapes=([32,256,256,3], [32,5]))ds&lt;FlatMapDataset shapes: ((32, 256, 256, 3), (32, 5)), types: (tf.float32, tf.float32)&gt;</code></pre><h3 id="从TFRecord格式文件构造Dataset"><a href="#从TFRecord格式文件构造Dataset" class="headerlink" title="从TFRecord格式文件构造Dataset"></a>从<code>TFRecord</code>格式文件构造<code>Dataset</code></h3><p>有些时候数据不在内存中，而是以特定格式存在磁盘上，比如 <code>TFRecord</code> 格式。这种情况我们可以使用 <code>tf.data.TFRecordDataset</code> 作为数据管道的一部分。</p><pre><code class="lang-python"># Creates a dataset that reads all of the examples from two files.fsns_test_file = tf.keras.utils.get_file(&quot;fsns.tfrec&quot;, &quot;https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001&quot;)dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file])</code></pre><p><code>tf.data.TFRecordDataset</code> API中，<code>filenames</code> 的输入很灵活，既可以是字符串，表明一个文件；也可以是字符串列表，表明多个文件。</p><h3 id="从-txt格式文件构造Dataset"><a href="#从-txt格式文件构造Dataset" class="headerlink" title="从.txt格式文件构造Dataset"></a>从<code>.txt</code>格式文件构造<code>Dataset</code></h3><p>如果是 <code>.txt</code> 格式，那么采用 <code>tf.data.TextLineDataset</code> 也可转成 <code>Dataset</code>。</p><pre><code class="lang-python">directory_url = &#39;https://storage.googleapis.com/download.tensorflow.org/data/illiad/&#39;file_names = [&#39;cowper.txt&#39;, &#39;derby.txt&#39;, &#39;butler.txt&#39;]file_paths = [    &#39;cowper.txt&#39;,&#39;derby.txt&#39;,&#39;butler.txt&#39;]dataset = tf.data.TextLineDataset(file_paths)</code></pre><p>看一下第一个文件的前5行：</p><pre><code class="lang-python">for line in dataset.take(5):  print(line.numpy())b&quot;\xef\xbb\xbfAchilles sing, O Goddess! Peleus&#39; son;&quot;b&#39;His wrath pernicious, who ten thousand woes&#39;b&quot;Caused to Achaia&#39;s host, sent many a soul&quot;b&#39;Illustrious into Ades premature,&#39;b&#39;And Heroes gave (so stood the will of Jove)&#39;</code></pre><p>前五行都是 <code>cowper.txt</code> 中的。如果我们希望生成的 <code>Dataset</code> 能够轮流选取三个文件中的元素，可以在构造之初，使用 <code>Dataset.interleave</code> ，并设置 <code>cycle_length</code>：</p><pre><code class="lang-python">files_ds = tf.data.Dataset.from_tensor_slices(file_paths)lines_ds = files_ds.interleave(tf.data.TextLineDataset, cycle_length=3)for i, line in enumerate(lines_ds.take(9)):  if i % 3 == 0:    print()  print(i, line.numpy())0 b&quot;\xef\xbb\xbfAchilles sing, O Goddess! Peleus&#39; son;&quot;1 b&quot;\xef\xbb\xbfOf Peleus&#39; son, Achilles, sing, O Muse,&quot;2 b&#39;\xef\xbb\xbfSing, O goddess, the anger of Achilles son of Peleus, that brought&#39;3 b&#39;His wrath pernicious, who ten thousand woes&#39;4 b&#39;The vengeance, deep and deadly; whence to Greece&#39;5 b&#39;countless ills upon the Achaeans. Many a brave soul did it send&#39;6 b&quot;Caused to Achaia&#39;s host, sent many a soul&quot;7 b&#39;Unnumbered ills arose; which many a soul&#39;8 b&#39;hurrying down to Hades, and many a hero did it yield a prey to dogs and&#39;</code></pre><p>有的时候我们不希望录入文件的第一行，或者只要文件中满足要求的特定行，可以分别使用 <code>Dataset.skip()</code> 和 <code>Dataset.filter()</code> 。比如下面的泰坦尼克数据集，去掉第一行后，筛选生存下来的人。</p><pre><code class="lang-python">titanic_file = tf.keras.utils.get_file(&quot;train.csv&quot;, &quot;https://storage.googleapis.com/tf-datasets/titanic/train.csv&quot;)titanic_lines = tf.data.TextLineDataset(titanic_file)for line in titanic_lines.take(10):  print(line.numpy())b&#39;survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone&#39;b&#39;0,male,22.0,1,0,7.25,Third,unknown,Southampton,n&#39;b&#39;1,female,38.0,1,0,71.2833,First,C,Cherbourg,n&#39;b&#39;1,female,26.0,0,0,7.925,Third,unknown,Southampton,y&#39;b&#39;1,female,35.0,1,0,53.1,First,C,Southampton,n&#39;b&#39;0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y&#39;b&#39;0,male,2.0,3,1,21.075,Third,unknown,Southampton,n&#39;b&#39;1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n&#39;b&#39;1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n&#39;b&#39;1,female,4.0,1,1,16.7,Third,G,Southampton,n&#39;def survived(line):  return tf.not_equal(tf.strings.substr(line, 0, 1), &quot;0&quot;)survivors = titanic_lines.skip(1).filter(survived)for line in survivors.take(10):  print(line.numpy())b&#39;1,female,38.0,1,0,71.2833,First,C,Cherbourg,n&#39;b&#39;1,female,26.0,0,0,7.925,Third,unknown,Southampton,y&#39;b&#39;1,female,35.0,1,0,53.1,First,C,Southampton,n&#39;b&#39;1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n&#39;b&#39;1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n&#39;b&#39;1,female,4.0,1,1,16.7,Third,G,Southampton,n&#39;b&#39;1,male,28.0,0,0,13.0,Second,unknown,Southampton,y&#39;b&#39;1,female,28.0,0,0,7.225,Third,unknown,Cherbourg,y&#39;b&#39;1,male,28.0,0,0,35.5,First,A,Southampton,y&#39;b&#39;1,female,38.0,1,5,31.3875,Third,unknown,Southampton,n&#39;</code></pre><h3 id="从csv格式构造Dataset"><a href="#从csv格式构造Dataset" class="headerlink" title="从csv格式构造Dataset"></a>从<code>csv</code>格式构造<code>Dataset</code></h3><p>除了<code>TFRecord</code>和<code>txt</code>格式，还有<code>csv</code>格式也很流行。<code>csv</code>格式能够以纯文本保存表格数据。<code>pandas</code>的<code>to_csv</code>是将<code>csv</code>搬运到内存的良好工具。</p><pre><code class="lang-python">titanic_file = tf.keras.utils.get_file(&quot;train.csv&quot;, &quot;https://storage.googleapis.com/tf-datasets/titanic/train.csv&quot;)df = pd.read_csv(titanic_file, index_col=None)df.head()titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))for feature_batch in titanic_slices.take(1):  for key, value in feature_batch.items():    print(&quot;  {!r:20s}: {}&quot;.format(key, value))</code></pre><p>当然<code>tf.data</code>强大之处在于可以处理<code>pandas</code>处理不了的文件大小。<code>experimental.make_csv_dataset</code>函数是用于读取csv文件集的高层接口，它可以自动推导每个<code>column</code>的文件类型。</p><pre><code class="lang-python">titanic_batches = tf.data.experimental.make_csv_dataset(    titanic_file, batch_size=4,    label_name=&quot;survived&quot;)</code></pre><p>查看第一个<code>batch</code>的内容。</p><pre><code class="lang-python">for feature_batch, label_batch in titanic_batches.take(1):  print(&quot;&#39;survived&#39;: {}&quot;.format(label_batch))  print(&quot;features:&quot;)  for key, value in feature_batch.items():    print(&quot;  {!r:20s}: {}&quot;.format(key, value))</code></pre><p>如果只需要<code>csv</code>的某一列，那么可以使用<code>select_columns</code>参数。</p><pre><code class="lang-python">titanic_batches = tf.data.experimental.make_csv_dataset(    titanic_file, batch_size=4,    label_name=&quot;survived&quot;, select_columns=[&#39;class&#39;, &#39;fare&#39;, &#39;survived&#39;])for feature_batch, label_batch in titanic_batches.take(1):  print(&quot;&#39;survived&#39;: {}&quot;.format(label_batch))  for key, value in feature_batch.items():    print(&quot;  {!r:20s}: {}&quot;.format(key, value))&#39;survived&#39;: [1 1 1 0]features:  &#39;sex&#39;               : [b&#39;female&#39; b&#39;female&#39; b&#39;male&#39; b&#39;female&#39;]  &#39;age&#39;               : [35. 31. 45. 28.]  &#39;n_siblings_spouses&#39;: [0 1 0 8]  &#39;parch&#39;             : [0 1 0 2]  &#39;fare&#39;              : [512.3292  20.525    8.05    69.55  ]  &#39;class&#39;             : [b&#39;First&#39; b&#39;Third&#39; b&#39;Third&#39; b&#39;Third&#39;]  &#39;deck&#39;              : [b&#39;unknown&#39; b&#39;unknown&#39; b&#39;unknown&#39; b&#39;unknown&#39;]  &#39;embark_town&#39;       : [b&#39;Cherbourg&#39; b&#39;Southampton&#39; b&#39;Southampton&#39; b&#39;Southampton&#39;]  &#39;alone&#39;             : [b&#39;y&#39; b&#39;n&#39; b&#39;y&#39; b&#39;n&#39;]</code></pre><p>还有一个底层的<code>experimental.CsvDataset</code>类，它可以更精细的控制读取<code>csv</code>的过程。不支持列类型推断。</p><pre><code class="lang-python">titanic_types  = [tf.int32, tf.string, tf.float32, tf.int32, tf.int32, tf.float32, tf.string, tf.string, tf.string, tf.string] dataset = tf.data.experimental.CsvDataset(titanic_file, titanic_types , header=True)for line in dataset.take(10):  print([item.numpy() for item in line])[0, b&#39;male&#39;, 22.0, 1, 0, 7.25, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 38.0, 1, 0, 71.2833, b&#39;First&#39;, b&#39;C&#39;, b&#39;Cherbourg&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 26.0, 0, 0, 7.925, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;y&#39;][1, b&#39;female&#39;, 35.0, 1, 0, 53.1, b&#39;First&#39;, b&#39;C&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][0, b&#39;male&#39;, 28.0, 0, 0, 8.4583, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Queenstown&#39;, b&#39;y&#39;][0, b&#39;male&#39;, 2.0, 3, 1, 21.075, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 27.0, 0, 2, 11.1333, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 14.0, 1, 0, 30.0708, b&#39;Second&#39;, b&#39;unknown&#39;, b&#39;Cherbourg&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 4.0, 1, 1, 16.7, b&#39;Third&#39;, b&#39;G&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][0, b&#39;male&#39;, 20.0, 0, 0, 8.05, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;y&#39;]</code></pre><p><code>CsvDataset</code>还可以指定每列的默认值，供元素为空时填充。</p><p>在Colab中，直接书写<code>csv</code>文件：</p><pre><code class="lang-bash">%%writefile missing.csv1,2,3,4,2,3,41,,3,41,2,,41,2,3,,,,</code></pre><p>设置每列默认值：</p><pre><code class="lang-python"># Creates a dataset that reads all of the records from two CSV files, each with# four float columns which may have missing values.record_defaults = [999,999,999,999]dataset = tf.data.experimental.CsvDataset(&quot;missing.csv&quot;, record_defaults)dataset = dataset.map(lambda *items: tf.stack(items))for line in dataset:  print(line.numpy())[1 2 3 4][999   2   3   4][  1 999   3   4][  1   2 999   4][  1   2   3 999][999 999 999 999]</code></pre><p>你也可以选择删除<code>header</code>，或者指定某列输出</p><pre><code class="lang-python"># Creates a dataset that reads all of the records from two CSV files with# headers, extracting float data from columns 2 and 4.record_defaults = [999, 999] # Only provide defaults for the selected columnsdataset = tf.data.experimental.CsvDataset(&quot;missing.csv&quot;, record_defaults, select_cols=[1, 3])dataset = dataset.map(lambda *items: tf.stack(items))for line in dataset:  print(line.numpy())[2 4][2 4][999   4][2 4][  2 999][999 999]</code></pre><h3 id="从文件夹中的每个文件构造Dataset"><a href="#从文件夹中的每个文件构造Dataset" class="headerlink" title="从文件夹中的每个文件构造Dataset"></a>从文件夹中的每个文件构造<code>Dataset</code></h3><p>如果每个单独的文件都是一个数据项（比如图片数据集），这样的数据集如何整理？</p><pre><code class="lang-python">flowers_root = tf.keras.utils.get_file(    &#39;flower_photos&#39;,    &#39;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&#39;,    untar=True)flowers_root = pathlib.Path(flowers_root)list_ds = tf.data.Dataset.list_files(str(flowers_root/&#39;*/*&#39;))for f in list_ds.take(5):  print(f.numpy())b&#39;/root/.keras/datasets/flower_photos/dandelion/8720503800_cab5c62a34.jpg&#39;b&#39;/root/.keras/datasets/flower_photos/dandelion/16510864164_3afa8ac37f.jpg&#39;b&#39;/root/.keras/datasets/flower_photos/tulips/7136973281_b2a935ce20.jpg&#39;b&#39;/root/.keras/datasets/flower_photos/sunflowers/14623719696_1bb7970208_n.jpg&#39;b&#39;/root/.keras/datasets/flower_photos/dandelion/4560663938_3557a1f831.jpg&#39;</code></pre><p>通过使用<code>tf.io.read_file</code>读取数据，并从路径中提取<code>label</code>，返回<code>(image, label)</code>数据对。</p><pre><code class="lang-python">def process_path(file_path):  label = tf.strings.split(file_path, os.sep)[-2]  return tf.io.read_file(file_path), labellabeled_ds = list_ds.map(process_path)for image_raw, label_text in labeled_ds.take(1):  print(repr(image_raw.numpy()[:100]))  print()  print(label_text.numpy())b&#39;\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x01\x03|\x03|\x00\x00\xff\xe2\x0cXICC_PROFILE\x00\x01\x01\x00\x00\x0cHLino\x02\x10\x00\x00mntrRGB XYZ \x07\xce\x00\x02\x00\t\x00\x06\x001\x00\x00acspMSFT\x00\x00\x00\x00IEC sRGB\x00\x00\x00\x00\x00\x00&#39;b&#39;sunflowers&#39;</code></pre><h2 id="批处理数据集元素"><a href="#批处理数据集元素" class="headerlink" title="批处理数据集元素"></a>批处理数据集元素</h2><p>批处理的最简单形式是将数据集的<code>n</code>个连续元素堆叠为单个元素。</p><p>batched_dataset = dataset.batch(4)</p><batchdataset shapes: ((none,), (none,)), types: (tf.int64, tf.int64)><p>最后一个批次可能未满，使用<code>drop_remainder</code>参数忽略最后一批，使得<code>shape</code>完整：</p><p>batched_dataset = dataset.batch(7, drop_remainder=True)<br>batched_dataset</p><batchdataset shapes: ((7,), (7,)), types: (tf.int64, tf.int64)><p>许多模型（例如序列模型）都可以使用大小可变（例如长度不同的序列）的输入数据。通过<code>Dataset.padded_batch</code>可以将不同长度的<code>tensor</code>转换成一个<code>batch</code>。</p><p>在多个<code>epochs</code>的情况下，需要重复迭代数据集，最简单的方法是将数据集重复<code>epochs</code>遍。可以使用 <code>Dataset.repeat()</code>完成。</p><p>原有的<code>titanic_lines</code>数据集中的数据数量为 628</p><pre><code class="lang-python">count = 0for data in titanic_lines:    count += 1    # print(data.numpy())print(count)</code></pre><p>titanic_lines数据集经过<code>repeat</code>之后数目变为原来的两倍，1256</p><pre><code class="lang-python">count = 0for data in titanic_lines.repeat(2):    count += 1    # print(data.numpy())print(count)</code></pre><p>将数据集打散的方法 <code>Dataset.shuffle()</code> 通过维护一个固定大小的缓冲区来实现。</p><pre><code class="lang-py">dataset = tf.data.Dataset.zip((counter, lines))dataset = dataset.shuffle(buffer_size=100)dataset = dataset.batch(20)</code></pre><p>由于<code>buffer_size</code>为100，而批大小为20，因此第一批不包含索引大于120的元素。</p><p>在实际使用中，<code>repeat</code>操作、<code>batch</code>操作和<code>shuffle</code>操作经常一起混用，但是一定要注意操作的先后顺序。</p><h2 id="预处理数据"><a href="#预处理数据" class="headerlink" title="预处理数据"></a>预处理数据</h2><p><code>Dataset.map(f)</code> 通过函数 <code>f</code> 对数据集执行变换，<code>f</code> 必须以单个Tensor为输入，单个Tensor为输出（这里指的单个tensor，其意思是由原始数据和标签组成的数据对）。</p><p>假设我们定义了单个图像变换函数 <code>parse_image</code> ，只需 <code>images_ds = list_ds.map(parse_image)</code> 即可对 <code>image_ds</code> 数据集中的所有图片执行变换了。</p><p>假设我们需要将照片随机旋转，可以定义函数，然后使用map将其应用于数据集的所有图片上。</p><pre><code class="lang-py">import scipy.ndimage as ndimagedef random_rotate_image(image):  image = ndimage.rotate(image, np.random.uniform(-30, 30), reshape=False)  return imageimage, label = next(iter(images_ds))image = random_rotate_image(image)show(image, label)</code></pre><p><img src="https://www.tensorflow.org/guide/data_files/output__wEyL7bS9S6t_1.png" srcset="/img/loading.gif" alt></p><p>这里使用的是 <code>scipy</code> 中的旋转函数，需套用 <code>tf.py_function()</code> 才能在Tensorflow的eager_mode里面使用。</p><p>接下来使用 <code>Dataset.map</code> ：</p><pre><code class="lang-py">def tf_random_rotate_image(image, label):  im_shape = image.shape  [image,] = tf.py_function(random_rotate_image, [image], [tf.float32])  image.set_shape(im_shape)  return image, label</code></pre><p>在函数内部不但要注意输入和返回值是<code>(image, label)</code>，而且需要描述数据的<code>shape</code>和<code>type</code>，方便调试。</p><h2 id="时间序列数据窗口化"><a href="#时间序列数据窗口化" class="headerlink" title="时间序列数据窗口化"></a>时间序列数据窗口化</h2><p>时间序列数据的标签有所不同，一般以下一时刻的输入数据为标签，对未来进行一步一步的密集预测。比如：</p><pre><code class="lang-py">range_ds = tf.data.Dataset.range(100000)batches = range_ds.batch(10, drop_remainder=True)def dense_1_step(batch):  # Shift features and labels one step relative to each other.  return batch[:-1], batch[1:]predict_dense_1_step = batches.map(dense_1_step)for features, label in predict_dense_1_step.take(3):  print(features.numpy(), &quot; =&gt; &quot;, label.numpy())[0 1 2 3 4 5 6 7 8]  =&gt;  [1 2 3 4 5 6 7 8 9][10 11 12 13 14 15 16 17 18]  =&gt;  [11 12 13 14 15 16 17 18 19][20 21 22 23 24 25 26 27 28]  =&gt;  [21 22 23 24 25 26 27 28 29]</code></pre><p>如果要预测整个时间窗口而不是固定的偏移量，比如</p><pre><code class="lang-py">batches = range_ds.batch(15, drop_remainder=True)def label_next_5_steps(batch):  return (batch[:-5],   # Take the first 5 steps          batch[-5:])   # take the remainderpredict_5_steps = batches.map(label_next_5_steps)for features, label in predict_5_steps.take(3):  print(features.numpy(), &quot; =&gt; &quot;, label.numpy())[0 1 2 3 4 5 6 7 8 9]  =&gt;  [10 11 12 13 14][15 16 17 18 19 20 21 22 23 24]  =&gt;  [25 26 27 28 29][30 31 32 33 34 35 36 37 38 39]  =&gt;  [40 41 42 43 44]</code></pre><p>或者一个批次的标签和下个批次的输入有重叠：</p><pre><code class="lang-py">feature_length = 10label_length = 5features = range_ds.batch(feature_length, drop_remainder=True)labels = range_ds.batch(feature_length).skip(1).map(lambda labels: labels[:-5])predict_5_steps = tf.data.Dataset.zip((features, labels))for features, label in predict_5_steps.take(3):  print(features.numpy(), &quot; =&gt; &quot;, label.numpy())[0 1 2 3 4 5 6 7 8 9]  =&gt;  [10 11 12 13 14][10 11 12 13 14 15 16 17 18 19]  =&gt;  [20 21 22 23 24][20 21 22 23 24 25 26 27 28 29]  =&gt;  [30 31 32 33 34]</code></pre><p>有更方便的方法，那就是使用<code>Dataset.window</code>方法</p><pre><code class="lang-py">def make_window_dataset(ds, window_size, shift, stride):  windows = ds.window(window_size, shift=shift, stride=stride)  def sub_to_batch(sub):    return sub.batch(window_size, drop_remainder=True)  windows = windows.flat_map(sub_to_batch)  return windows</code></pre><p><code>Dataset.window(window_size, shift=shift, stride=stride)</code> 中的window_size代表窗口大小，即每个batch的元素个数；shift代表每次窗口移动的距离；stride代表选择元素的间隔</p><pre><code class="lang-py">ds = make_window_dataset(range_ds, window_size=10, shift = 5, stride=3)for example in ds.take(10):  print(example.numpy())[ 0  3  6  9 12 15 18 21 24 27][ 5  8 11 14 17 20 23 26 29 32][10 13 16 19 22 25 28 31 34 37][15 18 21 24 27 30 33 36 39 42][20 23 26 29 32 35 38 41 44 47][25 28 31 34 37 40 43 46 49 52][30 33 36 39 42 45 48 51 54 57][35 38 41 44 47 50 53 56 59 62][40 43 46 49 52 55 58 61 64 67][45 48 51 54 57 60 63 66 69 72]</code></pre><p>提取这些数据的标签方法：</p><pre><code class="lang-py">dense_labels_ds = ds.map(dense_1_step)for inputs,labels in dense_labels_ds.take(3):  print(inputs.numpy(), &quot;=&gt;&quot;, labels.numpy())[ 0  3  6  9 12 15 18 21 24] =&gt; [ 3  6  9 12 15 18 21 24 27][ 5  8 11 14 17 20 23 26 29] =&gt; [ 8 11 14 17 20 23 26 29 32][10 13 16 19 22 25 28 31 34] =&gt; [13 16 19 22 25 28 31 34 37]</code></pre><h2 id="重采样"><a href="#重采样" class="headerlink" title="重采样"></a>重采样</h2><p>有一些数据集，不同类别的数据分布不均匀。这个时候需要对那些不足的类别进行重采样。</p><p>给定信用卡欺诈（二分类）数据集，下面首先检查数据集中不同类别的占比</p><pre><code class="lang-py">def count(counts, batch):  features, labels = batch  class_1 = labels == 1  class_1 = tf.cast(class_1, tf.int32)  class_0 = labels == 0  class_0 = tf.cast(class_0, tf.int32)  counts[&#39;class_0&#39;] += tf.reduce_sum(class_0)  counts[&#39;class_1&#39;] += tf.reduce_sum(class_1)  return countscounts = creditcard_ds.take(10).reduce(    initial_state={&#39;class_0&#39;: 0, &#39;class_1&#39;: 0},    reduce_func = count)counts = np.array([counts[&#39;class_0&#39;].numpy(),                   counts[&#39;class_1&#39;].numpy()]).astype(np.float32)fractions = counts/counts.sum()print(fractions)[0.9953 0.0047]</code></pre><p>偏差很大，这样训练的二分类器只需全预测为正类，即可达到99.53%的正确率。</p><p>重采样数据集的一种方法是使用<code>sample_from_datasets</code> 。当每个类都有单独的<code>data.Dataset</code>时，此方法更适用。</p><p>正类和反类分别构建 <code>Dataset</code></p><pre><code class="lang-py">negative_ds = (  creditcard_ds    .unbatch()    .filter(lambda features, label: label==0)    .repeat())positive_ds = (  creditcard_ds    .unbatch()    .filter(lambda features, label: label==1)    .repeat())</code></pre><p>要使用<code>tf.data.experimental.sample_from_datasets</code>传递数据集以及每个数据集的权重</p><pre><code class="lang-py">balanced_ds = tf.data.experimental.sample_from_datasets(    [negative_ds, positive_ds], [0.5, 0.5]).batch(10)for features, labels in balanced_ds.take(10):  print(labels.numpy())[0 1 1 0 0 1 0 1 0 0][0 1 0 1 1 0 0 1 1 0][1 1 1 0 0 1 1 0 1 1][0 0 0 0 1 1 0 0 1 0][0 1 1 0 0 0 1 0 0 0][1 1 0 0 0 0 0 0 0 0][0 1 0 1 1 0 0 1 1 1][0 0 1 0 1 0 1 0 1 1][1 0 1 1 0 1 0 0 1 0][0 0 0 1 1 1 1 0 1 1]</code></pre><p>现在数据集就平衡了。</p><p>上述<code>experimental.sample_from_datasets</code>方法的一个问题是，每个类需要一个单独的<code>tf.data.Dataset</code>。</p><p>可以将<code>data.experimental.rejection_resample</code>函数应用于数据集，它仅加载一次，通过将多余元素将从数据集中删除以实现平衡。</p><p><code>data.experimental.rejection_resample</code>采用<code>class_func</code>参数，用于标记每个数据集元素所属的类别。</p><p>由于<code>Dataset</code>已经是<code>(features, label)</code>标记好的状态，因此只需</p><pre><code class="lang-py">def class_func(features, label):  return label</code></pre><p>重采样器输入的数据不能为batch后的Dataset，必须经过unbatch。重采样器还需要目标分布，以及可选的初始分布估计。最后经过map中的函数，直接删除掉extra_label即可。</p><pre><code class="lang-py">resampler = tf.data.experimental.rejection_resample(    class_func, target_dist=[0.5, 0.5], initial_dist=fractions)resample_ds = creditcard_ds.unbatch().apply(resampler).batch(10)balanced_ds = resample_ds.map(lambda extra_label, features_and_label: features_and_label)</code></pre><h2 id="数据集迭代器的checkpoint"><a href="#数据集迭代器的checkpoint" class="headerlink" title="数据集迭代器的checkpoint"></a>数据集迭代器的checkpoint</h2><p>没想到吧？不只是模型能使用checkpoint，Dataset的处理过程也可以使用checkpoint。如果您有一个很大的数据集，并且不想在每次重新启动时都从头开始，则这可能很有用。但是请注意，迭代器检查点可能很大，因为诸如shuffle和prefetch需要迭代器中的缓冲元素。</p><p>下面是示例：</p><pre><code class="lang-py">range_ds = tf.data.Dataset.range(20)iterator = iter(range_ds)ckpt = tf.train.Checkpoint(step=tf.Variable(0), iterator=iterator)manager = tf.train.CheckpointManager(ckpt, &#39;/tmp/my_ckpt&#39;, max_to_keep=3)print([next(iterator).numpy() for _ in range(5)])[0, 1, 2, 3, 4]save_path = manager.save()print([next(iterator).numpy() for _ in range(5)])[5, 6, 7, 8, 9]ckpt.restore(manager.latest_checkpoint)print([next(iterator).numpy() for _ in range(5)])[5, 6, 7, 8, 9]</code></pre><h2 id="在Keras中使用tf-data"><a href="#在Keras中使用tf-data" class="headerlink" title="在Keras中使用tf.data"></a>在Keras中使用<code>tf.data</code></h2><p>数据集的处理：</p><pre><code class="lang-py">train, test = tf.keras.datasets.fashion_mnist.load_data()images, labels = trainimages = images/255.0labels = labels.astype(np.int32)fmnist_train_ds = tf.data.Dataset.from_tensor_slices((images, labels))fmnist_train_ds = fmnist_train_ds.shuffle(5000).batch(32)</code></pre><p>模型构建：</p><pre><code class="lang-py">model = tf.keras.Sequential([  tf.keras.layers.Flatten(),  tf.keras.layers.Dense(10)])model.compile(optimizer=&#39;adam&#39;,              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),               metrics=[&#39;accuracy&#39;])</code></pre><p>模型训练：</p><pre><code class="lang-py">model.fit(fmnist_train_ds, epochs=2)</code></pre><p>如果在fit过程中你要对Dataset进行repeat，只需指定每个epochs使用的数据个数，然后不给repeat指定参数，数据集就会变成无限个，一定会满足epochs的要求。</p><pre><code class="lang-py">model.fit(fmnist_train_ds.repeat(), epochs=2, steps_per_epoch=20)</code></pre><p>同理，evaluate时也是一样的</p><pre><code class="lang-py">loss, accuracy = model.evaluate(fmnist_train_ds.repeat(), steps=10)print(&quot;Loss :&quot;, loss)print(&quot;Accuracy :&quot;, accuracy)Loss : 0.3501795828342438Accuracy : 0.8968750238418579</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>如果在使用机器学习算法解决问题过程中，数据预处理是主要问题的话，采用合适的数据组织手段可以帮助解决问题。 <code>tf.data</code> 能够快速处理大量数据，并将各个来源的数据归一化成合适的 <code>Dataset</code> 格式。</p><p>你可以对构建好的 <code>tf.data.Dataset</code> 做预处理操作，比如随机打乱、分批次、规划时间窗口、重采样等等。</p><p>经过处理后的 <code>Dataset</code> 对象可以直接输入到keras进行训练。</p></batchdataset></batchdataset>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用 &lt;code&gt;tf.data&lt;/code&gt; API 可以轻松处理大量数据，支持多样化的数据格式，还可以方便执行复杂的转换。本文介绍了不同类别源数据转化为 &lt;code&gt;tf.data.Dataset&lt;/code&gt; 的方法，以及 &lt;code&gt;Dataset&lt;/code&gt; 常见的预处理方法。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="preprocessing" scheme="https://superlova.github.io/tags/preprocessing/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】使用TensorBoard分析模型性能</title>
    <link href="https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8TensorBoard%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD/"/>
    <id>https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8TensorBoard%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD/</id>
    <published>2020-08-14T08:39:06.000Z</published>
    <updated>2020-08-15T01:04:23.193Z</updated>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第六篇，讲述的是如何使用TensorBoard分析模型的性能，优化模型的资源消耗。<br><a id="more"></a></p><p>TensorBoard可以监控模型的各个组分运行过程中的时间消耗和资源消耗，并根据这些数据对模型下一步优化提出建议。</p><p>首先我们安装性能分析的插件</p><pre><code class="lang-bash">!pip install -U tensorboard_plugin_profile</code></pre><p>定义TensorBoard的回调函数（数据预处理和模型定义略去不表），注意这里新的参数<code>profile_batch</code>只监控第500到520之间的20个Batch，避免监控过多导致模型运行效率过低。</p><pre><code class="lang-python"># Create a TensorBoard callbacklogs = &quot;logs/&quot; + datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)tboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs,                        histogram_freq=1,                        profile_batch=&#39;500,520&#39;)model.fit(ds_train,    epochs=2,    validation_data=ds_test,    callbacks=[tboard_callback])</code></pre><p>打开TensorBoard：</p><pre><code class="lang-bash"># Load the TensorBoard notebook extension.%load_ext tensorboard# Launch TensorBoard and navigate to the Profile tab to view performance profile%tensorboard --logdir=logs</code></pre><p><img src="/2020/08/14/【学习笔记】使用TensorBoard分析模型性能/2020-08-14-16-47-10.png" srcset="/img/loading.gif" alt></p><p>有非常多有用的信息，比如每个batch消耗的时间都花在哪里了：</p><p><img src="/2020/08/14/【学习笔记】使用TensorBoard分析模型性能/2020-08-14-17-09-32.png" srcset="/img/loading.gif" alt></p><p>还有针对耗时的改进意见：</p><p><img src="/2020/08/14/【学习笔记】使用TensorBoard分析模型性能/2020-08-14-17-09-56.png" srcset="/img/loading.gif" alt></p><p>有耗时最长的10大操作：</p><p><img src="/2020/08/14/【学习笔记】使用TensorBoard分析模型性能/2020-08-14-17-10-27.png" srcset="/img/loading.gif" alt></p><p>有性能监控选项，查看CPU活动和GPU活动。根据一般经验，始终保持设备（GPU / TPU）处于活动状态是我们的优化目标。</p><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/profiler_trace_viewer_bad_ip.png" srcset="/img/loading.gif" alt></p><p>查看事件跟踪，可以看到<code>tf_data_iterator_get_next</code> 操作在CPU上运行时GPU不活动。该操作负责处理输入数据并将其发送到GPU进行训练。因此我们的优化方法可以是使用tf.data API优化输入管道，缓存训练数据集并预取数据，以确保始终有可供GPU处理的数据。</p><pre><code class="lang-python"> (ds_train, ds_test), ds_info = tfds.load(    &#39;mnist&#39;,    split=[&#39;train&#39;, &#39;test&#39;],    shuffle_files=True,    as_supervised=True,    with_info=True,)ds_train = ds_train.map(normalize_img)ds_train = ds_train.batch(128)ds_train = ds_train.cache()ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)ds_test = ds_test.map(normalize_img)ds_test = ds_test.batch(128)ds_test = ds_test.cache()ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)model.fit(ds_train,        epochs=2,        validation_data=ds_test,        callbacks = [tboard_callback])%tensorboard --logdir=logs</code></pre><p>跟踪查看器显示<code>tf_data_iterator_get_next</code>操作执行得更快。因此，GPU获得了稳定的数据流以进行训练，并通过模型训练获得了更好的利用率。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是TensorBoard笔记的第六篇，讲述的是如何使用TensorBoard分析模型的性能，优化模型的资源消耗。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="TensorBoard" scheme="https://superlova.github.io/tags/TensorBoard/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】通过TensorBoard可视化词嵌入空间</title>
    <link href="https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%8D%E5%B5%8C%E5%85%A5%E7%A9%BA%E9%97%B4/"/>
    <id>https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%8D%E5%B5%8C%E5%85%A5%E7%A9%BA%E9%97%B4/</id>
    <published>2020-08-14T08:37:06.000Z</published>
    <updated>2020-08-14T08:38:23.417Z</updated>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第五篇，讲述的是如何使用TensorBoard对词嵌入空间可视化。<br><a id="more"></a></p><p>TensorBoard可以将词嵌入空间二维化，方便我们理解词嵌入空间的含义。</p><pre><code class="lang-python">from tensorboard.plugins import projector</code></pre><p>引入IMDb数据集。tensorflow_datasets是tensorflow的官方数据集库。</p><pre><code class="lang-python">import tensorflow_datasets as tfds(train_data, test_data), info = tfds.load(    &quot;imdb_reviews/subwords8k&quot;,    split=(tfds.Split.TRAIN, tfds.Split.TEST),    with_info=True,    as_supervised=True,)encoder = info.features[&quot;text&quot;].encoder# shuffle and pad the data.train_batches = train_data.shuffle(1000).padded_batch(    10, padded_shapes=((None,), ()))test_batches = test_data.shuffle(1000).padded_batch(    10, padded_shapes=((None,), ()))train_batch, train_labels = next(iter(train_batches))</code></pre><p>训练词嵌入模型</p><pre><code class="lang-python"># Create an embedding layerembedding_dim = 16embedding = tf.keras.layers.Embedding(encoder.vocab_size, embedding_dim)# Train this embedding as part of a keras modelmodel = tf.keras.Sequential(    [        embedding, # The embedding layer should be the first layer in a model.        tf.keras.layers.GlobalAveragePooling1D(),        tf.keras.layers.Dense(16, activation=&quot;relu&quot;),        tf.keras.layers.Dense(1),    ])# Compile modelmodel.compile(    optimizer=&quot;adam&quot;,    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),    metrics=[&quot;accuracy&quot;],)# Train modelhistory = model.fit(    train_batches, epochs=1, validation_data=test_batches, validation_steps=20)</code></pre><p>将数据保存成方便TensorBoard读取的形式</p><pre><code class="lang-python"># Set up a logs directory, so Tensorboard knows where to look for fileslog_dir=&#39;/logs/imdb-example/&#39;if not os.path.exists(log_dir):    os.makedirs(log_dir)# Save Labels separately on a line-by-line manner.with open(os.path.join(log_dir, &#39;metadata.tsv&#39;), &quot;w&quot;) as f:  for subwords in encoder.subwords:    f.write(&quot;{}\n&quot;.format(subwords))  # Fill in the rest of the labels with &quot;unknown&quot;  for unknown in range(1, encoder.vocab_size - len(encoder.subwords)):    f.write(&quot;unknown #{}\n&quot;.format(unknown))# Save the weights we want to analyse as a variable. Note that the first# value represents any unknown word, which is not in the metadata, so# we will remove that value.weights = tf.Variable(model.layers[0].get_weights()[0][1:])# Create a checkpoint from embedding, the filename and key are# name of the tensor.checkpoint = tf.train.Checkpoint(embedding=weights)checkpoint.save(os.path.join(log_dir, &quot;embedding.ckpt&quot;))# Set up configconfig = projector.ProjectorConfig()embedding = config.embeddings.add()# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`embedding.tensor_name = &quot;embedding/.ATTRIBUTES/VARIABLE_VALUE&quot;embedding.metadata_path = &#39;metadata.tsv&#39;projector.visualize_embeddings(log_dir, config)</code></pre><pre><code class="lang-python">%tensorboard --logdir /logs/imdb-example/</code></pre><p>你可以看到2维、3维空间中的Embedding，搜索某个单词在词嵌入空间中的位置，甚至可以采取不同的可视化方法：<br><img src="/2020/08/14/【学习笔记】通过TensorBoard可视化词嵌入空间/2020-08-13-19-45-25.png" srcset="/img/loading.gif" alt></p><p>打开该工具的时候我的电脑很卡。。。希望大家注意。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是TensorBoard笔记的第五篇，讲述的是如何使用TensorBoard对词嵌入空间可视化。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="TensorBoard" scheme="https://superlova.github.io/tags/TensorBoard/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】通过TensorBoard调整超参数</title>
    <link href="https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87TensorBoard%E8%B0%83%E6%95%B4%E8%B6%85%E5%8F%82%E6%95%B0/"/>
    <id>https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87TensorBoard%E8%B0%83%E6%95%B4%E8%B6%85%E5%8F%82%E6%95%B0/</id>
    <published>2020-08-14T08:34:46.000Z</published>
    <updated>2020-08-14T08:36:27.555Z</updated>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第四篇，讲述的是如何借助TensorBoard调整模型的超参数。<br><a id="more"></a></p><p>TensorBoard中的HParams仪表板是比较新颖的工具包，提供了多种调节超参数的工具，并且该工具还在不断更新中。</p><pre><code class="lang-python">from tensorboard.plugins.hparams import api as hp</code></pre><p>用过sklearn进行机器学习模型调参的同学应该体验过交叉验证调参的方法。通过提供许多不同的超参数选项，<code>GridSearchCV</code>将训练多个模型，并取性能最优的模型超参数。</p><pre><code class="lang-python">from sklearn.model_selection import GridSearchCVfrom sklearn.pipeline import make_pipelineX_train, X_test, y_train, y_test = train_test_split(df_train[&#39;text&#39;][:10000], df_train[&#39;label&#39;][:10000], random_state=0)pipe_logis = make_pipeline(TfidfVectorizer(min_df=5, ngram_range=(1,3)), LogisticRegression())param_grid = {&#39;logisticregression__C&#39;: [0.001, 0.01, 0.1, 1, 10]}grid = GridSearchCV(pipe_logis, param_grid, cv=5)grid.fit(X_train, y_train)print(&quot;Best params:\n{}\n&quot;.format(grid.best_params_))print(&quot;Best cross-validation score: {:.2f}&quot;.format(grid.best_score_))print(&quot;Test-set score: {:.2f}&quot;.format(grid.score(X_test, y_test)))</code></pre><p>在上面的程序中，grid中包含一个需要调节的超参数，即逻辑回归的C值。候选C值有5个，因此grid在fit过程中会训练五个模型，每个模型执行5次交叉验证（因为fit中cv参数为5）。</p><p>HParams也是采用类似的方法找超参数。首先我们定义候选超参数的变化范围。我们选择三个参数进行网格搜索，分别是Dense层的Unit数目、dropout的比例和优化器，每个超参数都有两种选择，因此一共需要训练八个模型。</p><p>最终模型的评价标准以Accuracy为准。具体代码如下所示：</p><pre><code class="lang-python">HP_NUM_UNITS = hp.HParam(&#39;num_units&#39;, hp.Discrete([16, 32]))HP_DROPOUT = hp.HParam(&#39;dropout&#39;, hp.RealInterval(0.1, 0.2))HP_OPTIMIZER = hp.HParam(&#39;optimizer&#39;, hp.Discrete([&#39;adam&#39;, &#39;sgd&#39;]))METRIC_ACCURACY = &#39;accuracy&#39;</code></pre><p>设置write句柄，这已经是传统艺能了。</p><pre><code class="lang-python">with tf.summary.create_file_writer(&#39;logs/hparam_tuning&#39;).as_default():  hp.hparams_config(    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],    metrics=[hp.Metric(METRIC_ACCURACY, display_name=&#39;Accuracy&#39;)],  )</code></pre><p>接下来我们定义待训练模型。模型本身非常简单，而且只训练一个epoch，这是考虑到要消耗平时八倍的时间而采取的tradeoff。</p><pre><code class="lang-python">def train_test_model(hparams):  model = tf.keras.models.Sequential([    tf.keras.layers.Flatten(),    tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=tf.nn.relu),    tf.keras.layers.Dropout(hparams[HP_DROPOUT]),    tf.keras.layers.Dense(10, activation=tf.nn.softmax),  ])  model.compile(      optimizer=hparams[HP_OPTIMIZER],      loss=&#39;sparse_categorical_crossentropy&#39;,      metrics=[&#39;accuracy&#39;],  )  model.fit(x_train, y_train, epochs=1) # Run with 1 epoch to speed things up for demo purposes  _, accuracy = model.evaluate(x_test, y_test)  return accuracy</code></pre><p>进行训练并记录模型输出；</p><pre><code class="lang-python">def run(run_dir, hparams):  with tf.summary.create_file_writer(run_dir).as_default():    hp.hparams(hparams)  # record the values used in this trial    accuracy = train_test_model(hparams)    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)</code></pre><p>之后我们进行网格搜索（其实就是遍历每种可能。搜索方法完全是自己定义的，你也可以使用随机搜索方法）：</p><pre><code class="lang-python">session_num = 0for num_units in HP_NUM_UNITS.domain.values:  for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):    for optimizer in HP_OPTIMIZER.domain.values:      hparams = {          HP_NUM_UNITS: num_units,          HP_DROPOUT: dropout_rate,          HP_OPTIMIZER: optimizer,      }      run_name = &quot;run-%d&quot; % session_num      print(&#39;--- Starting trial: %s&#39; % run_name)      print({h.name: hparams[h] for h in hparams})      run(&#39;logs/hparam_tuning/&#39; + run_name, hparams)      session_num += 1</code></pre><p>最后使用TensorBoard进行可视化：</p><pre><code class="lang-bash">%tensorboard --logdir logs/hparam_tuning</code></pre><p><img src="https://www.tensorflow.org/tensorboard/images/hparams_table.png?raw=1" srcset="/img/loading.gif" alt></p><p>我们可以通过TensorBoard发现很多有趣的现象：比如在本模型中，adam优化器比sgd要好等等。</p><p><img src="/2020/08/14/【学习笔记】通过TensorBoard调整超参数/2020-08-13-19-19-41.png" srcset="/img/loading.gif" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是TensorBoard笔记的第四篇，讲述的是如何借助TensorBoard调整模型的超参数。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="TensorBoard" scheme="https://superlova.github.io/tags/TensorBoard/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】用TensorBoard生成模型图</title>
    <link href="https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8TensorBoard%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%9B%BE/"/>
    <id>https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8TensorBoard%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%9B%BE/</id>
    <published>2020-08-14T08:31:40.000Z</published>
    <updated>2020-08-14T08:34:09.831Z</updated>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第三篇，讲述的是如何令TensorBoard生成模型架构图。<br><a id="more"></a></p><p>TensorBoard不但可以展示存在的图片和张量，还可以生成图片，诸如模型图等。通过TensorBoard的GRAPHS选项卡，可以快速查看模型结构的预览图，并确保其符合设计预期。</p><p>比如我们定义模型如下：</p><pre><code class="lang-python">model = keras.models.Sequential([    keras.layers.Flatten(input_shape=(28, 28)),    keras.layers.Dense(32, activation=&#39;relu&#39;),    keras.layers.Dropout(0.2),    keras.layers.Dense(10, activation=&#39;softmax&#39;)])model.compile(    optimizer=&#39;adam&#39;,    loss=&#39;sparse_categorical_crossentropy&#39;,    metrics=[&#39;accuracy&#39;])</code></pre><p><img src="/2020/08/14/【学习笔记】用TensorBoard生成模型图/2020-08-13-13-31-51.png" srcset="/img/loading.gif" alt></p><p>通过TensorBoard中的GRAPHS选项卡，我们看到执行图。图是倒置的，数据从下到上流动，因此与代码相比是上下颠倒的。</p><p>可以更改Tag，选择Keras，选择左边的Conceptual Graph查看概念图，双击Sequential，得到概念图。概念图更像是代码。</p><p><img src="/2020/08/14/【学习笔记】用TensorBoard生成模型图/2020-08-13-13-35-03.png" srcset="/img/loading.gif" alt></p><p>有的时候我们希望得到计算图，研究数据经过了何种计算。比如下面这个函数：</p><pre><code class="lang-python"># Sample data for your function.x = tf.random.uniform((3, 3))y = tf.random.uniform((3, 3))# The function to be traced.@tf.functiondef my_func(x, y):  # A simple hand-rolled layer.  return tf.nn.relu(tf.matmul(x, y))# z = my_func(x, y)</code></pre><p>我们希望得到它的计算图。首先需要使用<code>@tf.function</code>修饰被监控的函数，然后使用<code>tf.summary.trace_on()</code>在<code>z = my_func(x, y)</code>函数运行之前开始记录。</p><p><img src="https://as2.bitinn.net/uploads/legacy/og/cistioqrt008t8q5nh9gtx9og.1200.jpg" srcset="/img/loading.gif" alt="Trace On"></p><pre><code class="lang-python">tf.summary.trace_on(graph=True, profiler=True)z = my_func(x, y)</code></pre><p>定义日志目录名称和文件写入句柄，这些都是刻在DNA里的操作：</p><pre><code class="lang-python"># Set up logging.stamp = datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)logdir = &#39;logs/func/%s&#39; % stampwriter = tf.summary.create_file_writer(logdir)</code></pre><p>最后执行记录：</p><pre><code class="lang-python"># Call only one tf.function when tracing.with writer.as_default():  tf.summary.trace_export(      name=&quot;my_func_trace&quot;,      step=0,      profiler_outdir=logdir)</code></pre><pre><code class="lang-bash">%tensorboard --logdir logs/func</code></pre><p><img src="/2020/08/14/【学习笔记】用TensorBoard生成模型图/2020-08-13-13-47-57.png" srcset="/img/loading.gif" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是TensorBoard笔记的第三篇，讲述的是如何令TensorBoard生成模型架构图。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="TensorBoard" scheme="https://superlova.github.io/tags/TensorBoard/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】用TensorBoard展示图片</title>
    <link href="https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8TensorBoard%E5%B1%95%E7%A4%BA%E5%9B%BE%E7%89%87/"/>
    <id>https://superlova.github.io/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8TensorBoard%E5%B1%95%E7%A4%BA%E5%9B%BE%E7%89%87/</id>
    <published>2020-08-14T08:28:29.000Z</published>
    <updated>2020-08-14T08:31:11.010Z</updated>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第二篇，讲述的是如何令TensorBoard展示外界已有的图片和Tensor。<br><a id="more"></a></p><p>我们可以利用TensorBoard展示图片类数据，或者通过tf.summary将张量类数据转化成图片。下面是对Fashion-MNIST数据集中部分图片的可视化：</p><h3 id="可视化单个图片"><a href="#可视化单个图片" class="headerlink" title="可视化单个图片"></a>可视化单个图片</h3><pre><code class="lang-python">import tensorflow as tffrom tensorflow import kerasfashion_mnist = keras.datasets.fashion_mnist(train_images, train_labels), (test_images, test_labels) = \    fashion_mnist.load_data()</code></pre><p>数据集中每个图像的形状都是2阶张量形状（28、28），分别表示高度和宽度</p><p>但是， tf.summary.image()期望包含(batch_size, height, width, channels)的4级张量。因此，张量需要重塑。</p><pre><code class="lang-python">img = np.reshape(train_images[0], (-1, 28, 28, 1))</code></pre><p>使用<code>tf.summary.image</code>将其转化为tensor，并利用TensorBoard可视化：</p><pre><code class="lang-python"> # Clear out any prior log data.!rm -rf logs# Sets up a timestamped log directory.logdir = &quot;logs/train_data/&quot; + datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)# Creates a file writer for the log directory.file_writer = tf.summary.create_file_writer(logdir)# Using the file writer, log the reshaped image.with file_writer.as_default():  tf.summary.image(&quot;Training data&quot;, img, step=0)</code></pre><p>转化后的图片被<code>tf.summary.create_file_writer</code>输出到logdir里面了。使用TensorBoard看看：</p><pre><code class="lang-bash">%tensorboard --logdir logs/train_data</code></pre><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_single.png" srcset="/img/loading.gif" alt></p><p>加载的过程可能有点慢，注意留足够的内存以免标签页崩溃。</p><p>你也可以使用左边的滑动条调节亮度、对比度和大小。</p><h3 id="可视化多张图片"><a href="#可视化多张图片" class="headerlink" title="可视化多张图片"></a>可视化多张图片</h3><p>调整<code>tf.summary.image</code>里面的参数<code>max_outputs</code>：</p><pre><code class="lang-python"> with file_writer.as_default():    # Don&#39;t forget to reshape.    images = np.reshape(train_images[0:25], (-1, 28, 28, 1))    tf.summary.image(&quot;25 training data examples&quot;, images, max_outputs=25, step=0)</code></pre><pre><code class="lang-bash">%tensorboard --logdir logs/train_data</code></pre><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_multiple.png" srcset="/img/loading.gif" alt></p><h3 id="可视化其他格式的图片"><a href="#可视化其他格式的图片" class="headerlink" title="可视化其他格式的图片"></a>可视化其他格式的图片</h3><p>有些图片不是tensor或者numpy.array，而是由诸如opencv、matplotlib生成的png图像，我们需要将其转化为tensor。</p><p>由于matplotlib适合生成复杂的数据图，因此先利用其他库生成图片，随后利用<code>tf.summary.image</code>将其转化为一个tensor再可视化，是一个比较方便的选择。</p><p>matplotlib生成数据集可视化：</p><pre><code class="lang-python"> # Clear out prior logging data.!rm -rf logs/plotslogdir = &quot;logs/plots/&quot; + datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)file_writer = tf.summary.create_file_writer(logdir)def plot_to_image(figure):  &quot;&quot;&quot;Converts the matplotlib plot specified by &#39;figure&#39; to a PNG image and  returns it. The supplied figure is closed and inaccessible after this call.&quot;&quot;&quot;  # Save the plot to a PNG in memory.  buf = io.BytesIO()  plt.savefig(buf, format=&#39;png&#39;)  # Closing the figure prevents it from being displayed directly inside  # the notebook.  plt.close(figure)  buf.seek(0)  # Convert PNG buffer to TF image  image = tf.image.decode_png(buf.getvalue(), channels=4)  # Add the batch dimension  image = tf.expand_dims(image, 0)  return imagedef image_grid():  &quot;&quot;&quot;Return a 5x5 grid of the MNIST images as a matplotlib figure.&quot;&quot;&quot;  # Create a figure to contain the plot.  figure = plt.figure(figsize=(10,10))  for i in range(25):    # Start next subplot.    plt.subplot(5, 5, i + 1, title=class_names[train_labels[i]])    plt.xticks([])    plt.yticks([])    plt.grid(False)    plt.imshow(train_images[i], cmap=plt.cm.binary)  return figure</code></pre><p>尔后，利用<code>tf.summary.image</code>转化：</p><pre><code class="lang-python"># Prepare the plotfigure = image_grid()# Convert to image and logwith file_writer.as_default():  tf.summary.image(&quot;Training data&quot;, plot_to_image(figure), step=0)</code></pre><p>最后，利用TensorBoard可视化：</p><pre><code class="lang-bash">%tensorboard --logdir logs/plots</code></pre><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_arbitrary.png" srcset="/img/loading.gif" alt></p><h3 id="在图片分类器中使用TensorBoard"><a href="#在图片分类器中使用TensorBoard" class="headerlink" title="在图片分类器中使用TensorBoard"></a>在图片分类器中使用TensorBoard</h3><p>之前我们通过TensorBoard了解了Fashion-MNIST数据集的概要，但是TensorBoard的功能不止于此。</p><p>首先构建分类模型：</p><pre><code class="lang-python">model = keras.models.Sequential([    keras.layers.Flatten(input_shape=(28, 28)),    keras.layers.Dense(32, activation=&#39;relu&#39;),    keras.layers.Dense(10, activation=&#39;softmax&#39;)])model.compile(    optimizer=&#39;adam&#39;,     loss=&#39;sparse_categorical_crossentropy&#39;,    metrics=[&#39;accuracy&#39;])</code></pre><p>我们想使用<a href="https://en.wikipedia.org/wiki/Confusion_matrix" target="_blank" rel="noopener">混淆矩阵</a>详细了解分类器对测试数据的性能。因此接下来定义一个函数，专门计算混淆矩阵。具体来说，</p><ol><li>使用model.predict预测该epoch的所有测试用例的标签，得到<code>test_pred</code></li><li>调用<code>sklearn.metrics.confusion_matrix</code>直接计算混淆矩阵</li><li>使用<code>matplotlib</code>将混淆矩阵可视化</li><li>将<code>matplotlib</code>生成的图片转为tensor，最后变成log储存</li></ol><p>下面是前两步所需的操作：</p><pre><code class="lang-python">def log_confusion_matrix(epoch, logs):    # Use the model to predict the values from the validation dataset.    test_pred_raw = model.predict(test_images)    test_pred = np.argmax(test_pred_raw, axis=1)    # Calculate the confusion matrix.    cm = sklearn.metrics.confusion_matrix(test_labels, test_pred)    # Log the confusion matrix as an image summary.    figure = plot_confusion_matrix(cm, class_names=class_names)    cm_image = plot_to_image(figure)    # Log the confusion matrix as an image summary.    with file_writer_cm.as_default():    tf.summary.image(&quot;Confusion Matrix&quot;, cm_image, step=epoch)</code></pre><p>下面是第三步所需的可视化函数：</p><pre><code class="lang-python">def plot_confusion_matrix(cm, class_names):    &quot;&quot;&quot;    Returns a matplotlib figure containing the plotted confusion matrix.    Args:    cm (array, shape = [n, n]): a confusion matrix of integer classes    class_names (array, shape = [n]): String names of the integer classes    &quot;&quot;&quot;    figure = plt.figure(figsize=(8, 8))    plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=plt.cm.Blues)    plt.title(&quot;Confusion matrix&quot;)    plt.colorbar()    tick_marks = np.arange(len(class_names))    plt.xticks(tick_marks, class_names, rotation=45)    plt.yticks(tick_marks, class_names)    # Normalize the confusion matrix.    cm = np.around(cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis], decimals=2)    # Use white text if squares are dark; otherwise black.    threshold = cm.max() / 2.    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):    color = &quot;white&quot; if cm[i, j] &gt; threshold else &quot;black&quot;    plt.text(j, i, cm[i, j], horizontalalignment=&quot;center&quot;, color=color)    plt.tight_layout()    plt.ylabel(&#39;True label&#39;)    plt.xlabel(&#39;Predicted label&#39;)    return figure</code></pre><p>下面是第四步所需的tensor转化和储存函数以及其他回调函数：</p><pre><code class="lang-bash"># Clear out prior logging data.!rm -rf logs/image</code></pre><pre><code class="lang-python">logdir = &quot;logs/image/&quot; + datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)# Define the basic TensorBoard callback.tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)file_writer_cm = tf.summary.create_file_writer(logdir + &#39;/cm&#39;)</code></pre><p>让我们开始训练：</p><pre><code class="lang-bash"># Start TensorBoard.%tensorboard --logdir logs/image</code></pre><pre><code class="lang-python"># Train the classifier.model.fit(    train_images,    train_labels,    epochs=5,    verbose=0, # Suppress chatty output    callbacks=[tensorboard_callback, cm_callback],    validation_data=(test_images, test_labels),)</code></pre><p>请注意，此时我先调用的TensorBoard，然后开始的训练，并且我设置了verbose=0，意味着信息完全通过TensorBoard动态展示。训练过程中你就可以看到参数的变化。</p><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_accuracy.png" srcset="/img/loading.gif" alt></p><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_cm.png" srcset="/img/loading.gif" alt></p><p>你还可以看到，随着训练的进行，矩阵是如何发生变化的：沿着对角线的正方形会逐渐变暗，而矩阵的其余部分趋向于0和白色。这意味着分类器正在不断改进。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是TensorBoard笔记的第二篇，讲述的是如何令TensorBoard展示外界已有的图片和Tensor。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Tensorflow" scheme="https://superlova.github.io/tags/Tensorflow/"/>
    
      <category term="TensorBoard" scheme="https://superlova.github.io/tags/TensorBoard/"/>
    
  </entry>
  
</feed>
