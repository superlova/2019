<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Superlova</title>
  
  <subtitle>Be a better man...</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://superlova.github.io/"/>
  <updated>2020-05-27T06:23:44.140Z</updated>
  <id>https://superlova.github.io/</id>
  
  <author>
    <name>Superlova</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Datawhale——SVHN——Task03：字符识别</title>
    <link href="https://superlova.github.io/2020/05/27/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task03%EF%BC%9A%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB/"/>
    <id>https://superlova.github.io/2020/05/27/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task03%EF%BC%9A%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB/</id>
    <published>2020-05-27T05:30:18.000Z</published>
    <updated>2020-05-27T06:23:44.140Z</updated>
    
    <content type="html"><![CDATA[<p>介绍常见的字符识别模型<br><a id="more"></a></p><h2 id="用-tf-data-加载图片"><a href="#用-tf-data-加载图片" class="headerlink" title="用 tf.data 加载图片"></a>用 tf.data 加载图片</h2><h2 id="用CNN建立字符识别模型"><a href="#用CNN建立字符识别模型" class="headerlink" title="用CNN建立字符识别模型"></a>用CNN建立字符识别模型</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍常见的字符识别模型&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="datawhale" scheme="https://superlova.github.io/tags/datawhale/"/>
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="Digit Recognition" scheme="https://superlova.github.io/tags/Digit-Recognition/"/>
    
  </entry>
  
  <entry>
    <title>Datawhale——SVHN——Task02：数据扩增</title>
    <link href="https://superlova.github.io/2020/05/23/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task02%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E/"/>
    <id>https://superlova.github.io/2020/05/23/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task02%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E/</id>
    <published>2020-05-23T13:15:18.000Z</published>
    <updated>2020-05-23T14:53:46.680Z</updated>
    
    <content type="html"><![CDATA[<p>训练模型第一步、数据读取和扩增！<br><a id="more"></a></p><h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><p>图像领域的数据读取方法，使用Pillow或者OpenCV内置的函数即可。</p><p>在读取图像时，还可以对原始图像添加扰动等，这就启发我们一件事：是不是对原数据增加一些扰动，就可以使其变成新的数据呢？</p><p>下面介绍利用该思想的数据扩增环节。</p><h2 id="数据扩增"><a href="#数据扩增" class="headerlink" title="数据扩增"></a>数据扩增</h2><h3 id="数据扩增为什么会有用"><a href="#数据扩增为什么会有用" class="headerlink" title="数据扩增为什么会有用"></a>数据扩增为什么会有用</h3><p>数据扩增的最常见作用，是增加数据集，用以缓解样本量不足导致的模型过拟合现象，从而提升模型的泛化性能。</p><p>究其本质，还是扩展数据集的多样性。</p><p>试想一下，我们如果想要试图训练一个完美模型，必然要利用完美的架构+完美的训练集，这个完美的训练集必然要覆盖到样本空间的方方面面。</p><p>我们当然不可能真的搞到无限多的样本。所以为了尽可能趋近于这个目标，就要试图以有限的数据集覆盖无限的样本空间。</p><p>每个样本在样本空间中就是一个坐标点。通过添加扰动，就能生成许多个在该样本点附近的增强样本。</p><p>上一个利用样本空间中添加扰动、从而生成与原样本很相似的应用，叫做<strong>生成对抗样本</strong>。</p><p>数据增强和对抗样本生成之间的区别在于，数据增强要保证扰动之后样本不能和原样本有区别；然而对抗样本生成则保证<strong>必须</strong>与原样本有区别。</p><p>有一些学者也通过将对抗样本添加至模型重训练的方法，使模型的泛化性能得到了提高。这说明数据增强和对抗样本的生效原理是一样的，都是通过扩大样本覆盖的样本空间的程度，通俗来讲就是模型见多识广了，再碰到新的问题也不怕了。</p><p>可以参考这篇论文：<a href="https://arxiv.org/abs/2003.08773" target="_blank" rel="noopener">Do CNNs Encode Data Augmentations?</a></p><h3 id="数据扩增方法"><a href="#数据扩增方法" class="headerlink" title="数据扩增方法"></a>数据扩增方法</h3><p>大家都用的Pytorch吗？不会只有我自己用tensorflow吧。我来给大家介绍一下tensorflow是怎么做数据扩增的。</p><p>参考：<a href="https://www.tensorflow.org/tutorials/images/data_augmentation" target="_blank" rel="noopener">TensorFlow Core</a></p><h4 id="1-准备"><a href="#1-准备" class="headerlink" title="1. 准备"></a>1. 准备</h4><pre><code class="lang-python"># 首先安装一个tensorflow_docs的库!pip install git+https://github.com/tensorflow/docsimport urllib # 负责下载网上的图片import tensorflow as tffrom tensorflow.keras.datasets import mnistfrom tensorflow.keras import layersAUTOTUNE = tf.data.experimental.AUTOTUNEimport tensorflow_docs as tfdocsimport tensorflow_docs.plotsimport tensorflow_datasets as tfdsimport PIL.Image # 大名鼎鼎PILimport matplotlib.pyplot as pltimport matplotlib as mplmpl.rcParams[&#39;figure.figsize&#39;] = (12, 5)import numpy as np</code></pre><p>下载一张示例图片：</p><pre><code class="lang-python">image_path = tf.keras.utils.get_file(&quot;cat.jpg&quot;, &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/320px-Felis_catus-cat_on_snow.jpg&quot;)PIL.Image.open(image_path)</code></pre><p><img src="/2020/05/23/Datawhale——SVHN——Task02：数据扩增/2020-05-23-22-29-50.png" srcset="/img/loading.gif" alt></p><p>将该图片解析成tensor</p><pre><code class="lang-python">image_string=tf.io.read_file(image_path)image=tf.image.decode_jpeg(image_string,channels=3)</code></pre><p>定义一个函数，用于可视化图像。</p><pre><code class="lang-python">def visualize(original, augmented):  fig = plt.figure()  plt.subplot(1,2,1)  plt.title(&#39;Original image&#39;)  plt.imshow(original)  plt.subplot(1,2,2)  plt.title(&#39;Augmented image&#39;)  plt.imshow(augmented)</code></pre><h4 id="2-执行数据扩增"><a href="#2-执行数据扩增" class="headerlink" title="2. 执行数据扩增"></a>2. 执行数据扩增</h4><p><strong>翻转图像</strong></p><pre><code class="lang-python">flipped = tf.image.flip_left_right(image)visualize(image, flipped)</code></pre><p><img src="/2020/05/23/Datawhale——SVHN——Task02：数据扩增/2020-05-23-22-32-30.png" srcset="/img/loading.gif" alt></p><p><strong>灰度处理</strong></p><pre><code class="lang-python">grayscaled = tf.image.rgb_to_grayscale(image)visualize(image, tf.squeeze(grayscaled))plt.colorbar()</code></pre><p><img src="/2020/05/23/Datawhale——SVHN——Task02：数据扩增/2020-05-23-22-33-51.png" srcset="/img/loading.gif" alt></p><p><strong>改变图像饱和度</strong></p><pre><code class="lang-python">saturated = tf.image.adjust_saturation(image, 3)visualize(image, saturated)</code></pre><p><img src="/2020/05/23/Datawhale——SVHN——Task02：数据扩增/2020-05-23-22-35-05.png" srcset="/img/loading.gif" alt></p><p><strong>改变图像亮度</strong></p><pre><code class="lang-python">bright = tf.image.adjust_brightness(image, 0.4)visualize(image, bright)</code></pre><p><img src="/2020/05/23/Datawhale——SVHN——Task02：数据扩增/2020-05-23-22-35-39.png" srcset="/img/loading.gif" alt></p><p><strong>旋转图像</strong></p><pre><code class="lang-python">rotated = tf.image.rot90(image)visualize(image, rotated)</code></pre><p><img src="/2020/05/23/Datawhale——SVHN——Task02：数据扩增/2020-05-23-22-36-11.png" srcset="/img/loading.gif" alt></p><p><strong>中心放大并裁剪图像</strong></p><pre><code class="lang-python">cropped = tf.image.central_crop(image, central_fraction=0.5)visualize(image,cropped)</code></pre><p><img src="/2020/05/23/Datawhale——SVHN——Task02：数据扩增/2020-05-23-22-36-51.png" srcset="/img/loading.gif" alt></p><p>等等此类操作，不一而足。大家感兴趣的可以查阅tensorflow的<code>tf.image</code>文档。</p><h4 id="3-使用扩增数据集训练"><a href="#3-使用扩增数据集训练" class="headerlink" title="3. 使用扩增数据集训练"></a>3. 使用扩增数据集训练</h4><p>我们构造一个模型，该模型架构为纯全连接网络，数据集为MNIST手写数字识别数据集。我们可以直接在tensorflow_datasets这个库中使用这个数据集。</p><pre><code class="lang-python">dataset, info =  tfds.load(&#39;mnist&#39;, as_supervised=True, with_info=True)train_dataset, test_dataset = dataset[&#39;train&#39;], dataset[&#39;test&#39;]num_train_examples= info.splits[&#39;train&#39;].num_examples</code></pre><p>编写函数执行对原来数据集的扩增操作。</p><pre><code class="lang-python">def convert(image, label):  image = tf.image.convert_image_dtype(image, tf.float32) # Cast and normalize the image to [0,1]  return image, labeldef augment(image,label):  image,label = convert(image, label)  image = tf.image.convert_image_dtype(image, tf.float32) # Cast and normalize the image to [0,1]  image = tf.image.resize_with_crop_or_pad(image, 34, 34) # Add 6 pixels of padding  image = tf.image.random_crop(image, size=[28, 28, 1]) # Random crop back to 28x28  image = tf.image.random_brightness(image, max_delta=0.5) # Random brightness  return image,label</code></pre><pre><code class="lang-python">BATCH_SIZE = 64# Only use a subset of the data so it&#39;s easier to overfit, for this tutorialNUM_EXAMPLES = 2048</code></pre><p>创建扩增后的数据集</p><pre><code class="lang-python">augmented_train_batches = (    train_dataset    # Only train on a subset, so you can quickly see the effect.    .take(NUM_EXAMPLES)    .cache()    .shuffle(num_train_examples//4)    # The augmentation is added here.    .map(augment, num_parallel_calls=AUTOTUNE)    .batch(BATCH_SIZE)    .prefetch(AUTOTUNE))</code></pre><p>为了对照，我们创建没有扩增的数据集。</p><pre><code class="lang-python">non_augmented_train_batches = (    train_dataset    # Only train on a subset, so you can quickly see the effect.    .take(NUM_EXAMPLES)    .cache()    .shuffle(num_train_examples//4)    # No augmentation.    .map(convert, num_parallel_calls=AUTOTUNE)    .batch(BATCH_SIZE)    .prefetch(AUTOTUNE))</code></pre><p>设置验证集。验证集与数据增不增强无关，反正我们不使用验证机训练，只用于最后的打分。</p><pre><code class="lang-python">validation_batches = (    test_dataset    .map(convert, num_parallel_calls=AUTOTUNE)    .batch(2*BATCH_SIZE))</code></pre><p>建立模型。注意这个模型纯粹是为了体现数据扩增的效果而专门构建的，因为卷积网络CNN即便是不用数据扩增也能很好地解决MNIST手写数字识别问题，这样比较起来效果就不明显了。两层4096个神经元的全连接网络，激活函数为RELU。最后是一个softmax层分类。</p><pre><code class="lang-python">def make_model():  model = tf.keras.Sequential([      layers.Flatten(input_shape=(28, 28, 1)),      layers.Dense(4096, activation=&#39;relu&#39;),      layers.Dense(4096, activation=&#39;relu&#39;),      layers.Dense(10)  ])  model.compile(optimizer = &#39;adam&#39;,                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),                metrics=[&#39;accuracy&#39;])  return model</code></pre><p>先使用<strong>没有经过数据扩增</strong>的数据训练模型，并记录其精度变化和loss变化：</p><pre><code class="lang-python">model_without_aug = make_model()no_aug_history = model_without_aug.fit(non_augmented_train_batches, epochs=50, validation_data=validation_batches)</code></pre><p>再使用<strong>经过扩增的数据</strong>训练模型，并记录。</p><pre><code class="lang-python">model_with_aug = make_model()aug_history = model_with_aug.fit(augmented_train_batches, epochs=50, validation_data=validation_batches)</code></pre><p>最后绘制图标，看一下表现。</p><p>首先是精度随着训练轮次的变化曲线：</p><pre><code class="lang-python">plotter = tfdocs.plots.HistoryPlotter()plotter.plot({&quot;Augmented&quot;: aug_history, &quot;Non-Augmented&quot;: no_aug_history}, metric = &quot;accuracy&quot;)plt.title(&quot;Accuracy&quot;)plt.ylim([0.75,1])</code></pre><p><img src="/2020/05/23/Datawhale——SVHN——Task02：数据扩增/2020-05-23-22-49-01.png" srcset="/img/loading.gif" alt></p><p>从图中可以看出，橙色线（没有数据增强的模型）在训练的时候很容易过拟合，但是在验证集上的精度不及蓝色线（数据增强的模型）。</p><p>再来看loss变化。</p><pre><code class="lang-python">plotter = tfdocs.plots.HistoryPlotter()plotter.plot({&quot;Augmented&quot;: aug_history, &quot;Non-Augmented&quot;: no_aug_history}, metric = &quot;loss&quot;)plt.title(&quot;Loss&quot;)plt.ylim([0,1])</code></pre><p><img src="/2020/05/23/Datawhale——SVHN——Task02：数据扩增/2020-05-23-22-51-30.png" srcset="/img/loading.gif" alt></p><p>这里看的就更明显了，橙色线在训练时的loss很快就下降到趋近0，这说明模型已经很难从未经增强的数据中学到东西了，产生了严重的过拟合。</p><p>而蓝色线直到最后也在逐步地学习之中，我们可以得出结论，数据增强的确有助于避免过拟合、增强模型的泛化性能。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;训练模型第一步、数据读取和扩增！&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="datawhale" scheme="https://superlova.github.io/tags/datawhale/"/>
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="Data Augmentation" scheme="https://superlova.github.io/tags/Data-Augmentation/"/>
    
  </entry>
  
  <entry>
    <title>如何评价推荐系统以及其他智能系统</title>
    <link href="https://superlova.github.io/2020/05/20/%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F/"/>
    <id>https://superlova.github.io/2020/05/20/%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F/</id>
    <published>2020-05-20T13:43:38.000Z</published>
    <updated>2020-05-21T00:51:02.937Z</updated>
    
    <content type="html"><![CDATA[<p>看到一篇介绍推荐系统的评价方法与指标的文章，刚好本课题组也有个同学在做推荐系统相关的课题，并且该文章的评价指标部分对我目前的课题有启发作用，因此转载。<br><a id="more"></a></p><h1 id="如何评价推荐系统以及其他智能系统"><a href="#如何评价推荐系统以及其他智能系统" class="headerlink" title="如何评价推荐系统以及其他智能系统"></a>如何评价推荐系统以及其他智能系统</h1><h2 id="1-评价推荐系统"><a href="#1-评价推荐系统" class="headerlink" title="1. 评价推荐系统"></a>1. <a href="https://www.jianshu.com/p/9d7c228eee59" target="_blank" rel="noopener">评价推荐系统</a></h2><p>评测一个推荐系统时，需要考虑用户、物品提供商、推荐系统提供网站的利益，一个好的推荐系统是能够令三方共赢的系统。比如弹窗广告就不是一个好的推荐系统。</p><h3 id="1-1-评测实验方法"><a href="#1-1-评测实验方法" class="headerlink" title="1.1 评测实验方法"></a>1.1 评测实验方法</h3><h4 id="1-1-1-离线实验（offline-experiment）"><a href="#1-1-1-离线实验（offline-experiment）" class="headerlink" title="1.1.1 离线实验（offline experiment）"></a>1.1.1 离线实验（offline experiment）</h4><p>离线实验的方法的步骤如下：</p><p>a）通过日志系统获得用户行为数据，并按照一定格式生成一个标准的数据集；<br>b）将数据集按照一定的规则分成训练集和测试集；<br>c）在训练集上训练用户兴趣模型，在测试集上进行预测；<br>d）通过事先定义的离线指标，评测算法在测试集上的预测结果。</p><p>从以上步骤看出，离线实验的都是在数据集上完成的。意味着，它不需要一个实际的系统作为支撑，只需要有一个从日志中提取的数据集即可。</p><p>离线实验的优点是：</p><ul><li>不需要有对实际系统的控制权；</li><li>不需要用户参与实践；</li><li>速度快，可以测试大量算法；</li></ul><p>缺点是：</p><ul><li>数据集的稀疏性限制了适用范围，例如一个数据集中没有包含某用户的历史行为，则无法评价对该用户的推荐结果；</li><li>评价结果的客观性，无法得到用户主观性的评价；</li><li>难以找到离线评价指标和在线真实反馈(如 点击率、转化率、点击深度、购买客单价、购买商 品类别等)之间的关联关系；</li></ul><h4 id="1-1-2-用户调查（user-study）"><a href="#1-1-2-用户调查（user-study）" class="headerlink" title="1.1.2 用户调查（user study）"></a>1.1.2 用户调查（user study）</h4><p>用户调查需要一些真实的用户，让他们在需要测试的推荐系统上完成一些任务。在他们完成任务时，需要观察和记录用户的行为，并让他们回答一些问题。</p><p>最后，我们通过分析他们的行为和答案，了解测试系统的性能。</p><p>用户调查的优点是：</p><ul><li>可以获得用户主观感受的指标，出错后容易弥补；</li></ul><p>缺点是：</p><ul><li>招募测试用户代价较大；</li><li>无法组织大规模的测试用户，统计意义不足；</li></ul><h4 id="1-1-3-在线实验（online-experiment）"><a href="#1-1-3-在线实验（online-experiment）" class="headerlink" title="1.1.3 在线实验（online experiment）"></a>1.1.3 在线实验（online experiment）</h4><p>在完成离线实验和用户调查之后，可以将系统上线做AB测试，将它和旧算法进行比较。</p><p>在线实验最常用的评测算法是【A/B测试】，它通过一定的规则将用户随机分成几组，对不同组的用户采用不同的算法，然后通过统计不同组的评测指标，比较不同算法的好坏。</p><p>它的核心思想是:</p><p>a) 多个方案并行测试;<br>b) 每个方案只有一个变量不同;<br>c) 以某种规则优胜劣汰。</p><p>其中第2点暗示了A/B 测试的应用范围：A/B测试必须是单变量。</p><p>对于推荐系统的评价中，唯一变量就是—推荐算法。</p><p>有个<a href="http://www.abtests.com" target="_blank" rel="noopener">很棒的网站</a>，里面有很多通过实际AB测试提高网站用户满意度的例子。</p><p>AB测试的优点是：</p><ul><li>可以公平获得不同算法实际在线时的性能指标，包括商业上关注的指标；</li></ul><p>缺点是：</p><ul><li>周期较长，必须进行长期的实验才能得到可靠的结果；</li></ul><h4 id="1-1-4-总结"><a href="#1-1-4-总结" class="headerlink" title="1.1.4 总结"></a>1.1.4 总结</h4><p>一般来说，一个新的推荐算法最终上线，需要完成上述的3个实验。</p><ul><li>首先，通过<strong>离线实验</strong>证明它在很多离线指标上优于现有的算法；</li><li>其次，通过<strong>用户调查</strong>确定用户满意度不低于现有的算法；</li><li>最后，通过<strong>在线AB测试</strong>确定它在我们关心的指标上优于现有的算法；</li></ul><h3 id="1-2-评测指标"><a href="#1-2-评测指标" class="headerlink" title="1.2 评测指标"></a>1.2 评测指标</h3><p>评测指标用于评测推荐系统的性能，有些可以定量计算，有些只能定性描述。</p><h4 id="1）用户满意度"><a href="#1）用户满意度" class="headerlink" title="1）用户满意度"></a>1）用户满意度</h4><p>用户满意度是评测推荐系统的重要指标，无法离线计算，只能通过用户调查或者在线实验获得。</p><p>调查问卷，需要考虑到用户各方面的感受，用户才能针对问题给出准确的回答。</p><p>在线系统中，用户满意度通过统计用户行为得到。比如用户如果购买了推荐的商品，就表示他们在一定程度上满意，可以用购买率度量用户满意度。</p><p>一般情况，我们可以用用户点击率、停留时间、转化率等指标度量用户的满意度。</p><h4 id="2）预测准确度"><a href="#2）预测准确度" class="headerlink" title="2）预测准确度"></a>2）预测准确度</h4><p>预测准确度，度量的是推荐系统预测用户行为的能力。 是推荐系统最重要的离线评测指标。</p><p>大部分的关于推荐系统评测指标的研究，都是针对预测准确度的。因为该指标可以通过离线实验计算，方便了学术界的研究人员。</p><p>由于离线的推荐算法有不同的研究方向，准确度指标也不同，根据研究方向，可分为：预测评分准确度和TopN推荐。</p><h5 id="a）预测评分准确度"><a href="#a）预测评分准确度" class="headerlink" title="a）预测评分准确度"></a>a）预测评分准确度</h5><p>预测评分的准确度，衡量的是算法预测的评分与用户的实际评分的贴近程度。</p><p>这针对于一些需要用户给物品评分的网站。</p><p>预测评分的准确度指标，一般通过以下指标计算：</p><p><strong>平均绝对误差（MAE）</strong></p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-51-32.png" srcset="/img/loading.gif" alt></p><p>MAE因其计算简单、通俗易懂得到了广泛的应用。但MAE指标也有一定的局限性，因为对MAE指标贡献比较大的往往是那种很难预测准确的低分商品。</p><p>所以即便推荐系统A的MAE值低于系统B，很可能只是由于系统A更擅长预测这部分低分商品的评分，即系统A比系统B能更好的区分用户非常讨厌和一般讨厌的商品，显然这样区分的意义不大。</p><p><strong>均方根误差（RMSE）</strong></p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-51-51.png" srcset="/img/loading.gif" alt></p><p>Netflix认为RMSE加大了对预测不准的用户物品评分的惩罚（平方项的惩罚），因而对系统的评测更加苛刻。</p><p>研究表明，如果评分系统是基于整数建立的（即用户给的评分都是整数），那么对预测结果取整数会降低MAE的误差。</p><h5 id="b）TopN推荐"><a href="#b）TopN推荐" class="headerlink" title="b）TopN推荐"></a>b）TopN推荐</h5><p>网站提供推荐服务时，一般是给用户一个个性化的推荐列表，这种推荐叫做TopN推荐。</p><p>TopN推荐的预测准确率，一般通过2个指标度量：</p><p><strong>准确率（precision）</strong></p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-52-10.png" srcset="/img/loading.gif" alt></p><p><strong>召回率（recall）</strong></p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-52-18.png" srcset="/img/loading.gif" alt></p><p>R(u)是根据用户在训练集上的行为给用户做出的推荐列表，T(u)是用户在测试集上的行为列表。</p><p>TopN推荐更符合实际的应用需求，比如预测用户是否会看一部电影，比预测用户看了电影之后会给它什么评分更重要。</p><p><strong>ROC曲线、AUC曲线、F值</strong></p><p>分类任务一般都有这三兄弟。</p><p>除此之外，还有Hit Rate (HR)等。</p><h4 id="3）覆盖率"><a href="#3）覆盖率" class="headerlink" title="3）覆盖率"></a>3）覆盖率</h4><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-52-29.png" srcset="/img/loading.gif" alt></p><p>覆盖率（coverage）是描述一个推荐系统对物品长尾的发掘能力。</p><p>最简单的定义是，推荐系统推荐出来的物品占总物品的比例。</p><p>假设系统的用户集合为U，推荐系统给每个用户推荐一个长度为N的物品列表R(u)，覆盖率公式为：</p><p>覆盖率是内容提供者关心的指标，覆盖率为100%的推荐系统可以将每个物品都推荐给至少一个用户。</p><p>除了推荐物品的占比，还可以通过研究物品在推荐列表中出现的次数分布，更好的描述推荐系统的挖掘长尾的能力。</p><p>如果分布比较平，说明推荐系统的覆盖率很高；如果分布陡峭，说明分布系统的覆盖率较低。</p><p>信息论和经济学中有两个著名指标，可以定义覆盖率：</p><p><strong>信息熵</strong></p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-52-40.png" srcset="/img/loading.gif" alt></p><p>p(i)是物品i的流行度除以所有物品流行度之和。</p><p><strong>基尼系数（Gini Index）</strong></p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-52-57.png" srcset="/img/loading.gif" alt></p><p>p(ij)是按照物品流行度p()从小到大排序的物品列表中第j个物品。</p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-53-09.png" srcset="/img/loading.gif" alt></p><p><strong>评测马太效应</strong></p><p>马太效应，是指强者越强，弱者越弱的效应。推荐系统的初衷是希望消除马太效应，使得各物品都能被展示给对它们感兴趣的人群。</p><p>但是，很多研究表明，现在的主流推荐算法（协同过滤）是具有马太效应的。评测推荐系统是否具有马太效应可以使用基尼系数。</p><p>如，G1是从初始用户行为中计算出的物品流行度的基尼系数，G2是从推荐列表中计算出的物品流行度的基尼系数，那么如果G1&gt;G2，就说明推荐算法具有马太效应。</p><h4 id="4）多样性"><a href="#4）多样性" class="headerlink" title="4）多样性"></a>4）多样性</h4><p>为了满足用户广泛的兴趣，推荐列表需要能够覆盖用户不同兴趣的领域，即需要具有多样性。</p><p>多样性描述了推荐列表中物品两两之间的不相似性。假设s(i,j)在[0,1]区间定义了物品i和j之间的相似度，那么用户u的推荐列表R(u)的多样性定义如下：</p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-53-44.png" srcset="/img/loading.gif" alt></p><p>推荐系统整体多样性可以定义为所有用户推荐列表多样性的平均值：</p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-53-51.png" srcset="/img/loading.gif" alt></p><h4 id="5）新颖性"><a href="#5）新颖性" class="headerlink" title="5）新颖性"></a>5）新颖性</h4><p>新颖性也是影响用户体验的重要指标之一。它指的是向用户推荐非热门非流行物品的能力。</p><p>评测新颖度最简单的方法，是利用推荐结果的平均流行度，因为越不热门的物品，越可能让用户觉得新颖。</p><p>此计算比较粗糙，需要配合用户调查准确统计新颖度。</p><h4 id="6）惊喜度"><a href="#6）惊喜度" class="headerlink" title="6）惊喜度"></a>6）惊喜度</h4><p>推荐结果和用户的历史兴趣不相似，但却让用户满意，这样就是惊喜度很高。</p><p>目前惊喜度还没有公认的指标定义方式，最近几年研究的人很多，深入研究可以参考一些论文。</p><h4 id="7）信任度"><a href="#7）信任度" class="headerlink" title="7）信任度"></a>7）信任度</h4><p>如果用户信任推荐系统，就会增加用户和推荐系统的交互。</p><p>提高信任度的方式有两种：</p><ul><li>增加系统透明度：提供推荐解释，让用户了解推荐系统的运行机制。</li><li>利用社交网络，通过好友信息给用户做推荐</li></ul><p>度量信任度的方式，只能通过问卷调查。</p><h4 id="8）实时性"><a href="#8）实时性" class="headerlink" title="8）实时性"></a>8）实时性</h4><p>推荐系统的实时性，包括两方面：</p><ul><li>实时更新推荐列表满足用户新的行为变化；</li><li>将新加入系统的物品推荐给用户；</li></ul><h4 id="9）健壮性"><a href="#9）健壮性" class="headerlink" title="9）健壮性"></a>9）健壮性</h4><p>任何能带来利益的算法系统都会被攻击，最典型的案例就是搜索引擎的作弊与反作弊斗争。</p><p>健壮性（robust，鲁棒性）衡量了推荐系统抗击作弊的能力。</p><p>2011年的推荐系统大会专门有一个推荐系统健壮性的教程，作者总结了很多作弊方法，最著名的是行为注入攻击（profile injection attack）。</p><p>就是注册很多账号，用这些账号同时购买A和自己的商品。此方法针对亚马逊的一种推荐方法，“购买商品A的用户也经常购买的其他商品”。</p><p>评测算法的健壮性，主要利用模拟攻击：</p><ul><li>a）给定一个数据集和算法，用算法给数据集中的用户生成推荐列表；</li><li>b）用常用的攻击方法向数据集中注入噪声数据；</li><li>c）利用算法在有噪声的数据集上再次生成推荐列表；</li><li>d）通过比较攻击前后推荐列表的相似度评测算法的健壮性。</li></ul><p>提高系统健壮性的方法：</p><ul><li>选择健壮性高的算法；</li><li>选择代价较高的用户行为，如购买行为比浏览行为代价高；</li><li>在使用数据前，进行攻击检测，从而对数据进行清理。</li></ul><h4 id="10）商业目标"><a href="#10）商业目标" class="headerlink" title="10）商业目标"></a>10）商业目标</h4><p>设计推荐系统时，需要考虑最终的商业目标。不同网站具有不同的商业目标，它与网站的盈利模式息息相关。</p><p>总结：</p><p><img src="/2020/05/20/如何评价推荐系统以及其他智能系统/2020-05-20-21-54-09.png" srcset="/img/loading.gif" alt></p><p>作者认为，对于可以离线优化的指标，在给定覆盖率、多样性、新颖性等限制条件下，应尽量优化预测准确度。</p><h3 id="1-3-评测维度"><a href="#1-3-评测维度" class="headerlink" title="1.3 评测维度"></a>1.3 评测维度</h3><p>如果推荐系统的评测报告中，包含了不同维度下的系统评测指标，就能帮我们全面了解系统性能。一般评测维度分3种：</p><p>用户维度，主要包括用户的人口统计学信息、活跃度以及是不是新用户等；<br>物品维度，包括物品的属性信息、流行度、平均分以及是不是新加入的物品等；<br>时间维度，包括季节，是工作日还是周末，白天还是晚上等；</p><h2 id="2-评价智能系统"><a href="#2-评价智能系统" class="headerlink" title="2. 评价智能系统"></a>2. 评价智能系统</h2><p>【未完成】</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;看到一篇介绍推荐系统的评价方法与指标的文章，刚好本课题组也有个同学在做推荐系统相关的课题，并且该文章的评价指标部分对我目前的课题有启发作用，因此转载。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="testing" scheme="https://superlova.github.io/tags/testing/"/>
    
      <category term="Recommendation System" scheme="https://superlova.github.io/tags/Recommendation-System/"/>
    
      <category term="Metrics" scheme="https://superlova.github.io/tags/Metrics/"/>
    
  </entry>
  
  <entry>
    <title>Datawhale——SVHN——Task01：赛题理解</title>
    <link href="https://superlova.github.io/2020/05/20/Datawhale%E5%B0%8F%E7%BB%84%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%E4%BB%BB%E5%8A%A1%E2%80%94%E2%80%94Task01%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/"/>
    <id>https://superlova.github.io/2020/05/20/Datawhale%E5%B0%8F%E7%BB%84%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%E4%BB%BB%E5%8A%A1%E2%80%94%E2%80%94Task01%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/</id>
    <published>2020-05-20T08:38:22.000Z</published>
    <updated>2020-05-23T14:55:33.288Z</updated>
    
    <content type="html"><![CDATA[<p>本次新人赛是Datawhale与天池联合发起的0基础入门系列赛事第二场 —— 零基础入门CV之街景字符识别比赛。<br><a id="more"></a></p><h1 id="Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解"><a href="#Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解" class="headerlink" title="Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解"></a>Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解</h1><h2 id="0-学习目标"><a href="#0-学习目标" class="headerlink" title="0. 学习目标"></a>0. 学习目标</h2><ul><li>理解赛题背景和赛题数据</li><li>完成赛题报名和数据下载，理解赛题的解题思路</li><li>了解赛题</li></ul><h2 id="1-大赛简介"><a href="#1-大赛简介" class="headerlink" title="1. 大赛简介"></a>1. 大赛简介</h2><p>本次新人赛是Datawhale与天池联合发起的0基础入门系列赛事第二场 —— 零基础入门CV之街景字符识别比赛。</p><h3 id="1-1-赛题数据介绍"><a href="#1-1-赛题数据介绍" class="headerlink" title="1.1 赛题数据介绍"></a>1.1 赛题数据介绍</h3><p>赛题来源自Google街景图像中的门牌号数据集（The Street View House Numbers Dataset, SVHN），该数据来自真实场景的门牌号。</p><p>训练集数据包括3W张照片，验证集数据包括1W张照片，每张照片包括颜色图像和对应的编码类别和具体位置</p><h3 id="1-2-参赛规则"><a href="#1-2-参赛规则" class="headerlink" title="1.2 参赛规则"></a>1.2 参赛规则</h3><ul><li>比赛允许使用CIFAR-10和ImageNet数据集的预训练模型，不允许使用其他任何预训练模型和任何外部数据；</li><li>报名成功后，选手下载数据，在本地调试算法，提交结果；</li><li>提交后将进行实时评测；每天排行榜更新时间为12:00和20:00，按照评测指标得分从高到低排序；排行榜将选择历史最优成绩进行展示。</li></ul><h3 id="1-3-数据集简介"><a href="#1-3-数据集简介" class="headerlink" title="1.3 数据集简介"></a>1.3 数据集简介</h3><p>所有的数据（训练集、验证集和测试集）的标注使用JSON格式，并使用文件名进行索引。</p><div class="table-container"><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td>top</td><td>左上角坐标X</td></tr><tr><td>height</td><td>字符高度</td></tr><tr><td>left</td><td>左上角最表Y</td></tr><tr><td>width</td><td>字符宽度</td></tr><tr><td>label</td><td>字符编码</td></tr></tbody></table></div><p>字符的坐标具体如下所示：<br><img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/字符坐标.png" srcset="/img/loading.gif" alt="坐标">  </p><p>在比赛数据（训练集和验证集）中，同一张图片中可能包括一个或者多个字符，因此在比赛数据的JSON标注中，会有两个字符的边框信息：<br>|原始图片|图片JSON标注|<br>|——|——-|<br><img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/原始图片.png" srcset="/img/loading.gif" alt="19">    | <img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/原始图片标注.png" srcset="/img/loading.gif" alt="标注">  |</p><h3 id="1-4-成绩评定方式"><a href="#1-4-成绩评定方式" class="headerlink" title="1.4 成绩评定方式"></a>1.4 成绩评定方式</h3><p>评价标准为准确率。<br>选手提交结果与实际图片的编码进行对比，以编码整体识别准确率为评价指标，结果越大越好，具体计算公式如下：</p><p> Score=编码识别正确的数量/测试集图片数量   </p><h3 id="1-5-结果提交格式"><a href="#1-5-结果提交格式" class="headerlink" title="1.5 结果提交格式"></a>1.5 结果提交格式</h3><p>提交前请确保预测结果的格式与sample_submit.csv中的格式一致，以及提交文件后缀名为csv。<br>形式如下：<br>file_name, file_code<br>0010000.jpg,451<br>0010001.jpg,232<br>0010002.jpg,45<br>0010003.jpg,67<br>0010004.jpg,191<br>0010005.jpg,892 </p><h2 id="2-数据读取"><a href="#2-数据读取" class="headerlink" title="2. 数据读取"></a>2. 数据读取</h2><p>JSON中标签的读取方式：  </p><pre><code class="lang-python">import jsontrain_json = json.load(open(&#39;../input/train.json&#39;))# 数据标注处理def parse_json(d):    arr = np.array([        d[&#39;top&#39;], d[&#39;height&#39;], d[&#39;left&#39;],  d[&#39;width&#39;], d[&#39;label&#39;]    ])    arr = arr.astype(int)    return arrimg = cv2.imread(&#39;../input/train/000000.png&#39;)arr = parse_json(train_json[&#39;000000.png&#39;])plt.figure(figsize=(10, 10))plt.subplot(1, arr.shape[1]+1, 1)plt.imshow(img)plt.xticks([]); plt.yticks([])for idx in range(arr.shape[1]):    plt.subplot(1, arr.shape[1]+1, idx+2)    plt.imshow(img[arr[0, idx]:arr[0, idx]+arr[1, idx],arr[2, idx]:arr[2, idx]+arr[3, idx]])    plt.title(arr[4, idx])    plt.xticks([]); plt.yticks([])</code></pre><p><img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/19.png" srcset="/img/loading.gif" alt="19"></p><h2 id="3-解题思路"><a href="#3-解题思路" class="headerlink" title="3. 解题思路"></a>3. 解题思路</h2><p>赛题思路分析：赛题本质是分类问题，需要对图片的字符进行识别。但赛题给定的数据图片中不同图片中包含的字符数量不等，如下图所示。有的图片的字符个数为2，有的图片字符个数为3，有的图片字符个数为4。 </p><div class="table-container"><table><thead><tr><th>字符属性</th><th>图片</th></tr></thead><tbody><tr><td>字符：42   字符个数：2</td><td><img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/42.png" srcset="/img/loading.gif" alt="标注"></td></tr><tr><td>字符：241   字符个数：3</td><td><img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/2411.png" srcset="/img/loading.gif" alt="标注"></td></tr><tr><td>字符：7358   字符个数：4</td><td><img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/7358.png" srcset="/img/loading.gif" alt="标注"></td></tr></tbody></table></div><p>因此本次赛题的难点是需要对不定长的字符进行识别，与传统的图像分类任务有所不同。为了降低参赛难度，我们提供了一些解题思路供大家参考：</p><ul><li>简单入门思路：定长字符识别    </li></ul><p>可以将赛题抽象为一个定长字符识别问题，在赛题数据集中大部分图像中字符个数为2-4个，最多的字符    个数为6个。<br>因此可以对于所有的图像都抽象为6个字符的识别问题，字符23填充为23XXXX，字符231填充为231XXX。<br><img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/23xxxxxx.png" srcset="/img/loading.gif" alt="标注">   </p><p>经过填充之后，原始的赛题可以简化了6个字符的分类问题。在每个字符的分类中会进行11个类别的分类，假如分类为填充字符，则表明该字符为空。    </p><ul><li>专业字符识别思路：不定长字符识别 </li></ul><p><img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/不定长字符识别.png" srcset="/img/loading.gif" alt="标注"> </p><p>在字符识别研究中，有特定的方法来解决此种不定长的字符识别问题，比较典型的有CRNN字符识别模型。<br>在本次赛题中给定的图像数据都比较规整，可以视为一个单词或者一个句子。   </p><ul><li>专业分类思路：检测再识别</li></ul><p>在赛题数据中已经给出了训练集、验证集中所有图片中字符的位置，因此可以首先将字符的位置进行识别，利用物体检测的思路完成。   </p><p><img src="/2020/05/20/Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解/检测.png" srcset="/img/loading.gif" alt="IMG"> </p><p>此种思路需要参赛选手构建字符检测模型，对测试集中的字符进行识别。选手可以参考物体检测模型SSD或者YOLO来完成。    </p><h2 id="4-Baseline思路：将不定长字符转换为定长字符的识别问题，并使用CNN完成训练和验证"><a href="#4-Baseline思路：将不定长字符转换为定长字符的识别问题，并使用CNN完成训练和验证" class="headerlink" title="4. Baseline思路：将不定长字符转换为定长字符的识别问题，并使用CNN完成训练和验证"></a>4. Baseline思路：将不定长字符转换为定长字符的识别问题，并使用CNN完成训练和验证</h2><h3 id="4-1-运行环境及安装示例"><a href="#4-1-运行环境及安装示例" class="headerlink" title="4.1 运行环境及安装示例"></a>4.1 运行环境及安装示例</h3><ul><li>运行环境要求：Python2/3，Pytorch1.x，内存4G，有无GPU都可以。         </li></ul><p>下面给出python3.7+ torch1.3.1gpu版本的环境安装示例：      </p><ul><li><p>首先在Anaconda中创建一个专门用于本次天池练习赛的虚拟环境。          </p><blockquote><p>$conda create -n py37_torch131 python=3.7      </p></blockquote></li><li><p>激活环境，并安装pytorch1.3.1                                     </p><blockquote><p>$source activate py37_torch131<br>$conda install pytorch=1.3.1 torchvision cudatoolkit=10.0                     </p></blockquote></li><li><p>通过下面的命令一键安装所需其它依赖库     </p><blockquote><p>$pip install jupyter tqdm opencv-python matplotlib pandas                                  </p></blockquote></li><li><p>启动notebook，即可开始baseline代码的学习                  </p><blockquote><p>$jupyter-notebook   </p></blockquote></li><li><p>假设所有的赛题输入文件放在../input/目录下，首先导入常用的包：</p></li></ul><pre><code class="lang-python">import os, sys, glob, shutil, jsonos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &#39;0&#39;import cv2from PIL import Imageimport numpy as npfrom tqdm import tqdm, tqdm_notebookimport torchtorch.manual_seed(0)torch.backends.cudnn.deterministic = Falsetorch.backends.cudnn.benchmark = Trueimport torchvision.models as modelsimport torchvision.transforms as transformsimport torchvision.datasets as datasetsimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torch.autograd import Variablefrom torch.utils.data.dataset import Dataset</code></pre><h3 id="4-2-步骤"><a href="#4-2-步骤" class="headerlink" title="4.2 步骤"></a>4.2 步骤</h3><ul><li>赛题数据读取（封装为Pytorch的Dataset和DataLoder）</li><li>构建CNN模型（使用Pytorch搭建）</li><li>模型训练与验证</li><li>模型结果预测</li></ul><h4 id="步骤1：定义好读取图像的Dataset"><a href="#步骤1：定义好读取图像的Dataset" class="headerlink" title="步骤1：定义好读取图像的Dataset"></a>步骤1：定义好读取图像的Dataset</h4><pre><code class="lang-python">class SVHNDataset(Dataset):    def __init__(self, img_path, img_label, transform=None):        self.img_path = img_path        self.img_label = img_label         if transform is not None:            self.transform = transform        else:            self.transform = None    def __getitem__(self, index):        img = Image.open(self.img_path[index]).convert(&#39;RGB&#39;)        if self.transform is not None:            img = self.transform(img)        # 设置最长的字符长度为5个        lbl = np.array(self.img_label[index], dtype=np.int)        lbl = list(lbl)  + (5 - len(lbl)) * [10]        return img, torch.from_numpy(np.array(lbl[:5]))    def __len__(self):        return len(self.img_path)</code></pre><h4 id="步骤2：定义好训练数据和验证数据的Dataset"><a href="#步骤2：定义好训练数据和验证数据的Dataset" class="headerlink" title="步骤2：定义好训练数据和验证数据的Dataset"></a>步骤2：定义好训练数据和验证数据的Dataset</h4><pre><code class="lang-python">train_path = glob.glob(&#39;../input/train/*.png&#39;)train_path.sort()train_json = json.load(open(&#39;../input/train.json&#39;))train_label = [train_json[x][&#39;label&#39;] for x in train_json]print(len(train_path), len(train_label))train_loader = torch.utils.data.DataLoader(    SVHNDataset(train_path, train_label,                transforms.Compose([                    transforms.Resize((64, 128)),                    transforms.RandomCrop((60, 120)),                    transforms.ColorJitter(0.3, 0.3, 0.2),                    transforms.RandomRotation(5),                    transforms.ToTensor(),                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])    ])),     batch_size=40,     shuffle=True,     num_workers=10,)val_path = glob.glob(&#39;../input/val/*.png&#39;)val_path.sort()val_json = json.load(open(&#39;../input/val.json&#39;))val_label = [val_json[x][&#39;label&#39;] for x in val_json]print(len(val_path), len(val_label))val_loader = torch.utils.data.DataLoader(    SVHNDataset(val_path, val_label,                transforms.Compose([                    transforms.Resize((60, 120)),                    # transforms.ColorJitter(0.3, 0.3, 0.2),                    # transforms.RandomRotation(5),                    transforms.ToTensor(),                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])    ])),     batch_size=40,     shuffle=False,     num_workers=10,)</code></pre><h4 id="步骤3：定义好字符分类模型，使用renset18的模型作为特征提取模块"><a href="#步骤3：定义好字符分类模型，使用renset18的模型作为特征提取模块" class="headerlink" title="步骤3：定义好字符分类模型，使用renset18的模型作为特征提取模块"></a>步骤3：定义好字符分类模型，使用renset18的模型作为特征提取模块</h4><pre><code class="lang-python">class SVHN_Model1(nn.Module):    def __init__(self):        super(SVHN_Model1, self).__init__()        model_conv = models.resnet18(pretrained=True)        model_conv.avgpool = nn.AdaptiveAvgPool2d(1)        model_conv = nn.Sequential(*list(model_conv.children())[:-1])        self.cnn = model_conv        self.fc1 = nn.Linear(512, 11)        self.fc2 = nn.Linear(512, 11)        self.fc3 = nn.Linear(512, 11)        self.fc4 = nn.Linear(512, 11)        self.fc5 = nn.Linear(512, 11)    def forward(self, img):                feat = self.cnn(img)        # print(feat.shape)        feat = feat.view(feat.shape[0], -1)        c1 = self.fc1(feat)        c2 = self.fc2(feat)        c3 = self.fc3(feat)        c4 = self.fc4(feat)        c5 = self.fc5(feat)        return c1, c2, c3, c4, c5</code></pre><h4 id="步骤4：定义好训练、验证和预测模块"><a href="#步骤4：定义好训练、验证和预测模块" class="headerlink" title="步骤4：定义好训练、验证和预测模块"></a>步骤4：定义好训练、验证和预测模块</h4><pre><code class="lang-python">def train(train_loader, model, criterion, optimizer):    # 切换模型为训练模式    model.train()    train_loss = []    for i, (input, target) in enumerate(train_loader):        if use_cuda:            input = input.cuda()            target = target.cuda()        c0, c1, c2, c3, c4 = model(input)        loss = criterion(c0, target[:, 0]) + \                criterion(c1, target[:, 1]) + \                criterion(c2, target[:, 2]) + \                criterion(c3, target[:, 3]) + \                criterion(c4, target[:, 4])        # loss /= 6        optimizer.zero_grad()        loss.backward()        optimizer.step()        if i % 100 == 0:            print(loss.item())        train_loss.append(loss.item())    return np.mean(train_loss)def validate(val_loader, model, criterion):    # 切换模型为预测模型    model.eval()    val_loss = []    # 不记录模型梯度信息    with torch.no_grad():        for i, (input, target) in enumerate(val_loader):            if use_cuda:                input = input.cuda()                target = target.cuda()            c0, c1, c2, c3, c4 = model(input)            loss = criterion(c0, target[:, 0]) + \                    criterion(c1, target[:, 1]) + \                    criterion(c2, target[:, 2]) + \                    criterion(c3, target[:, 3]) + \                    criterion(c4, target[:, 4])            # loss /= 6            val_loss.append(loss.item())    return np.mean(val_loss)def predict(test_loader, model, tta=10):    model.eval()    test_pred_tta = None    # TTA 次数    for _ in range(tta):        test_pred = []        with torch.no_grad():            for i, (input, target) in enumerate(test_loader):                if use_cuda:                    input = input.cuda()                c0, c1, c2, c3, c4 = model(input)                output = np.concatenate([                    c0.data.numpy(),                     c1.data.numpy(),                    c2.data.numpy(),                     c3.data.numpy(),                    c4.data.numpy()], axis=1)                test_pred.append(output)        test_pred = np.vstack(test_pred)        if test_pred_tta is None:            test_pred_tta = test_pred        else:            test_pred_tta += test_pred    return test_pred_tta</code></pre><h4 id="步骤5：迭代训练和验证模型"><a href="#步骤5：迭代训练和验证模型" class="headerlink" title="步骤5：迭代训练和验证模型"></a>步骤5：迭代训练和验证模型</h4><pre><code class="lang-python">model = SVHN_Model1()criterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), 0.001)best_loss = 1000.0use_cuda = Falseif use_cuda:    model = model.cuda()for epoch in range(2):    train_loss = train(train_loader, model, criterion, optimizer, epoch)    val_loss = validate(val_loader, model, criterion)    val_label = [&#39;&#39;.join(map(str, x)) for x in val_loader.dataset.img_label]    val_predict_label = predict(val_loader, model, 1)    val_predict_label = np.vstack([        val_predict_label[:, :11].argmax(1),        val_predict_label[:, 11:22].argmax(1),        val_predict_label[:, 22:33].argmax(1),        val_predict_label[:, 33:44].argmax(1),        val_predict_label[:, 44:55].argmax(1),    ]).T    val_label_pred = []    for x in val_predict_label:        val_label_pred.append(&#39;&#39;.join(map(str, x[x!=10])))    val_char_acc = np.mean(np.array(val_label_pred) == np.array(val_label))    print(&#39;Epoch: {0}, Train loss: {1} \t Val loss: {2}&#39;.format(epoch, train_loss, val_loss))    print(val_char_acc)    # 记录下验证集精度    if val_loss &lt; best_loss:        best_loss = val_loss        torch.save(model.state_dict(), &#39;./model.pt&#39;)</code></pre><p>训练两个2 Epoch后，输出的训练日志为：</p><p>Epoch: 0, Train loss: 3.1      Val loss: 3.4 验证集精度：0.3439<br>Epoch: 1, Train loss: 2.1      Val loss: 2.9 验证集精度：0.4346     </p><h4 id="步骤6：对测试集样本进行预测，生成提交文件"><a href="#步骤6：对测试集样本进行预测，生成提交文件" class="headerlink" title="步骤6：对测试集样本进行预测，生成提交文件"></a>步骤6：对测试集样本进行预测，生成提交文件</h4><pre><code class="lang-python">test_path = glob.glob(&#39;../input/test_a/*.png&#39;)test_path.sort()test_label = [[1]] * len(test_path)print(len(val_path), len(val_label))test_loader = torch.utils.data.DataLoader(    SVHNDataset(test_path, test_label,                transforms.Compose([                    transforms.Resize((64, 128)),                    transforms.RandomCrop((60, 120)),                    # transforms.ColorJitter(0.3, 0.3, 0.2),                    # transforms.RandomRotation(5),                    transforms.ToTensor(),                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])    ])),     batch_size=40,     shuffle=False,     num_workers=10,)test_predict_label = predict(test_loader, model, 1)test_label = [&#39;&#39;.join(map(str, x)) for x in test_loader.dataset.img_label]test_predict_label = np.vstack([    test_predict_label[:, :11].argmax(1),    test_predict_label[:, 11:22].argmax(1),    test_predict_label[:, 22:33].argmax(1),    test_predict_label[:, 33:44].argmax(1),    test_predict_label[:, 44:55].argmax(1),]).Ttest_label_pred = []for x in test_predict_label:    test_label_pred.append(&#39;&#39;.join(map(str, x[x!=10])))import pandas as pddf_submit = pd.read_csv(&#39;../input/test_A_sample_submit.csv&#39;)df_submit[&#39;file_code&#39;] = test_label_preddf_submit.to_csv(&#39;renset18.csv&#39;, index=None)</code></pre><p><strong>在训练完成2个Epoch后，模型在测试集上的成绩应该在0.33左右。</strong>    </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次新人赛是Datawhale与天池联合发起的0基础入门系列赛事第二场 —— 零基础入门CV之街景字符识别比赛。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="datawhale" scheme="https://superlova.github.io/tags/datawhale/"/>
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>numpy中的einsum使用方法</title>
    <link href="https://superlova.github.io/2020/05/19/numpy%E4%B8%AD%E7%9A%84einsum%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>https://superlova.github.io/2020/05/19/numpy%E4%B8%AD%E7%9A%84einsum%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</id>
    <published>2020-05-19T06:52:52.000Z</published>
    <updated>2020-05-20T02:48:40.857Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍爱因斯坦求和约定，以及在numpy中的使用<br><a id="more"></a></p><p>numpy里面有很多奇技淫巧，爱因斯坦求和约定就是其中之一。</p><p>爱因斯坦求和约定能够很方便和简介地表示点积、外积、转置、矩阵-向量乘法、矩阵-矩阵乘法等，这在深度学习公式推导中的用处很大。</p><p>其实我不认为einsum在numpy中用处很大，我认为其顶多就是一种统一的矩阵运算写法罢了。这种技巧，是在牺牲可读性基础上，对代码的简化。而且由于numpy对其他运算也有进行优化，所以仅凭借爱因斯坦乘数法还不一定能提升代码执行效率。</p><p>可能是我还没有体会到高维张量相互计算时的痛苦吧。</p><p>先看一下einsum的api：</p><pre><code class="lang-python">np.einsum(equation, *arr)</code></pre><p>最开始需要一个字符串，用以描述想要完成的计算。后面是计算需要的操作数，也就是你的矩阵等。</p><p>来看具体的例子：</p><h3 id="对于向量"><a href="#对于向量" class="headerlink" title="对于向量"></a>对于向量</h3><pre><code class="lang-python">arr1 = np.arange(5) # 0,1,2,3,4arr2 = np.arange(5) # 0,1,2,3,4</code></pre><ol><li>计算向量所有分量的和，即<code>np.sum(arr)</code>。如何利用einsum完成？</li></ol><pre><code class="lang-python">np.einsum(&quot;i-&gt;&quot;, arr) # 10</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c = \sum_{i} a_i,\quad i = 1, 2, \dots</script><ol><li>计算两向量内积，即<code>np.dot(arr1, arr2)</code>或<code>np.inner(arr1, arr2)</code></li></ol><pre><code class="lang-python"># 0*0 + 1*1 + 2*2 + 3*3 + 4*4np.einsum(&quot;i,i-&gt;&quot;, arr1, arr2) # 30</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c = \sum_{i} a_i \times b_i,\quad i = 1, 2, \dots</script><ol><li>计算两向量逐元素乘积，即<code>arr1 * arr2</code></li></ol><pre><code class="lang-python">np.einsum(&quot;i,i-&gt;i&quot;, arr1, arr2) # 0,1,4,9,16</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_i = a_i \times b_i,\quad i = 1, 2, \dots</script><ol><li>计算两向量外积，即<code>np.outer(arr1, arr2)</code></li></ol><pre><code class="lang-python">[[ 0  0  0  0  0] [ 0  1  2  3  4] [ 0  2  4  6  8] [ 0  3  6  9 12] [ 0  4  8 12 16]]np.einsum(&quot;i,j-&gt;ij&quot;, arr1, arr2)</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i,j} = a_i \times b_j,\quad i,j = 1, 2, \dots</script><h3 id="对于矩阵"><a href="#对于矩阵" class="headerlink" title="对于矩阵"></a>对于矩阵</h3><pre><code class="lang-python">A = np.arange(4).reshape(2,2)B = np.arange(4,8).reshape(2,2)[[0 1] [2 3]][[4 5] [6 7]]</code></pre><ol><li>计算矩阵转置，即<code>A.T</code></li></ol><pre><code class="lang-python">[[0 2] [1 3]]print(np.einsum(&quot;ij-&gt;ji&quot;, A))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i,j} = a_{j,i},\quad i,j = 1, 2, \dots</script><ol><li>计算矩阵各元素求和，即<code>np.sum(A)</code></li></ol><pre><code class="lang-python">6print(np.einsum(&quot;ij-&gt;&quot;, A))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c = \sum_{i}\sum_{j}a_{i,j},\quad i,j = 1, 2, \dots</script><ol><li>计算矩阵按列求和，即<code>np.sum(A, axis=0)</code></li></ol><pre><code class="lang-python">[2 4]print(np.einsum(&quot;ij-&gt;j&quot;, A))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{j} = \sum_{i}a_{i,j},\quad i,j = 1, 2, \dots</script><ol><li>计算矩阵按行求和，即<code>np.sum(A, axis=1)</code></li></ol><pre><code class="lang-python">[1 5]print(np.einsum(&quot;ij-&gt;i&quot;, A))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i} = \sum_{j}a_{i,j},\quad i,j = 1, 2, \dots</script><ol><li>求矩阵对角线元素，即<code>np.diag(A)</code></li></ol><pre><code class="lang-python">[0 3]print(np.einsum(&quot;ii-&gt;i&quot;, A))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i} = a_{i,i},\quad i = 1, 2, \dots</script><ol><li>计算矩阵的迹，即对角线元素和，即<code>np.trace(A)</code></li></ol><pre><code class="lang-python">3print(np.einsum(&quot;ii-&gt;&quot;, A))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c = \sum_{i}a_{i,i},\quad i = 1, 2, \dots</script><ol><li>计算两矩逐元素乘积，即<code>A*B</code></li></ol><pre><code class="lang-python">[[ 0  5] [12 21]] print(np.einsum(&quot;ij,ij-&gt;ij&quot;, A, B))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i,j} = a_{i,j} \times b_{i,j}, i,j = 1, 2, \dots</script><ol><li>计算<code>A*B.T</code></li></ol><pre><code class="lang-python">[[ 0  6] [10 21]]print(np.einsum(&quot;ij,ji-&gt;ij&quot;, A, B))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i,j} = a_{i,j} \times b_{j,i}, i,j = 1, 2, \dots</script><ol><li>计算两矩阵乘积<code>np.dot(A, B)</code></li></ol><pre><code class="lang-python">[[ 6  7] [26 31]]print(np.einsum(&quot;ij,jk-&gt;ik&quot;, A, B))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i,k} = a_{i,j} \times b_{j,k}, i,j = 1, 2, \dots</script><p>停一下，停一下。</p><p><img src="/2020/05/19/numpy中的einsum使用方法/2020-05-20-09-45-48.png" srcset="/img/loading.gif" alt></p><p>你们懂了吗？反正我没有。网上的文章指望着我们光看例子就能学会，这是把我们都当成模型训练了吗？</p><p>仔细看一下上面的两个例子，其实每个equation都拥有一个箭头<code>-&gt;</code>。对应数学公式不难得出，箭头左边对应数学公式右边，箭头右边对应数学公式左边。</p><p>比如这个式子：</p><pre><code class="lang-python">np.einsum(&quot;ij,ji-&gt;i&quot;, A, B)</code></pre><p><code>&quot;ij,ji-&gt;i&quot;</code>解释成自然语言：将A中第<code>{i,j}</code>个元素与B中第<code>{j,i}</code>个元素相乘（逗号理解成相乘），结果中没有j分量，只有i分量，所以所有j分量求和。</p><p>就是对应这个数学公式：</p><script type="math/tex; mode=display">c_i = \sum_{j}a_{i,j}\times b_{j,i}</script><p>实际含义代表：<code>np.sum(A*B.T, axis=1)</code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍爱因斯坦求和约定，以及在numpy中的使用&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="numpy" scheme="https://superlova.github.io/tags/numpy/"/>
    
      <category term="einsum" scheme="https://superlova.github.io/tags/einsum/"/>
    
  </entry>
  
  <entry>
    <title>numpy中axis的简单理解</title>
    <link href="https://superlova.github.io/2020/05/19/numpy%E4%B8%ADaxis%E7%9A%84%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3/"/>
    <id>https://superlova.github.io/2020/05/19/numpy%E4%B8%ADaxis%E7%9A%84%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3/</id>
    <published>2020-05-19T06:52:25.000Z</published>
    <updated>2020-05-19T10:36:47.657Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍numpy中的axis<br><a id="more"></a></p><p>我对于numpy中的axis的理解，一直处于似懂非懂、似是而非的状态。看到网上大神的文章，也只能点个赞之后，该不会还是不会。每次看完博客，都会觉得自己懂了；但是每次使用的时候，又要想老半天才行。因此今天我想借此机会，彻底扫清使用numpy时，axis的障碍。</p><p>在numpy中，数据的基本类型是array。array有个基本的数据属性，是它的维度。</p><p>比如下面的这个array，在逻辑上来看这就是个2维的数据，是一个矩阵。</p><pre><code class="lang-python">A = np.random.randint(0, 19, 9).reshape(3, 3)print(A)[[12 15  0] [ 3  3  7] [ 9 18  4]]</code></pre><p>接下来我要对其中的元素进行求和。</p><pre><code class="lang-python">print(np.sum(A))print(np.sum(A, axis=0))print(np.sum(A, axis=1))71[24 36 11][27 13 31]</code></pre><p>显然，第一个sum是对所有元素累加。第二个参数为axis=0的求和，则是这样计算的：</p><p><code>A[0][X] + A[1][X] + A[2][X]</code><br><code>--|---------|---------|----</code></p><p>也就是说，axis=0意味着在求和的过程中，只有A的第0个分量会变化，将第0个分量的所有情况穷举出来，再作为被操作元素，求和之。</p><p>第0个分量的元素计算完毕、得到一个结果时，计算并没有结束，因为我们的X还有很多种可能。</p><p>同理，axis=1时，变化的只有A的第1个（从逻辑上讲是第二个）分量有变化：</p><p><code>A[X][0] + A[X][1] + A[X][2]</code><br><code>-----|---------|---------|-</code></p><p>把该结论推广到更高维度的数据也不会有问题。我们看一个4维的张量是如何指定axis求和的：</p><pre><code class="lang-python">np.random.seed(0)A = np.random.randint(0, 9, 16).reshape(2, 2, 2, 2)print(&quot;orignal A&quot;, A)orignal A [[[[5 0]   [3 3]]  [[7 3]   [5 2]]] [[[4 7]   [6 8]]  [[8 1]   [6 7]]]]</code></pre><pre><code class="lang-python">print(np.sum(A))75</code></pre><pre><code class="lang-python">print(np.sum(A, axis=0))# 相当于print(A[0,:,:,:]+A[1,:,:,:])[[[ 9  7]  [ 9 11]] [[15  4]  [11  9]]]</code></pre><pre><code class="lang-python">print(np.sum(A, axis=1))# 相当于print(A[:,0,:,:] + A[:,1,:,:])[[[12  3]  [ 8  5]] [[12  8]  [12 15]]]</code></pre><pre><code class="lang-python">print(np.sum(A, axis=2))# 相当于print(A[:,:,0,:] + A[:,:,1,:])[[[ 8  3]  [12  5]] [[10 15]  [14  8]]]</code></pre><pre><code class="lang-python">print(np.sum(A, axis=3))# 相当于print(A[:,:,:,0]+A[:,:,:,1])[[[ 5  6]  [10  7]] [[11 14]  [ 9 13]]]</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍numpy中的axis&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="numpy" scheme="https://superlova.github.io/tags/numpy/"/>
    
      <category term="axis" scheme="https://superlova.github.io/tags/axis/"/>
    
  </entry>
  
  <entry>
    <title>闭包的迷思</title>
    <link href="https://superlova.github.io/2020/05/19/%E9%97%AD%E5%8C%85%E7%9A%84%E8%BF%B7%E6%80%9D/"/>
    <id>https://superlova.github.io/2020/05/19/%E9%97%AD%E5%8C%85%E7%9A%84%E8%BF%B7%E6%80%9D/</id>
    <published>2020-05-19T06:50:52.000Z</published>
    <updated>2020-05-19T08:05:48.783Z</updated>
    
    <content type="html"><![CDATA[<p>闭包是什么？如果你与我有同样的疑问，敬请阅读。<br><a id="more"></a></p><h2 id="什么是闭包？"><a href="#什么是闭包？" class="headerlink" title="什么是闭包？"></a><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-closure/#note_1" target="_blank" rel="noopener">什么是闭包？</a></h2><p>这个问题困扰了我很长时间。</p><p>第一次接触闭包这个概念，是在“形式语言”这门课上。好像“离散数学”这门课上也教过闭包，但是这都不重要，因为我们这里讨论的闭包与数学上的闭包没什么关系。本文讨论的闭包，是程序设计语言中的闭包。</p><h3 id="专业概念："><a href="#专业概念：" class="headerlink" title="专业概念："></a>专业概念：</h3><p><img src="/2020/05/19/闭包的迷思/2020-05-19-15-20-04.png" srcset="/img/loading.gif" alt></p><p>闭包是在其词法上下文中引用了自由变量的<strong>函数</strong>，自由变量是指除局部变量以外的变量。</p><p>又有一种说法是闭包<strong>不是函数</strong>，而是由函数和与其相关的引用环境组合而成的实体。</p><p><a href="https://zh.wikipedia.org/wiki/%E9%97%AD%E5%8C%85_(%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6" target="_blank" rel="noopener">维基百科</a>)的解释：闭包在实现上是一个结构体，它存储了一个函数（通常是其入口地址）和一个关联的环境（相当于一个符号查找表）。</p><p>看到这里我彻底懵逼了。是是是，你们说的都对！<br><img src="/2020/05/19/闭包的迷思/2020-05-18-14-59-42.png" srcset="/img/loading.gif" alt></p><p>身为新手小白，我需要通过判断闭包是做什么的，之后再讨论为什么叫做闭包。</p><h2 id="闭包有什么用？"><a href="#闭包有什么用？" class="headerlink" title="闭包有什么用？"></a>闭包有什么用？</h2><p>如果你是从C++来的，那么阅读下面没有什么障碍。如果不是也没有关系，反正各种语言的设计原理都是类似的，只要你掌握的语言有<strong>匿名函数</strong>的功能即可。</p><p>我们都知道，C++11标准引入了lambda表达式，就是一个匿名函数。这个函数长成这样：</p><pre><code class="lang-cpp">[](const string&amp;a, const string&amp;b) {    return a.size() &lt; b.size();};</code></pre><p>上面的这个匿名函数负责比较两个字符串的大小。匿名函数的好处就是节省代码。</p><p>比如我现在想要实现自定义字符串排序函数，按照字符串长度从小到大排序，而不是按照字典排序。这个排序函数就可以用lambda表达式定义。</p><pre><code class="lang-cpp">stable_sort(words.begin(), words.end(),             [](const string&amp;a, const string&amp;b) {                return a.size() &lt; b.size();};)</code></pre><p>lambda前面的中括号是干啥的？是用来捕获外部变量的。比如我想判断字符串长度有没有大于阈值threshold，这个threshold是在函数外面定义的。按照C++的语法，一般的函数不能访问函数外部的变量。但是lambda可以把外部的变量“捕获”，就像下面这样：</p><pre><code class="lang-cpp">int threshold = 10;[threshold](const string&amp; a) {    return a.size() &gt; threshold;};</code></pre><p>可以看到，这个lambda不但使用了lambda内部的变量和参数，而且还“偷取”了不属于它的全局变量threshold。<strong>我们把lambda表达式定义的这种函数叫做闭包。</strong></p><h2 id="为什么叫做闭包？"><a href="#为什么叫做闭包？" class="headerlink" title="为什么叫做闭包？"></a>为什么叫做闭包？</h2><p>有人说这不是脑子有坑吗，闭包哪里“闭”了？这明明比普通函数更“开放”好吧？是不是名字起错了？</p><p>其实不然。闭包并不是对内部封闭，而是给当前外部环境取了个快照，相当于封闭了外部状态。下面是著名营养快线经销商vczh的回答：</p><p><img src="/2020/05/19/闭包的迷思/2020-05-19-15-40-47.png" srcset="/img/loading.gif" alt></p><h2 id="Python中的闭包"><a href="#Python中的闭包" class="headerlink" title="Python中的闭包"></a>Python中的闭包</h2><p>Python中写闭包就要方便多了，毕竟Python的设计哲学就是“一切皆对象”，函数都是对象。</p><p>我们来看这样一个问题：利用闭包和生成器返回一个计数器函数，每次调用它返回递增整数。</p><pre><code class="lang-python"># 利用闭包和生成器返回一个计数器函数，每次调用它返回递增整数。def createCounter():        [...]# 检验部分counterA = createCounter()print(counterA(), counterA(), counterA(), counterA(), counterA()) # 1 2 3 4 5counterB = createCounter()if [counterB(), counterB(), counterB(), counterB()] == [1, 2, 3, 4]:    print(&#39;测试通过!&#39;)else:    print(&#39;测试失败!&#39;)</code></pre><p>你想怎么写？我能想到的，就是在函数内部定义一个生成器，每次调用生成一个整数；然后利用next函数构造一个迭代器，每次调用让这个整数+1，最后返回这个迭代器。</p><pre><code class="lang-python">def createCounter():        def counter():        &#39;&#39;&#39;定义一个生成器        &#39;&#39;&#39;        n = 0        while 1:            n += 1            yield n    g = counter() # 取生成器    def g_fn():        &#39;&#39;&#39;定义一个迭代器，利用next迭代生成器g        &#39;&#39;&#39;        return next(g)    return g_fn # 返回这个迭代器</code></pre><p>我们看一下上面这个函数，函数内部定义的<code>g_fn</code>函数，它使用了外部变量<code>g</code>，也就是说<code>g_fn</code>是个闭包。</p><h2 id="总结一下："><a href="#总结一下：" class="headerlink" title="总结一下："></a>总结一下：</h2><p><strong>引用了自由变量的函数，就是闭包。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;闭包是什么？如果你与我有同样的疑问，敬请阅读。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="C++" scheme="https://superlova.github.io/tags/C/"/>
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="closure" scheme="https://superlova.github.io/tags/closure/"/>
    
      <category term="lambda" scheme="https://superlova.github.io/tags/lambda/"/>
    
  </entry>
  
  <entry>
    <title>Python装饰器为什么这么难以理解</title>
    <link href="https://superlova.github.io/2020/05/18/Python%E8%A3%85%E9%A5%B0%E5%99%A8%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E9%9A%BE%E4%BB%A5%E7%90%86%E8%A7%A3/"/>
    <id>https://superlova.github.io/2020/05/18/Python%E8%A3%85%E9%A5%B0%E5%99%A8%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E9%9A%BE%E4%BB%A5%E7%90%86%E8%A7%A3/</id>
    <published>2020-05-18T04:43:08.000Z</published>
    <updated>2020-05-19T10:06:38.373Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍Python中的装饰器，以及设计模式中的装饰模式。<br><a id="more"></a></p><p>从C/C++或Java迁移来的新Python程序员一定会对Python的装饰器功能感到陌生，尤其是在函数定义前加<code>@func</code>这一功能感到困惑。装饰器到底是什么？Python背后做了什么？在仔细研究网上的资料之后，我总结了此文，与大家分享。</p><p><a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017451662295584" target="_blank" rel="noopener">参考文章</a></p><h2 id="1-提出需求"><a href="#1-提出需求" class="headerlink" title="1. 提出需求"></a>1. 提出需求</h2><p>我们想在函数增加一点功能，比如每次函数执行之前打印一段话，但是又不想更改函数的定义。</p><p>这种想要给原来函数增加需求的同时，不修改原来代码的行为，非常有“面向对象编程思想”内味儿，因为它符合“开放封闭原则”。</p><p>现在就有请大名鼎鼎的设计模式之——装饰器模式登场！</p><blockquote><p>装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。</p></blockquote><p><img src="/2020/05/18/Python装饰器为什么这么难以理解/2020-05-19-17-27-00.png" srcset="/img/loading.gif" alt></p><h2 id="2-Python中的装饰器模式"><a href="#2-Python中的装饰器模式" class="headerlink" title="2. Python中的装饰器模式"></a>2. Python中的装饰器模式</h2><p>在Python中实现装饰器模式很方便。在Python中，有个功能模块直接就叫装饰器。在Python中的装饰器是指一个返回其他函数的函数。外部的高阶函数在执行内部的原函数的前后，再私藏一点干货，然后把修改后的函数对象赋值给原来的函数变量。这样就能在不修改原函数的基础上，增加一些功能。</p><p>总结下来，实现装饰器三步走：</p><ol><li>定义原函数</li><li>定义高阶函数，在里面除了执行原函数之外，再添加一些功能</li><li>将高阶函数对象赋值为原函数变量，以后调用原函数的时候都会执行高阶函数了</li></ol><pre><code class="lang-python">def log(func):    def wrapper(*args, **kw):        print(&#39;call %s():&#39; % func.__name__)        return func(*args, **kw)    return wrapper</code></pre><p>上面的函数，输入参数为原函数变量，在内部构造了一个高阶函数对象wrapper，wrapper里面负责执行一个print语句。最后返回构造好的wrapper。</p><p>以后我们使用<code>func</code>的时候，只要使用<code>log(func)</code>就可以在执行<code>func</code>的同时，打印一段话了。</p><p>看起来不咋地啊，毕竟我们还是修改了代码，把<code>func</code>全都替换成<code>log(func)</code>才能执行。</p><p>或者我们来这样一句：</p><pre><code class="lang-python">func = log(func)</code></pre><p>这个log函数就是一个装饰器，它现在装饰的是func函数。</p><h2 id="3-Python的语法糖"><a href="#3-Python的语法糖" class="headerlink" title="3. Python的语法糖"></a>3. Python的语法糖</h2><p>借助Python的@语法，把decorator置于函数的定义处，我们可以直接完成<code>func = log(func)</code>的操作。</p><pre><code class="lang-python">@logdef basic_fun():    print(&quot;basic_func&quot;)</code></pre><p>以后使用basic_func就会默认执行log(basic_func)了。</p><h2 id="4-改函数名"><a href="#4-改函数名" class="headerlink" title="4. 改函数名"></a>4. 改函数名</h2><p>Python的设计思想就是“一切皆对象”，就连函数也不例外。既然是对象，那么对象可以赋值给一个变量，也可以直接使用。通过变量也可以调用该函数对象。</p><pre><code class="lang-python">def f():    return 0f_obj = f # 注意，这里f为函数名，不加括号则为将函数对象赋值为变量f_res = f() # f后面跟了括号，则此时执行函数，并把返回值赋值给变量</code></pre><p>Python有个特别方便的功能，那就是函数对象可以在运行时打印自己的名字。接上面的代码：</p><pre><code class="lang-python">print(f.__name__) # fprint(f_obj.__name__) # 本质上还是调用上面的函数对象，结果仍为f</code></pre><p>前面我们做了赋值操作<code>func = log(func)</code>，但是其变量代表的函数名称发生了变化。</p><pre><code class="lang-python">print(func.__name__) # funcfunc = log(func)print(func.__name__) # wrapper</code></pre><p>我们希望装饰器完全包裹原函数，也就是说令外界环境感觉不到内部逻辑的变化。那么就需要我们把函数名字也给保持住。这个功能不难，我们使用<code>functools</code>库中自带的装饰器<code>wraps</code>就可以保持函数名称了。</p><pre><code class="lang-python">import functoolsdef log(func):    @functools.wraps(func) # 将被装饰函数名变成参数中函数名    def wrapper(*args, **kw):        print(&#39;call %s():&#39; % func.__name__)        return func(*args, **kw)    return wrapper</code></pre><h2 id="5-带参数的装饰器"><a href="#5-带参数的装饰器" class="headerlink" title="5. 带参数的装饰器"></a>5. 带参数的装饰器</h2><p>在上面我们可以看到，装饰器也是可以带参数的。这是怎么做到的呢？</p><p>其实我们不难想到，只需装饰一个装饰器即可。比如下面这个问题：</p><p><strong>实现log(str)：在函数每次执行前打印str和函数名</strong></p><pre><code class="lang-python">@log(&#39;end&#39;)def now():    print(np.datetime64(&#39;today&#39;, &#39;D&#39;))&gt;&gt;&gt; now()end now():2019-10-13</code></pre><p>解法如下：</p><pre><code class="lang-python">import functoolsdef log(text):    def decorator(func):        @functools.wraps(func)        def wrapper(*args, **kw):            print(&#39;%s %s():&#39; % (text, func.__name__))            return func(*args, **kw)        return wrapper    return decorator</code></pre><p>相当于<code>fun = log(&#39;text&#39;)(fun)</code>，实际上函数变成了<code>wrapper</code><br>但是由于<code>@functools.wraps(func)</code>，函数的<code>__name__</code>不变</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍Python中的装饰器，以及设计模式中的装饰模式。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="Python" scheme="https://superlova.github.io/tags/Python/"/>
    
      <category term="decorator" scheme="https://superlova.github.io/tags/decorator/"/>
    
  </entry>
  
  <entry>
    <title>深度学习工作站调研--结合政府采购网信息</title>
    <link href="https://superlova.github.io/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%E8%B0%83%E7%A0%94-%E7%BB%93%E5%90%88%E6%94%BF%E5%BA%9C%E9%87%87%E8%B4%AD%E7%BD%91%E4%BF%A1%E6%81%AF/"/>
    <id>https://superlova.github.io/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%E8%B0%83%E7%A0%94-%E7%BB%93%E5%90%88%E6%94%BF%E5%BA%9C%E9%87%87%E8%B4%AD%E7%BD%91%E4%BF%A1%E6%81%AF/</id>
    <published>2020-05-11T03:59:50.000Z</published>
    <updated>2020-05-20T02:36:25.578Z</updated>
    
    <content type="html"><![CDATA[<h1 id="服务器调研-2020年5月10日"><a href="#服务器调研-2020年5月10日" class="headerlink" title="服务器调研 2020年5月10日"></a>服务器调研 2020年5月10日</h1><p><strong>调研目标：</strong></p><ul><li>目前典型的计算机，包括商用台式机、工作站、服务器</li><li>搭配目前典型的GPU卡</li><li>GPU适配计算机，需要厂家网站公开的列表，特别是对于服务器。如果厂家没有，需要致电厂商（非销售商）的技术支持。</li></ul><h2 id="1-制约性能的典型项目"><a href="#1-制约性能的典型项目" class="headerlink" title="1. 制约性能的典型项目"></a>1. 制约性能的典型项目</h2><h3 id="1-1-主板"><a href="#1-1-主板" class="headerlink" title="1.1 主板"></a>1.1 主板</h3><p>Intel部分芯片组不支持PCIe 3.0接口，无法发挥显卡的最佳速度。</p><h4 id="名词解释："><a href="#名词解释：" class="headerlink" title="名词解释："></a>名词解释：</h4><p><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-21-33-30.png" srcset="/img/loading.gif" alt><br><strong>PCI Express / PCI-e</strong><br>PCI-E的全名叫PCI Express，简称PCI-E，官方简称PCIe，他是计算机内部的一种高速总线。PCI-E既是通道，也是接口，当他以接口形式存在的时候，就是我们主板上那长长的槽。PCI-E接口目前最大的作用就是插显卡<br><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-21-32-30.png" srcset="/img/loading.gif" alt></p><p><strong>PCI Express 修订版 / PCIe版本</strong><br>PCIe所能承受的带宽一般以版本和长度来区分，目前最流行的PCIe版本是3.0，最新的版本是4.0，目前只有高端主板支持4.0，只有比2080ti还要高端的显卡才需要4.0。</p><p><strong>PCI Express 配置</strong><br>通俗的说就是插槽长度。X1长度是最短的，所能承受的带宽大约是986MB/S。X2长度就是2GB/S，X4长度就是4GB/S，那X16长度就是16GB/S。当前主流显卡，均采用PCIE×16插槽结构。只要具有PCIE×16插槽的主板，都是可以安装独立显卡的。<br>英特尔官网的意义没大看懂，真正有意义的是“支持的处理器 PCI Express 端口配置”这一项。</p><p><strong>支持的处理器 PCI Express 端口配置</strong><br>以Z390主板为例，该主板1x16 or 2x8 or 1x8+2x4，意思就是可以插1个长度为16X的显卡，也可以插两个长度为8X的固态硬盘之类的，但是如果同时插上显卡和固态硬盘，就会出现抢通道的现象：显卡占用16个通道，两个固态占用16个通道，然而<strong>PCI Express 通道数的最大值</strong>就只有24个，通道不够用就会导致限速，甚至无法正常运转。</p><p><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-22-13-34.png" srcset="/img/loading.gif" alt><br><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-22-13-57.png" srcset="/img/loading.gif" alt><br><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-22-14-07.png" srcset="/img/loading.gif" alt><br><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-22-14-15.png" srcset="/img/loading.gif" alt></p><p>以上是当前在售处理器搭配主板（芯片组）的特性支持情况，仅供参考，并不是说某块主板用了上述某个芯片组芯片就会具备这么多的扩展接口及能力，具体还要看主板厂商针对这个版型作出什么样的“阉割”调整。</p><h3 id="1-2-电源"><a href="#1-2-电源" class="headerlink" title="1.2 电源"></a>1.2 电源</h3><p><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-22-11-24.png" srcset="/img/loading.gif" alt="顶级游戏显卡及需要的电源功率大小"></p><h2 id="2-目前典型计算机"><a href="#2-目前典型计算机" class="headerlink" title="2. 目前典型计算机"></a>2. 目前典型计算机</h2><p>服务器对显卡的支持不如工作站，台式机的性能过低，因此本调查汇聚于工作站查询。</p><h3 id="2-1-服务器"><a href="#2-1-服务器" class="headerlink" title="2.1 服务器"></a>2.1 服务器</h3><p>服务器按外形划分可以划分为：塔式服务器、机架式服务器、刀片式服务器。<br>服务器除了一些低端的塔式机能用显卡以外，其他的都不支持显度卡，当然机架式服务器很薄根本就没有显卡的空间。<br>如果购买服务器，官方售后将不会主动为你安装个人家用系列显卡，转而推销商业计算卡。<br><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-15-26-49.png" srcset="/img/loading.gif" alt></p><p>截至2020年5月，服务器热销品牌Top-10（取自<a href="http://top.zol.com.cn/compositor/server.html" target="_blank" rel="noopener">ZOL网</a>）：<br>Dell、华为、浪潮、联想、惠普、H3C、ThinkServer、中科曙光、宝德、IBM。</p><h3 id="2-2-工作站"><a href="#2-2-工作站" class="headerlink" title="2.2 工作站"></a>2.2 工作站</h3><p>工作站的机箱主要以塔式为主，和一般家用主机机箱差距不大。<br>工作站对显卡的支持比服务器强很多，具体来说，工作站的主板对PCI-E的接口支持更好。</p><p>以下价格和资料全部取自于北京市政府采购网。</p><p><strong>神舟</strong><br>HFMPB2O8型号支持双路2080ti或TITAN<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=1r6e15444412038777n5&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">HFMPB2O8</a>    78,016.00 自带2080ti<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=2g0v15681729710791z0&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">HFMPB99K</a>  55,691.28   自带2080ti<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=6u7m15621197126648s6&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">HFMPB3IR</a>  32,870.00   C422可更换更高级显卡<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=8n5x15444417182251i6&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">HFMPB3J9</a>  29,980.00   自带2080</p><p><strong>联想</strong><br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=6h7y15571256822209c5&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">Think Station P520</a>   46,920.00    C422可更换更高级显卡<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=1s8z15571265520623u2&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">ThinkStation P720</a> 35,000.00   C622可更换更高级 </p><p><strong>宏碁</strong><br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=7w0g15281019011939v9&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">AP150 F4</a> 38,500.00    C622可更换更高级 </p><p><strong>浪潮</strong><br>浪潮是自研主板，不过其主板支持PCIe 16x，理论上只要供电足够即可安装包括2080Ti在内的显卡<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=3f4m15287143876139q2&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">P8000</a>    37,260.00</p><p><strong>惠普</strong><br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=4g2q15281081290990x3&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">HP Z4 G4</a>  15,900.00   C622可更换更高级<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=4v9p15281082398434h6&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">HP Z6 G4</a>  23,500.00   C622可更换更高级<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=0z0m15314586871347z2&amp;tiitPk=BG_002X&amp;tioPk=" target="_blank" rel="noopener">HP Z8 G4</a>  35,800.00   C622可更换更高级 </p><p><strong>苹果</strong><br>苹果的主板仅支持AMD的显卡，A卡不能用作深度学习。</p><h3 id="2-3-商用台式机"><a href="#2-3-商用台式机" class="headerlink" title="2.3 商用台式机"></a>2.3 商用台式机</h3><p>即普通台式机。普通台式机难以支撑深度学习任务。</p><h2 id="3-显卡介绍"><a href="#3-显卡介绍" class="headerlink" title="3. 显卡介绍"></a>3. 显卡介绍</h2><p>显卡分为Nvidia显卡和AMD显卡，其中Nvidia显卡可以用来深度学习训练和推理。</p><p>比较显卡性能，可以去<a href="https://versus.com/cn" target="_blank" rel="noopener">这个网站</a></p><h3 id="3-1-Nvidia显卡简介"><a href="#3-1-Nvidia显卡简介" class="headerlink" title="3.1 Nvidia显卡简介"></a>3.1 Nvidia显卡简介</h3><p><a href="https://www.bybusa.com/gpu-rank" target="_blank" rel="noopener">2020年显卡天梯图</a></p><p><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-22-23-18.png" srcset="/img/loading.gif" alt="2020年显卡天梯图"></p><p>目前最强的显卡是2080ti。预计在2020年底的3080ti发布之前，2080ti还会持续称霸显卡江湖。</p><h3 id="3-2-游戏显卡"><a href="#3-2-游戏显卡" class="headerlink" title="3.2 游戏显卡"></a>3.2 游戏显卡</h3><p>对游戏显卡的调研，参考<a href="https://post.smzdm.com/p/a6lrwk3e/" target="_blank" rel="noopener">“什么值得买”上的调研</a>以及<a href="https://www.cnblogs.com/xiaozhi_5638/p/10923351.html" target="_blank" rel="noopener">这个网址</a>。</p><p><img src="/2020/05/11/深度学习工作站调研-结合政府采购网信息/2020-05-10-22-09-38.png" srcset="/img/loading.gif" alt="游戏显卡一览"></p><p><strong>Geforce系列</strong></p><p>这个系列是销量最多、大众最为熟悉的显卡，一般用来打游戏。价格便宜，最新出来的旗舰卡RTX 2080Ti京东售价大概1w左右，根据不同的品牌，价格有所波动。低配置的便宜的一千就能买到。官方定位是消费级，但是它在深度学习上的表现也非常不错，很多人用来做推理、训练，单张卡的性能跟深度学习专业卡Tesla系列比起来其实差不太多，但是性价比却高很多。比如已经停产的GTX 1080显卡的参数基本和深度学习入门级显卡Tesla P4一样，用来做训练和推理的效果比Tesla P4还要好，可是GTX 1080一张卡才卖5000~6000左右，而Tesla P4要卖到1.4w。</p><p>究其原因，很大程度上在于英伟达官方禁止使用GTX、RTX系列显卡用于深度学习等用途，一经使用，自动过保。除了商业考虑外，还包括：Tesla多块显卡合起来的性能不会受很大影响，且Tesla系列显卡功耗优化非常明显，基本都是被动散热，不提供风扇，更适合数据中心机房工作环境等。</p><h3 id="3-3-计算显卡"><a href="#3-3-计算显卡" class="headerlink" title="3.3 计算显卡"></a>3.3 计算显卡</h3><p>专业级显卡的介绍参考<a href="https://product.pconline.com.cn/itbk/diy/graphics/1802/10846244.html" target="_blank" rel="noopener">“什么值得买”上的调研</a>以及<a href="https://www.cnblogs.com/xiaozhi_5638/p/10923351.html" target="_blank" rel="noopener">这个网址</a>。</p><p><strong>Quadro系列</strong><br>Quadro系列显卡一般用于特定行业，比如设计、建筑等，图像处理专业显卡，比如CAD、Maya等软件，一般人很少用到，价格相对来讲也稍微贵一些，最新的包括RTX 3000/4000/6000/8000型号。</p><p><strong>Tesla系列</strong><br>Tesla系列显卡定位并行计算，一般用于数据中心，具体点，比如用于深度学习，做训练、推理等。阿里云、Amazon云有非常多的GPU服务器，基本都采用Tesla系列显卡。这个系列显卡有个特别明显的特征，那就是贵。Tesla系列入门级显卡 Tesla P4，前面提到过，用来做深度学习的效果比GTX 1080还差，但是价格是后者的3倍多。像其他更高级别的Tesla V100、Tesla P100 价格高达8w、4w，这种价位的显卡虽然性能强劲，但是一般人是买不起的，只有企业数据中心才会部署这种显卡。</p><h3 id="3-4-显卡性能指标"><a href="#3-4-显卡性能指标" class="headerlink" title="3.4 显卡性能指标"></a>3.4 显卡性能指标</h3><p>本部分请参考<a href="https://www.cnblogs.com/xiaozhi_5638/p/10923351.html" target="_blank" rel="noopener">这里</a>。</p><h3 id="3-4-显卡罗列"><a href="#3-4-显卡罗列" class="headerlink" title="3.4 显卡罗列"></a>3.4 显卡罗列</h3><p>政府采购网上，值得采购的显卡如下</p><div class="table-container"><table><thead><tr><th>型号</th><th>价格</th></tr></thead><tbody><tr><td>p5000</td><td>27000</td></tr><tr><td>p6000</td><td>43500</td></tr><tr><td>2080</td><td>13500</td></tr><tr><td>k4000</td><td>42450</td></tr><tr><td>p4000</td><td>6800</td></tr><tr><td>2070s</td><td>8000</td></tr><tr><td>8000</td><td>93350</td></tr><tr><td>2080ti</td><td>16000</td></tr><tr><td>1080ti</td><td>8620</td></tr><tr><td>p4</td><td>28000</td></tr><tr><td>2080</td><td>10290</td></tr><tr><td>2060</td><td>5000</td></tr><tr><td>titan rtx</td><td>30000</td></tr><tr><td>p1000</td><td>3500</td></tr><tr><td>2070</td><td>7500</td></tr><tr><td>m2000</td><td>2982</td></tr><tr><td>titan v</td><td>37500</td></tr></tbody></table></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;服务器调研-2020年5月10日&quot;&gt;&lt;a href=&quot;#服务器调研-2020年5月10日&quot; class=&quot;headerlink&quot; title=&quot;服务器调研 2020年5月10日&quot;&gt;&lt;/a&gt;服务器调研 2020年5月10日&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;调研目标：&lt;
      
    
    </summary>
    
    
      <category term="record" scheme="https://superlova.github.io/categories/record/"/>
    
    
      <category term="PCIe" scheme="https://superlova.github.io/tags/PCIe/"/>
    
      <category term="显卡" scheme="https://superlova.github.io/tags/%E6%98%BE%E5%8D%A1/"/>
    
      <category term="工作站" scheme="https://superlova.github.io/tags/%E5%B7%A5%E4%BD%9C%E7%AB%99/"/>
    
  </entry>
  
  <entry>
    <title>chrome升级版本失败解决办法</title>
    <link href="https://superlova.github.io/2020/05/11/chrome%E5%8D%87%E7%BA%A7%E7%89%88%E6%9C%AC%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <id>https://superlova.github.io/2020/05/11/chrome%E5%8D%87%E7%BA%A7%E7%89%88%E6%9C%AC%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</id>
    <published>2020-05-11T03:46:55.000Z</published>
    <updated>2020-05-11T04:22:34.367Z</updated>
    
    <content type="html"><![CDATA[<h2 id="错误描述："><a href="#错误描述：" class="headerlink" title="错误描述："></a>错误描述：</h2><p>在Win7电脑上试图将Chrome从32位的72版本升级到64位的80版本时发生问题，升级进度到62%报错：<br>Chrome安装 未知错误导致安装失败  “0x80040902”</p><p>从chrome官网下载“chromesetup.exe”，打开梯子之后下载成功，在安装过程中也出现未知错误。<br>从Chrome官网下载“Chromestandalonesetup64.exe”，即离线安装包，最后也出现同样的错误。<br>重新启动、进入安全模式、试图结束所有有关google的进程的方法对我都没用。</p><h2 id="最后有效的方法："><a href="#最后有效的方法：" class="headerlink" title="最后有效的方法："></a>最后有效的方法：</h2><p>把原来的Chrome从控制面板的“添加删除程序”中卸载；</p><p>按住windows+R，在“开始”运行中输入“regedit”，打开注册表编辑器，依次进入HKEY_CURRENT_USER\Software\Google\Chrome；</p><p>把Chrome这一项删除，然后重启。再安装就不会存在问题了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;错误描述：&quot;&gt;&lt;a href=&quot;#错误描述：&quot; class=&quot;headerlink&quot; title=&quot;错误描述：&quot;&gt;&lt;/a&gt;错误描述：&lt;/h2&gt;&lt;p&gt;在Win7电脑上试图将Chrome从32位的72版本升级到64位的80版本时发生问题，升级进度到62%报错：&lt;br&gt;
      
    
    </summary>
    
    
      <category term="record" scheme="https://superlova.github.io/categories/record/"/>
    
    
      <category term="chrome" scheme="https://superlova.github.io/tags/chrome/"/>
    
  </entry>
  
  <entry>
    <title>testRNN--Coverage-guided Testing on Recurrent Neural Networks 论文阅读笔记</title>
    <link href="https://superlova.github.io/2020/03/25/testRNN%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>https://superlova.github.io/2020/03/25/testRNN%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</id>
    <published>2020-03-25T02:37:35.000Z</published>
    <updated>2020-05-20T02:37:02.929Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;

      
    
    </summary>
    
    
      <category term="paper" scheme="https://superlova.github.io/categories/paper/"/>
    
    
      <category term="RNN" scheme="https://superlova.github.io/tags/RNN/"/>
    
      <category term="testing" scheme="https://superlova.github.io/tags/testing/"/>
    
      <category term="testRNN" scheme="https://superlova.github.io/tags/testRNN/"/>
    
  </entry>
  
  <entry>
    <title>RNN-Test--Adversarial Testing Framework for Recurrent Neural Network Systems 论文阅读笔记</title>
    <link href="https://superlova.github.io/2020/03/25/RNN-Test%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>https://superlova.github.io/2020/03/25/RNN-Test%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</id>
    <published>2020-03-25T02:37:16.000Z</published>
    <updated>2020-05-20T02:36:55.897Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;

      
    
    </summary>
    
    
      <category term="paper" scheme="https://superlova.github.io/categories/paper/"/>
    
    
      <category term="RNN" scheme="https://superlova.github.io/tags/RNN/"/>
    
      <category term="testing" scheme="https://superlova.github.io/tags/testing/"/>
    
      <category term="RNN-Test" scheme="https://superlova.github.io/tags/RNN-Test/"/>
    
  </entry>
  
  <entry>
    <title>lintcode-138 子数组求和问题</title>
    <link href="https://superlova.github.io/2020/03/24/lintcode-138-%E5%AD%90%E6%95%B0%E7%BB%84%E4%B9%8B%E5%92%8C/"/>
    <id>https://superlova.github.io/2020/03/24/lintcode-138-%E5%AD%90%E6%95%B0%E7%BB%84%E4%B9%8B%E5%92%8C/</id>
    <published>2020-03-24T09:30:31.000Z</published>
    <updated>2020-03-24T12:04:57.628Z</updated>
    
    <content type="html"><![CDATA[<ul><li>给定一个整数数组，找到和为零的子数组。</li><li>你的代码应该返回满足要求的子数组的起始位置和结束位置<a id="more"></a><h1 id="lintcode-138：子数组之和"><a href="#lintcode-138：子数组之和" class="headerlink" title="lintcode 138：子数组之和"></a>lintcode 138：子数组之和</h1><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2></li></ul><p>给定一个整数数组，找到和为零的子数组。你的代码应该返回满足要求的子数组的起始位置和结束位置<br><strong>样例 1:</strong><br>输入: [-3, 1, 2, -3, 4]<br>输出: [0,2] 或 [1,3]<br>样例解释： 返回任意一段和为0的区间即可。<br><strong>样例 2:</strong><br>输入: [-3, 1, -4, 2, -3, 4]<br>输出: [1,5]<br><strong>注意事项</strong><br>至少有一个子数组的和为 0</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>子数组之和问题。看看哪个区间段，段内所有元素加起来刚好等于0（或者某个值）。类似这种求区间段，段内元素满足什么条件的问题。</p><p>关键是下面这个结论：</p><p>准备一个数组array，其中第i个元素保存num[0]~num[i]之和。那么index_value中一旦出现两个元素其值相同，就说明这两个下标之间所有元素加起来等于0。</p><script type="math/tex; mode=display">\begin{aligned}& if & \sum_{i=0}^{\operatorname{index_1}}nums(i) = \sum_{i=0}^{\operatorname{index_2}}nums(i) \\ & then\quad & return \left[ \operatorname{index_1}+1, \operatorname{index_2} \right] \end{aligned}</script><p>举个例子：对于数组<code>num = [-3, 1, 2, -3, 4]</code>，我们可以构建array数组如下：</p><div class="table-container"><table><thead><tr><th>index</th><th>nums[index]</th><th>$\sum_{i=0}^{index}nums(i)$</th></tr></thead><tbody><tr><td>0</td><td>-3</td><td>-3</td></tr><tr><td>1</td><td>1</td><td>-2</td></tr><tr><td>2</td><td>2</td><td>0</td></tr><tr><td>3</td><td>-3</td><td>-3</td></tr><tr><td>4</td><td>4</td><td>1</td></tr></tbody></table></div><p>返回 [0, 2] 或 [1, 3]</p><p>在代码实现中，当我们采用数组实现array时，会受限于查询array内元素的线型时间复杂度，为了找某个值对应的下标，遍历array数组的过程，可能耗费线性复杂度的时间，导致代码TLE超时。</p><p>因此我们采用散列，将散列的key设置为前i个元素的和值，value为该值对应的下标位置。</p><p>在Python中查找元素，用<strong>字典</strong>可以大大加快查找速度。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><pre><code class="lang-python">class Solution:    &quot;&quot;&quot;    @param nums: A list of integers    @return: A list of integers includes the index of the first number and the index of the last number    &quot;&quot;&quot;    def subarraySum(self, nums):        index_value = {}        accumulator = 0        for i in range(len(nums)):            accumulator += nums[i]            if accumulator in index_value:                return [index_value[accumulator] + 1, i]            else:                index_value[accumulator] = i        else:            if accumulator == 0:                return [0, i]        return [0, 0]</code></pre><h2 id="变种：子数组元素之和等于k"><a href="#变种：子数组元素之和等于k" class="headerlink" title="变种：子数组元素之和等于k"></a>变种：子数组元素之和等于k</h2><script type="math/tex; mode=display">\begin{aligned}& if & \sum_{i=0}^{\operatorname{index_1}}nums(i) - \bold{k} = \sum_{i=0}^{\operatorname{index_2}}nums(i) \\ & then\quad & return \left[ \operatorname{index_1}+1, \operatorname{index_2} \right] \end{aligned}</script><pre><code class="lang-python">class Solution:    &quot;&quot;&quot;    @param nums: A list of integers    @return: A list of integers includes the index of the first number and the index of the last number    &quot;&quot;&quot;    def subarraySum(self, nums, obj_num):        index_value = {}        accumulator = 0        for i in range(len(nums)):            accumulator += nums[i]            if accumulator - obj_num in index_value:                return [index_value[accumulator - obj_num] + 1, i]            else:                index_value[accumulator] = i        else:            if accumulator == 0:                return [0, i]        return [0, 0]</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;给定一个整数数组，找到和为零的子数组。&lt;/li&gt;
&lt;li&gt;你的代码应该返回满足要求的子数组的起始位置和结束位置&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
      <category term="code_exercises" scheme="https://superlova.github.io/categories/code-exercises/"/>
    
    
      <category term="algorithm" scheme="https://superlova.github.io/tags/algorithm/"/>
    
      <category term="lintcode" scheme="https://superlova.github.io/tags/lintcode/"/>
    
      <category term="array" scheme="https://superlova.github.io/tags/array/"/>
    
  </entry>
  
  <entry>
    <title>C++字符串高级操作总结</title>
    <link href="https://superlova.github.io/2020/03/23/C-%E5%AD%97%E7%AC%A6%E4%B8%B2%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E6%80%BB%E7%BB%93/"/>
    <id>https://superlova.github.io/2020/03/23/C-%E5%AD%97%E7%AC%A6%E4%B8%B2%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E6%80%BB%E7%BB%93/</id>
    <published>2020-03-23T15:50:02.000Z</published>
    <updated>2020-03-25T02:32:11.470Z</updated>
    
    <content type="html"><![CDATA[<p>C++的字符串操作非常多，功能也非常多样化，熟练使用标准库提供的字符串操作函数能够高效提升我们编写代码的效率和可读性。除了常用的<string>库中包含的几项基本操作之外，本文总结了几项特别好用而又不为人所知的高级操作。<br><a id="more"></a></string></p><h2 id="常见的基本操作回顾"><a href="#常见的基本操作回顾" class="headerlink" title="常见的基本操作回顾"></a>常见的基本操作回顾</h2><p>必须指明，<string>中字符串方法可以按照输入参数的类型不同调用不同的重载方法，这些函数名相同，但是参数类型和顺序完全不同，返回值也略有差别。</string></p><p>部分函数，包括insert和erase等函数可分为两类，如果输入的位置参数<code>pos</code>为整数<code>int</code>，此时返回值为<strong>被插入字符串</strong>的引用；而输入的位置参数类型为迭代器<code>iterator</code>，则会调用返回迭代器的函数，该迭代器<strong>指向被插入部分的头部</strong>。</p><p>而且<string>部分函数为了兼容C原生字符串，提供了一批适用于C String构造接口，这又产生了一大批只有参数顺序不同的同名函数。比如对于string构造方法上，输入原生string和输入C String的参数含义完全不同。</string></p><p>这些同名、功能相似但不同参数的函数使得C++新人学习标准库时容易产生极大的困扰。</p><h3 id="1-构造string"><a href="#1-构造string" class="headerlink" title="1. 构造string"></a>1. 构造string</h3><pre><code class="lang-cpp">const char * cp = &quot;Hello World!!!&quot;;char noNull[] = {&#39;H&#39;, &#39;i&#39;};</code></pre><p>输入一个char类型的指针，以及偏移量。转换从指针开始的偏移量个字符。<br>如果未指定偏移量，则默认转化到碰到’\0’为止。<br>没有’\0’结尾则该行为未定义。  </p><pre><code class="lang-cpp">string s1(cp); // 从C风格字符串转化string s2(noNull, 2); // 指定转化的字符个数string s3(noNull); // 未定义，因为noNull不是以空字符结尾string s4(cp + 6, 5);</code></pre><p>拷贝构造函数，从其他string拷贝<br>指定拷贝位置和拷贝字符个数<br>不指定pos则默认从头拷贝<br>不指定len则默认从pos开始全拷贝<br>pos越界则抛出异常<br>len越界没问题，只到’\0’  </p><pre><code class="lang-cpp">string s5(s1, 6, 5);string s6(s1, 6);string s7(s1, 6, 20);string s8(s1, 16);</code></pre><p>substr函数，输入pos和len<br>返回由该字符串的第pos位置拷贝len个字符组成的新子串  </p><pre><code class="lang-cpp">string s9 = s1.substr(0, 5);</code></pre><h3 id="2-改变string"><a href="#2-改变string" class="headerlink" title="2. 改变string"></a>2. 改变string</h3><p>以insert为例，简单介绍不同参数重载的不同insert。</p><p>insert除了接受迭代器的版本之外，还有直接接受下标的版本。返回值为被插入字符串的引用。<br>s.insert(pos, count, char)<br>s.insert(pos, char_ptr, len)<br>s.insert(pos, string, pos, len)</p><p>第一个位置总会是pos，表示被插入位置；</p><p>第二个参数如果是个数，那么你调用的是第一个insert函数，其含义为重复插入第三个参数char所制定的内容；</p><p>第二个参数如果是C风格字符串，那么第三个参数可以指出插入长度，不指名就默认把该C风格字符串全插到pos的位置；</p><p>第二个参数如果是string，那么你还需在string参数后指定从哪个pos开始插，并且指定len表示插入多少个。相对于插入C风格字符串的insert来说，插入string更灵活。</p><p>其他函数及其说明见下表。</p><p><img src="/2020/03/23/C-字符串高级操作总结/2020-03-24-20-38-14.png" srcset="/img/loading.gif" alt><br><img src="/2020/03/23/C-字符串高级操作总结/2020-03-25-08-58-36.png" srcset="/img/loading.gif" alt></p><h2 id="搜索字符串"><a href="#搜索字符串" class="headerlink" title="搜索字符串"></a>搜索字符串</h2><p><string>定义了六种不同的搜索方法，每种方法拥有四个重载版本。</string></p><p><img src="/2020/03/23/C-字符串高级操作总结/2020-03-25-09-00-50.png" srcset="/img/loading.gif" alt><br><img src="/2020/03/23/C-字符串高级操作总结/2020-03-25-09-02-09.png" srcset="/img/loading.gif" alt></p><p>需要注意的有两点：</p><ol><li>搜索函数返回类型为string::size_type，为无符号整数类型。</li><li>搜索失败时，返回string::npos，该值为-1，也就是无符号整数最大的值。</li></ol><h2 id="正则表达式库"><a href="#正则表达式库" class="headerlink" title="正则表达式库"></a>正则表达式库</h2><p>正则表达式是字符串匹配的有力工具。C++11加入了对正则表达式的支持，具体定义位于<regex>头文件中。</regex></p><p>在C++中，正则表达式可以做的工作有：</p><ul><li>Match 将整个输入拿来比对（匹配）某个正则表达式</li><li>Search 查找与正则表达式吻合的子串</li><li>Tokenize 根据正则表达式切分字符串</li><li>Replace 根据正则表达式替换字符串</li></ul><h3 id="第一种应用：Match和Search"><a href="#第一种应用：Match和Search" class="headerlink" title="第一种应用：Match和Search"></a>第一种应用：Match和Search</h3><p>具体流程可概括为：定义、匹配、判断</p><pre><code class="lang-cpp">regex reg(&quot;&lt;.*&gt;.*&lt;/.*&gt;&quot;); // 定义bool isExist = regex_match(string, reg); // 匹配整体//orbool isExist = regex_search(string, reg); // 匹配部分cout &lt;&lt; boolalpha &lt;&lt; isExist &lt;&lt; endl; // 判断</code></pre><p>你可能已经注意到了，regex_match和regex_search返回的仅仅是一个bool值，表明是否匹配。我们还需要匹配的位置。此时我们需要一个<code>match</code>对象来保存结果。<code>match</code>对象的方法如下所示。</p><pre><code class="lang-cpp">smatch m;bool isExist = regex_search(string, m, reg); // 结果保存在m中m.empty()m.size() // 返回匹配个数m.str(i) // 类似于python中group，返回第i个匹配位置的字符串。i=0则返回全部m.length(i) // 同上，返回第i个匹配字符串的长度m.position(i) // 同上，返回第i个匹配字符串的位置m.prefix().str() // 已匹配位置之前的字符串，字符串前缀m.suffix().str() // 已匹配位置之后的字符串，字符串后缀for (auto pos = m.begin(); pos != m.end(); ++pos) {    cout &lt;&lt; *pos &lt;&lt; endl;}代码中的参数i，代表了正则表达式中存在分组，i为提取分组i的被匹配内容。类似python中的group。分组操作是正则表达式的语法，本文不再赘述。</code></pre><p><code>match</code>对象根据保存内容类型不同分成</p><ul><li><code>smatch</code> 匹配string</li><li><code>cmatch</code> 匹配C风格字符串</li><li><code>wsmatch</code> 匹配wstring</li><li><code>wcmatch</code> 匹配const wchar_t*</li></ul><h3 id="第二种应用：Regex-Iterator"><a href="#第二种应用：Regex-Iterator" class="headerlink" title="第二种应用：Regex Iterator"></a>第二种应用：Regex Iterator</h3><p>data可能很长，reg可能会多次匹配。为了迭代所有的匹配成果，我们可以使用regex_iterator。根据类型不同，分别是<br><code>sregex_iterator</code><br><code>cregex_iterator</code><br><code>wsregex_iterator</code><br><code>wcregex_iterator</code></p><pre><code class="lang-cpp">regex reg(&quot;...&quot;);sregex_iterator pos(data.cbegin(), data.cend(), reg);sregex_iterator end;for (; pos != end; ++pos) {    cout &lt;&lt; pos-&gt;str() &lt;&lt; endl;}</code></pre><h3 id="第三种应用：Regex-Token-Iterator"><a href="#第三种应用：Regex-Token-Iterator" class="headerlink" title="第三种应用：Regex Token Iterator"></a>第三种应用：Regex Token Iterator</h3><p>你可能关注的不是被匹配的字符串，而是其余的字符串。此时正则表达式串就像是切割刀一样，将data分割成不含被匹配串的几部分。我们可以利用此功能实现C++中一直没能实现的字符串分割函数split。</p><pre><code class="lang-cpp">string data = &quot;qqq www    eee rrr&quot;;regex r(&quot;\\s+&quot;);sregex_token_iterator pos(data.cbegin(), data.cend(), r, -1); // -1代表你对正则表达式匹配的内容不感兴趣sregex_token_iterator end;for (; pos != end; ++pos) {    cout &lt;&lt; *pos &lt;&lt; endl;}</code></pre><h3 id="第四种应用：替换"><a href="#第四种应用：替换" class="headerlink" title="第四种应用：替换"></a>第四种应用：替换</h3><p>下面的代码将</p><pre><code class="lang-html">&lt;person&gt;&lt;first&gt;Nico&lt;/first&gt;&lt;/person&gt;</code></pre><p>替换成</p><pre><code class="lang-html">&lt;person&gt;&lt;first value=&quot;Nico&quot;/&gt;&lt;/person&gt;</code></pre><pre><code class="lang-cpp">string data = &quot;&lt;person&gt;&lt;first&gt;Nico&lt;/first&gt;&lt;/person&gt;&quot;regex reg(&quot;&lt;(.*)&gt;(.*)&lt;/(\\1)&gt;&quot;);string replace_pattern = &quot;&lt;$1 value=\&quot;$2\&quot;/&gt;&quot;;cout &lt;&lt; regex_replace(data, reg, replace_pattern) &lt;&lt; endl;</code></pre><p>模式替换串用$n指定第几个匹配部分group(n)<br>$1 value=$2 含义即为原来是group(1)的部分替换成group(2)的内容。</p><p><img src="/2020/03/23/C-字符串高级操作总结/2020-03-25-10-21-38.png" srcset="/img/loading.gif" alt></p><h2 id="string-view"><a href="#string-view" class="headerlink" title="string_view"></a>string_view</h2><p>C++17加入了string_view对象，能够避免string类型的复制临时对象操作。</p><ul><li>string_view对象由两部分组成，分别是<strong>数据的起始指针</strong>和<strong>数据的长度</strong>。有点类似于带其他语言表示字符串的方法，不依赖<code>&#39;\0&#39;</code>在结尾，而是通过一个变量记忆长度。</li><li>string_view只读，不能修改。可以很好地作为函数的参数和返回值</li></ul><pre><code class="lang-cpp">//使用string的拷贝操作string s(1000, &#39;0&#39;);string sub_s = s.substr(100, 200); // O(n)//使用view则不需要拷贝string_view sv(s); // no copystring_view sv2 = sv.substr(100, 200); // O(1)</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;C++的字符串操作非常多，功能也非常多样化，熟练使用标准库提供的字符串操作函数能够高效提升我们编写代码的效率和可读性。除了常用的&lt;string&gt;库中包含的几项基本操作之外，本文总结了几项特别好用而又不为人所知的高级操作。&lt;br&gt;&lt;/string&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="C++" scheme="https://superlova.github.io/tags/C/"/>
    
      <category term="STL" scheme="https://superlova.github.io/tags/STL/"/>
    
      <category term="string" scheme="https://superlova.github.io/tags/string/"/>
    
      <category term="regex" scheme="https://superlova.github.io/tags/regex/"/>
    
  </entry>
  
  <entry>
    <title>C++关联容器学习笔记</title>
    <link href="https://superlova.github.io/2020/03/20/C-%E5%85%B3%E8%81%94%E5%AE%B9%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>https://superlova.github.io/2020/03/20/C-%E5%85%B3%E8%81%94%E5%AE%B9%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</id>
    <published>2020-03-19T16:10:13.000Z</published>
    <updated>2020-03-20T11:21:43.587Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-关联容器特点简介"><a href="#1-关联容器特点简介" class="headerlink" title="1. 关联容器特点简介"></a>1. 关联容器特点简介</h2><p>关联容器和顺序容器是两种适用范围不同的容器。许多C++程序员只用过顺序容器诸如vector和string，但他们从未使用过set和map等关联数据结构。</p><p><img src="/2020/03/20/C-关联容器学习笔记/2020-03-20-09-47-21.png" srcset="/img/loading.gif" alt></p><p><code>set</code>是元素的简单集合，用来保存类型相同的一组元素。当你只是想知道一个值<strong>是否存在</strong>时，<code>set</code>是最有用的。</p><ul><li><code>set</code><strong>不会出现重复元素</strong></li><li>内部元素永远<strong>有序</strong></li><li><code>set</code>中的元素一经添加就<strong>不能修改</strong>。</li></ul><p><code>map</code>可以看做特殊的<code>vector</code>，其特殊之处在于此<code>map</code>中的每个元素都由两部分 <code>(key, value)</code> 构成，C++将每个这样的 <code>(key, value)</code> 封装成一个对象，其类别为<code>pair</code>。<code>map</code>内部元素都为<code>pair</code>类型。</p><ul><li><code>map</code><strong>不会出现重复<code>key</code></strong></li><li>内部元素永远有序，按照<code>pair</code>类型元素的key字段排序（key字段必须能够被排序）。</li><li><code>map</code>中的key一经添加就<strong>不能修改</strong>。</li></ul><h2 id="2-有序容器基本操作"><a href="#2-有序容器基本操作" class="headerlink" title="2. 有序容器基本操作"></a>2. 有序容器基本操作</h2><p>以下所有操作需要添加头文件：</p><pre><code class="lang-cpp">#include &lt;map&gt;#include &lt;set&gt;</code></pre><h3 id="2-1-初始化"><a href="#2-1-初始化" class="headerlink" title="2.1 初始化"></a>2.1 初始化</h3><h4 id="2-1-1-map"><a href="#2-1-1-map" class="headerlink" title="2.1.1 map"></a>2.1.1 map</h4><pre><code class="lang-cpp">map&lt;int, int&gt; m; // 默认初始化map&lt;int, int&gt; m{{1, 2}, {2, 4}}; // 初始化列表map&lt;int, int&gt; m2(m); // 拷贝构造函数map&lt;int, int&gt; m = {{1, 2}, {2, 4}}; // 初始化列表 + 拷贝构造函数// pair和make_pair()在#include&lt;utility&gt;中map&lt;int, int&gt; m3 = {    std::pair&lt;int, int&gt;(1, 2), std::pair&lt;int, int&gt;(2, 4)}; // 初始化列表的方法展开来说就是这样map&lt;int, int&gt; m4{    std::make_pair(1, 2), std::make_pair(2, 4)}; // 或者使用make_pair函数，免得输入参数类型，效果等价。// std::begin()和end()在#include &lt;iterator&gt; 中map&lt;int, int&gt; m5 = {std::begin(m), std::end(m)}; // 迭代器，前提是被迭代的对象内部元素类型是pair</code></pre><h4 id="2-1-2-set"><a href="#2-1-2-set" class="headerlink" title="2.1.2 set"></a>2.1.2 set</h4><pre><code class="lang-cpp">set&lt;int&gt; s; // 默认初始化set&lt;int&gt; s{1, 2, 3, 6, 9, 10}; // 初始化列表set&lt;int&gt; s2(s); // 拷贝构造函数set&lt;int&gt; s = {1, 2, 3, 6, 9, 10}; // 初始化列表 + 拷贝构造函数// std::begin()和end()在#include &lt;iterator&gt; 中int arr[] = {1, 2, 5, 8, 9};set&lt;int&gt; s2{std::begin(arr), std::end(arr)}; // 迭代器</code></pre><h4 id="2-1-3-定义排序方法"><a href="#2-1-3-定义排序方法" class="headerlink" title="2.1.3 定义排序方法"></a>2.1.3 定义排序方法</h4><p>可以给map或set初始化时输入一个比较器，用以替代原有的比较key大小的方法（原来一般是小于号）。这个比较器可以是一个函数，也可以是一个当做函数用的函数对象。下面是一个示例。</p><pre><code class="lang-cpp">struct comp {    template&lt;typename T&gt;    bool operator() (const T&amp; l, const T&amp; r) const {        return l &gt; r;    }};int main() {    map&lt;int, int, comp&gt; m = {        {1, 5}, {2, 3}, {7, 6}    };    for (auto&amp;&amp; [key, val] : m) {        cout &lt;&lt; key &lt;&lt; &quot; &quot; &lt;&lt; val &lt;&lt; endl;    }}output&gt;&gt; 7 6&gt;&gt; 2 3&gt;&gt; 1 5</code></pre><p>实现自己写的比较器，简单来说就是实现一个自定义的“&lt;”小于号。<br>原有的比较器<code>bool compare(object&amp; left, object&amp; right)</code>作用如下：</p><ol><li>compare输出true，map 认为 left 小于 right ，把left放在right前面。</li><li>compare输出false，map 认为 left 大于等于 right。</li><li>如果compare(left, right)为false，compare(right, left)也为false，就认为left==right。否则left放在right后面。</li></ol><blockquote><p>自己实现的比较器，必须让关键字集合满足以下性质：</p><ul><li>关键字自己不能小于自己</li><li>两个关键字不能互相小于对方</li><li>如果按照比较器，key1小于key2，key2小于key3，则key1小于key3</li></ul><p>学过离散数学的同学应该知道，上面描述的这种二元关系满足反自反性、反对称性和传递性。满足这三个性质的关系称之为“严格偏序关系”。我们日常生活中见到的数字比较的小于号、集合中“真包含于”都是这种关系。</p></blockquote><h3 id="2-2-添加元素"><a href="#2-2-添加元素" class="headerlink" title="2.2 添加元素"></a>2.2 添加元素</h3><h4 id="2-2-1-insert"><a href="#2-2-1-insert" class="headerlink" title="2.2.1 insert"></a>2.2.1 insert</h4><p>对于map</p><pre><code class="lang-cpp">map&lt;string, int&gt; m;m.insert({&quot;str&quot;, 1});m.insert(make_pair(&quot;ser&quot;, 1));m.insert(pair&lt;string, int&gt;(&quot;ssr&quot;, 1));m.insert(map&lt;string, int&gt;::value_type(&quot;sdr&quot;, 1));</code></pre><p>对于set</p><pre><code class="lang-cpp">vector&lt;int&gt; ivec = {2, 4, 6, 8};set&lt;int&gt; set2;set2.insert(1);set2.insert({2, 4, 6, 8});set2.insert(ivec.cbegin(), ivec.cend());</code></pre><p>insert函数和emplace函数返回pair对象，pair.first为迭代器，指向刚插入的元素，pair.second为bool，表示插入是否成功。如果由于存在重复导致插入失败，则除了second为false之外，first指向那个重复元素。</p><pre><code class="lang-cpp">set&lt;int&gt; s;const auto [iter, success] = s.insert(x); // 返回值拆成两个</code></pre><h4 id="2-2-2-对map使用下标-操作"><a href="#2-2-2-对map使用下标-操作" class="headerlink" title="2.2.2 对map使用下标[]操作"></a>2.2.2 对map使用下标[]操作</h4><p>map使用下标操作首先会查找该key的元素，找不到就新建一个key的pair，将其初始化。最后执行赋值操作。</p><pre><code class="lang-cpp">map&lt;char, int&gt; mp;mp[&#39;a&#39;] = 5;mp[&#39;b&#39;] = 4;mp[&#39;c&#39;] = 3;</code></pre><h3 id="2-3-访问和查找元素"><a href="#2-3-访问和查找元素" class="headerlink" title="2.3 访问和查找元素"></a>2.3 访问和查找元素</h3><p>map除了使用下标操作访问元素之外，还可以用<code>at()</code>函数。</p><pre><code class="lang-cpp">map&lt;char, int&gt; mp;mp.at(k) // 查找关键字为k的元素，找不到就抛出异常</code></pre><p>关联容器内置的<code>find</code>函数和<code>count</code>函数可以执行查找操作</p><pre><code class="lang-cpp">//c为一个map容器c.find(k) // 返回一个迭代器，指向关键字为k的元素。若k不在容器中，则返回尾后迭代器c.count(k) // 返回关键字等于k的元素数量。对于map和set而言，返回值永远是0或1。</code></pre><p>当我们要在map容器中查找一个元素时，我们可以使用find函数查找。</p><pre><code class="lang-cpp">auto it = word_count.find(&quot;foobar&quot;);if(it==word_count.end())   cout&lt;&lt;&quot;foobar is not in the map&quot;&lt;&lt;endl;else   cout&lt;&lt;it-&gt;first&lt;&lt;&quot; &quot;&lt;&lt;it-&gt;second&lt;&lt;endl;</code></pre><p>在有序容器中，我们还可以找到关键字k附近的元素。</p><pre><code class="lang-cpp">s.lower_bound(k); // 返回迭代器，指向第一个关键字**不小于**k的元素s.upper_bound(k); // 返回迭代器，指向第一个关键字**大于**k的元素s.equal_range(k); // 返回pair&lt;iter, iter&gt;，表示关键字为k的元素范围。适用于multiset/multimap。若是没有k，则返回两个end()</code></pre><p>在对于允许重复关键字的容器来说，查找元素的过程稍微复杂些，因为一个关键字可能对应多个值，我们需要把这么对应的值都找出来。<br>如果multimap中有多个元素具有相同的关键字，则这些关键字在容器中会相邻存储。我们可以通过这一特性，将一个关键字对应的多个值全部找出来。</p><pre><code class="lang-cpp">//《C++ Primer》示例，查找某作者对应的所有书籍//authors是一个multimap容器string search_item(&quot;Alain&quot;);int numbers=authors.count(search_item);auto it=authors.find(search_item);while(numbers){   cout&lt;&lt;iter-&gt;second&lt;&lt;endl;   ++it;   numbers--;}// 或者采用一种其他方式for (auto beg = authors.lower_bound(search_item),          end = authors.upper_bound(search_item);     beg != end; ++beg) {    cout &lt;&lt; beg-&gt;second &lt;&lt; endl; }// 或者采用一种更加直接的方式for (auto pos = authors.equal_range(search_item);     pos.first != pos.second; ++pos.first){    cout &lt;&lt; pos.first-&gt;second &lt;&lt; endl; // 打印每本书}</code></pre><h3 id="2-4-删除元素"><a href="#2-4-删除元素" class="headerlink" title="2.4 删除元素"></a>2.4 删除元素</h3><p>使用erase</p><pre><code class="lang-cpp">// s为关联容器，可能为set/map/multiset/multimaps.erase(k); // 删除指定关键字的元素，返回删除的个数。s.erase(iter); // iter必须指向s中的一个真实元素，返回指向删除元素之后的元素的迭代器。s.erase(iter1, iter2); // 删除迭代器[iter1, iter2)，其中必须是真是的元素。iter2指向的元素不删除。返回iter2</code></pre><h2 id="3-无序容器特有操作"><a href="#3-无序容器特有操作" class="headerlink" title="3. 无序容器特有操作"></a>3. 无序容器特有操作</h2><h3 id="3-1-无序容器特点"><a href="#3-1-无序容器特点" class="headerlink" title="3.1 无序容器特点"></a>3.1 无序容器特点</h3><p>如果我们不关心容器中元素的次序，那么我们就可以使用无序容器。在无序容器中，元素没有明确的排列次序，当你迭代容器内的所有元素时，会发现他们的次序个有可能。我们唯一关心的是某个元素特定元素是否位于容器内。</p><p>无需容器，常常以Hash table实现出来，内部结构是一个类似于<code>vector&lt;list&gt;</code>的列表，列表的元素是链表<code>linked list</code>。通过某个hash函数的运算，确定元素落于这个列表的位置。</p><p>Hash函数的运算目标是让每个元素的落点（位置）有助于用户快速访问任何一个元素（前提则是哈希函数本身也必须够快）。</p><p>由于这样一个快速而完美的哈希函数不一定存在。抑或由于造成array耗费巨额内存而显得不切实际，因此退而求其次的哈希函数有可能让多个元素落于同一位置上，所以设计上就让vector的元素再被放进一个linked list中。如此一来，vector的每个位置就得以存放一个以上的元素。</p><p><img src="/2020/03/20/C-关联容器学习笔记/2020-03-20-18-01-59.png" srcset="/img/loading.gif" alt></p><p>无序容器的主要优点是，当你打算查找一个特定值的元素，其速度甚至可能快过有序关联式容器（时间复杂度O(1)）。前提是你有一个良好的哈希函数。然而这样的哈希函数可能需要许多内存。</p><p>以下所有操作需要添加头文件：</p><pre><code class="lang-cpp">#include &lt;unordered_map&gt;#include &lt;unordered_set&gt;</code></pre><p><code>unordered_map</code>/<code>unordered_multimap</code>/<code>unordered_set</code>/<code>unordered_multiset</code>的初始化、插入、查找和删除的方法与普通的<code>map</code>/<code>multimap</code>/<code>set</code>/<code>multiset</code>没有大的区别。注意以下主要区别要点：</p><ol><li>无序容器内部不含比较器，因此你也不能提供自定义比较器。</li><li>每次对无序容器的添加操作可能会引起无序容器次序的改变。即便是相同元素，在不同的电脑上也可能得到不同的次序。</li><li>删除元素虽然不会引起无序容器次序改变，但是删除之后的第一次插入必然会引发次序改变。</li></ol><p>次序究竟会不会变化、怎样变化取决于使用的rehashing策略，该策略可由程序员自定义，就像在有序关联容器里定义排序函数那样。</p><h3 id="3-2-管理桶"><a href="#3-2-管理桶" class="headerlink" title="3.2 管理桶"></a>3.2 管理桶</h3><p>1.桶接口</p><pre><code class="lang-cpp">m.bucket_count()        正在使用的桶的数目m.max_bucket_count()    容器能容纳的最多的桶的数量m.bucket_size(n)        第n个桶中有多少个元素m.bucket(k)             关键字为k的元素在哪个桶</code></pre><p>2.桶迭代</p><pre><code class="lang-cpp">local_iterator            可以用来访问桶中元素的迭代器类型const_local_iterator      桶迭代器的const版本m.begin(n)、m.end(n)      桶n的首元素迭代器和尾后迭代器（n是什么类型？）m.cbegin(n)、m.cend(n)    与前两个函数类似，但返回const_local_iterator</code></pre><p>3.哈希策略</p><pre><code class="lang-cpp">//每个桶的平均元素数量，返回float值m.load_factor() //m试图维护的平均桶大小，返回float值，要求创建的新桶的load_factor&lt;=max_load_factor         m.max_load_factor() //重新存储，使得bucket_count&gt;=n，且bucket_count&gt;size/max_load_factor         m.rehash(n)  //重新存储，使得m可以保存n个元素且不必rehash m.reserve(n)</code></pre><h3 id="3-3-自定义哈希函数和比较函数"><a href="#3-3-自定义哈希函数和比较函数" class="headerlink" title="3.3 自定义哈希函数和比较函数"></a>3.3 自定义哈希函数和比较函数</h3><p>默认情况下，无序容器使用<code>==</code>来判断两key是否相等，并使用系统内置的哈希函数生成哈希值。不同类型的key会应用到不同的哈希函数，如下都是STL内置的哈希函数对象：</p><pre><code class="lang-cpp">struct hash&lt;char*&gt;struct hash&lt;const char*&gt;struct hash&lt;char&gt; struct hash&lt;unsigned char&gt; struct hash&lt;signed char&gt;struct hash&lt;short&gt;struct hash&lt;unsigned short&gt; struct hash&lt;int&gt; struct hash&lt;unsigned int&gt;struct hash&lt;long&gt; struct hash&lt;unsigned long&gt;</code></pre><p>如果key使用的是以上类型中的一种，可以使用缺省的hash函数。当然你程序员可以定义自己的hash函数。对于自定义对象，只能自定义hash函数。</p><p>下面是《C++ Primer》的一个自定义哈希函数的一个例子：</p><pre><code class="lang-cpp">/* 定义哈希函数和判等器 */size_t hasher(const Sales_data &amp;sd){    // 对书籍对象的哈希    return hash&lt;string&gt;() (sd.isbn()); // 返回其isbn编号的哈希，调用内置的string哈希函数}bool eqOp(const Sales_data &amp;lhs, const Sales_data &amp;rhs){    // 如何判断两本书是否相等？    return lhs.isbn() == rhs.isbn(); // 判断两书的isbn编号是否相等}/* 使用哈希函数和判等器 */using SD_multiset = unordered_multiset&lt;Sales_data, decltype(hasher)*, decltype(eqOp)*&gt;; // 类型名太长了，将类型名保存成别的变量名SD_multiset bookstore(42, hasher, eqOp);</code></pre><h2 id="4-小结"><a href="#4-小结" class="headerlink" title="4. 小结"></a>4. 小结</h2><p>对C++ 关联容器的总结到此告一段落。本篇文章从开始着手写作到完成，不间断地工作了八个小时，期间不断重温已经遗忘的知识，查阅资料，其中很多还是自己曾经收藏过的资料。</p><p>写作时，我多次问自己：写一篇不会有人看的文章值得吗？我也多次想要像我以前很多文章、像CSDN大多数的文章那样，随便水水，记录一下，反正只有自己看。</p><p>但是这次我觉得，我要为自己负责，要为已经付出的精力和时间负责。我相信大家都会有这种迷茫的时候，怀疑自己手头上的工作有没有意义，甚至想放弃。</p><p>不要轻言放弃，尤其是当你怀疑它的意义的时候。因为这个时候你可能是在为自己的懒惰找借口。将一件事情的意义贬低，这种想法出现的太容易，又太能让自己解脱了。这是一种让人没有负罪感的放弃方式。但是回头看，很多好想法，明明只要坚持一下就可以实现。因为对意义的评价，近乎于预测未来，我们大多数平凡人是没有这种本事的。</p><p>Be a better man, 每天进步一点点。大家共勉！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-关联容器特点简介&quot;&gt;&lt;a href=&quot;#1-关联容器特点简介&quot; class=&quot;headerlink&quot; title=&quot;1. 关联容器特点简介&quot;&gt;&lt;/a&gt;1. 关联容器特点简介&lt;/h2&gt;&lt;p&gt;关联容器和顺序容器是两种适用范围不同的容器。许多C++程序员只用过顺序容
      
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="C++" scheme="https://superlova.github.io/tags/C/"/>
    
      <category term="map" scheme="https://superlova.github.io/tags/map/"/>
    
      <category term="set" scheme="https://superlova.github.io/tags/set/"/>
    
      <category term="STL" scheme="https://superlova.github.io/tags/STL/"/>
    
  </entry>
  
  <entry>
    <title>DeepStellar--Model-Based Quantitative Analysis of Stateful Deep Learning Systems 论文阅读笔记</title>
    <link href="https://superlova.github.io/2020/03/19/DeepStellar%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>https://superlova.github.io/2020/03/19/DeepStellar%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</id>
    <published>2020-03-19T07:38:00.000Z</published>
    <updated>2020-03-25T02:37:49.035Z</updated>
    
    <content type="html"><![CDATA[<p>DeepStellar 是少有的针对RNN进行测试的工具，它提出了RNN测试的新思路。<br>封面上的大神就是谢肖飞博士，他是DeepStellar一问的作者。<br><a id="more"></a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DeepStellar 是少有的针对RNN进行测试的工具，它提出了RNN测试的新思路。&lt;br&gt;封面上的大神就是谢肖飞博士，他是DeepStellar一问的作者。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://superlova.github.io/categories/paper/"/>
    
    
      <category term="RNN" scheme="https://superlova.github.io/tags/RNN/"/>
    
      <category term="testing" scheme="https://superlova.github.io/tags/testing/"/>
    
      <category term="DeepStellar" scheme="https://superlova.github.io/tags/DeepStellar/"/>
    
  </entry>
  
  <entry>
    <title>循环神经网络模型的覆盖率调研</title>
    <link href="https://superlova.github.io/2020/03/19/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A6%86%E7%9B%96%E7%8E%87%E8%B0%83%E7%A0%94/"/>
    <id>https://superlova.github.io/2020/03/19/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A6%86%E7%9B%96%E7%8E%87%E8%B0%83%E7%A0%94/</id>
    <published>2020-03-19T01:07:47.000Z</published>
    <updated>2020-03-20T01:12:40.514Z</updated>
    
    <content type="html"><![CDATA[<h1 id="循环神经网络模型的覆盖率调研"><a href="#循环神经网络模型的覆盖率调研" class="headerlink" title="循环神经网络模型的覆盖率调研"></a>循环神经网络模型的覆盖率调研</h1><h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><h3 id="1-1-循环神经网络"><a href="#1-1-循环神经网络" class="headerlink" title="1.1 循环神经网络"></a>1.1 循环神经网络</h3><p>循环神经网络（Recurrent neural network：RNN）是神经网络的一种。</p><p>单纯的RNN因为无法处理随着递归，权重指数级爆炸或梯度消失问题，难以捕捉长期时间关联；而结合不同的LSTM可以很好解决这个问题。循环神经网络可以描述动态时间行为。和前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。手写识别是最早成功利用RNN的研究结果。</p><p>为了更好地理解循环神经网络，首先需要介绍前馈神经网络。</p><h4 id="1-前馈神经网络"><a href="#1-前馈神经网络" class="headerlink" title="1) 前馈神经网络"></a>1) 前馈神经网络</h4><p>前馈网络通过在网络的每个节点上做出的一系列操作传递信息。前馈网络每次通过每个层直接向后传递信息。这与循环神经网络不同。一般而言，前馈网络接受一个输入并据此产生输出，这也是大多数监督学习的步骤，输出结果可能是一个分类结果。输出可以是以猫狗等作为标签的类别。我们常见的卷积神经网络（CNN）就是一类经典的前馈网络。</p><p>前馈网络是基于一系列预先标注过的数据训练的。训练阶段的目的是减少前馈网络猜类别时的误差。一旦训练完成，我们就可以用训练后的权重对新批次的数据进行分类。</p><p><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-18-22-02-26.png" srcset="/img/loading.gif" alt="img1"></p><p>在前馈网络中，在<strong>测试阶段</strong>无论展示给分类器的图像是什么，都不会改变权重，所以也不会影响第二个决策。这是前馈网络和循环网络之间一个非常大的不同。也就是说，前馈网络<strong>在测试时</strong>不会记得之前的输入数据。它们只会<strong>在训练阶段</strong>记得历史输入数据。</p><p>与前馈神经网络不同，循环网络不仅将当前的输入样例作为网络输入，还将它们之前感知到的一并作为输入。</p><h4 id="2-前馈网络到循环网络的转变"><a href="#2-前馈网络到循环网络的转变" class="headerlink" title="2) 前馈网络到循环网络的转变"></a>2) 前馈网络到循环网络的转变</h4><p>下图是一个多层感知机示意图，该图所示的模型只拥有一个隐藏层（Hidden Layer），其接受来自输入层经过ReLU处理后的信号，输出的信号再经过Softmax层，从而产生一个分类结果。</p><p><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-18-22-06-04.png" srcset="/img/loading.gif" alt="img2"></p><p>如果在上述示例中的层数增加了，并且我们令隐藏层也接收输入，那么第一个隐藏层将激活传递到第二个隐藏层上，以此类推，最后到达输出层，每一层都有自己的权重（W）、偏置项（B）和激活函数（F）。</p><p><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-18-22-09-56.png" srcset="/img/loading.gif" alt="img3"></p><p>我们令所有隐藏层的权重和偏置项替换成相同的值，从而能使得隐藏层在某种意义上“合并”，如下图所示（注意图中方框内部隐藏层的参数）：</p><p><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-18-22-12-56.png" srcset="/img/loading.gif" alt="img4"></p><p>现在我们就可以将所有层合并在一起了。所有的隐藏层都可以结合在一个循环层中，如下图：</p><p><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-18-22-13-27.png" srcset="/img/loading.gif" alt="img5"></p><p>我们在每一步都会向隐藏层提供输入。现在一个循环神经元存储了所有之前步的输入，并将这些信息和当前步的输入合并。因此，它还捕获到一些当前数据步和之前步的相关性信息。t-1 步的决策影响到第 t 步做的决策。</p><p>如果我们在向网络输入 7 个字母后试着找出第 8 个字母，隐藏层会经历 8 次迭代。如果展开网络的话就是一个 8 层的网络，每一层对应一个字母。所以一个普通的神经网络被重复了多次。展开的次数与它记得多久之前的数据是直接相关的。</p><h4 id="3-循环神经网络基本结构"><a href="#3-循环神经网络基本结构" class="headerlink" title="3) 循环神经网络基本结构"></a>3) 循环神经网络基本结构</h4><p>下图是一个简单的循环神经网络如，它由输入层、一个隐藏层和一个输出层组成：<br><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-18-23-31-00.png" srcset="/img/loading.gif" alt></p><ul><li>x是一个向量，它表示输入层的值；<br>s是一个向量，它表示隐藏层的值（你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；<br>U是输入层到隐藏层的权重矩阵；<br>o也是一个向量，它表示输出层的值；<br>V是隐藏层到输出层的权重矩阵；<br>W是隐藏层上一次的值作为这一次的输入的权重。</li></ul><p>把上面的图展开，循环神经网络也可以画成下面这个样子：<br><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-18-23-33-05.png" srcset="/img/loading.gif" alt></p><p>网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$。关键一点是，$s_t$的值不仅仅取决于$x_t$，还取决于$s_{t-1}$。</p><p>我们可以用下面的公式来表示循环神经网络的计算方法：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{o}_{t} &=g\left(V \mathbf{s}_{t}\right) \tag{1}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}\mathbf{s}_{t} &=f\left(U \mathbf{x}_{t}+W \mathbf{s}_{t-1}\right) \tag{2}\end{aligned}</script><p>(1)是输出层的计算公式，输出层是一个全连接层，它的每个节点都和隐藏层的各个节点相连。V是输出层的权重矩阵，g是激活函数。(2)是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一次的值作为这一次的输入的权重矩阵，f是激活函数。</p><p>从上面的公式我们可以看出，循环层和全连接层的区别就是循环层多了一个权重矩阵 W。</p><h4 id="4-长短期记忆网络"><a href="#4-长短期记忆网络" class="headerlink" title="4) 长短期记忆网络"></a>4) 长短期记忆网络</h4><p>长短期记忆（英语：Long Short-Term Memory，LSTM）是一种时间循环神经网络（RNN），论文首次发表于1997年。由于独特的设计结构，LSTM适合于处理和预测时间序列中间隔和延迟非常长的重要事件。</p><p>LSTM是一种含有LSTM区块（blocks）或其他的一种类神经网络，文献或其他资料中LSTM区块可能被描述成智能网络单元，因为它可以记忆不定时间长度的数值，区块中有一个gate能够决定input是否重要到能被记住及能不能被输出output。</p><p>下图中，底下是四个S函数单元，最左边的单元为input，右边三个gate决定input是否能传入下个；左边第二个为input gate，如果这里gate近似于零，将把这里的值挡住，不会进到下一层。左数第三个是forget gate，当这产生值近似于零，将把过去记住的值忘掉。第四个也就是最右边的input为output gate，他可以决定在区块记忆中的input是否能输出 。</p><p><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-18-23-51-41.png" srcset="/img/loading.gif" alt></p><p>LSTM有很多个版本，其中一个重要的版本是GRU（Gated Recurrent Unit），根据谷歌的测试表明，LSTM中最重要的是Forget gate，其次是Input gate，最次是Output gate。</p><h3 id="1-2-循环神经网络测试的机遇和挑战"><a href="#1-2-循环神经网络测试的机遇和挑战" class="headerlink" title="1.2 循环神经网络测试的机遇和挑战"></a>1.2 循环神经网络测试的机遇和挑战</h3><p>当前研究大多集中于针对前馈神经网络的测试，诸如DeepXplore、DeepGauge针对CNN的测试等，而对RNN鲜有研究。由于RNN的循环特性，适用于CNN的分析方法不能简单地迁移到RNN上。目前学者大多采用模糊测试的方法，通过随机干扰数据集产生对抗性样本，而后分析RNN内部状态信息，引导数据集的扰动方向的方法，快速生成能使得RNN模型判断错误的对抗测试用例集合，达到模型测试的目的。</p><p>根据此思路，一方面可以针对数据集添加干扰的方式进行优化，采用启发式搜索改善对测试样本的Mutation过程；另一方面则是针对RNN内部状态信息的分析，引导算法快速找到有效的、难以察觉变化的对抗样本，这个思路类似于模型攻击。</p><p>目前RNN的对抗性测试主要面临三方面的挑战：</p><ol><li>对于非分类模型而言，没有较好的标准识别对抗样本能不能让模型发生错误。</li></ol><blockquote><p>For the sequential outputs not then applied to classification, there is no standard to decide the outputs as wrong outputs with respect to the changing degree.</p></blockquote><ol><li>对于序列输入的Mutation来说，很难保证添加的扰动是最小的</li></ol><blockquote><p>Applying the perturbations to words in a discrete space always cannot obtain a legal input and the explicit modification is distinguishable for humans.</p></blockquote><ol><li>现有应用于CNN等的覆盖率指标没有考虑到RNN内部结构特性，因此不能直接应用到RNN上。</li></ol><h2 id="2-覆盖指标调研"><a href="#2-覆盖指标调研" class="headerlink" title="2. 覆盖指标调研"></a>2. 覆盖指标调研</h2><p>目前针对循环神经网络的测试 (testing) 和验证 (verification) 等工作的研究还十分有限，根据目前调研取得的结果，现有学者的研究思路大体分成两类：抽象替代模型法和门覆盖率法。下文分别对这两个模型进行简述。</p><h3 id="2-1-抽象替代模型法"><a href="#2-1-抽象替代模型法" class="headerlink" title="2.1 抽象替代模型法"></a>2.1 抽象替代模型法</h3><h4 id="2-1-1-代表论文："><a href="#2-1-1-代表论文：" class="headerlink" title="2.1.1 代表论文："></a>2.1.1 代表论文：</h4><blockquote><p>Du, X., Xie, X., Li, Y., Ma, L., Liu, Y., &amp; Zhao, J. (2019, August). <strong>Deepstellar: model-based quantitative analysis of stateful deep learning systems.</strong> In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 477-487).</p><p>Du, X., Xie, X., Li, Y., Ma, L., Liu, Y., &amp; Zhao, J. (2019, November). <strong>A Quantitative Analysis Framework for Recurrent Neural Network.</strong> In 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE) (pp. 1062-1065). IEEE.</p></blockquote><h4 id="2-1-2-关键方法"><a href="#2-1-2-关键方法" class="headerlink" title="2.1.2 关键方法"></a>2.1.2 关键方法</h4><p>由于直接分析RNN内部结构具有状态转移的特性，因此DeepStellar一文提出<strong>将RNN建模成马尔科夫链</strong>，来模拟其内部状态和动态行为特性。基于马尔科夫链的抽象，该文设计了两个相似度指标和五个覆盖率标准，来衡量输入测试用例差异和测试用例的测试充分性。</p><p><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-19-02-00-02.png" srcset="/img/loading.gif" alt></p><ul><li><p>抽象模型结构模块：输入训练好的RNN，通过Profiling，分析其内部行为。一系列的RNN状态向量叫做trace。每个输入序列会通过Profiling分析得到一个trace。profiling结束之后就可以得到一系列的trace，记载着RNN训练过程访问过的和经过的状态。</p></li><li><p>分析内部状态空间和被训练集激活的trace的过程计算量很大，因此进一步抽象模型来简化状态和trace。首先对状态向量采用主成分分析，以保留其前k个主成分，并将这k个主成分等分成m部分。在状态转换方面，根据抽象状态将具体的转换概括成为抽象的转换。并且根据每个状态向不同方向转换的频率，导出了训练RNN的离散时间Markov链（DTMC）模型。</p></li><li><p>设计了两个相似度指标，用于衡量不同输入下激活的两个trace的相似度。分别是state-based trace similarity和transition-based trace similarity，简写为SBTSIM和TBTSIM。</p></li><li><p>五个覆盖率包括basic state coverage/n-step state boundary coverage, weighted state coverage/ basic transition coverage/weighted transition coverage，简写为BSCov/n-SBCov/WSCov/BTCov/WTCov。</p></li><li><p>将两个指标和五个标准应用于对抗样本检测和覆盖引导的测试上，来缓解来自对抗样本的威胁。运用两个相似度，即可在运行时检测对抗样本（<strong>Monitor</strong>）；运用五个覆盖率，我们将其用于指导测试用例生成上，生成的测试用例以提升覆盖率和找到更多的未被发现的defects为目标。这两个应用互相补充。</p></li></ul><h4 id="2-1-3-实验效果及评价"><a href="#2-1-3-实验效果及评价" class="headerlink" title="2.1.3 实验效果及评价"></a>2.1.3 实验效果及评价</h4><p>DeepStellar在精心的调参下，通过trace相似度检测算法能够很好地检测出当前输入样本是否为对抗样本（音频），准确度达到了89%。</p><p>DeepStellar提出的测试方法本质上是对一个等价模型进行分析的方法，抽象掉了RNN模型的很多细节，只保留了主干部分。思路值得借鉴。</p><p>DeepStellar的缺点也很明显，其高识别率的背后是精心的调参，并且作者也提到，面对更复杂的模型，结果未必会这么好。</p><blockquote><p>With finer-grained model, the result is not necessarily better.</p></blockquote><h3 id="2-2-门覆盖率法"><a href="#2-2-门覆盖率法" class="headerlink" title="2.2 门覆盖率法"></a>2.2 门覆盖率法</h3><h4 id="2-2-1-代表论文："><a href="#2-2-1-代表论文：" class="headerlink" title="2.2.1 代表论文："></a>2.2.1 代表论文：</h4><blockquote><p>Huang, W., Sun, Y., Huang, X., &amp; Sharp, J. (2019). <strong>testRNN: Coverage-guided Testing on Recurrent Neural Networks.</strong> arXiv preprint arXiv:1906.08557.</p></blockquote><h4 id="2-2-2-关键方法"><a href="#2-2-2-关键方法" class="headerlink" title="2.2.2 关键方法"></a>2.2.2 关键方法</h4><p>testRNN关注LSTM和其鲁棒性，鲁棒性指对输入添加小的扰动并不影响LSTM的判断结果的特性。注意，该工具只针对LSTM及相似的网络结构进行分析，原因在于其算法依赖于内部门结构的实现，而这种门结构只存在于LSTM类型的网络中。</p><p>testRNN的特色在于，其直接分析RNN内部结构并加以分析的思路非常类似于其前辈DeepXplore分析前馈神经网络的思路。但由于RNN网络内部关于“层”和“节点”的概念不同于CNN，因此对如何实现CNN中覆盖率迁移到RNN的应用中，testRNN提出了自己的方法。</p><ul><li><p>Cell覆盖率。Cell（后文称之为<strong>单元</strong>）覆盖旨在覆盖每个时间步的隐藏状态发生的显著变化$\Delta\xi_t$。当单元值$\Delta\xi_t$大于用户定义的阈值参数$\alpha_h$时，该单元将被激活并被测试用例覆盖。然后使用覆盖率来衡量由生成的测试用例激活至少一次的单元的百分比。（单元的隐藏状态变化大了，超过了用户定义的某个阈值，就算激活，测试用例激活的单元个数占总个数的比例就是该测试用例的覆盖率）</p></li><li><p>Gate覆盖率。门的覆盖率类似于单元覆盖率，但是信息是从LSTM单元的门中筛选的。上文提到，Google的研究团队发现，LSTM的四种门中最重要的门是忘记门（forget gate），因此testRNN专注于统计忘记门的覆盖率。忘记门的值$Rt(f，x)$表示可以从最后一个单元继承多少信息。由于LSTM以其长期的存储能力而闻名，因此检查一个单元格是否丢弃了从先前输入中学习到的适当数量的信息非常有意义。（忘记们忘记的信息量用Rt函数表示，则Rt太大了就激活？）</p></li></ul><p>下图是testRNN的具体处理流程。</p><p><img src="/2020/03/19/循环神经网络模型的覆盖率调研/2020-03-19-02-43-32.png" srcset="/img/loading.gif" alt></p><h4 id="2-2-3-实验效果"><a href="#2-2-3-实验效果" class="headerlink" title="2.2.3 实验效果"></a>2.2.3 实验效果</h4><p>testRNN着眼于门覆盖率，成功将分析CNN的那一套迁移了过来。但是testRNN的实验结果仅限于小数据集训练下的小网络，诸如MNIST分类数据集训练的双层LSTM网络等。因此能否将此方法推广到更加复杂的大型网络中，还有待探究。</p><h3 id="2-3-优化引导法"><a href="#2-3-优化引导法" class="headerlink" title="2.3 优化引导法"></a>2.3 优化引导法</h3><h4 id="2-3-1-代表论文："><a href="#2-3-1-代表论文：" class="headerlink" title="2.3.1 代表论文："></a>2.3.1 代表论文：</h4><blockquote><p>Guo, J., Zhao, Y., Han, X., Jiang, Y., &amp; Sun, J. (2019). <strong>RNN-Test: Adversarial Testing Framework for Recurrent Neural Network Systems.</strong> arXiv preprint arXiv:1911.06155.</p></blockquote><h4 id="2-3-2-关键方法"><a href="#2-3-2-关键方法" class="headerlink" title="2.3.2 关键方法"></a>2.3.2 关键方法</h4><p>RNN-Test一方面采用了和testRNN类似的“门覆盖率”方法来引导测试用例生成，另一方面采用了一种全新的优化函数思想，计算能同时使得扰动添加最小并且最有可能令模型发生判断错误的扰动方向。RNN-Test将二者结合起来，但正是因为该文仅仅是将这两种方法求得的偏移方向简单的加和，让人不禁怀疑其工作是否没有进行完全。</p><ul><li><p>状态不连续方向（State inconsistency orientation）：</p><script type="math/tex; mode=display">obj_{orient} = h_{t-1}^l + c_t^l - h_t^l</script><p>该优化函数的设计思想是，若一个样本能使得从隐状态t-1时刻输入的信息尽量大，而输出尽量小，那么这种样本更容易出现问题。这是因为t时刻$h_t^l$的值完全取决于$h_{t-1}^l $和$ c_t^l$，让这两部分产生大小差异更容易引发不确定行为。</p></li><li><p>损失函数优化方向（Cost orientation）:</p><script type="math/tex; mode=display">obj_{orient} = L_{seq}(y, \hat{y})</script><p>这一部分引用自FGSM的优化算法，该论文是对抗样本生成领域的开山之作。讲的是如何通过梯度上升算法求得添加扰动的方向，从而使得扰动最小的同时模型的变化最大。</p></li><li><p>决策边界方向（Decision boundary orientation）：</p><script type="math/tex; mode=display">obj_{orient} = (\sum_{i=0}^{k}\hat{y_{t_i}}) - \hat{y_t}</script><p>这一部分的灵感来自于RNN内部结构，由于每个时间步的隐状态事实上都会产生中间输出$y=o_t^l$，但一般我们认为只有最后阶段的输出向量才是有意义的。该优化函数将除了原来预测的最大值y之外前k个最大的y加起来，并减去原来的y。</p></li></ul><h4 id="2-3-3-实验效果"><a href="#2-3-3-实验效果" class="headerlink" title="2.3.3 实验效果"></a>2.3.3 实验效果</h4><p>该文章使用自己生成的对抗样本集合，对模型进行重新训练，模型的复杂度（Perplexity）有大约1.159%的降低。这说明模型更稳定了。</p><p>然而提升不大，运行效率却很低，计算代价较大。并且原文为了提升算法运行性能，Mutation过程采用了一遍Mutation，若不成功直接放弃的做法，原来的测试用例利用率较低、为了达到较高的突变利用率，算法不得不放宽添加干扰力度，这就导致生成太多无用的假测试用例。算法本身可以被优化。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;循环神经网络模型的覆盖率调研&quot;&gt;&lt;a href=&quot;#循环神经网络模型的覆盖率调研&quot; class=&quot;headerlink&quot; title=&quot;循环神经网络模型的覆盖率调研&quot;&gt;&lt;/a&gt;循环神经网络模型的覆盖率调研&lt;/h1&gt;&lt;h2 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1
      
    
    </summary>
    
    
      <category term="paper" scheme="https://superlova.github.io/categories/paper/"/>
    
    
      <category term="RNN" scheme="https://superlova.github.io/tags/RNN/"/>
    
      <category term="testing" scheme="https://superlova.github.io/tags/testing/"/>
    
  </entry>
  
  <entry>
    <title>如何使用VS Code编写github pages博客</title>
    <link href="https://superlova.github.io/2020/01/11/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8VS-Code%E7%BC%96%E5%86%99github-pages%E5%8D%9A%E5%AE%A2/"/>
    <id>https://superlova.github.io/2020/01/11/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8VS-Code%E7%BC%96%E5%86%99github-pages%E5%8D%9A%E5%AE%A2/</id>
    <published>2020-01-11T13:55:17.000Z</published>
    <updated>2020-03-19T01:42:00.352Z</updated>
    
    <content type="html"><![CDATA[<p>使用VS Code写博客，需要你按照我之前写的两篇博客，将github pages平台搭建起来。</p><p><a href="https://superlova.github.io/2019/04/14/%E9%85%8D%E7%BD%AEhexo+GitHub%20Pages%E7%BA%AA%E5%AE%9E/">配置hexo+GitHub Pages纪实</a><br><a href="https://superlova.github.io/2019/04/25/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/">hexo图片加载失败解决方案</a></p><p>之后我们安装VSCode。接下来介绍我一直使用的几个插件，和它们的配置小技巧。</p><p>第一个是<strong>Markdown Preview Enhanced</strong>，有了该插件，就可以提前预览markdown文件的渲染效果。方法是使用VSCode打开以md后缀名结尾的文件，右键点击<strong>Markdown Preview Enhanced： Open Preview To The Side</strong>，即可在侧边栏生成即时渲染的md效果文件。</p><p>第二个是<strong>Markdown PDF</strong>，该插件可以令写好的md文件打印成pdf格式。该插件需要安装chromium内核。</p><p>第三个是<strong><strong>Paste Image</strong></strong>插件，可以很方便地在md文章中粘贴位于剪切板的图片。</p><p>粘贴的快捷键是Ctrl+Alt+V。</p><p>在Paste Image插件的Path设置部分，改成如下所示：<br><img src="/2020/01/11/如何使用VS-Code编写github-pages博客/2020-01-11-23-28-36.png" srcset="/img/loading.gif" alt><br>这样图片粘贴的位置就变成了<strong>当前文章目录下，与该文章同名的文件夹内</strong>，方便我们进行进一步整理。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;使用VS Code写博客，需要你按照我之前写的两篇博客，将github pages平台搭建起来。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://superlova.github.io/2019/04/14/%E9%85%8D%E7%BD%AEhexo+GitHub%20P
      
    
    </summary>
    
    
      <category term="record" scheme="https://superlova.github.io/categories/record/"/>
    
    
      <category term="github pages" scheme="https://superlova.github.io/tags/github-pages/"/>
    
      <category term="hexo" scheme="https://superlova.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>总结论文中常用的Matplotlib和Seaborn绘图技术</title>
    <link href="https://superlova.github.io/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/"/>
    <id>https://superlova.github.io/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/</id>
    <published>2020-01-11T13:47:26.000Z</published>
    <updated>2020-03-19T01:45:18.572Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、使用matplotlib绘制图像"><a href="#一、使用matplotlib绘制图像" class="headerlink" title="一、使用matplotlib绘制图像"></a>一、使用matplotlib绘制图像</h1><p>matplotlib是一个Python的数据可视化2D图形库。matplotlib的特点是可以采用面向对象的方法，模仿MATLAB中的图形命令。matplotlib经常与numpy、pandas等库结合起来使用。<br>matplotlib可以采用MATLAB的命令风格使用，也可以采用面向对象的风格使用。</p><h2 id="matplotlib的图像中各组件名称"><a href="#matplotlib的图像中各组件名称" class="headerlink" title="matplotlib的图像中各组件名称"></a>matplotlib的图像中各组件名称</h2><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-11-23-42-39.png" srcset="/img/loading.gif" alt></p><h2 id="新建图像"><a href="#新建图像" class="headerlink" title="新建图像"></a>新建图像</h2><pre><code class="lang-python">fig, axes = plt.subplots(2,1,figsize=(5,10)) #两行一列组成一张图，图像大小宽5高10</code></pre><p>上面的语句创建了一个figure，由两个ax组成。把它想象成一张画布上面的两个贴画，会比较容易理解。</p><p>plt.figure()函数的前两个参数是设置figure是由几行几列的ax组成。figure(2,1)说明figure是由两行一列的ax一共两个ax组成。</p><p>后面的figsize参数设置画布的宽和高，单位为英寸。</p><h1 id="二、使用Seaborn绘制图像"><a href="#二、使用Seaborn绘制图像" class="headerlink" title="二、使用Seaborn绘制图像"></a>二、使用Seaborn绘制图像</h1><p>首先确定我们需要可视化的数据的结构。以iris鸢尾花数据集为例，</p><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-07-35.png" srcset="/img/loading.gif" alt></p><p>每一行代表一个数据对象，每一列代表数据对象的一个属性。但是现实生活的数据很多不长这样，只不过组织成一个表格的形式，内容大相径庭。因此在进行数据可视化时一定要保证你的数据也是<strong>用行代表数据对象，用列表示数据的属性</strong>。</p><h2 id="2-1-关联图"><a href="#2-1-关联图" class="headerlink" title="2.1 关联图"></a>2.1 关联图</h2><p>我们是用 <code>relplot</code>函数进行进一步绘制。实际上，<code>relplot</code> 可以看作是 <code>scatterplot</code> 和 <code>lineplot</code> 的结合版本。但是relplot包装层级更加高，这意味着它更适合快速应用，不适合自定义。如果你对它的效果不满意，恐怕还是得诉诸<code>scatterplot</code> 和 <code>lineplot</code>等与matplotlib结合更紧密的api，或者直接使用matplotlib。</p><pre><code class="lang-python">sns.relplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-10-26.png" srcset="/img/loading.gif" alt></p><p>x为花萼长度，y为花萼宽度。这样分x，y其实有一定道理，我们的目的是能够把不同类型的数据对象在图上区分开。因为同类花朵一般个头差不多，花萼的长度和宽度聚集在图的一部分区域。但是在上图我们是看不出来的。我们希望给不同类别添加不同颜色。</p><pre><code class="lang-python">sns.relplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, hue=&quot;species&quot;, data=iris)</code></pre><p>可以看到我们添加了<code>hue</code>字段，并要求按照<code>species</code>进行进一步分类。<code>hue</code>字段就是进行二次分类的参数。</p><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-15-48.png" srcset="/img/loading.gif" alt></p><p>如果是论文，则我们要使得读者在黑白打印的条件下也能发现区别。添加<code>stype</code>参数为<code>species</code>或许会有帮助。</p><pre><code class="lang-python">sns.relplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;,            hue=&quot;species&quot;, style=&quot;species&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-18-31.png" srcset="/img/loading.gif" alt></p><p>不只是散点图，该方法还支持线形图，只需要指定 <code>kind=&quot;line&quot;</code> 参数即可。</p><pre><code class="lang-python">sns.relplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;,            hue=&quot;species&quot;, style=&quot;species&quot;, kind=&quot;line&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-20-55.png" srcset="/img/loading.gif" alt></p><p>上图其实就是折线图，我们使用一个与matplotlib结合更紧密的api来探究花萼长度和花瓣长度之间的关系。</p><pre><code class="lang-python">sns.lineplot(x=&quot;sepal_length&quot;, y=&quot;petal_length&quot;,             hue=&quot;species&quot;, style=&quot;species&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-26-06.png" srcset="/img/loading.gif" alt></p><h2 id="2-2-类别图"><a href="#2-2-类别图" class="headerlink" title="2.2 类别图"></a>2.2 类别图</h2><p>懒人函数是<code>catplot</code>，<code>catplot</code>是下面几个底层函数的封装：</p><ul><li><p>分类散点图:</p><ul><li><a href="https://seaborn.pydata.org/generated/seaborn.stripplot.html" target="_blank" rel="noopener"><code>stripplot()</code></a> (<code>kind=&quot;strip&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.swarmplot.html" target="_blank" rel="noopener"><code>swarmplot()</code></a> (<code>kind=&quot;swarm&quot;</code>)</li></ul></li><li><p>分类分布图:</p><ul><li><a href="https://seaborn.pydata.org/generated/seaborn.boxplot.html" target="_blank" rel="noopener"><code>boxplot()</code></a> (<code>kind=&quot;box&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.violinplot.html" target="_blank" rel="noopener"><code>violinplot()</code></a> (<code>kind=&quot;violin&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.boxenplot.html" target="_blank" rel="noopener"><code>boxenplot()</code></a> (<code>kind=&quot;boxen&quot;</code>)</li></ul></li><li><p>分类估计图:</p><ul><li><a href="https://seaborn.pydata.org/generated/seaborn.pointplot.html" target="_blank" rel="noopener"><code>pointplot()</code></a> (<code>kind=&quot;point&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.barplot.html" target="_blank" rel="noopener"><code>barplot()</code></a> (<code>kind=&quot;bar&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.countplot.html" target="_blank" rel="noopener"><code>countplot()</code></a> (<code>kind=&quot;count&quot;</code>)</li></ul></li></ul><p>我们想知道不同类别下花萼长度的散点图。</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&#39;strip&#39;,data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-36-33.png" srcset="/img/loading.gif" alt></p><p><code>kind=&quot;swarm&quot;</code> 可以让散点按照 beeswarm 的方式防止重叠，可以更好地观测数据分布。</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;swarm&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-38-46.png" srcset="/img/loading.gif" alt></p><p>箱线图</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;box&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-41-06.png" srcset="/img/loading.gif" alt><br>变种箱线图</p><pre><code class="lang-python">sns.catplot(x=&quot;species&quot;, y=&quot;sepal_length&quot;, kind=&quot;boxen&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-41-24.png" srcset="/img/loading.gif" alt><br>提琴图</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;violin&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-41-16.png" srcset="/img/loading.gif" alt><br>点线图</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;point&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-41-36.png" srcset="/img/loading.gif" alt><br>柱状图</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;bar&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-41-48.png" srcset="/img/loading.gif" alt></p><h2 id="2-3-分布图"><a href="#2-3-分布图" class="headerlink" title="2.3 分布图"></a>2.3 分布图</h2><p>如果想看一个变量到底是正态分布、卡方分布还是指数分布，此时就要使用分布图进行可视化了。一维分布图比较常见，二维以上分布图不太直观。绘制分布图的函数有这几个：<code>jointplot</code> <code>pairplot</code> <code>distplot</code> <code>kdeplot</code>。</p><p><code>distplot</code>可以方便的查看单变量的分布图。</p><pre><code class="lang-python">sns.distplot(iris[&quot;sepal_length&quot;])</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-48-17.png" srcset="/img/loading.gif" alt><br>图上那条曲线是根据数据拟合出来的核密度估计kde曲线（原理有待学习）。如果不想要这条线，可以在参数中设置<code>kde=False</code>。更可以只要kde曲线，设置<code>hist=False</code>即可。</p><p><code>jointplot</code>绘制二元变量的分布图，比如花瓣长度和宽度的关系。</p><pre><code class="lang-python">sns.jointplot(x=&quot;petal_length&quot;, y=&quot;petal_width&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-55-40.png" srcset="/img/loading.gif" alt></p><p>kde估计图也可以在二元变量分布图中出现。还有蜂巢图<code>kind=&quot;hex&quot;</code>、回归图<code>kind=&quot;reg&quot;</code>等。</p><pre><code class="lang-python">sns.jointplot(x=&quot;petal_length&quot;, y=&quot;petal_width&quot;, data=iris, kind=&quot;kde&quot;)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-57-09.png" srcset="/img/loading.gif" alt></p><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-16-58-51.png" srcset="/img/loading.gif" alt></p><p>最后注意到我们的鸢尾花数据集含有四组属性。我们想探究这四组属性两两之间的关系，就需要用到<code>pairplot</code></p><pre><code class="lang-python">sns.pairplot(iris, hue=&quot;species&quot;)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-17-01-04.png" srcset="/img/loading.gif" alt></p><h2 id="2-4-回归图"><a href="#2-4-回归图" class="headerlink" title="2.4 回归图"></a>2.4 回归图</h2><p><code>regplot</code> 绘制回归图，只会绘制一组回归曲线。</p><pre><code class="lang-python">sns.regplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-17-02-28.png" srcset="/img/loading.gif" alt></p><p><code>lmplot</code> 可以引入<code>hue</code>变量，绘制不同类别数据的回归图</p><pre><code class="lang-python">sns.lmplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, hue=&quot;species&quot;, data=iris)</code></pre><p><img src="/2020/01/11/总结论文中常用的Matplotlib绘图技术/2020-01-12-17-03-57.png" srcset="/img/loading.gif" alt></p><h2 id="2-5-矩阵图"><a href="#2-5-矩阵图" class="headerlink" title="2.5 矩阵图"></a>2.5 矩阵图</h2><p><code>heatmap</code>用来画热图，数据值大的格子颜色比较深。热力图在某些场景下非常实用，例如绘制出变量相关性系数热力图。<br><code>clustermap</code>用来画层次聚类结构图。对于iris数据集来说，这两类图没有用武之地。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一、使用matplotlib绘制图像&quot;&gt;&lt;a href=&quot;#一、使用matplotlib绘制图像&quot; class=&quot;headerlink&quot; title=&quot;一、使用matplotlib绘制图像&quot;&gt;&lt;/a&gt;一、使用matplotlib绘制图像&lt;/h1&gt;&lt;p&gt;matplo
      
    
    </summary>
    
    
      <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
      <category term="matplotlib" scheme="https://superlova.github.io/tags/matplotlib/"/>
    
      <category term="seaborn" scheme="https://superlova.github.io/tags/seaborn/"/>
    
  </entry>
  
  <entry>
    <title>用于科学计算的GPU选购参考</title>
    <link href="https://superlova.github.io/2019/07/01/%E7%94%A8%E4%BA%8E%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E7%9A%84GPU%E9%80%89%E8%B4%AD%E5%8F%82%E8%80%83/"/>
    <id>https://superlova.github.io/2019/07/01/%E7%94%A8%E4%BA%8E%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E7%9A%84GPU%E9%80%89%E8%B4%AD%E5%8F%82%E8%80%83/</id>
    <published>2019-07-01T10:11:15.000Z</published>
    <updated>2020-03-19T01:44:37.971Z</updated>
    
    <content type="html"><![CDATA[<p>实验室最近要采购一批显卡，需要调研显卡的型号和价格。</p><h2 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h2><p>首先说一下需求：</p><ul><li>首先显卡的用途是科学计算，更具体一点是深度学习，有人做图像，有人做NLP；</li><li>其次预算有限，得买性价比最好的；</li><li>然后可能会有很多人要用服务器训练模型。</li></ul><p>然后这是戴尔服务器的售后人员发来的建议采购清单：<br><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-18-19-43.png" srcset="/img/loading.gif" alt></p><p>其中M10：19999￥ P100： 49999￥ V100：59999￥ P40：49999￥</p><p>值得一提的是，谷歌的Colab上面用的是这款：<br><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-18-21-48.png" srcset="/img/loading.gif" alt><br><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-18-21-58.png" srcset="/img/loading.gif" alt></p><p>但是一个从事深度学习研究的学长建议我买1080Ti。他好像提都没提过Tesla啊？难道显卡水这么深？</p><h2 id="GPU参数"><a href="#GPU参数" class="headerlink" title="GPU参数"></a>GPU参数</h2><p>GPU的性能主要由下面三个主要参数构成：</p><p><strong>计算能力</strong>。通常我们关心的是32位浮点计算能力。当然，对于高玩来说也可以考虑16位浮点用来训练，8位整数来预测。</p><p><strong>内存大小</strong>。神经网络越深，或者训练时批量大小越大，所需要的GPU内存就越多。</p><p><strong>内存带宽</strong>。内存带宽要足够才能发挥出所有计算能力。</p><p>此外，针对不同深度学习架构，GPU参数的选择优先级是不一样的，总体来说分两条路线：</p><p><strong>卷积网络和Transformer</strong>：张量核心&gt;FLOPs（每秒浮点运算次数）&gt;显存带宽&gt;16位浮点计算能力</p><p><strong>循环神经网络</strong>：显存带宽&gt;16位浮点计算能力&gt;张量核心&gt;FLOPs</p><p>这个排序背后有一套逻辑，下面将详细解释一下。</p><p>在说清楚哪个GPU参数对速度尤为重要之前，先看看两个最重要的张量运算：矩阵乘法和卷积。</p><p>举个栗子，以运算矩阵乘法A×B=C为例，将A、B复制到显存上比直接计算A×B更耗费资源。也就是说，如果你想用LSTM等处理大量小型矩阵乘法的循环神经网络，显存带宽是GPU最重要的属性。</p><p>矩阵乘法越小，内存带宽就越重要。</p><p>相反，卷积运算受计算速度的约束比较大。因此，要衡量GPU运行ResNets等卷积架构的性能，最佳指标就是FLOPs。张量核心可以明显增加FLOPs。</p><p>Transformer中用到的大型矩阵乘法介于卷积运算和RNN的小型矩阵乘法之间，16位存储、张量核心和TFLOPs都对大型矩阵乘法有好处，但它仍需要较大的显存带宽。</p><h2 id="性价比分析"><a href="#性价比分析" class="headerlink" title="性价比分析"></a>性价比分析</h2><p>下面总结了一张GPU和TPU的标准性能数据，值越高代表性能越好。RTX系列假定用了16位计算，Word RNN数值是指长度&lt;100的段序列的biLSTM性能。</p><p>这项基准测试是用PyTorch 1.0.1和CUDA 10完成的。</p><p><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-19-09-53.png" srcset="/img/loading.gif" alt="GPU和TPU的性能数据"></p><p>性价比可能是选择一张GPU最重要的考虑指标。</p><p>性价比可能是选择一张GPU最重要的考虑指标。在攻略中，小哥进行了如下运算测试各显卡的性能：</p><ul><li>用语言模型Transformer-XL和BERT进行Transformer性能的基准测试。</li><li>用最先进的biLSTM进行了单词和字符级RNN的基准测试。</li><li>上述两种测试是针对Titan Xp、Titan RTX和RTX 2080 Ti进行的，对于其他GPU则线性缩放了性能差异。</li><li>借用了现有的CNN基准测试。</li><li>用了亚马逊和eBay上显卡的平均售价作为GPU的参考成本。<br>最后，可以得出CNN、RNN和Transformer的归一化性能/成本比值，如下所示：</li></ul><p><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-19-11-20.png" srcset="/img/loading.gif" alt="CNN、RNN和Transformer的每美元性能"></p><p>在上面这张图中，数字越大代表每一美元能买到的性能越强。可以看出， RTX 2060比RTX 2070，RTX 2080或RTX 2080 Ti更具成本效益，<strong>甚至是Tesla V100性价比的5倍以上</strong>。</p><p>所以此轮的性价比之王已经确定，是RTX 2060无疑了。</p><p>下图是李沐老师画了900和1000系列里各个卡的32位浮点计算能力和价格的对比（价格是wikipedia的推荐价格，真实价格通常会有浮动）。</p><p><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-19-06-00.png" srcset="/img/loading.gif" alt></p><p>由于GPU的功耗，散热和体积，需要一些额外考虑。</p><ul><li>机箱体积<br>GPU尺寸较大，通常不考虑太小的机箱。而且机箱自带的风扇要好。</li><li>电源<br>购买GPU时需要查下GPU的功耗，50w到300w不等。因此买电源时需要功率足够的。</li><li>主板的PCIe卡槽<br>推荐使用PCIe 3.0 16x来保证足够的GPU到主内存带宽。如果是多卡的话，要仔细看主板说明，保证多卡一起使用时仍然是16x带宽。（有些主板插4卡时会降到8x甚至4x）</li></ul><h2 id="Tesla为什么那么贵？"><a href="#Tesla为什么那么贵？" class="headerlink" title="Tesla为什么那么贵？"></a>Tesla为什么那么贵？</h2><p>英伟达现在有一项非常坑爹的政策，如果在数据中心使用CUDA，那么只允许使用Tesla GPU而不能用GTX或RTX GPU。</p><p>由于担心法律问题，研究机构和大学经常被迫购买低性价比的Tesla GPU。<strong>然而，Tesla与GTX和RTX相比并没有真正的优势，价格却高出10倍。</strong></p><p>Nvidia卡有面向个人用户（例如GTX系列）和企业用户（例如Tesla系列）两种。企业用户卡通常使用被动散热和增加了内存校验从而更加适合数据中心。但计算能力上两者相当。<strong>企业卡通常要贵上10倍</strong>。</p><p>Tesla显卡那么贵，其实是贵在双精度浮点数运算能力上了，外加一个鸡肋的ECC校验功能，实在不值。</p><h2 id="总结建议："><a href="#总结建议：" class="headerlink" title="总结建议："></a>总结建议：</h2><p><strong>最佳GPU</strong>：RTX 2070</p><p><strong>避免的坑</strong>：所有Tesla、Quadro、创始人版（Founders Edition）的显卡，还有Titan RTX、Titan V、Titan XP</p><p><strong>高性价比</strong>：RTX 2070（高端），RTX 2060或GTX 1060 (6GB)（中低端）</p><p><strong>计算机视觉或机器翻译研究人员</strong>：采用鼓风设计的GTX 2080 Ti，如果训练非常大的网络，请选择RTX Titans</p><p><strong>NLP研究人员</strong>：RTX 2080 Ti</p><p><strong>已经开始研究深度学习</strong>：RTX 2070起步，以后按需添置更多RTX 2070</p><h2 id="其他配件要求："><a href="#其他配件要求：" class="headerlink" title="其他配件要求："></a>其他配件要求：</h2><p><img src="/2019/07/01/用于科学计算的GPU选购参考/2019-07-01-19-19-14.png" srcset="/img/loading.gif" alt="各硬件性能要求"></p><ul><li><p>CPU:<br>因为主要使用显卡进行cuda计算，因此对CPU的要求并不是很高，频率越高、线程数越多越好，一般最低要求cpu核心数大于显卡个数。其中一个制约因素：cpu的最大PCI-E 通道数。每张显卡占用16条pcie通道才能达到最大性能，而单cpu最大支持40条pcie，也就是即使有4个pcie x16接口，只能最多达到2路x16加一路x8，插上的显卡不能发挥全部性能。不过，主板芯片组其实也可以扩充一部分pcie通道。（x99主板可以扩宽2.0的8lanes，z170可以扩充3.0的20lanes）</p></li><li><p>主板:<br>前面提到了cpu提供的pcie通道数的限制，如果要使用多块显卡，就需要主板提供额外的pcie通道，一般只有服务器级别的主板才会提供扩展pcie通道如x99、x299等主板，但是使用此类主板必须搭配具有该接口的服务器级cpu（xeon系列、i7 7900x以上、i9系列等），如果不需要三块以上的显卡，使用cpu提供的40lane pcie即可。</p></li><li><p>内存：<br>深度学习需要大量数据，中间计算过程也会临时储存大量数据，一般要求具有显存2~3倍的内存，32G或64G乃至更高。内存频率越高越好。<br>最低建议32G DDR4 3200MHz内存(16G*2)约2000元，预算宽裕可升级到64G（约4000元）</p></li><li><p>硬盘：<br>深度学习需要大量数据，和较快的访问速度，一般使用一个较大的固态硬盘作为系统盘和训练数据仓储盘，另外使用hdd机械硬盘作为仓储盘。<br>建议使用512G以上nVME固态硬盘（800元）搭配几TB(2TB约300元）Hdd作为储存空间</p></li><li><p>电源、机箱：电源其实还是要买个比较稳定的，因为要保证长期稳定运行会有“无休止”的training。一般使用大品牌的经过80PLUS金牌或铂金认证的电源。只搭配一张显卡700w即可，每多一张增加400w。4*titan V大概使用1600w电源。</p></li></ul><p>深度学习实验室共享服务器，7x24小时运行  2080ti或者4titan V ，预算充裕可以专门购置一台高性能多显卡深度学习服务器，24*7小时运行，其他用户可以在自己的笔记本电脑和台式机上编写和初步调试卷积神经网络，本地验证无误后，上传至服务器进行训练任务。这样做可以极大的节省设备开支，最大限度的利用计算资源，也避免了每个用户单独配置复杂的软件环境。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;实验室最近要采购一批显卡，需要调研显卡的型号和价格。&lt;/p&gt;
&lt;h2 id=&quot;需求分析&quot;&gt;&lt;a href=&quot;#需求分析&quot; class=&quot;headerlink&quot; title=&quot;需求分析&quot;&gt;&lt;/a&gt;需求分析&lt;/h2&gt;&lt;p&gt;首先说一下需求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先显卡的用
      
    
    </summary>
    
    
      <category term="record" scheme="https://superlova.github.io/categories/record/"/>
    
    
      <category term="deep learning" scheme="https://superlova.github.io/tags/deep-learning/"/>
    
      <category term="GPU" scheme="https://superlova.github.io/tags/GPU/"/>
    
  </entry>
  
</feed>
